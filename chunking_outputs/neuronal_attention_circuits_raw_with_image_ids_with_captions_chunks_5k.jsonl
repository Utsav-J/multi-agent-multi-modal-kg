{"id": "neuronal_attention_circuits_raw_with_image_ids_with_captions_chunks_5k_0", "content": "## **Neuronal Attention Circuit (NAC) for Representation Learning**\n\n**Waleed Razzaq** [1] **Izis Kankaraway** [1] **Yun-Bo Zhao** [1 2]\n\n\n\n**Abstract**\n\nAttention improves representation learning over\nRNNs, but its discrete nature limits continuoustime (CT) modeling. We introduce Neuronal Attention Circuit (NAC), a novel, biologically plausible CT-Attention mechanism that reformulates\n\nattention logits computation as the solution to a\nlinear first-order ODE with nonlinear interlinked\ngates derived from repurposing _C. elegans_ Neuronal Circuit Policies (NCPs) wiring mechanism.\nNAC replaces dense projections with sparse sensory gates for key-query projections and a sparse\nbackbone network with two heads for computing\n_content-target_ and _learnable time-constant_ gates,\nenabling efficient adaptive dynamics. NAC supports three attention logit computation modes: (i)\nexplicit Euler integration, (ii) exact closed-form\nsolution, and (iii) steady-state approximation. To\nimprove memory intensity, we implemented a\nsparse Top- _K_ pairwise concatenation scheme that\nselectively curates key-query interactions. We\nprovide rigorous theoretical guarantees, including\nstate stability, bounded approximation errors, and\nuniversal approximation. Empirically, we implemented NAC in diverse domains, including irregular time-series classification, lane-keeping for\nautonomous vehicles, and industrial prognostics.\nWe observed that NAC matches or outperforms\ncompeting baselines in accuracy and occupies an\nintermediate position in runtime and memory efficiency compared with several CT baselines.\n\n\n**1. Introduction**\n\n\nLearning representations of sequential data in temporal or\nspatio-temporal domains is essential for capturing patterns\nand enabling accurate forecasting. Discrete-time Recurrent neural networks (DT-RNNs) such as RNN (Rumelhart et al., 1985; Jordan, 1997), Long-short term memory\n\n\n1Department of Automation, University of Science & Technology of China, Hefei, China [2] Institute of Artificial Intelligence,\nHefei Comprehensive National Science Center. Correspondence to:\nYun-Bo Zhao _<_ ybzhao@ustc.edu.cn _>_, Waleed Razzaq _<_ waleedrazzaq@mail.ustc.edu.cn _>_ .\n\n\n_Preprint. December 12, 2025._\n\n\n\n(LSTM) (Hochreiter & Schmidhuber, 1997), and Gated Recurrent Unit (GRU) (Cho et al., 2014) model sequential dependencies by iteratively updating hidden states to represent\nor predict future elements in a sequence. While effective\nfor regularly sampled sequences, DT-RNNs face challenges\nwith irregularly sampled data because they assume uniform\ntime intervals. In addition, vanishing gradients can make\nit difficult to capture long-term dependencies (Hochreiter,\n1998).\nContinuous-time RNNs (CT-RNNs) (Rubanova et al., 2019)\nmodel hidden states as ordinary differential equations\n(ODEs), allowing them to process inputs that arrive at arbitrary or irregular time intervals. Mixed-memory RNNs\n(mmRNNs) (Lechner & Hasani, 2022) build on this idea\nby separating memory compartments from time-continuous\nstates, helping maintain stable error propagation while capturing continuous-time dynamics. Liquid neural networks\n(LNNs) (Hasani et al., 2021; 2022) take a biologically inspired approach by assigning variable time-constants to hidden states, improving adaptability and robustness, though\nvanishing gradients can still pose challenges during training.\nThe attention mechanisms (Vaswani et al., 2017) mitigate\nthis limitation by treating all time steps equally and allowing models to focus on the most relevant observations. It\ncomputes the similarity between queries ( _q_ ) and keys ( _k_ ),\nscaling by the key dimension to keep gradients stable. MultiHead Attention (MHA) (Vaswani et al., 2017) extends this\nby allowing the model to attend to different representation\nsubspaces in parallel. Variants like Sparse Attention (Tay\net al., 2020; Roy et al., 2021), BigBird (Zaheer et al., 2020),\nand Longformer (Beltagy et al., 2020) modify the attention\npattern to reduce computational cost, particularly for long sequences, by attending only to selected positions rather than\nall pairs. Even with these improvements, attention-based\nmethods still rely on discrete scaled dot-product operations,\nlimiting their ability to model continuous trajectories often\ncaptured by CT counterparts.\nRecent work has explored bridging this gap through NeuralODE (Chen et al., 2018) formulation. mTAN (Shukla &\nMarlin, 2021) learns CT embeddings and uses time-based\nattention to interpolate irregular observations into a fixedlength representation for downstream encoder-decoder modeling. ODEFormer (d\u2019Ascoli et al., 2023) trains a sequenceto-sequence transformer on synthetic trajectories to directly\ninfer symbolic ODE systems from noisy, irregular data,\nthough it struggles with chaotic systems and generalization\n\n\n\n1\n\n\n**Neuronal Attention Circuit (NAC) for Representation Learning**\n\n\n\nbeyond observed conditions. Continuous-time Attention\n(CTA) (Chien & Chen, 2021) embeds a continuous-time\nattention mechanism within a Neural ODE, allowing attention weights and hidden states to evolve jointly over time.\nStill, it remains computationally intensive and sensitive to\nthe accuracy of the ODE solver. ContiFormer (Chen et al.,\n2023) builds a CT-transformer by pairing ODE-defined latent trajectories with a time-aware attention mechanism to\nmodel dynamic relationships in data.\nDespite these innovations, a persistent and underexplored\ngap remains in developing a biologically plausible attention\nmechanism that seamlessly integrates CT dynamics with the\nabstraction of the brain\u2019s connectome to handle irregular sequences without prohibitive computational costs. Building\non this, we propose a novel attention mechanism called the\n_Neuronal Attention Circuit_ (NAC), in which attention logits\nare computed as the solution to a first-order ODE modulated\nby nonlinear, interlinked gates derived from repurposing\nNeuronal Circuit Policies (NCPs) from the nervous system\nof _C. elegans_ nematode (refer to Appendix A.2 for more\ninformation). Unlike standard attention, which projects keyquery pairs through a dense layer, NAC employs a sensory\ngate to transform input features and a backbone to model\nnonlinear interactions, with multiple heads producing outputs structured for attention logits computation. Based on\nthe solutions to ODE, we define three computation modes:\n(i) Exact, using the closed-form ODE solution; (ii) Euler,\napproximating the solution via _explicit Euler_ integration;\nand (iii) Steady, using only the steady-state solution, analogous to standard attention scores. To reduce computational\ncomplexity, we implemented a sparse Top- _K_ pairwise concatenation algorithm that selectively curates key-query inputs. We evaluate NAC across multiple domains, including\nirregularly sampled time series, autonomous vehicle lanekeeping, and Industry 4.0, comparing it to state-of-the-art\nbaselines. NAC consistently matches or outperforms these\nmodels, while runtime and peak memory benchmarks place\nit between CT-RNNs in terms of speed and CT-Attentions\nin terms of memory requirements.\n\n\n**2. Neuronal Attention Circuit (NAC)**\n\n\nWe propose a simple alternative formulation of the attention\nlogits _a_ (refer to Appendix A.1 for more information), interpreting them as the solution to a first-order linear ODE\nmodulated by nonlinear, interlinked gates:\n\n\n\nfrom repurposing NCPs. We refer to this formulation as\nthe Neuronal Attention Circuit (NAC). It enables the logits\n_at_ to evolve dynamically with input-dependent, variable\ntime constants, mirroring the adaptive temporal dynamics\nfound in _C. elegans_ nervous systems while improving\ncomputational efficiency and expressiveness. Moreover, it\nintroduces continuous depth into the attention mechanism,\nbridging discrete-layer computation with dynamic temporal\nevolution.\n\n**Motivation behind this formulation:** The proposed\nformulation is loosely motivated by the input-dependent\ntime-constant mechanism of Liquid Neural Networks\n(LNNs), a class of CT-RNNs inspired by biological nervous\nsystems and synaptic transmission. In this framework, the\ndynamics of non-spiking neurons are described by a linear\nODE with nonlinear, interlinked gates: _ddt_ **xt** = **[x]** _\u03c4_ **[t]** [+] **[ S][t]** _[,]_\n\nwhere **xt** denotes the hidden state and **St** _\u2208_ R _[M]_ represents\na nonlinear contribution defined as _f_ ( **xt** _,_ **u** _, t, \u03b8_ )( _A \u2212_ **xt** ).\nHere, _A_ and _\u03b8_ are learnable parameters. Plugging **St**\nyields _[d]_ **[x][t]** [=] _[ \u2212]_ \ufffd _\u2212_ [1] _[\u2212]_ _[f]_ [(] **[x][t]** _[,]_ **[ u]** _[, t, \u03b8]_ [)] \ufffd **xt** + _f_ ( **xt** _,_ **u** _, t, \u03b8_ ) _A_ .\n\n\n\n_an_ +1 = _an_ + \u2206 _t_ ( _\u2212\u03c9\u03c4_ _an_ + _\u03d5_ ) _._ (2)\n\n\n**Closed-form (Exact) Computation of NAC:** We now devise the analytical solution for Eqn. 1. Let both _\u03c9\u03c4_ and\n_\u03d5_ be fixed in pseudo-time interval (frozen-coefficient approximation (John, 1952)) with initial condition _a_ 0, then\nclosed-form solution is:\n\n\n\n**xt** = **[x][t]**\n\n_dt_ _\u03c4_\n\n\n\n\n**[x][t]** _\u2212_ [1]\n\n_dt_ [=] _[ \u2212]_ \ufffd _\u03c4_\n\n\n\nyields _[d]_ _dt_ **[x][t]** [=] _[ \u2212]_ \ufffd _\u2212_ _\u03c4_ [1] _[\u2212]_ _[f]_ [(] **[x][t]** _[,]_ **[ u]** _[, t, \u03b8]_ [)] \ufffd **xt** + _f_ ( **xt** _,_ **u** _, t, \u03b8_ ) _A_ .\n\nLNNs are known for their strong expressivity, stability, and\nperformance in irregularly sampled time-series modeling\n(Hasani et al., 2021; 2022).\n**NAC\u2019s forward-pass update using ODE solver:** The\nstate of NAC at time _t_ can be computed using a numerical\nODE solver that simulates the dynamics from an initial\nstate _a_ 0 to _at_ . The solver discretizes the continuous interval\n\n[0 _, T_ ] into steps [ _t_ 0 _, t_ 1 _, t_ 2 _, . . ., tn_ ], with each step updating\nthe state from _ti_ to _ti_ +1. For our purposes, we use the\n_explicit Euler_ solver, which is simple, efficient, and easy to\nimplement. Although methods such as Runge-Kutta may\noffer higher accuracy, their computational overhead makes\nthem less suitable for large-scale neural simulations that\nrequire numerous updates, especially since the logits are\nnormalized, and exact precision is not necessary. Let the\nstep size be \u2206 _t_, with discrete times _tn_ = _n_ \u2206 _t_ and logit\nstates _an_ = _a_ ( _tn_ ). Using the _explicit Euler_ method, the\nupdate is\n\n\n\n_dat_\n\n_dt_ [=] _[ \u2212]_ ~~\ufffd~~ _[f][\u03c9][\u03c4]_ [ ([] **[q]** \ufffd [;] **[ k]** ~~\ufffd~~ []] _[, \u03b8][\u03c9][\u03c4]_ [ )] ~~\ufffd~~\n_\u03c9\u03c4_ ( **u** )\n\n\n\n_at_ + _f\u03d5_ ([ **q** ; **k** ] _, \u03b8\u03d5_ ) _,_ (1)\n\ufffd ~~\ufffd~~ \ufffd ~~\ufffd~~\n_\u03d5_ ( **u** )\n\n\n\n_at_ = _a_ _[\u2217]_ + ( _a_ 0 _\u2212_ _a_ _[\u2217]_ ) _e_ _[\u2212][\u03c9][\u03c4][ t]_\n\ufffd\ufffd\ufffd\ufffd ~~\ufffd~~ ~~\ufffd\ufffd~~ ~~\ufffd~~\nsteady-state transient\n\n\n\n(3)\n\n\n\nwhere **u** = [ **q** ; **k** ] denotes the sparse Top- _K_ concatenated\nquery\u2013key input. _\u03c9\u03c4_ represents a learnable time-constant\ngate head, _\u03d5_ denotes a nonlinear content-target head. Both\ngates are parameterized by a backbone network derived\n\n\n\nHere, _a_ _[\u2217]_ = _\u03d5/\u03c9\u03c4_ is the steady-state solution. The full\nderivation is provided in Appendix B.1.\n\n\n\n2\n\n\n**Neuronal Attention Circuit (NAC) for Representation Learning**\n\n\n\n**2.1. Stability Analysis of NAC**\n\n\nWe now investigate the stability bounds of _NAC_ under both\nthe ODE-based and the Closed-Form formulations.\n\n\n2.1.1. STATE STABILITY\n\n\nWe analyze state stability in both single-connection and\nmulti-connection settings. This analysis establishes the\nboundedness of the attention logit state trajectory, ensuring that, under positive decay rates, the dynamics remain\nwell-behaved without divergence or overshoot.\n\n**Theorem 1** (State Stability) **.** _Let a_ [(] _t_ _[i]_ [)] _denote the state of_\n_the i-th attention logit governed by da_ [(] _t_ _[i]_ [)] _[/dt]_ [ =] _[ \u2212][\u03c9][\u03c4]_ _[a]_ [(] _t_ _[i]_ [)] +\n_\u03d5. Assume that \u03d5 and \u03c9\u03c4 decompose across M incom-_\n_ing connections as \u03d5_ = [\ufffd] _[M]_ _j_ =1 _[f][\u03d5]_ [([] **[q]** _[i]_ [;] **[ k]** _[j]_ [])] _[, and][ \u03c9][\u03c4]_ [ =]\n_M_\n\ufffd _j_ =1 _[f][\u03c9]_ _\u03c4_ [([] **[q]** _[i]_ [;] **[ k]** _[j]_ [])] _[,][ with][ f][\u03c9]_ _\u03c4_ _>_ 0 _._ _Define the per-_\n_connection equilibrium Ai,j_ = _f\u03d5_ ([ **q** _i_ ; **k** _j_ ]) _/f\u03c9\u03c4_ ([ **q** _i_ ; **k** _j_ ]) _,_\n_and let A_ [min] _i_ = min _j Ai,j and A_ [max] _i_ = max _j Ai,j. Then_\n_for any finite horizon t \u2208_ [0 _, T_ ] _, the state trajectory satisfies_\n\n\nmin(0 _, A_ [min] _i_ ) _\u2264_ _a_ [(] _t_ _[i]_ [)] _\u2264_ max(0 _, A_ [max] _i_ ) _,_ (4)\n\n\n_provided the initial condition ai_ (0) _lies within this range. In_\n_the special case of a single connection (M_ = 1 _), the bounds_\n_collapse to_\n\n\nmin(0 _, Ai_ ) _\u2264_ _a_ [(] _t_ _[i]_ [)] _\u2264_ max(0 _, Ai_ ) _,_ (5)\n\n\n_where Ai_ = _f\u03d5/\u03c9\u03c4 is exactly the steady-state solution from_\n_Eqn. 3. The proof is provided in the Appendix B.2._\n\n\n2.1.2. CLOSED-FORM ERROR & EXPONENTIAL BOUNDS\n\n\nWe now examine the asymptotic stability, error characterization, and exponential boundedness of the closed-form\nformulation. We begin by quantifying the deviation of the\ntrajectory from its steady-state solution. Define the instanta\nneous error\n_\u03b5t_ = _at \u2212_ _a_ _[\u2217]_ _,_ (6)\n\n\nwhich measures the distance of the system state to equilibrium at time _t_ . From Eqn. 3, the error admits the exact\nrepresentation\n\n\n_\u03b5t_ = ( _a_ 0 _\u2212_ _a_ _[\u2217]_ ) _e_ _[\u2212][\u03c9][\u03c4][ t]_ (7)\n\n\nIn particular, the pointwise absolute error is given by\n\n\n_|\u03b5t|_ = _|a_ 0 _\u2212_ _a_ _[\u2217]_ _| e_ _[\u2212][\u03c9][\u03c4][ t]_ (8)\n\n\nThis reveals that convergence is not merely asymptotic but\nfollows an exact exponential law, controlled by the rate parameter _\u03c9\u03c4_ . This yields the following finite-time guarantee.\n\n**Corollary 1** (Exponential decay bound) **.** _If \u03c9\u03c4 >_ 0 _, then_\n_for all t \u2265_ 0 _,_\n\n\n_|at \u2212_ _a_ _[\u2217]_ _| \u2264|a_ 0 _\u2212_ _a_ _[\u2217]_ _| e_ _[\u2212][\u03c9][\u03c4][ t]_ _._ (9)\n\n\n\n_Remark:_ If _\u03c9\u03c4 >_ 0 then lim _t\u2192\u221e_ _e_ _[\u2212][\u03c9][\u03c4][ t]_ = 0, therefore\n_at \u2192_ _a_ _[\u2217]_ . The convergence is exponential with rate _\u03c9\u03c4_ .\nIf _\u03c9\u03c4 <_ 0 then _e_ _[\u2212][\u03c9][\u03c4][ t]_ = _e_ _[|][\u03c9][\u03c4][ |][t]_ diverges so _at_ grows\nexponentially away from _a_ _[\u2217]_ in magnitude (unless initial\noffset _a_ 0 _\u2212_ _a_ _[\u2217]_ = 0, a measure-zero case). If _\u03c9\u03c4_ = 0 the\nODE is \u02d9 _a_ = _\u03d5_ and the solution is linear in _t_ (unless _\u03d5_ = 0).\nFor bounded dynamics that converge to an interpretable\nsteady-state, it is required that _\u03c9\u03c4 >_ 0.\n\n\n**Corollary 2** (Uniform initialization) **.** _If the initialization is_\n_only known to belong to a bounded set, i.e., |a_ 0 _\u2212_ _a_ _[\u2217]_ _| \u2264_ _M_\n_for some M >_ 0 _, then the error admits the uniform bound_\n\n\n_|at \u2212_ _a_ _[\u2217]_ _| \u2264_ _Me_ _[\u2212][\u03c9][\u03c4][ t]_ _._ (10)\n\n\n_Remark:_ This bound highlights that exponential convergence holds uniformly across all admissible initial\nconditions, with the constant _M_ capturing the worst-case\ndeviation.\n\n\n**Corollary 3** (Sample complexity to _\u03b4_ -accuracy) **.** _A natural_\n_operational question is the time required to achieve a target_\n_tolerance \u03b4 >_ 0 _. Solving_\n\n\n_|a_ 0 _\u2212_ _a_ _[\u2217]_ _|e_ _[\u2212][\u03c9][\u03c4][ t]_ _\u2264_ _\u03b4,_ (11)\n\n\n_We obtain the threshold_\n\n\n\n\n[1] ln _[|][a]_ [0] _[ \u2212]_ _[a][\u2217][|]_\n\n_\u03c9\u03c4_ _\u03b4_\n\n\n\n_t \u2265_ [1]\n\n\n\n_._ (12)\n_\u03b4_\n\n\n\n_Remark:_ The convergence rate is inversely proportional to\n_\u03c9\u03c4_, and the required time scales only logarithmically in\nthe accuracy level 1 _/\u03b4_ . Intuitively, larger _\u03c9\u03c4_ accelerates\ncontraction towards equilibrium, yielding faster attainment\nof any prescribed tolerance.\n\n\n_Figure 1._ Illustration of **(a)** NCPs with pre-determined wiring; **(b)**\nSensory gate, where sensory neurons are active, and the remaining\nneurons are disabled for the _q_, _k_, and _v_ projections; **(c)** Backbone,\nshowing inter-motor projections with sensory neurons disabled in\nextended heads for computing _\u03d5_ and _\u03c9\u03c4_ .\n\n\n\n3", "metadata": {"source": "neuronal_attention_circuits_raw_with_image_ids_with_captions.md"}, "chunk_index": 0, "token_size_config": 5000}
{"id": "neuronal_attention_circuits_raw_with_image_ids_with_captions_chunks_5k_1", "content": "_|at \u2212_ _a_ _[\u2217]_ _| \u2264_ _Me_ _[\u2212][\u03c9][\u03c4][ t]_ _._ (10)\n\n\n_Remark:_ This bound highlights that exponential convergence holds uniformly across all admissible initial\nconditions, with the constant _M_ capturing the worst-case\ndeviation.\n\n\n**Corollary 3** (Sample complexity to _\u03b4_ -accuracy) **.** _A natural_\n_operational question is the time required to achieve a target_\n_tolerance \u03b4 >_ 0 _. Solving_\n\n\n_|a_ 0 _\u2212_ _a_ _[\u2217]_ _|e_ _[\u2212][\u03c9][\u03c4][ t]_ _\u2264_ _\u03b4,_ (11)\n\n\n_We obtain the threshold_\n\n\n\n\n[1] ln _[|][a]_ [0] _[ \u2212]_ _[a][\u2217][|]_\n\n_\u03c9\u03c4_ _\u03b4_\n\n\n\n_t \u2265_ [1]\n\n\n\n_._ (12)\n_\u03b4_\n\n\n\n_Remark:_ The convergence rate is inversely proportional to\n_\u03c9\u03c4_, and the required time scales only logarithmically in\nthe accuracy level 1 _/\u03b4_ . Intuitively, larger _\u03c9\u03c4_ accelerates\ncontraction towards equilibrium, yielding faster attainment\nof any prescribed tolerance.\n\n\n_Figure 1._ Illustration of **(a)** NCPs with pre-determined wiring; **(b)**\nSensory gate, where sensory neurons are active, and the remaining\nneurons are disabled for the _q_, _k_, and _v_ projections; **(c)** Backbone,\nshowing inter-motor projections with sensory neurons disabled in\nextended heads for computing _\u03d5_ and _\u03c9\u03c4_ .\n\n\n\n3\n\n\n\n\n```json\n\"img_neuronal_attention_circuits_2_0\": {\n    \"path\": \"E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/neuronal_attention_circuits.pdf-2-0.png\",\n    \"page\": 2,\n    \"section\": \"Abstract\",\n    \"image_relevance\": \"high\",\n    \"image_type\": \"architecture\",\n    \"semantic_role\": \"illustrates\",\n    \"caption\": \"The image presents three distinct neural circuit architectures: a) Neuronal Circuit Policies, b) Sensory Gate, and c) Backbone. Each circuit is composed of sensory (dark purple triangle), inter (blue circle), command (purple square), and motor (pink triangle) neurons, with some command neurons exhibiting feedback loops. The diagrams also delineate 'active group' (light green) and 'disabled group' (light grey) components, illustrating variations in neuron activation and connectivity from input to output.\",\n    \"depicted_concepts\": [\n      \"Neuronal circuit\",\n      \"Sensory neuron\",\n      \"Interneuron\",\n      \"Command neuron\",\n      \"Motor neuron\",\n      \"Neural network architecture\",\n      \"Feedback loop\",\n      \"Sensory Gate\",\n      \"Backbone circuit\",\n      \"Active group\",\n      \"Disabled group\"\n    ],\n    \"confidence\": \"high\"\n}\n```\n**Neuronal Attention Circuit (NAC) for Representation Learning**\n\n\n\n**Algorithm 1** Repurposed NCPCell\n\n\n**Require:** Wiring _W_ with ( _A_ in _, A_ rec), groups ( _Ns, Ni, Nc, Nm_ ),\nactivation _\u03b1_, input group _G_ input, output group _G_ output, disabled\ngroups _D_\n**Ensure:** Output _yt \u2208_ R _[B][\u00d7][d]_ [out], state **x** _t \u2208_ R _[B][\u00d7][d][h]_\n\nBinary mask: _M_ rec _\u2190|A_ rec _|_, _M_ in _\u2190|A_ in _|_\nInitialize parameters : _W_ in _, W_ rec _, b, w_ in _, b_ in _, w_ out _, b_ out\nInput neurons: _I_ in _\u2190_ _G_ input\nOutput neurons: _I_ out _\u2190_ _G_ output\nDefine activation mask: maskact _,i_ = 0 if _i \u2208D_ else 1\nInput Projections: \u02dc _ut \u2190_ _ut \u2299_ _w_ in + _b_ in\nRecurrent computation:Sparse computation: _st \u2190 rt \u2190u_ \u02dc _t_ ( **x** _Wt\u2212_ in1 _\u2299_ ( _WM_ recin _\u2299_ ) _M_ rec)\nNeuron update: **x** _t \u2190_ _\u03b1_ ( _rt_ + _st_ + _b_ ) _\u2299_ maskact\nOutput mapping: _yt \u2190_ ( **x** _t_ [ _I_ out] _\u2299_ _w_ out) + _b_ out\n**return** ( _yt,_ **x** _t_ )\n\n\n**Algorithm 2** Sparse Top- _K_ Pairwise Concatenation\n\n\n**Require:** Keys _K \u2208_ R _[B][\u00d7][H][\u00d7][T][k][\u00d7][D]_, Top- _K_ value _K_\n**Ensure:** concatenated tensor _U \u2208_ R _[B][\u00d7][H][\u00d7][T][q]_ _[\u00d7][K]_ [eff] _[\u00d7]_ [2] _[D]_\n\nScores: _S \u2190_ _Q \u00b7 K_ _[\u22a4]_\n\nEffective Top- _K_ : _K_ eff _\u2190_ min( _K, Tk_ )\nIndices: _I_ topk _\u2190_ top ~~k~~ ( _S, K_ eff)\nGather: _K_ selected _\u2190_ gather( _K, I_ topk) _\u2208_ R _[B][\u00d7][H][\u00d7][T][q]_ _[\u00d7][K]_ [eff] _[\u00d7][D]_\n\nTiled: _Q_ tiled _\u2190_ tile( _Q, K_ eff) _\u2208_ R _[B][\u00d7][H][\u00d7][T][q]_ _[\u00d7][K]_ [eff] _[\u00d7][D]_\n\nConcatenate: _U_ topk _\u2190_ [ _Q_ tiled; _K_ selected ] _\u2208_ R _[B][\u00d7][H][\u00d7][T][q]_ _[\u00d7][K]_ [eff] _[\u00d7]_ [2] _[D]_\n\n**return** _U_ topk\n\n\n**2.2. Designing the Neural Network**\n\n\nWe now outline the design of a neural network layer guided\nby the preceding analysis. The process involves five steps:\n(i) repurposing NCPs; (ii) input curation; (iii) construction\nof the time vector ( _t_ ); (iv) computing attention logits and\nweights; and (v) generating the attention output. Figure 2\nprovides a graphical overview of NAC.\n**Repurposing NCPs:** We repurpose the NCPs framework\nby converting its fixed, biologically derived wiring (see Figure 1(a)) into a flexible recurrent architecture that allows\nconfigurable input\u2013output mappings. Instead of enforcing\na static connectome, our approach exposes adjacency matrices as modifiable structures defining sparse input and\nrecurrent connections. This enables selective information\n\nrouting across neuron groups while retaining the original circuit topology. Decoupling wiring specifications from model\ninstantiation allows dynamic connectivity adjustments to\naccommodate different input modalities without full retraining. Algorithm 1 summarizes the steps for repurposing the\nNCPs wiring mechanism. Key features include group-wise\nmasking for neuron isolation, adaptive remapping of inputs\nand outputs for task-specific adaptation, and tunable sparsity\n_s_ to balance expressiveness and efficiency.\nIn our implementation, the sensory neuron gate ( _NN_ sensory)\nprojects the _q_, _k_, and _v_ representations (see Figure 1(b)).\nThis enables sensory neurons to maintain structured, contextaware representations rather than collapsing inputs into fully\n\n\n\nconnected layers. As a result, the network preserves locality\nand modularity, which improves information routing.\n\n\n_NN_ sensory = NCPCell( _G_ input = [ _Ns_ ] _, G_ output = [ _Ns_ ] _,_\n\n_D_ = [ _Ni, Nc, Nm_ ] _, s_ )\n(13)\nThe inter-to-motor pathways form a backbone network\n( _NN_ backbone) with branches that compute _\u03d5_ and _\u03c9\u03c4_ (see\nFigure 1(c)). Instead of learning _\u03d5_ and _\u03c9\u03c4_ independently,\nthis backbone allows the model to learn shared representations, enabling multiple benefits: (i) separate head layers\nenable the system to capture temporal and structural dependencies independently; (ii) accelerates convergence during\ntraining.\n\n\n_NN_ backbone = NCPCell( _G_ input = [ _Ni_ ] _, G_ output = [ _Nm_ ] _,_\n\n_D_ = [ _Ns_ ] _, s_ )\n(14)\nThe output heads are defined as:\n\n\n_\u03d5_ = _\u03c3_ ( _NN_ backbone( **u** )) (15)\n\n_\u03c9\u03c4_ = softplus( _NN_ backbone( **u** )) + _\u03b5,_ _\u03b5 >_ 0 (16)\n\n\nHere, _\u03d5_ serves as a _content\u2013target gate_ head, where the\nsigmoid function _\u03c3_ ( _\u00b7_ ) determines the target signal strength.\nIn contrast, _\u03c9\u03c4_ is a strictly positive _time\u2013constant gate_ head\nthat controls the rate of convergence and the steady-state\namplitude. Conceptually, this parallels recurrent gating: _\u03d5_\nregulates _what_ content to emphasize, while _\u03c9\u03c4_ governs _how_\n_quickly_ and _to what extent_ it is expressed.\n**Input Curation:** We experimented with different\nstrategies for constructing query\u2013key inputs. Initially, we implemented full pairwise concatenation,\nwhere queries _Q \u2208_ R _[B][\u00d7][H][\u00d7][T][q][\u00d7][D]_ are combined with\nall keys _K_ _\u2208_ R _[B][\u00d7][H][\u00d7][T][k][\u00d7][D]_ to form a joint tensor\n_U \u2208_ R _[B][\u00d7][H][\u00d7][T][q][\u00d7][T][k][\u00d7]_ [2] _[D]_ . While this preserved complete\nfeature information and enabled expressive, learnable\nsimilarity functions, it was memory-intensive, making it impractical for longer sequences. To mitigate this, we applied\na sparse Top- _K_ optimization: for each query, we compute\npairwise scores _S_ = _Q \u00b7 K_ _[\u22a4]_ _\u2208_ R _[B][\u00d7][H][\u00d7][T][q][\u00d7][T][k]_, select the\nTop- _K_ eff = min( _K, Tk_ ) keys, and construct concatenated\npairs _U_ topk _\u2208_ R _[B][\u00d7][H][\u00d7][T][q][\u00d7][K]_ [eff] _[\u00d7]_ [2] _[D]_ . This approach preserves\nthe most relevant interactions while substantially reducing\nmemory requirements in the concatenation and subsequent\nbackbone processing stages, allowing the method to scale\nlinearly with the sequence length in those components.\nHowever, the initial computation of _S_ remains quadratic\n(see Appendix C.3). Algorithm 2 outlines the steps required\nfor input curation.\n**Time Vector:** NAC builds on continuous-depth models\nas (Hasani et al., 2022) that adapt their temporal dynamics to the task. It constructs an internal, normalized\npseudo-time vector _t_ pseudo using a sigmoidal transformation,\n_t_ pseudo = _\u03c3_ ( _ta \u00b7 t_ + _tb_ ), where _ta_ and _tb_ are learnable affine\n\n\n\n4\n\n\n**Neuronal Attention Circuit (NAC) for Representation Learning**\n\n\n\nparameters and _\u03c3_ is the sigmoid function. For time-varying\ndatasets (e.g., irregularly sampled series), each time point\n_t_ is derived from the sample\u2019s timestamp, while for tasks\nwithout meaningful timing, _t_ is set to 1. The resulting _t_ pseudo\nlies in [0 _,_ 1] and provides a smooth, bounded representation\nof time for modulating the network\u2019s dynamics.\n**Attention logits and weights:** Starting from Eqn. 3,\nconsider the trajectory of a query\u2013key pair with initial\ncondition _a_ 0 = 0:\n\n\n\n_at_ = _[\u03d5]_\n\n_\u03c9\u03c4_\n\n\n\n\ufffd1 _\u2212_ _e_ _[\u2212][\u03c9][\u03c4][ t]_ [\ufffd] _,_ (17)\n\n\n\nfollowed by the _softmax_ normalization to calculate attention weights. The resulting attention weights _\u03b1t_ [(] _[h]_ [)] are then\nused to integrate with the value vector _v_ [(] _[h]_ [)], producing headspecific attention outputs. Finally, these outputs are concatenated and linearly projected back into the model dimension.\nThis formulation ensures that each head learns distinct dynamic compatibilities governed by its own parameterization\nof _\u03d5_ and _\u03c9\u03c4_, while the aggregation across heads preserves\nthe expressive capacity of the standard multi-head attention\nmechanism.\n\n\n**2.3. NAC as Universal Approximator**\n\n\nWe now establish the universal approximation capability of\nNAC by extending the classical Universal Approximation\nTheorem (UAT) (Nishijima, 2021) to the proposed mechanism. For brevity, we consider a network with a single\nNAC layer processing fixed-dimensional inputs, though the\nargument generalizes to sequences.\n\n\n**Theorem 2** (Universal Approximation by NAC) **.** _Let K \u2282_\nR _[n]_ _be a compact set and f_ : _K \u2192_ R _[m]_ _be a continuous_\n_function. For any \u03f5 >_ 0 _, there exists a neural network_\n_consisting of a single NAC layer, with sufficiently large_\n_model dimension dmodel, number of heads H, sparsity s,_\n_and nonlinear activations, such that the network\u2019s output_\n_g_ : R _[n]_ _\u2192_ R _[m]_ _satisfies_\n\n\nsup _\u2225f_ ( _x_ ) _\u2212_ _g_ ( _x_ ) _\u2225_ _< \u03f5._ (20)\n_x\u2208K_\n\n\n_The proof is provided in Appendix B.3._\n\n\n**3. Evaluation**\n\n\nWe evaluate the proposed architecture against a range of\nbaselines, including (DT & CT) RNN, (DT & CT) attention,\nand multiple NAC ablation configurations. Experiments\nare conducted across diverse domains, including irregular\ntime-series modeling, lane keeping of autonomous vehicles,\nand Industry 4.0 prognostics. All results are obtained using 5-fold cross-validation, where models are trained using\nBPTT (see Appendix C.2) on each fold and evaluated across\nall folds. We report the mean ( _\u00b5_ ) and standard deviation ( _\u03c3_ )\nto capture variability and quantify uncertainty in the predictions. Table 1 provides results for all experiments, and the\ndetails of the baselines, ablation, environment utilized, the\ndata curation and preprocessing, and neural network architectures for all experiments are provided in the Appendix\nD.3.\n\n\n**3.1. Irregular Time-series**\n\n\nWe evaluate the proposed architecture on two irregular timeseries datasets: (i) Event-based MNIST; and (ii) Person\nActivity Recognition (PAR).\n\n\n\nFor finite _t_, the exponential factor (1 _\u2212_ _e_ _[\u2212][\u03c9][\u03c4][ t]_ ) regulates the\nbuildup of attention, giving _\u03c9\u03c4_ a temporal gating role. Normalizing across all keys via _softmax_ yields attention weights\n_\u03b1t_ = softmax( _at_ ), defining a valid probability distribution\nwhere _\u03d5_ amplifies or suppresses content alignments, and _\u03c9\u03c4_\nshapes both the speed and saturation of these preferences.\nAs _t \u2192\u221e_, the trajectory converges to the steady state\n\n\n_a_ _[\u2217]_ _t_ [=] _[\u03d5]_ _\u2248_ _[q][\u22a4][k]_ _,_ (18)\n\n_\u03c9\u03c4_ ~~_\u221a_~~ _dk_\n\n\nwhich is analogous to scaled-dot attention under specific\nparameterization when the backbone _NN_ backbone is configured as a linear projection such that _\u03d5_ ( **u** ) = _q_ _[\u22a4]_ _k_ and\n_\u03c9\u03c4_ ( _u_ ) = _[\u221a]_ _dk_ (e.g., by setting NCP weights to emulate\nbilinear forms and disabling nonlinearities). In general, the\nnonlinear backbone allows for more expressive similarities,\nwith the approximation holding when trained to mimic dot\nproducts.\n**Attention output:** Finally, the attention output is computed\nby integrating the attention weights with the value matrix:\n\n\nNAC( _q, k, v_ ) = _\u03b1tvtdt_ (19)\n\ufffd _T_\n\n\nIn practice, the integration is approximated using a Riemannstyle approach, where the weighted elements are computed\nby multiplying each _vt_ with its corresponding _\u03b1t_ . These are\nthen summed and multiplied by a fixed pseudo-time step\n_\u03b4t_, chosen as a scalar (typically between 0.5\u20131.0) hyperparameter during layer initialization. This yields a continuous\nanalogue of standard weighted sums, giving finer resolution\nof the attention trajectory without altering the underlying\nvalues. Sensitivity to attention output w.r.t _\u03b4t_ is visualized\nin Appendix D.2.\n\n\n2.2.1. EXTENSION TO MULTI-HEAD\n\n\nTo scale this mechanism to multi-head attention, we project\nthe input sequence into _H_ independent subspaces (heads)\nof dimension _d_ model _/H_, yielding query, key, and value tensors ( _q_ [(] _[h]_ [)] _, k_ [(] _[h]_ [)] _, v_ [(] _[h]_ [)] ) for _h \u2208{_ 1 _, . . ., H}_ . For each head,\npairwise logits are computed according to Eqns. 2,3 or 18,\n\n\n\n5\n\n\n**Neuronal Attention Circuit (NAC) for Representation Learning**\n\n\n_Figure 2._ Illustration of the architecture of **(a)** Neuronal Attention Circuit mechanism ; **(b)** Multi-Head Extension\n\n\n\n\n```json\n\"img_neuronal_attention_circuits_5_0\": {\n    \"path\": \"E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/neuronal_attention_circuits.pdf-5-0.png\",\n    \"page\": 5,\n    \"section\": \"Abstract\",\n    \"image_relevance\": \"high\",\n    \"image_type\": \"architecture\",\n    \"semantic_role\": \"defines\",\n    \"caption\": \"The image presents two architectural diagrams: a Neuronal Attention Circuit (NAC) and a Multi-Head Neuronal Attention Circuit. The single-head NAC processes time-series Query, Key, and Value inputs through Sparse Topk-Pairwise, a Backbone module, a logit computation step, softmax, and an Attention Score function to yield NAC(q,k,v). The Multi-Head NAC extends this by channeling Query, Key, and Value inputs through Sensory gates into multiple instances of the Neuronal Attention Circuit, whose outputs are then concatenated, transformed by a linear layer, and subjected to an activation function.\",\n    \"depicted_concepts\": [\n      \"Neuronal Attention Circuit (NAC)\",\n      \"Multi-Head Neuronal Attention Circuit\",\n      \"Query\",\n      \"Key\",\n      \"Value\",\n      \"Sparse Topk-Pairwise\",\n      \"Backbone\",\n      \"Compute logits\",\n      \"softmax\",\n      \"Attention Score\",\n      \"Sensory gate\",\n      \"Concatenation\",\n      \"Linear layer\",\n      \"Activation function\"\n    ],\n    \"confidence\": \"high\"\n}\n```", "metadata": {"source": "neuronal_attention_circuits_raw_with_image_ids_with_captions.md"}, "chunk_index": 1, "token_size_config": 5000}
{"id": "neuronal_attention_circuits_raw_with_image_ids_with_captions_chunks_5k_2", "content": "```json\n\"img_neuronal_attention_circuits_5_0\": {\n    \"path\": \"E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/neuronal_attention_circuits.pdf-5-0.png\",\n    \"page\": 5,\n    \"section\": \"Abstract\",\n    \"image_relevance\": \"high\",\n    \"image_type\": \"architecture\",\n    \"semantic_role\": \"defines\",\n    \"caption\": \"The image presents two architectural diagrams: a Neuronal Attention Circuit (NAC) and a Multi-Head Neuronal Attention Circuit. The single-head NAC processes time-series Query, Key, and Value inputs through Sparse Topk-Pairwise, a Backbone module, a logit computation step, softmax, and an Attention Score function to yield NAC(q,k,v). The Multi-Head NAC extends this by channeling Query, Key, and Value inputs through Sensory gates into multiple instances of the Neuronal Attention Circuit, whose outputs are then concatenated, transformed by a linear layer, and subjected to an activation function.\",\n    \"depicted_concepts\": [\n      \"Neuronal Attention Circuit (NAC)\",\n      \"Multi-Head Neuronal Attention Circuit\",\n      \"Query\",\n      \"Key\",\n      \"Value\",\n      \"Sparse Topk-Pairwise\",\n      \"Backbone\",\n      \"Compute logits\",\n      \"softmax\",\n      \"Attention Score\",\n      \"Sensory gate\",\n      \"Concatenation\",\n      \"Linear layer\",\n      \"Activation function\"\n    ],\n    \"confidence\": \"high\"\n}\n```\n\n**Event-based MNIST:** Event-based MNIST is the trans\nformation of the widely recognized MNIST dataset with\nirregular sampling added originally proposed in (Lechner\n& Hasani, 2022). The transformation was done in two\nsteps: (i) flattening each 28\u00d728 image into a time series\nof length 784, and (ii) encoding the binary time series into\nan event-based format by collapsing consecutive identical\nvalues (e.g., 1,1,1,1 \u2192 (1, t=4)). This representation requires models to handle temporal dependencies effectively.\nNAC-PW achieved first place with an accuracy of 96.64%,\nfollowed by NAC-Exact/05s/8k at 96.12%. GRU-ODE and\nContiFormer ranked third with 96.04%.\n\n**Person Activity Recognition (PAR):** We employed the\nLocalized Person Activity dataset from UC Irvine (Vidulin\net al., 2010). The dataset contains data from five participants,\neach equipped with inertial measurement sensors sampled\nevery 211 ms. The goal of this experiment is to predict a\nperson\u2019s activity from a set of predefined actions, making it\na classification task. All models performed well on this task,\nwith NAC-PW achieving 89.15% accuracy and taking first\nplace. NAC-Exact/05s/8k and GRU-ODE ranked second\nwith 89.01% accuracy, while NAC-02s ranked third with\n88.84% mean accuracy.\n\n\n**3.2. Lane-Keeping of Autonomous Vehicles**\n\n\nLane keeping in autonomous vehicles (AVs) is a fundamental problem in robotics and AI. Early works (Tiang et al.,\n2018; Park et al., 2021) primarily emphasized accuracy,\noften relying on large models. More recent research (Lechner et al., 2020; Razzaq & Hongwei, 2023) has shifted toward designing compact architectures suitable for resourceconstrained devices. The goal of this experiment is to create\na long causal structure between the road\u2019s horizon and the\n\n\n\ncorresponding steering commands. To evaluate, we used\ntwo widely adopted simulation environments: (i) OpenAI\nCarRacing (Brockman et al., 2016); and (ii) the Udacity SelfDriving Car Simulator (uda). In OpenAI CarRacing, the\ntask is to classify steering actions from a predefined action\nset. In contrast, the Udacity Simulator requires predicting a\ncontinuous trajectory of steering values. We implemented\nthe AI models proposed by (Razzaq & Hongwei, 2023),\nreplacing the recurrent layer with NAC and its counterparts.\nAll models achieved around 80% accuracy on average in\nthe CarRacing benchmark. Notably, NAC-PW performed\nthe best, reaching the highest accuracy of 80.72%, followed\nby NAC-Steady, ranked second with 80.62%. LSTM and\nGRU took third position, achieving 80.60% on average.\nIn the Udacity benchmark, NAC-32k performed the best,\nachieving the lowest MSE of 0.0170. NAC-Exact followed\nwith 0.0173, and ContiFormer ranked third with 0.0174.\nTo visualize saliency maps for these experiments, refer to\nAppendix D.3.3. Experimental videos are available for the\n[OpenAI CarRacing [click here] and for the Udacity Simula-](https://www.youtube.com/watch?v=kwTNU8aV8-I)\n[tor [click here].](https://www.youtube.com/watch?v=mMRVsNUQ8i0)\n\n\n**3.3. Industry 4.0**\n\n\nIndustry 4.0 has transformed manufacturing, making prognostic health management (PHM) systems essential. A key\nPHM task is estimating the remaining useful life (RUL) of\ncomponents, particularly rolling element bearings (REB),\nwhich account for 40\u201350% of machine failures (Ding et al.,\n2021; Zhuang et al., 2021). The objective is to learn degradation features from one operating condition of a dataset\nand generalize to unseen conditions within the same dataset.\nFurthermore, the model should provide accurate RUL estimation on entirely different datasets, while maintaining a\n\n\n\n6\n\n\n**Neuronal Attention Circuit (NAC) for Representation Learning**\n\n\n_Table 1._ Model Performance Across All Categories and Datasets\n\n\n**Irregular Time-Series** **Lane-Keeeping of AVs** **Industry 4.0**\n**Model**\n\n**E-MNIST (\u2191)** **PAR (\u2191)** **CarRacing (\u2191)** **Udacity (\u2193)** **PRONOSTIA (\u2193)** **XJTU-SY (\u2193)** **HUST (\u2193)**\n\n\nRNN 95.59 _[\u00b1]_ [0.37] 88.77 _[\u00b1]_ [0.58] 78.90 _[\u00b1]_ [3.35] 0.0210 _[\u00b1]_ [0.0014] 42.05 _[\u00b1]_ [7.49] 31.07 _[\u00b1]_ [6.63] 42.22 _[\u00b1]_ [8.16]\n\nLSTM 95.88 _[\u00b1]_ [0.23] 88.36 _[\u00b1]_ [0.79] 80.60 _[\u00b1]_ [0.12] 0.0181 _[\u00b1]_ [0.0014] 41.87 _[\u00b1]_ [2.88] 31.99 _[\u00b1]_ [8.32] 44.09 _[\u00b1]_ [2.14]\n\nGRU 95.85 _[\u00b1]_ [0.22] 88.68 _[\u00b1]_ [1.35] 80.60 _[\u00b1]_ [0.22] 0.0206 _[\u00b1]_ [0.0014] 44.22 _[\u00b1]_ [4.60] 26.65 _[\u00b1]_ [4.49] 41.86 _[\u00b1]_ [7.96]\n\nCT-RNN 95.18 _[\u00b1]_ [0.20] 88.71 _[\u00b1]_ [0.87] 80.21 _[\u00b1]_ [0.27] 0.0206 _[\u00b1]_ [0.0013] 44.32 _[\u00b1]_ [8.69] 26.01 _[\u00b1]_ [8.74] 39.99 _[\u00b1]_ [6.33]\n\nGRU-ODE **96.04** _[\u00b1]_ **[0.13]** **89.01** _[\u00b1]_ **[1.55]** 80.29 _[\u00b1]_ [0.72] 0.0188 _[\u00b1]_ [0.0016] 45.11 _[\u00b1]_ [3.19] 31.20 _[\u00b1]_ [8.69] 43.91 _[\u00b1]_ [7.10]\n\nPhasedLSTM 95.79 _[\u00b1]_ [0.14] 88.93 _[\u00b1]_ [1.08] 80.35 _[\u00b1]_ [0.38] 0.0186 _[\u00b1]_ [0.0015] 44.15 _[\u00b1]_ [4.80] 35.49 _[\u00b1]_ [5.54] 38.66 _[\u00b1]_ [5.55]\n\nmmRNN 95.74 _[\u00b1]_ [0.27] 88.48 _[\u00b1]_ [0.46] 80.13 _[\u00b1]_ [0.54] 0.0205 _[\u00b1]_ [0.0027] 48.50 _[\u00b1]_ [4.60] 27.84 _[\u00b1]_ [4.05] 40.11 _[\u00b1]_ [9.56]\n\nLTC 95.25 _[\u00b1]_ [0.00] 88.12 _[\u00b1]_ [0.68] 76.37 _[\u00b1]_ [3.01] 0.0245 _[\u00b1]_ [0.0024] 48.14 _[\u00b1]_ [5.01] 36.83 _[\u00b1]_ [8.57] 61.82 _[\u00b1]_ [15.64]\n\nCfC 94.16 _[\u00b1]_ [0.49] 88.60 _[\u00b1]_ [0.34] 80.59 _[\u00b1]_ [0.33] 0.0198 _[\u00b1]_ [0.0022] 47.78 _[\u00b1]_ [3.54] 35.51 _[\u00b1]_ [3.94] 54.09 _[\u00b1]_ [10.13]\n\nAttention 95.68 _[\u00b1]_ [0.23] 88.29 _[\u00b1]_ [0.98] 80.40 _[\u00b1]_ [0.26] 0.0193 _[\u00b1]_ [0.0009] 41.89 _[\u00b1]_ [6.98] 26.29 _[\u00b1]_ [4.06] 40.28 _[\u00b1]_ [4.23]\n\nMHA 95.94 _[\u00b1]_ [0.15] 88.36 _[\u00b1]_ [1.06] 79.99 _[\u00b1]_ [0.49] 0.0185 _[\u00b1]_ [0.0017] 45.36 _[\u00b1]_ [5.16] 37.31 _[\u00b1]_ [12.20] 41.40 _[\u00b1]_ [7.72]\n\nmTAN 95.97 _[\u00b1]_ [0.25] 88.08 _[\u00b1]_ [0.94] 80.86 _[\u00b1]_ [0.22] 0.0178 _[\u00b1]_ [0.0005] 44.41 _[\u00b1]_ [7.15] 41.34 _[\u00b1]_ [3.72] 66.29 _[\u00b1]_ [4.25]\n\nCTA 95.86 _[\u00b1]_ [0.14] 88.10 _[\u00b1]_ [1.10] 80.54 _[\u00b1]_ [0.40] 0.0197 _[\u00b1]_ [0.0016] 39.16 _[\u00b1]_ [3.54] **25.86** _[\u00b1]_ [1.47] 38.41 _[\u00b1]_ [4.51]\n\nODEFormer 95.62 _[\u00b1]_ [0.20] 88.25 _[\u00b1]_ [0.66] 80.54 _[\u00b1]_ [0.40] 0.0190 _[\u00b1]_ [0.0012] 42.42 _[\u00b1]_ [6.98] 35.63 _[\u00b1]_ [9.24] 40.60 _[\u00b1]_ [6.83]\n\nContiFormer **96.04** _[\u00b1]_ **[0.23]** 81.28 _[\u00b1]_ [0.85] 80.47 _[\u00b1]_ [0.50] **0.0174** _[\u00b1]_ **[0.01]** **27.82** _[\u00b1]_ **[7.09]** 34.71 _[\u00b1]_ [4.98] 43.81 _[\u00b1]_ [10.18]\n\n\nNAC-2k 95.73 _[\u00b1]_ [0.07] 88.84% _[\u00b1]_ [0.81] 80.59 _[\u00b1]_ [0.46] 0.0208 _[\u00b1]_ [0.0015] 43.78 _[\u00b1]_ [2.71] 37.43 _[\u00b1]_ [9.28] 40.51 _[\u00b1]_ [6.61]\n\nNAC-32k 95.15 _[\u00b1]_ [0.11] 88.80 _[\u00b1]_ [0.76] 80.38 _[\u00b1]_ [0.16] **0.0170** _[\u00b1]_ **[0.0007]** 49.53 _[\u00b1]_ [4.89] 32.45 _[\u00b1]_ [10.84] 39.17 _[\u00b1]_ [12.23]\n\nNAC-PW **96.64** _[\u00b1]_ [0.12] **89.15** _[\u00b1]_ **[1.01]** **80.72** _[\u00b1]_ **[0.41]** 0.0177 _[\u00b1]_ [0.0008] **37.50** _[\u00b1]_ **[2.56]** 28.01 _[\u00b1]_ [4.93] **30.14** _[\u00b1]_ **[6.87]**\n\n\nNAC-FC 95.31 _[\u00b1]_ [0.07] 88.45 _[\u00b1]_ [0.91] 80.49 _[\u00b1]_ [0.46] 0.0192 _[\u00b1]_ [0.0012] 40.36 _[\u00b1]_ [6.09] **24.89** _[\u00b1]_ **[5.30]** **35.35** _[\u00b1]_ **[6.64]**\n\nNAC-02s 95.31 _[\u00b1]_ [0.07] 88.84 _[\u00b1]_ [1.33] 80.47 _[\u00b1]_ [0.27] 0.0188 _[\u00b1]_ [0.0013] 39.43 _[\u00b1]_ [5.94] 35.59 _[\u00b1]_ [3.86] 38.90 _[\u00b1]_ [6.43]\n\nNAC-09s 95.86 _[\u00b1]_ [0.11] 88.61% _[\u00b1]_ [1.25] 80.43% _[\u00b1]_ [0.17] 0.0188 _[\u00b1]_ [0.0013] 47.29 _[\u00b1]_ [5.52] 40.40 _[\u00b1]_ [8.85] 44.39 _[\u00b1]_ [6.82]\n\n\nNAC-Exact/05s/8k **96.12** _[\u00b1]_ [0.11] **89.01** _[\u00b1]_ **[1.01]** 80.59 _[\u00b1]_ [1.82] **0.0173** _[\u00b1]_ **[0.0006]** **37.75** _[\u00b1]_ **[4.72]** **19.87** _[\u00b1]_ **[1.75]** **27.82** _[\u00b1]_ **[7.09]**\n\nNAC-Euler 95.67 _[\u00b1]_ [0.26] 88.52 _[\u00b1]_ [0.68] **80.61** _[\u00b1]_ [0.28] 0.0181 _[\u00b1]_ [0.0017] 42.08 _[\u00b1]_ [6.14] 28.46 _[\u00b1]_ [8.18] 39.32 _[\u00b1]_ [9.15]\n\nNAC-Steady 95.75 _[\u00b1]_ [0.28] 88.36 _[\u00b1]_ [1.05] **80.62** _[\u00b1]_ **[0.26]** 0.0181 _[\u00b1]_ [0.0012] 40.95 _[\u00b1]_ [5.77] 26.76 _[\u00b1]_ [7.36] 37.12 _[\u00b1]_ [12.43]\n\n\n**Note:** (\u2191) higher is better; (\u2193) lower is better.\n\n\nthe lowest score of 27.82. NAC-Exact/05s/8k and NAC-PW\n\nachieved nearly identical scores, obtaining 37.75 and 37.50\non average, respectively. On the XJTU-SY dataset, NACExact/05s/8k has the lowest score of 19.87. NAC-FC ranked\n\nsecond with a score of 24.89, followed by NAC-PW in third\nplace with an average score of 28.01. A similar trend was\nobserved on the HUST dataset, where NAC-Exact/05s/8k\nachieved first place with a score of 27.82, NAC-PW ranked\nsecond with 30.14, and NAC-FC ranked third with 35.35.\nThese results demonstrated the strong cross-validation capability of NAC.\n\n\n\n\n```json\n\"img_neuronal_attention_circuits_6_0\": {\n    \"path\": \"E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/neuronal_attention_circuits.pdf-6-0.png\",\n    \"page\": 6,\n    \"section\": \"Abstract\",\n    \"image_relevance\": \"high\",\n    \"image_type\": \"plot\",\n    \"semantic_role\": \"supports_result\",\n    \"caption\": \"A line plot illustrates degradation estimation by comparing Normalized Degradation over Time for 'Expected' versus 'NAC' (Neural Architecture Search) curves across three distinct datasets: PRONOSTIA, XJTU-SY, and HUST. The plot shows the progression of degradation, ranging from 0.0 to 1.0, against time for each dataset's expected and NAC-estimated values.\",\n    \"depicted_concepts\": [\n      \"Degradation Estimation\",\n      \"Normalized Degradation\",\n      \"Time\",\n      \"PRONOSTIA dataset\",\n      \"XJTU-SY dataset\",\n      \"HUST dataset\",\n      \"Expected degradation\",\n      \"NAC degradation\"\n    ],\n    \"confidence\": \"high\"\n}\n```\n\n_Figure 3._ Degradation Estimation Results.\n\n\ncompact architecture suitable for resource-constrained devices, thereby supporting localized safety.\nWe utilized three benchmark datasets: (i) PRONOSTIA\n(Nectoux et al., 2012), (ii) XJTU-SY (Wang et al., 2018),\nand (iii) HUST (Thuan & Hong, 2023). Training is performed on PRONOSTIA, while XJTU-SY and HUST are\n\nused to assess cross-validation. We used the Score met\nric (Nectoux et al., 2012) to assess the performance. We\nevaluate generalization using _Bearing 1_ from the first operating condition of each dataset. Figure 3 visualizes the\nexpected degradation alongside the outputs of NAC. On the\nPRONOSTIA dataset, ContiFormer performed the best with\n\n\n\n**3.4. Run Time and Memory Experiments**\n\n\nWe evaluate computational requirements on fixed-length\nsequences of 1024 steps, 64-dimensional features, 4 heads,\nand a Batch size of 1. Each model is run for ten for\nward passes on Google Colab T4-GPU, and we report the\nmean runtime with standard deviation, throughput, and peak\nmemory usage. NAC occupies an intermediate position\nin runtime relative to several CT-RNN models, including\nGRU-ODE, CfC, and LTC. In terms of memory consumption, NAC uses significantly less memory than mTAN, with\nNAC-2k being the least memory-consuming among the CTAttention models. Reducing NAC sparsity from 90% to 20%\nhas minimal effect on memory, decreasing usage slightly\n\n\n\n7\n\n\n**Neuronal Attention Circuit (NAC) for Representation Learning**\n\n\n\n_Table 2._ Run-Time and Memory Benchmark Results", "metadata": {"source": "neuronal_attention_circuits_raw_with_image_ids_with_captions.md"}, "chunk_index": 2, "token_size_config": 5000}
{"id": "neuronal_attention_circuits_raw_with_image_ids_with_captions_chunks_5k_3", "content": "_Figure 3._ Degradation Estimation Results.\n\n\ncompact architecture suitable for resource-constrained devices, thereby supporting localized safety.\nWe utilized three benchmark datasets: (i) PRONOSTIA\n(Nectoux et al., 2012), (ii) XJTU-SY (Wang et al., 2018),\nand (iii) HUST (Thuan & Hong, 2023). Training is performed on PRONOSTIA, while XJTU-SY and HUST are\n\nused to assess cross-validation. We used the Score met\nric (Nectoux et al., 2012) to assess the performance. We\nevaluate generalization using _Bearing 1_ from the first operating condition of each dataset. Figure 3 visualizes the\nexpected degradation alongside the outputs of NAC. On the\nPRONOSTIA dataset, ContiFormer performed the best with\n\n\n\n**3.4. Run Time and Memory Experiments**\n\n\nWe evaluate computational requirements on fixed-length\nsequences of 1024 steps, 64-dimensional features, 4 heads,\nand a Batch size of 1. Each model is run for ten for\nward passes on Google Colab T4-GPU, and we report the\nmean runtime with standard deviation, throughput, and peak\nmemory usage. NAC occupies an intermediate position\nin runtime relative to several CT-RNN models, including\nGRU-ODE, CfC, and LTC. In terms of memory consumption, NAC uses significantly less memory than mTAN, with\nNAC-2k being the least memory-consuming among the CTAttention models. Reducing NAC sparsity from 90% to 20%\nhas minimal effect on memory, decreasing usage slightly\n\n\n\n7\n\n\n**Neuronal Attention Circuit (NAC) for Representation Learning**\n\n\n\n_Table 2._ Run-Time and Memory Benchmark Results\n\n\n**Run-Time** **Throughput** **Peak Memory**\n**Model**\n(s) (seq/s) (MB)\n\n\nRNN 1 _._ 8392 _[\u00b1]_ [0] _[.]_ [1933] 0.544 0.29\nCT-RNN 7 _._ 1097 _[\u00b1]_ [0] _[.]_ [3048] 0.141 0.67\nLSTM 2 _._ 6241 _[\u00b1]_ [0] _[.]_ [2906] 0.381 0.42\nPhasedLSTM 4 _._ 9812 _[\u00b1]_ [0] _[.]_ [272] 0.201 0.80\nGRU 3 _._ 216 _[\u00b1]_ [0] _[.]_ [2566] 0.311 0.54\nGRU-ODE 12 _._ 2498 _[\u00b1]_ [0] _[.]_ [0525] 0.082 0.64\nmmRNN 7 _._ 5852 _[\u00b1]_ [0] _[.]_ [2785] 0.132 0.96\nLTC 14 _._ 643 _[\u00b1]_ [0] _[.]_ [2445] 0.068 0.99\nCfC 6 _._ 0988 _[\u00b1]_ [0] _[.]_ [2135] 0.164 0.76\n\n\nAttention 0 _._ 0016 _[\u00b1]_ [0] _[.]_ [0001] 625.00 16.86\nMHA 0 _._ 0041 _[\u00b1]_ [0] _[.]_ [0001] 243.90 69.05\nmTAN 0 _._ 0272 _[\u00b1]_ [0] _[.]_ [0054] 36.76 790.16\nODEFormer 0 _._ 0317 _[\u00b1]_ [0] _[.]_ [0016] 31.55 67.71\nCTA 8 _._ 5275 _[\u00b1]_ [0] _[.]_ [2355] 0.117 1.43\nContiFormer 0 _._ 066 _[\u00b1]_ [0] _[.]_ [0075] 15.15 67.71\n\n\nNAC-2k 7 _._ 3071 _[\u00b1]_ [0] _[.]_ [1547] 0.137 44.75\nNAC-32k 7 _._ 2313 _[\u00b1]_ [0] _[.]_ [219] 0.138 549.86\nNAC-PW 8 _._ 5649 _[\u00b1]_ [0] _[.]_ [0203] 0.117 5042.09\nNAC-FC 0 _._ 0195 ~~_[\u00b1]_~~ ~~[0]~~ ~~_[.]_~~ ~~[0002]~~ 51.28 29.92\nNAC-02s 7 _._ 252 _[\u00b1]_ [0] _[.]_ [2018] 0.138 151.54\nNAC-09s 7 _._ 222 _[\u00b1]_ [0] _[.]_ [176] 0.139 150.85\nNAC-Exact/05s/8k 7 _._ 4101 ~~_[\u00b1]_~~ ~~[0]~~ ~~_[.]_~~ ~~[1586]~~ 0.135 151.50\nNAC-Euler 7 _._ 3367 _[\u00b1]_ [0] _[.]_ [1719] 0.136 152.22\nNAC-Steady 7 _._ 2942 _[\u00b1]_ [0] _[.]_ [1451] 0.137 150.86\n\n\nfrom 151.54 MB to 150.85 MB. In constrast, decreasing\nthe Top- _K_ selection from _PW_ to _k_ = 2 drastically reduces\nmemory consumption from 5042 MB to 44.75 MB, demonstrating the flexibility of NAC.\n**Interpreting the Results:** From the experiments, we observe that increasing the sparsity of the NAC layer improves\nthe robustness of the system and leads to higher overall accuracy. Similarly, increasing the Top- _K_ interactions enhances\naccuracy too; however, the benefits diminish as memory\nconsumption grows. Using Exact mode, Top- _K_ =8 with 50%\nsparsity achieves the best balance between accuracy and\nefficiency. Steady mode is the fastest, while Euler mode\nhandles adaptive temporal dynamics.\n\n\n**4. Discussions**\n\n\n\nThis research is part of ongoing work on biologically plausible attention mechanisms and represents a pioneering step,\nwith limitations to be addressed in future work.\n\n**Architectural improvement:** Currently, NAC uses predetermined wiring (AutoNCP) requiring three inputs: number of units (sensory + interneuron + motor), output motor\nneurons, and sparsity, with typically 60% of units assigned\nto sensory neurons. To integrate with the attention mechanism while preserving wiring, sensory units for _NN_ sensory\nare set as unitssensory = \ufffd _d_ model0 _.\u2212_ 6 0 _._ 5 \ufffd and backbone units as\n\n\n\nare set as unitssensory = \ufffd _d_ model0 _.\u2212_ 6 0 _._ 5 \ufffd and backbone units as\n\nunits _backbone_ = _d_ model + \ufffd _d_ 0model _._ 6 \ufffd, where _\u2308\u00b7\u2309_ and _\u230a\u00b7\u230b_ denote\n\n\n\nunits _backbone_ = _d_ model + \ufffd _d_ 0model _._ 6 \ufffd, where _\u2308\u00b7\u2309_ and _\u230a\u00b7\u230b_ denote\n\nthe ceiling and floor functions, respectively. This results\nin a larger overall architectural size and increased runtime.\n\n\n\nFuture work will support user-defined NCPs configurations\nor randomized wiring to enable more efficient architectures.\n**Learnable sparse Top-** _**K**_ **selection:** Sparse Top- _K_ attention can miss important context, is sensitive to _k_, and may\nbe harder to optimize. A further limitation is that it still\ncomputes the full _QK_ _[\u22a4]_ matrix, which can dominate the\ncost for very long sequences. Future work includes adaptive\nor learnable Top- _K_ selection, improved key scoring, and\nhardware-aware optimization to strengthen accuracy and\nrobustness.\n\n\n**5. Conclusion**\n\n\nIn this paper, we introduce the Neuronal Attention Circuit\n(NAC), a biologically inspired attention mechanism that\nreformulates attention logits as the solution to a first-order\nODE modulated by nonlinear, interlinked gates derived from\nrepurposing _C.elegans_ nematode NCPs. NAC bridges discrete attention with continuous-time dynamics, enabling\nadaptive temporal processing without the limitations inherent to traditional scaled dot-product attention. Based on the\nsolution to ODE, we introduce three computational modes:\n(i) Euler, based on _explicit Euler_ integration; (ii) Exact, providing closed-form solutions; and (iii) Steady, approximating equilibrium states. In addition, a sparse Top- _K_ pairwise\nconcatenation scheme is introduced to mitigate the memory intensity. Theoretically, we establish NAC\u2019s log-state\nstability, exponential error bounds, and universal approximation, thereby providing rigorous guarantees of convergence\nand expressiveness. Empirical evaluations demonstrate that\nNAC achieves state-of-the-art performance across diverse\ntasks, including irregularly sampled time-series benchmarks,\nautonomous vehicle lane-keeping, and industrial prognostics. Moreover, NAC occupies an intermediate position\nbetween CT-RNNs and CT-Attention, offering robust temporal modeling while requiring less runtime than CT-RNNs\nand less memory than CT-Attention models.\n\n\n**Reproducibility Statement**\n\n\nThe code for reproducibility is available at\n[https://github.com/itxwaleedrazzaq/](https://github.com/itxwaleedrazzaq/neuronal_attention_circuit)\n\n[neuronal_attention_circuit](https://github.com/itxwaleedrazzaq/neuronal_attention_circuit)\n\n\n**Impact Statement**\n\n\nThe work addresses the growing field of continuous-time\nattention and pioneers a biologically plausible mechanism.\nIt encourages research into sparse, adaptive networks that\nresemble natural wiring. From a societal perspective, it\nsupports more robust AI in resource-limited settings, but it\nalso raises ethical concerns when applied to areas such as\nsurveillance or autonomous systems.\n\n\n\n8\n\n\n**Neuronal Attention Circuit (NAC) for Representation Learning**\n\n\n\n**References**\n\n\nIntroduction to self-driving cars. URL\n[https://www.udacity.com/course/](https://www.udacity.com/course/intro-to-self-driving-cars--nd113)\n[intro-to-self-driving-cars--nd113.](https://www.udacity.com/course/intro-to-self-driving-cars--nd113)\n\n\nAguiar-Conraria, L. and Soares, M. J. The continuous\nwavelet transform: Moving beyond uni-and bivariate analysis. _Journal of economic surveys_, 28(2):344\u2013375, 2014.\n\n\nBeltagy, I., Peters, M. E., and Cohan, A. Longformer: The long-document transformer. _arXiv preprint_\n_arXiv:2004.05150_, 2020.\n\n\nBojarski, M., Del Testa, D., Dworakowski, D., Firner, B.,\nFlepp, B., Goyal, P., Jackel, L. D., Monfort, M., Muller,\nU., Zhang, J., et al. End to end learning for self-driving\ncars. _arXiv preprint arXiv:1604.07316_, 2016.\n\n\nBrockman, G., Cheung, V., Pettersson, L., Schneider, J.,\nSchulman, J., Tang, J., and Zaremba, W. Openai gym.\n_arXiv preprint arXiv:1606.01540_, 2016.\n\n\nCao, Y., Li, S., Petzold, L., and Serban, R. Adjoint sensitivity analysis for differential-algebraic equations: The\nadjoint dae system and its numerical solution. _SIAM_\n_journal on scientific computing_, 24(3):1076\u20131089, 2003.\n\n\nChen, R. T., Rubanova, Y., Bettencourt, J., and Duvenaud,\nD. K. Neural ordinary differential equations. _Advances_\n_in neural information processing systems_, 31, 2018.\n\n\nChen, Y., Ren, K., Wang, Y., Fang, Y., Sun, W., and Li, D.\nContiformer: Continuous-time transformer for irregular\ntime series modeling. _Advances in Neural Information_\n_Processing Systems_, 36:47143\u201347175, 2023.\n\n\nChien, J.-T. and Chen, Y.-H. Continuous-time attention for\nsequential learning. In _Proceedings of the AAAI confer-_\n_ence on artificial intelligence_, volume 35, pp. 7116\u20137124,\n2021.\n\n\nCho, K., Van Merrienboer, B., Gulcehre, C., Bahdanau,\u00a8\nD., Bougares, F., Schwenk, H., and Bengio, Y. Learning phrase representations using rnn encoder-decoder\nfor statistical machine translation. _arXiv preprint_\n_arXiv:1406.1078_, 2014.\n\n\nd\u2019Ascoli, S., Becker, S., Mathis, A., Schwaller, P., and\nKilbertus, N. Odeformer: Symbolic regression of dynamical systems with transformers. _arXiv preprint_\n_arXiv:2310.05573_, 2023.\n\n\nDe Brouwer, E., Simm, J., Arany, A., and Moreau, Y.\nGru-ode-bayes: Continuous modeling of sporadicallyobserved time series. _Advances in neural information_\n_processing systems_, 32, 2019.\n\n\n\nDeng, L. The mnist database of handwritten digit images\nfor machine learning research [best of the web]. _IEEE_\n_signal processing magazine_, 29(6):141\u2013142, 2012.\n\n\nDing, Y., Jia, M., Miao, Q., and Huang, P. Remaining\nuseful life estimation using deep metric transfer learning\nfor kernel regression. _Reliability Engineering & System_\n_Safety_, 212:107583, 2021.\n\n\nHasani, R., Lechner, M., Amini, A., Rus, D., and Grosu,\nR. Liquid time-constant networks. In _Proceedings of the_\n_AAAI Conference on Artificial Intelligence_, volume 35,\npp. 7657\u20137666, 2021.\n\n\nHasani, R., Lechner, M., Amini, A., Liebenwein, L., Ray,\nA., Tschaikowski, M., Teschl, G., and Rus, D. Closed\nform continuous-time neural networks. _Nature Machine_\n\n_Intelligence_, 4(11):992\u20131003, 2022.\n\n\nHochreiter, S. The vanishing gradient problem during learning recurrent neural nets and problem solutions. _Interna-_\n_tional Journal of Uncertainty, Fuzziness and Knowledge-_\n_Based Systems_, 6(02):107\u2013116, 1998.\n\n\nHochreiter, S. and Schmidhuber, J. Long short-term memory.\n_Neural computation_, 9(8):1735\u20131780, 1997.\n\n\nHong, H. S. and Thuan, N. Hust bearing: a practical dataset\nfor ball bearing fault diagnosis. _Mendeley Data_, 3, 2023.\n\n\nJohn, F. On integration of parabolic equations by difference methods: I. linear and quasi-linear equations for the\ninfinite interval. _Communications on Pure and Applied_\n_Mathematics_, 5(2):155\u2013211, 1952.\n\n\nJordan, M. I. Serial order: A parallel distributed processing\napproach. In _Advances in psychology_, volume 121, pp.\n471\u2013495. Elsevier, 1997.\n\n\nLechner, M. and Hasani, R. Mixed-memory rnns for learning long-term dependencies in irregularly sampled time\nseries. 2022.\n\n\nLechner, M., Hasani, R. M., and Grosu, R. Neuronal circuit\npolicies. _arXiv preprint arXiv:1803.08554_, 2018.\n\n\nLechner, M., Hasani, R., Amini, A., Henzinger, T. A., Rus,\nD., and Grosu, R. Neural circuit policies enabling auditable autonomy. _Nature Machine Intelligence_, 2(10):\n642\u2013652, 2020.\n\n\nLeCun, Y., Touresky, D., Hinton, G., and Sejnowski, T. A\ntheoretical framework for back-propagation. In _Proceed-_\n_ings of the 1988 connectionist models summer school_,\nvolume 1, pp. 21\u201328, 1988.\n\n\nLin, J. and Qu, L. Feature extraction based on morlet\nwavelet and its application for mechanical fault diagnosis.\n_Journal of sound and vibration_, 234(1):135\u2013148, 2000.\n\n\n\n9\n\n\n**Neuronal Attention Circuit (NAC) for Representation Learning**\n\n\n\nNectoux, P., Gouriveau, R., Medjaher, K., Ramasso, E.,\nChebel-Morello, B., Zerhouni, N., and Varnier, C. Pronostia: An experimental platform for bearings accelerated\ndegradation tests. In _IEEE International Conference on_\n_Prognostics and Health Management, PHM\u201912._, pp. 1\u20138.\nIEEE Catalog Number: CPF12PHM-CDR, 2012.\n\n\nNeil, D., Pfeiffer, M., and Liu, S.-C. Phased lstm: Accelerating recurrent network training for long or event-based\nsequences. _Advances in neural information processing_\n_systems_, 29, 2016.\n\n\nNishijima, T. Universal approximation theorem for neural\nnetworks. _arXiv preprint arXiv:2102.10993_, 2021.\n\n\nPark, M., Kim, H., and Park, S. A convolutional neural\nnetwork-based end-to-end self-driving using lidar and\ncamera fusion: Analysis perspectives in a real-world environment. _Electronics_, 10(21):2608, 2021.\n\n\nRazzaq, W. and Hongwei, M. Neural circuit policies imposing visual perceptual autonomy. _Neural Processing_\n_Letters_, 55(7):9101\u20139116, 2023.\n\n\nRazzaq, W. and Zhao, Y.-B. Carle: a hybrid deep-shallow\nlearning framework for robust and explainable rul estimation of rolling element bearings. _Soft Computing_, 29(23):\n6269\u20136292, 2025a.\n\n\nRazzaq, W. and Zhao, Y.-B. Developing distance-aware uncertainty quantification methods in physics-guided neural\nnetworks for reliable bearing health prediction, 2025b.\n[URL https://arxiv.org/abs/2512.08499.](https://arxiv.org/abs/2512.08499)\n\n\nRoy, A., Saffar, M., Vaswani, A., and Grangier, D. Efficient\ncontent-based sparse attention with routing transformers. _Transactions of the Association for Computational_\n_Linguistics_, 9:53\u201368, 2021.\n\n\nRubanova, Y., Chen, R. T., and Duvenaud, D. K. Latent\nordinary differential equations for irregularly-sampled\ntime series. _Advances in neural information processing_\n_systems_, 32, 2019.\n\n\nRumelhart, D. E., Hinton, G. E., and Williams, R. J. Learning internal representations by error propagation. Technical report, 1985.\n\n\nShibuya, N. Car behavioral cloning, 2017. URL\n[https://github.com/naokishibuya/](https://github.com/naokishibuya/car-behavioral-cloning)\n[car-behavioral-cloning.](https://github.com/naokishibuya/car-behavioral-cloning) Accessed: 202510-05.\n\n\nShukla, S. N. and Marlin, B. M. Multi-time attention networks for irregularly sampled time series. _arXiv preprint_\n_arXiv:2101.10318_, 2021.\n\n\n\nStinchcomb, M. Multilayered feedforward networks are\nuniversal approximators. _Neural Networks_, 2:356\u2013359,\n1989.\n\n\nTay, Y., Bahri, D., Yang, L., Metzler, D., and Juan, D.-C.\nSparse sinkhorn attention. In _International conference on_\n_machine learning_, pp. 9438\u20139447. PMLR, 2020.\n\n\nThuan, N. D. and Hong, H. S. Hust bearing: a practical\ndataset for ball bearing fault diagnosis. _BMC research_\n_notes_, 16(1):138, 2023.\n\n\nTiang, Y., Gelernter, J. R., Wang, X., Chen, W., Gao, J.,\nZhang, Y., and Li, X. Lane marking detection via fast\nend-to-end deep convolutional neural network that is our\npatch proposal network (ppn). 2018.", "metadata": {"source": "neuronal_attention_circuits_raw_with_image_ids_with_captions.md"}, "chunk_index": 3, "token_size_config": 5000}
{"id": "neuronal_attention_circuits_raw_with_image_ids_with_captions_chunks_5k_4", "content": "Rubanova, Y., Chen, R. T., and Duvenaud, D. K. Latent\nordinary differential equations for irregularly-sampled\ntime series. _Advances in neural information processing_\n_systems_, 32, 2019.\n\n\nRumelhart, D. E., Hinton, G. E., and Williams, R. J. Learning internal representations by error propagation. Technical report, 1985.\n\n\nShibuya, N. Car behavioral cloning, 2017. URL\n[https://github.com/naokishibuya/](https://github.com/naokishibuya/car-behavioral-cloning)\n[car-behavioral-cloning.](https://github.com/naokishibuya/car-behavioral-cloning) Accessed: 202510-05.\n\n\nShukla, S. N. and Marlin, B. M. Multi-time attention networks for irregularly sampled time series. _arXiv preprint_\n_arXiv:2101.10318_, 2021.\n\n\n\nStinchcomb, M. Multilayered feedforward networks are\nuniversal approximators. _Neural Networks_, 2:356\u2013359,\n1989.\n\n\nTay, Y., Bahri, D., Yang, L., Metzler, D., and Juan, D.-C.\nSparse sinkhorn attention. In _International conference on_\n_machine learning_, pp. 9438\u20139447. PMLR, 2020.\n\n\nThuan, N. D. and Hong, H. S. Hust bearing: a practical\ndataset for ball bearing fault diagnosis. _BMC research_\n_notes_, 16(1):138, 2023.\n\n\nTiang, Y., Gelernter, J. R., Wang, X., Chen, W., Gao, J.,\nZhang, Y., and Li, X. Lane marking detection via fast\nend-to-end deep convolutional neural network that is our\npatch proposal network (ppn). 2018.\n\n\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. _Advances in neural information_\n_processing systems_, 30, 2017.\n\n\nVidulin, V., Lustrek, M., Kaluza, B., Piltaver, R.,\nand Krivec, J. Localization Data for Person Activity. UCI Machine Learning Repository, 2010. DOI:\nhttps://doi.org/10.24432/C57G8X.\n\n\nWang, B., Lei, Y., Li, N., et al. Xjtu-sy bearing datasets.\n_GitHub, GitHub Repository_, 2018.\n\n\nZaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Alberti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q.,\nYang, L., et al. Big bird: Transformers for longer sequences. _Advances in neural information processing_\n_systems_, 33:17283\u201317297, 2020.\n\n\nZhuang, J., Dvornek, N., Li, X., Tatikonda, S., Papademetris,\nX., and Duncan, J. Adaptive checkpoint adjoint method\nfor gradient estimation in neural ode. In _International_\n_Conference on Machine Learning_, pp. 11639\u201311649.\nPMLR, 2020.\n\n\nZhuang, J., Jia, M., Ding, Y., and Ding, P. Temporal\nconvolution-based transferable cross-domain adaptation\napproach for remaining useful life estimation under variable failure behaviors. _Reliability Engineering & System_\n_Safety_, 216:107946, 2021.\n\n\n\n10\n\n\n**Neuronal Attention Circuit (NAC) for Representation Learning**\n\n\n\n**Appendix**\n\n\n**A. Preliminaries**\n\n\n**A.1. Attention Mechanism**\n\n\nAttention mechanisms have become a cornerstone in\n\nmodern neural architectures, enabling models to dynamically focus on relevant parts of the input. The concept\nwas first introduced in the context of neural machine\ntranslation, where it allowed the decoder to weight encoder\noutputs according to their importance for generating each\ntarget token. Formally, given a query vector _q \u2208_ R _[d]_, key\nvectors _K_ = [ _k_ 1 _, k_ 2 _, . . ., kn_ ] _\u2208_ R _[n][\u00d7][d]_, and value vectors\n_V_ = [ _v_ 1 _, v_ 2 _, . . ., vn_ ] _\u2208_ R _[n][\u00d7][d]_, the attention mechanism can\nbe expressed in two steps:\n\n\n1. Compute the scaled dot attention logits:\n\n\n_ai_ = _[q][T][ k][i]_ (21)\n~~_\u221a_~~ _d_\n\n\n2. Normalize the logits to get attention weights and compute the output:\n\n\n_e_ _[a][i]_\n_\u03b1i_ = softmax( _ai_ ) = ~~_n_~~ (22)\n~~\ufffd~~ _j_ =1 _[e][a][j]_\n\n\n\nexhibit a resting potential at _\u2212_ 70 mV and an activation potential near _\u2212_ 20 mV. Similarly, each _Nm_ is composed of\ntwo subneurons, _Mp_ and _Mn_, and is driven by a controllable variable _y_, which also maps to a biologically plausible\nrange [ _\u2212_ 70 mV _, \u2212_ 20 mV]. The connections in the NCP\narchitecture are designed to reflect the biological sparsity\nand abstraction of neural circuits. Specifically, connections\nfrom _Ns_ to _Ni_ are feedforward, while those between _Nc_\nand _Nm_ are highly recurrent (Lechner et al., 2018). Figure\n1(a) illustrates the connectome of NCPs.\n\n\n**B. Proofs**\n\n\nIn this section, we provide all the proofs.\n\n\n**B.1. Deriving Closed-form (Exact) Solution**\n\n\nAlthough _\u03d5_ and _\u03c9\u03c4_ are nonlinear functions of the input\n**u** = [ **q** ; **k** ], we derive closed-form solution by treating them\nas locally constant over the pseudo-time integration interval\nfor each query\u2013key pair based on frozen-coefficient approximation (John, 1952). This is accurate whenever the interval\nis short or when input variations are slow compared with\nthe relaxation rate _\u03c9\u03c4_ . Under approximation assumption,\nrewrite Eqn. 1 as\n\n\n_dadtt_ [+] _[ \u03c9][\u03c4]_ _[a][t]_ [ =] _[ \u03d5.]_ (24)\n\n\nThis is now a linear first-order ODE. The integrating factor\nis\n\n\n\nAttention( _q, k, v_ ) =\n\n\n\n_n_\n\ufffd _\u03b1ivi_ (23)\n\n\n_i_ =1\n\n\n\nHere, _ai_ is the raw attention logit between the query and\neach key, and the scaling factor _\u221ad_ prevents large dot prod\nucts from destabilizing the softmax (Vaswani et al., 2017).\n\n\n**A.2. Neuronal Circuit Policies (NCPs)**\n\n\nNCPs represent a biologically inspired framework for developing interpretable neural control agents by adapting the\ntap-withdrawal circuit found in the nematode _C. elegans_\n(Lechner et al., 2018). Unlike traditional spiking neural\nnetworks, the majority of neurons in this circuit exhibit\nelectronic dynamics, characterized by the passive flow of\nelectrical charges, resulting in graded potentials. NCPs\nare structured as a four-layer hierarchical architecture comprising sensory neurons ( _Ns_ ), interneurons ( _Ni_ ), command\nneurons ( _Nc_ ), and motor neurons ( _Nm_ ). The _Ns_ perceive\nand respond to external stimulus inputs and are responsible for the initial signal transduction. Each _Ns_ consists\nof subneurons _Sp_ and _Sn_ and a system variable _x_ . The\nactivation of _Sp_ and _Sn_ depends upon the sign of _x_ : _Sp_\nbecomes activated for _x >_ 0, whereas _Sn_ becomes activated for _x <_ 0. The variable _x_ is mapped to the membrane\npotential range of [ _\u2212_ 70 mV _, \u2212_ 20 mV], which is consistent\nwith the biophysical behavior of nerve cells, which typically\n\n\n\n\ufffd _\u03c9\u03c4 dt_\n\n_\u00b5_ = _e_ \ufffd \ufffd\n\n\nMultiply both sides by _\u00b5_ ( _t_ ):\n\n\n\nSubstitute back:\n\n\n_e_ _[\u03c9][\u03c4][ t]_ _at \u2212_ _a_ 0 = _\u03d5 \u00b7_ _[e][\u03c9][\u03c4][ t][ \u2212]_ [1] _._ (30)\n\n_\u03c9\u03c4_\n\n\n\n= _e_ _[\u03c9][\u03c4][ t]_ _._ (25)\n\n\n\n\n_[da][t]_\n_e_ _[\u03c9][\u03c4][ t]_ _dt_ [+] _[ \u03c9][\u03c4]_ _[e][\u03c9][\u03c4][ t][a][t]_ [ =] _[ \u03d5e][\u03c9][\u03c4][ t][.]_ (26)\n\n\nRecognize the left-hand side as the derivative of _e_ _[\u03c9][\u03c4][ t]_ _at_ :\n\n\n_d_\n\ufffd _e_ _[\u03c9][\u03c4][ t]_ _at_ \ufffd = _\u03d5e_ _[\u03c9][\u03c4][ t]_ _._ (27)\n_dt_\n\n\nIntegrate from 0 to _t_ :\n\n\n_t_\n_e_ _[\u03c9][\u03c4][ t]_ _at \u2212_ _e_ [0] _a_ 0 = _\u03d5_ _e_ _[\u03c9][\u03c4][ s]_ _ds._ (28)\n\ufffd0\n\n\nCompute the integral (since _\u03c9\u03c4 \u0338_ = 0):\n\n\n\n\ufffd0 _t_\n\n\n\n_e_ _[\u03c9][\u03c4][ s]_ _ds_ = [1]\n0 _\u03c9\u03c4_\n\n\n\n_\u03c9\u03c4_\n\n\n\n\ufffd _e_ _[\u03c9][\u03c4][ t]_ _\u2212_ 1\ufffd _._ (29)\n\n\n\n11\n\n\nRearrange:\n\n\n\n**Neuronal Attention Circuit (NAC) for Representation Learning**\n\n\nCASE 2: MULTIPLE CONNECTIONS ( _M >_ 1).\n\n\n\n_e_ _[\u03c9][\u03c4][ t]_ _at_ = _a_ 0 + _[\u03d5]_\n\n_\u03c9\u03c4_\n\n\nDivide both sides by _e_ _[\u03c9][\u03c4][ t]_ :\n\n\n_at_ = _a_ 0 _e_ _[\u2212][\u03c9][\u03c4][ t]_ + _[\u03d5]_\n\n_\u03c9\u03c4_\n\n\n\n\ufffd _e_ _[\u03c9][\u03c4][ t]_ _\u2212_ 1\ufffd _._ (31)\n\n\n\ufffd1 _\u2212_ _e_ _[\u2212][\u03c9][\u03c4][ t]_ [\ufffd] _._ (32)\n\n\n\nThe ODE is\n\n\n\nwith per-connection equilibria _Aj_ = _\u03d5j/fj_ . The effective\nequilibrium is\n\n\n\n_da_\n\n_dt_ [=] _[ \u2212]_ \ufffd \ufffd _[M]_\n\n\n\n_da_\n\n\n\n\ufffd _fj_ \ufffd _a_ +\n\n_j_ =1\n\n\n\n_M_\n\ufffd _fjAj,_ (37)\n\n_j_ =1\n\n\n\nSet _a_ _[\u2217]_ := _[\u03d5]_ . Then _at_ = _a_ _[\u2217]_ + ( _a_ 0 _\u2212_ _a_ _[\u2217]_ ) _e_ _[\u2212][\u03c9][\u03c4][ t]_, proved.\n\n_\u03c9\u03c4_\n\n\n**B.2. Proof of Theorem 1**\n\n\nWe divide the proof into two parts: (i) the single-connection\ncase _M_ = 1, and (ii) the general multi-connection case\n_M >_ 1. The main technique is to evaluate the ODE at\nboundary values of the proposed invariant interval and show\nthat the derivative points inward, ensuring that trajectories\n\ncannot escape.\n\n\nCASE 1: SINGLE CONNECTION ( _M_ = 1).\n\n\nThe ODE reduces to\n\n\n\n_A_ =\n\n\n\n\ufffd _Mj_ =1 _[f][j][A][j]_\n~~\ufffd~~ _Mj_ =1 _[f][j]_ _._ (38)\n\n\n\nSince the weights ~~\ufffd~~ _fjfj_ are positive and sum to 1, _A_ is a\nconvex combination of _{Aj}_ . Therefore,\n\n\n_A \u2208_ [ _A_ [min] _, A_ [max] ] _._ (39)\n\n\n- _Upper bound:_ Let _M_ = max(0 _, A_ [max] ). Then\n\n\n\n_da_\n\n_dt_ \ufffd\ufffd\ufffd _a_ = _M_ [=]\n\n\n\n_M_\n\ufffd _fj_ ( _Aj \u2212_ _M_ ) _._ (40)\n\n_j_ =1\n\n\n\n_da_\n\n_A_ = _[\u03d5]_\n_dt_ [=] _[ \u2212][\u03c9][\u03c4]_ _[a]_ [ +] _[ \u03d5]_ [ =] _[ \u2212][\u03c9][\u03c4]_ [(] _[a][ \u2212]_ _[A]_ [)] _[,]_ _\u03c9_\n\n\n\n_da_\n\n\n\n_._ (33)\n_\u03c9\u03c4_\n\n\n\nSince _Aj \u2264_ _A_ [max] _\u2264_ _M_, each term ( _Aj_ _\u2212M_ ) _\u2264_ 0, and thus\n\ufffd _fj_ ( _Aj \u2212_ _M_ ) _\u2264_ 0. Hence _dadt_ _[\u2264]_ [0][, proving trajectories]\n\ncannot exceed _M_ .\n\n\n- _Lower bound:_ Let _m_ = min(0 _, A_ [min] ). Then\n\n\n\nHere _A_ is the unique equilibrium. We now check both\nbounds.\n\n\n- _Upper bound:_ Let _M_ = max(0 _, A_ ). At _a_ = _M_,\n\n\n\n_da_\n\n_dt_ \ufffd\ufffd\ufffd _a_ = _m_ [=]\n\n\n\n_M_\n\ufffd _fj_ ( _Aj \u2212_ _m_ ) _._ (41)\n\n_j_ =1\n\n\n\n_da_\n\n(34)\n\n_dt_ \ufffd\ufffd\ufffd _a_ = _M_ [=] _[ \u2212][\u03c9][\u03c4]_ [(] _[M][ \u2212]_ _[A]_ [)] _[.]_\n\n\n\nIf _A \u2265_ 0, then _M_ = _A_ and _[da]_\n\n\n\nIf _A \u2265_ 0, then _M_ = _A_ and _dt_ [= 0][. If] _[ A <]_ [ 0][, then] _[ M]_ [ = 0][,]\n\nso _[da]_ _dt_ [=] _[ \u2212][\u03c9][\u03c4]_ [(0] _[ \u2212]_ _[A]_ [) =] _[ \u03c9][\u03c4]_ _[A][ \u2264]_ [0][ since] _[ A <]_ [ 0][. In both]\n\ncases, _[da]_ _[\u2264]_ [0][. Thus trajectories cannot cross above] _[ M]_ [.]\n\n\n\n_dt_ _[\u2264]_ [0][. Thus trajectories cannot cross above] _[ M]_ [.]\n\n\n\n- _Lower bound:_ Let _m_ = min(0 _, A_ ). At _a_ = _m_,\n\n\n\n_da_\n\n(35)\n\n_dt_ \ufffd\ufffd\ufffd _a_ = _m_ [=] _[ \u2212][\u03c9][\u03c4]_ [(] _[m][ \u2212]_ _[A]_ [)] _[.]_\n\n\n\nSince _Aj \u2265_ _A_ [min] _\u2265_ _m_, each ( _Aj \u2212_ _m_ ) _\u2265_ 0, so [\ufffd] _fj_ ( _Aj \u2212_\n_m_ ) _\u2265_ 0. Hence _[da]_ _dt_ _[\u2265]_ [0][, proving trajectories cannot fall]\n\nbelow _m_ . Thus, the interval [ _m, M_ ] is forward-invariant.\n\n_Remark_ 1 _._ This result guarantees that the continuous-time\nattention state converges within a well-defined interval dictated by the per-connection equilibria. In particular, for the\nsingle-connection case ( _M_ = 1), the state trajectory converges monotonically toward the closed-form equilibrium\nsolution (Eqn. 18) without overshoot.\n\n\n**B.3. Proof for Theorem 2**\n\n\nThe proof proceeds constructively by showing that the NAC\nlayer can emulate a single-hidden-layer feedforward neural\nnetwork with nonlinear activations, which is a universal\napproximator under the Universal Approximation Theorem\n(UAT). We assume self-attention on a single-token input\n_x \u2208_ R _[n]_ (setting sequence length _T_ = 1) and focus on\nthe steady mode for simplicity. Without loss of generality,\nset _d_ model = _n_ + _m_ or adjust as needed for dimensionality.\nConstructively, set NCP sparsity _s_ = 0 for full connectivity, ensuring the backbone\u02dc _NN_ backbone approximates any\n_\u03d5_ : R [2] _[d]_ _\u2192_ [0 _,_ 1] with error _< \u03b4_ via stacked layers. For\n\n\n\nIf _A \u2264_ 0, then _m_ = _A_ and _[da]_ _dt_ [= 0][. If] _[ A >]_ [ 0][, then]\n\n_m_ = 0, so _[da]_ _dt_ [=] _[ \u2212][\u03c9][\u03c4]_ [(0] _[ \u2212]_ _[A]_ [) =] _[ \u03c9][\u03c4]_ _[A][ \u2265]_ [0][. In both cases,]\n\n_dadt_ _[\u2265]_ [0][. Thus trajectories cannot cross below] _[ m]_ [. Therefore,]\n\nthe interval [ _m, M_ ] is forward-invariant.\n\n\nTo see this explicitly under Euler discretization with step\nsize \u2206 _t >_ 0,\n\n\n_a_ ( _t_ + \u2206 _t_ ) = _at_ + \u2206 _t \u00b7_ _[da]_ (36)\n\n_dt_ _[.]_\n\n\n\nAt _a_ = _M_, _[da]_ _dt_ _[\u2264]_ [0 =] _[\u21d2]_ _[a]_ [(] _[t]_ [ + \u2206] _[t]_ [)] _[ \u2264]_ _[M]_ [. At] _[ a]_ [ =] _[ m]_ [,]\n\n_dadt_ _[\u2265]_ [0 =] _[\u21d2]_ _[a]_ [(] _[t]_ [ + \u2206] _[t]_ [)] _[ \u2265]_ _[m]_ [. By induction over steps,]\n\n_at \u2208_ [ _m, M_ ] for all _t \u2208_ [0 _, T_ ].\n\n\n\n12\n\n\n**Neuronal Attention Circuit (NAC) for Representation Learning**\n\n\n\nmulti-head, scale _H_ proportionally to target complexity,\nwith output projection _Wo_ aggregating as in classical UAT\nproofs (Stinchcomb, 1989).\n**Input Projections:** The input _x_ is projected via NCPbased sensory projections to obtain query _q_ = _q_ proj( _x_ ), key\n_k_ = _k_ proj( _x_ ), and value _v_ = _v_ proj( _x_ ), each in R _[d]_ [model] . For\nemulation, set _q_ proj = _k_ proj = _In_ (identity on R _[n]_ ) and adjust", "metadata": {"source": "neuronal_attention_circuits_raw_with_image_ids_with_captions.md"}, "chunk_index": 4, "token_size_config": 5000}
{"id": "neuronal_attention_circuits_raw_with_image_ids_with_captions_chunks_5k_5", "content": "_dadt_ _[\u2265]_ [0][. Thus trajectories cannot cross below] _[ m]_ [. Therefore,]\n\nthe interval [ _m, M_ ] is forward-invariant.\n\n\nTo see this explicitly under Euler discretization with step\nsize \u2206 _t >_ 0,\n\n\n_a_ ( _t_ + \u2206 _t_ ) = _at_ + \u2206 _t \u00b7_ _[da]_ (36)\n\n_dt_ _[.]_\n\n\n\nAt _a_ = _M_, _[da]_ _dt_ _[\u2264]_ [0 =] _[\u21d2]_ _[a]_ [(] _[t]_ [ + \u2206] _[t]_ [)] _[ \u2264]_ _[M]_ [. At] _[ a]_ [ =] _[ m]_ [,]\n\n_dadt_ _[\u2265]_ [0 =] _[\u21d2]_ _[a]_ [(] _[t]_ [ + \u2206] _[t]_ [)] _[ \u2265]_ _[m]_ [. By induction over steps,]\n\n_at \u2208_ [ _m, M_ ] for all _t \u2208_ [0 _, T_ ].\n\n\n\n12\n\n\n**Neuronal Attention Circuit (NAC) for Representation Learning**\n\n\n\nmulti-head, scale _H_ proportionally to target complexity,\nwith output projection _Wo_ aggregating as in classical UAT\nproofs (Stinchcomb, 1989).\n**Input Projections:** The input _x_ is projected via NCPbased sensory projections to obtain query _q_ = _q_ proj( _x_ ), key\n_k_ = _k_ proj( _x_ ), and value _v_ = _v_ proj( _x_ ), each in R _[d]_ [model] . For\nemulation, set _q_ proj = _k_ proj = _In_ (identity on R _[n]_ ) and adjust\n\n\n**Head Splitting and Sparse Top-** _**k**_ **Pairwise Computation:**\nSplit into _H_ heads, yielding _q_ [(] _[h]_ [)] _, k_ [(] _[h]_ [)] _\u2208_ R _[d]_ per head _h_,\nwhere _d_ = _d_ model _/H_ . For _T_ = 1, compute sparse top- _k_\npairs, but since _T_ = 1, _K_ eff = 1, yielding concatenated\npair _u_ [(] _[h]_ [)] = [ _q_ [(] _[h]_ [)] ; _k_ [(] _[h]_ [)] ] _\u2208_ R [2] _[d]_ . Since _q_ [(] _[h]_ [)] = _k_ [(] _[h]_ [)], this is\n\n[ _x_ [(] _[h]_ [)] ; _x_ [(] _[h]_ [)] ], but the NCP processes it generally.\n**Computation of** _\u03d5_ [(] _[h]_ [)] **and** _\u03c9\u03c4_ [(] _[h]_ [)] **:** The scalar _\u03d5_ [(] _[h]_ [)] is computed via the NCP-based inter-to-motor projection on the\npair:\n_\u03d5_ [(] _[h]_ [)] = _\u03c3_ ( _NN_ backbone( _u_ [(] _[h]_ [)] )) (42)\n\n\nwhere _\u03c3_ ( _z_ ) = (1 + _e_ _[\u2212][z]_ ) _[\u2212]_ [1] is the sigmoid. This NCP, with\nsufficiently large units and low sparsity, approximates any\ncontinuous scalar function _\u03d5_ [\u02dc] : R [2] _[d]_ _\u2192_ [0 _,_ 1] to arbitrary precision on compact sets (by the UAT for multi-layer networks\n(Stinchcomb, 1989)). Similarly, _\u03c9\u03c4_ [(] _[h]_ [)] is computed via:\n\n\n_\u03c9\u03c4_ [(] _[h]_ [)] = softplus( _NN_ backbone( _u_ [(] _[h]_ [)] )) + _\u03b5,_ _\u03b5 >_ 0 (43)\n\n\nBy setting weights to make _\u03c9\u03c4_ [(] _[h]_ [)] _\u2261_ 1 (constant), the steadymode logit simplifies to _a_ [(] _[h]_ [)] = _\u03d5_ [(] _[h]_ [)] _/\u03c9\u03c4_ [(] _[h]_ [)] = _\u03d5_ [(] _[h]_ [)] . Thus,\n_a_ [(] _[h]_ [)] _\u2248_ _\u03c3_ \ufffd _w_ [(] _[h]_ [)] _x_ + _b_ [(] _[h]_ [)][\ufffd] for chosen weights _w_ [(] _[h]_ [)] _, b_ [(] _[h]_ [)], emulating a sigmoid hidden unit.\n**Attention Weights and output:** For _T_ = 1, the softmax\nover one \u201ckey\u201d yields _\u03b1_ [(] _[h]_ [)] = exp( _a_ [(] _[h]_ [)] ) _/_ exp( _a_ [(] _[h]_ [)] ) = 1.\nThe head output is _y_ [(] _[h]_ [)] = \ufffd _T_ _[\u03b1]_ [(] _[h]_ [)] _[v]_ [(] _[h]_ [)] _[dt]_ [. Set] _[ v]_ [proj][ such that]\n\n_v_ [(] _[h]_ [)] = 1 (scalar), yielding _y_ [(] _[h]_ [)] _\u2248_ _\u03c3_ \ufffd _w_ [(] _[h]_ [)] _x_ + _b_ [(] _[h]_ [)][\ufffd] . For\nvector-valued _v_ [(] _[h]_ [)], more complex combinations are possible, but scalars suffice here.\n**Output Projection:** Concatenate head outputs: _Y_ =\n\n[ _y_ [(1)] ; _y_ [(2)] ; _. . ._ ; _y_ [(] _[H]_ [)] ] _\u2208_ R _[H]_ . Apply the final dense layer:\n\n\n_g_ ( _x_ ) = ( _Y \u00b7 Wo_ ) + _bo \u2208_ R _[m]_ _._ (44)\n\n\nWith _y_ [(] _[h]_ [)] _\u2248_ _\u03c3_ \ufffd _w_ [(] _[h]_ [)] _x_ + _b_ [(] _[h]_ [)][\ufffd], this matches a single-hiddenlayer network with _H_ units. By the UAT, for large _H_, such\nnetworks approximate any continuous _f_ on compact _K_ to\naccuracy _\u03f5_, by choosing appropriate _w_ [(] _[h]_ [)] _, b_ [(] _[h]_ [)] _, Wo, bo_ .\n\n\n**C. Training, Gradients and Complexity**\n\n\n**C.1. Gradient Characterization**\n\n\nWe analyze the sensitivity of the dynamics with respect to\nthe underlying learnable parameters. Specifically, we compute closed-form derivatives of both the steady state and the\nfull trajectory _at_ with respect to the parameters _\u03d5_ and _\u03c9\u03c4_ .\nThese expressions illuminate how gradients flow through\n\n\n\nthe system, and provide guidance for selecting parameterizations that avoid vanishing or exploding gradients.\n\n\nC.1.1. TRAJECTORY SENSITIVITIES FOR CLOSED-FORM\n\n\nFORMULATION\n\n\nThe trajectory is given by\n\n\n_at_ = _a_ _[\u2217]_ + ( _a_ 0 _\u2212_ _a_ _[\u2217]_ ) _e_ _[\u2212][\u03c9][\u03c4][ t]_ _,_ (45)\n\n\nwhich depends on ( _\u03d5, \u03c9\u03c4_ ) both through the equilibrium _a_ _[\u2217]_\n\nand the exponential term.\n**Derivative with respect to** _\u03d5_ **:** We obtain\n\n\n\n_Interpretation_ : The gradient with respect to _\u03c9\u03c4_ contains\na transient term proportional to _te_ _[\u2212][\u03c9][\u03c4][ t]_, which dominates\nat intermediate times, and a steady-state contribution proportional to _\u2212\u03d5/\u03c9\u03c4_ [2][, which persists asymptotically. Thus,]\nsensitivity to _\u03c9\u03c4_ is time-dependent, peaking before vanishing exponentially in the transient component.\n\n\n**C.2. Gradient-Based Training**\n\n\nLike Neural ODEs (Chen et al., 2018) and CT-RNNs\n(Rubanova et al., 2019), NAC produces differentiable computational graphs and can be trained using gradient-based\noptimization, such as the adjoint sensitivity method (Cao\net al., 2003) or backpropagation through time (BPTT) (LeCun et al., 1988). In this work, we use BPTT exclusively,\nas the adjoint sensitivity method can introduce numerical\nerrors (Zhuang et al., 2020).\n\n\n**C.3. Efficiency and Complexity**\n\n\nTable 3 summarizes the computational complexity of different sequence models. For sequence prediction over length\n_n_ with hidden dimension _k_, RNNs scale linearly, _O_ ( _nk_ ),\nwhile Attention and NAC scale quadratically, _O_ ( _n_ [2] _k_ ).\nODE-based models, such as LNNs, incur an additional\nmultiplicative factor _S_ for the number of solver steps.\nFor single-time-step prediction, RNNs and LSTMs require\n_O_ ( _k_ ), whereas Attention and NAC require _O_ ( _nk_ ) when\nrecomputing attention over the full sequence.\n\n\n\n_\u2202at_\n\n_\u2202\u03d5_ [= 1] _[ \u2212]_ _\u03c9_ _[e]_ _\u03c4_ _[\u2212][\u03c9][\u03c4][ t]_\n\n\n\n_\u2202at_\n\n\n\n(46)\n_\u03c9\u03c4_\n\n\n\n_Interpretation_ : For large _\u03c9\u03c4_, the gradient with respect to _\u03d5_\nsaturates quickly but shrinks to scale _O_ (1 _/\u03c9\u03c4_ ), potentially\nslowing learning of _\u03d5_ . Conversely, very small _\u03c9\u03c4_ leads\nto large steady-state gradients, which may destabilize optimization.\n\n**Derivative with respect to** _\u03c9\u03c4_ **:** Here, both the equilibrium\nand the decay rate depend on _\u03c9\u03c4_, yielding\n\n\n\n_\u2202at_\n= _\u2212_ _[\u03d5]_\n_\u2202\u03c9\u03c4_ _\u03c9\u03c4_ [2]\n\n\n\n\ufffd1 _\u2212_ _e_ _[\u2212][\u03c9][\u03c4][ t]_ [\ufffd] _\u2212_ ( _a_ 0 _\u2212_ _a_ _[\u2217]_ ) _t e_ _[\u2212][\u03c9][\u03c4][ t]_ _._ (47)\n\n\n\n13\n\n\n**Neuronal Attention Circuit (NAC) for Representation Learning**\n\n\n\n_Table 3._ Sequence and time-step prediction complexity. _n_ is the\nsequence length and _k_ is the hidden/model dimension.\n\n\n**Model** **Sequence** **Time-step**\nRNN _O_ ( _nk_ ) _O_ ( _k_ )\nAttention _O_ ( _n_ [2] _k_ ) _O_ ( _nk_ )\nLNN (ODEsolve) _O_ ( _nk \u00b7 S_ ) _O_ ( _k \u00b7 S_ )\nNAC-Exact _O_ ( _n_ [2] _k_ ) _O_ ( _nk_ )\nNAC-Euler _O_ ( _n_ [2] _k_ ) _O_ ( _nk_ )\n\n\n**D. Evaluation**\n\n\n**D.1. Related Works**\n\n\nThe brief description for related works is divided into four\nsubcategories.\n**DT-RNNs:** RNN (Rumelhart et al., 1985) captures sequential dependencies in time-series data by updating a hidden state from the current observation and the previous\nstate. LSTM (Hochreiter & Schmidhuber, 1997) extends\nRNNs with input, output, and forget gates, allowing the\nnetwork to maintain and update long-term memory, which\nimproves modeling of long-term dependencies in time-series\nsequences. GRU (Cho et al., 2014) simplifies the LSTM\narchitecture by combining the forget and input gates into a\nsingle update gate, allowing efficient modeling of long-term\ndependencies in time-series sequences.\n**CT-RNNs:** CT-RNN (Rubanova et al., 2019) model temporal dynamics using differential equations, enabling hidden\nstates to evolve continuously over time in response to inputs, which is particularly useful for irregularly sampled\ntime-series data. PhasedLSTM (Neil et al., 2016) introduces a time gate that updates hidden states according to\na rhythmic schedule, enabling efficient modeling of asynchronous or irregularly sampled time-series. GRU-ODE\n(De Brouwer et al., 2019) extends the GRU to continuous\ntime, evolving hidden states via ODEs to handle sequences\nwith non-uniform time intervals. mmRNN (Lechner &\nHasani, 2022) combines short-term and long-term memory units to capture both fast-changing and slowly evolving\npatterns in sequential data. LTC (Hasani et al., 2021) use\nneurons with learnable, input-dependent time constants to\nadapt the speed of dynamics and capture complex temporal\npatterns in continuous-time data. CfC (Hasani et al., 2022)\napproximate LTC dynamics analytically, providing efficient\ncontinuous-time modeling without relying on numerical\nODE solvers.\n\n**DT-Attentions:** Attention (Vaswani et al., 2017) computes\nattention weights by measuring similarity between queries\nand keys, scaling the results, and applying softmax to weigh\ntime-step contributions. Multi-Head Attention (Vaswani\net al., 2017) applies multiple parallel scaled dot-product\nattention mechanisms, capturing different types of temporal\ndependencies simultaneously for complex time-series modeling.\n\n\n\n**CT-Attentions:** mTAN (Shukla & Marlin, 2021) learns\ncontinuous-time embeddings and uses time-based attention\nto interpolate irregular observations into a fixed-length representation for downstream encoder-decoder modeling. CTA\n(Chien & Chen, 2021) generalizes discrete-time attention\nto continuous-time by representing hidden states, context\nvectors, and attention scores as functions whose dynamics are modeled via neural networks and integrated using\nODE solvers for irregular sequences. ODEFormer(d\u2019Ascoli\net al., 2023) trains a sequence-to-sequence transformer on\nsynthetic trajectories to directly output a symbolic ODE\nsystem from noisy, irregular time-series data. ContiFormer\n(Chen et al., 2023) builds a continuous-time Transformer by\npairing ODE-defined latent trajectories with a time-aware\nattention mechanism to model dynamic relationships in irregular time-series data.\n\n\n**D.2. Ablations Details**\n\n\nThe brief descriptions of variants and ablation are also divided into four subcategories:\n**Top-** _**K**_ **Ablations:** _NAC-2k_ uses Top- _K_ =2 to compute the\nlogits and _NAC-32k_ uses Top- _K_ =32. All variants use the\nexact computation mode with 50% sparsity.\n**Sparsity Ablations:** _NAC-02s_ uses 20% sparsity to compute the logits and _NAC-09s_ uses 90%. _NAC-PW_ employs\nfull pairwise (non-sparse) concatenation for input curation.\n_NAC-FC_ replaces the sparse NCP gating mechanism with\na simple fully connected layer. All variants use the exact\ncomputation mode with Top- _K_ =8.\n**Modes variants:** _NAC-Euler_ computes attention logits using the explicit Euler integration method. _NAC-Steady_ derives attention logits from the steady-state solution of the\nexact formulation. _NAC-Exact/05s/8k_ computes attention\nlogits using the closed-form exact solution. It also overlaps\nwith other ablations, so we combined it into a single one.\nAll modes use Top- _K_ =8, 50% sparsity and _\u03b4t_ =1.0. The\nsensitivity of NAC to _\u03b4t_ is visualized in Figure 4\n\n\n_Figure 4._ Effect of _\u03b4t_ on output of NAC.\n\n\n\n14\n\n\n\n\n```json\n\"img_neuronal_attention_circuits_13_0\": {\n    \"path\": \"E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/neuronal_attention_circuits.pdf-13-0.png\",\n    \"page\": 13,\n    \"section\": \"References\",\n    \"image_relevance\": \"high\",\n    \"image_type\": \"plot\",\n    \"semantic_role\": \"illustrates\",\n    \"caption\": \"A line plot illustrating the sensitivity of Accuracy to the parameter \u03b4t. The x-axis represents \u03b4t, ranging from approximately 0.1 to 2.0, while the y-axis denotes Accuracy values, which fluctuate between roughly 0.82 and 0.87 across the observed range of \u03b4t.\",\n    \"depicted_concepts\": [\n      \"Sensitivity analysis\",\n      \"Accuracy\",\n      \"\u03b4t (delta t)\",\n      \"Line plot\"\n    ],\n    \"confidence\": \"high\"\n}\n```\n**Neuronal Attention Circuit (NAC) for Representation Learning**\n\n\n\n**D.3. Experimental Details**\n\n\nD.3.1. EVENT-BASED MNIST\n\n\n**Dataset Explanation and Curation:** The MNIST dataset,\nintroduced by (Deng, 2012), is a widely used benchmark for\ncomputer vision and image classification tasks. It consists\nof 70,000 grayscale images of handwritten digits (0\u20139), each\nof size 28 _\u00d7_ 28 pixels, split into 60,000 training and 10,000\ntesting samples.\n**Preprocessing:** We follow the preprocessing pipeline described in (Lechner & Hasani, 2022), which proceeds as\nfollows. First, a threshold is applied to convert the 8-bit\npixel values into binary values, with 128 as the threshold\non a scale from 0 (minimum intensity) to 255 (maximum\nintensity). Second, each 28 _\u00d7_ 28 image is reshaped into a\none-dimensional time series of length 784. Third, the binary\ntime series is encoded in an event-based format, eliminating\nconsecutive occurrences of the same value; for example, the\nsequence [1 _,_ 1 _,_ 1 _,_ 1] is transformed into (1 _, t_ = 4). This encoding introduces a temporal dimension and compresses the\nsequences from 784 to an average of 53 time steps. Finally,\nto facilitate efficient batching and training, each sequence\nis padded to a fixed length of 256, and the time dimension\nis normalized such that each symbol corresponds to one\nunit of time. The resulting dataset defines a per-sequence\nclassification problem on irregularly sampled time series.\n**Neural Network Architecture:** We develop an end-to-end\nhybrid neural network by combining compact convolutional\nlayers with NAC or counterparts baselines for fair comparison. Detailed hyperparameters and architectural specifications are provided in Table 4.\n\n\nD.3.2. PERSON ACTIVITY RECOGNITION (PAR)\n\n\n**Dataset Explanation and Curation:** We used the\nLocalized Person Activity Recognition dataset provided by\nUC Irvine (Vidulin et al., 2010). The dataset comprises\n25 recordings of human participants performing different\nphysical activities. The eleven possible activities are\n\u201cwalking,\u201d \u201cfalling,\u201d \u201clying down,\u201d \u201clying,\u201d \u201csitting down,\u201d\n\u201csitting,\u201d \u201cstanding up from lying,\u201d \u201con all fours,\u201d \u201csitting\non the ground,\u201d \u201cstanding up from sitting,\u201d and \u201cstanding\nup from sitting on the ground.\u201d The objective of this\nexperiment is to recognize the participant\u2019s activity from\ninertial sensors, formulating the task as a per-time-step\nclassification problem. The input data consist of sensor\nreadings from four inertial measurement units placed on\nparticipants\u2019 arms and feet. While the sensors are sampled\nat a fixed interval of 211 ms, recordings exhibit different\nphase shifts and are thus treated as irregularly sampled time\nseries.\n\n**Preprocessing:** We first separated each participant\u2019s\nrecordings based on sequence identity and calculated\nelapsed time in seconds using the sampling period. To", "metadata": {"source": "neuronal_attention_circuits_raw_with_image_ids_with_captions.md"}, "chunk_index": 5, "token_size_config": 5000}
{"id": "neuronal_attention_circuits_raw_with_image_ids_with_captions_chunks_5k_6", "content": "D.3.2. PERSON ACTIVITY RECOGNITION (PAR)\n\n\n**Dataset Explanation and Curation:** We used the\nLocalized Person Activity Recognition dataset provided by\nUC Irvine (Vidulin et al., 2010). The dataset comprises\n25 recordings of human participants performing different\nphysical activities. The eleven possible activities are\n\u201cwalking,\u201d \u201cfalling,\u201d \u201clying down,\u201d \u201clying,\u201d \u201csitting down,\u201d\n\u201csitting,\u201d \u201cstanding up from lying,\u201d \u201con all fours,\u201d \u201csitting\non the ground,\u201d \u201cstanding up from sitting,\u201d and \u201cstanding\nup from sitting on the ground.\u201d The objective of this\nexperiment is to recognize the participant\u2019s activity from\ninertial sensors, formulating the task as a per-time-step\nclassification problem. The input data consist of sensor\nreadings from four inertial measurement units placed on\nparticipants\u2019 arms and feet. While the sensors are sampled\nat a fixed interval of 211 ms, recordings exhibit different\nphase shifts and are thus treated as irregularly sampled time\nseries.\n\n**Preprocessing:** We first separated each participant\u2019s\nrecordings based on sequence identity and calculated\nelapsed time in seconds using the sampling period. To\n\n\n\nmitigate class imbalance, we removed excess samples from\noverrepresented classes to match the size of the smallest\nclass. Subsequently, the data were normalized using a\nstandard scaler. Finally, the dataset was split into a 90:10\nratio for training and testing.\n**Neural Network Architecture:** Following the approach in\nSection D.3.1, we developed an end-to-end hybrid neural\nnetwork combining convolutional heads with NAC or other\nbaselines. Hyperparameter details are summarized in Table\n4.\n\n\nD.3.3. AUTONOMOUS VEHICLE\n\n\n**Dataset Explanation and Curation:** We followed the data\ncollection methodology described in (Razzaq & Hongwei,\n2023). For OpenAI-CarRacing, a PPO-trained agent (5M\ntimesteps) was used to record 20 episodes, yielding approximately 48,174 RGB images of size 92 _\u00d7_ 92 _\u00d7_ 3 with\ncorresponding action labels across five discrete actions (noact, move left, forward, move right, stop). The dataset was\nsplit with 10% reserved for testing and the remaining 90%\nfor training. For the Udacity simulator, we manually controlled the vehicle for 50 minutes, producing 15647 RGB\nimages of size 320 _\u00d7_ 160 _\u00d7_ 3, captured from three camera\nstreams (left, center, right) along with their corresponding\ncontinuous steering values. This dataset was split into 20%\ntesting and 80% training.\n**Preprocessing:** No preprocessing was applied to the\nOpenAI-CarRacing dataset. For the Udacity simulator, we\nfollowed the preprocessing steps in (Shibuya, 2017). Each\nimage was first cropped to remove irrelevant regions and\nresized to 66 _\u00d7_ 120 _\u00d7_ 3. Images were then converted from\nRGB to YUV color space to match the network input. To improve robustness, data augmentation techniques, including\nrandom flips, translations, shadow overlays, and brightness\nvariations, were applied to simulate lateral shifts and diverse\nlighting conditions.\n**Neural Network Architecture:** For OpenAI-CarRacing,\nwe modified the neural network architecture proposed in\n(Razzaq & Hongwei, 2023), which combines compact CNN\nlayers for spatial feature extraction with LNNs to capture\ntemporal dynamics. In our implementation, the LNN layers\nwere replaced with NAC and its comparable alternatives\nfor fair evaluation. Full hyperparameter configurations are\nprovided in Table 4. For the Udacity simulator, we modified\nthe network proposed in (Bojarski et al., 2016) by replacing\nthree latent MLP layers with NAC and its counterparts. Full\nhyperparameters for this configuration are summarized in\nTable 4.\n\n**Saliency Maps:** A saliency map visualizes the regions of\nthe input that a model attends to when making decisions.\nFigure 5 shows the saliency maps for the OpenAI CarRacing environment. We observe that only NAC (Steady, Euler,\n\n\n\n15\n\n\n**Neuronal Attention Circuit (NAC) for Representation Learning**\n\n\n_Figure 5._ Saliency maps for OpenAI CarRacing\n\n\n_Figure 6._ Saliency maps for Udacity Simulator\n\n\n\n\n```json\n\"img_neuronal_attention_circuits_15_0\": {\n    \"path\": \"E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/neuronal_attention_circuits.pdf-15-0.png\",\n    \"page\": 15,\n    \"section\": \"References\",\n    \"image_relevance\": \"high\",\n    \"image_type\": \"attention_map\",\n    \"semantic_role\": \"illustrates\",\n    \"caption\": \"The image displays saliency maps generated for various recurrent neural network architectures applied to the OpenAI CarRacing environment. It includes traditional models like RNN, LSTM, and GRU, alongside continuous-time variants such as CT-RNN, PhasedLSTM, GRU-ODE, ODEFormer, ContiFormer, and NAC models. The top-left panel shows the input frame, while the subsequent panels visualize the regions of the input image that are most salient for each specified model.\",\n    \"depicted_concepts\": [\n      \"Saliency map\",\n      \"OpenAI CarRacing\",\n      \"Recurrent Neural Network\",\n      \"RNN\",\n      \"LSTM\",\n      \"GRU\",\n      \"Continuous-time Neural Network\",\n      \"CT-RNN\",\n      \"PhasedLSTM\",\n      \"GRU-ODE\",\n      \"mmRNN\",\n      \"LTC\",\n      \"CfC\",\n      \"Attention mechanism\",\n      \"MHA (Multi-Head Attention)\",\n      \"mTAN\",\n      \"CTA\",\n      \"ODEFormer\",\n      \"ContiFormer\",\n      \"Neural Accumulator (NAC)\",\n      \"NAC-Exact\",\n      \"NAC-Euler\",\n      \"NAC-Steady\"\n    ],\n    \"confidence\": \"high\"\n}\n```\n\n\n```json\n\"img_neuronal_attention_circuits_15_1\": {\n    \"path\": \"E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/neuronal_attention_circuits.pdf-15-1.png\",\n    \"page\": 15,\n    \"section\": \"References\",\n    \"image_relevance\": \"high\",\n    \"image_type\": \"attention_map\",\n    \"semantic_role\": \"illustrates\",\n    \"caption\": \"A grid of saliency maps visualizes the regions of an input image from the Udacity Simulator that are most attended to by 16 different recurrent neural network architectures. Each map corresponds to a specific model, including RNN, LSTM, GRU, CT-RNN, PhasedLSTM, GRU-ODE, mmRNN, LTC, CfC, Attention, MHA, mTAN, CTA, ODEFormer, ContiFormer, NAC-Exact, NAC-Euler, and NAC-Steady models, showing varying patterns of attention across the road scene.\",\n    \"depicted_concepts\": [\n      \"Saliency map\",\n      \"Udacity Simulator\",\n      \"Recurrent Neural Network\",\n      \"RNN\",\n      \"LSTM\",\n      \"GRU\",\n      \"CT-RNN\",\n      \"PhasedLSTM\",\n      \"GRU-ODE\",\n      \"mmRNN\",\n      \"LTC\",\n      \"CfC\",\n      \"Attention mechanism\",\n      \"MHA\",\n      \"mTAN\",\n      \"CTA\",\n      \"ODEFormer\",\n      \"ContiFormer\",\n      \"NAC-Exact\",\n      \"NAC-Euler\",\n      \"NAC-Steady\"\n    ],\n    \"confidence\": \"high\"\n}\n```\n\nand Steady) maintains focus on the road\u2019s horizon, while\nother models either focus on the sides or remain largely\nunresponsive to the task. Figure 6 also presents the saliency\nmaps for the Udacity Simulator. In this case, NAC-Exact,\nCTA produces the most accurate visual maps, maintaining\nattention on the road\u2019s horizon, followed by ContiFormer\nand mTAN, which achieve comparable performance. Attention and PhasedLSTM also generate reasonable saliency\nmaps, although their focus is more dispersed across the\nscene. In contrast, other models either fail to identify relevant regions, producing blurry maps, or focus solely on\none side of the road. These results demonstrate the NAC\u2019s\n\nability to understand the underlying task.\n\n\nD.3.4. INDUSTRY 4.0\n\n\n**Dataset Explanation and Curation:** _PRONOSTIA dataset_\nis a widely recognized benchmark dataset in the field of\ncondition monitoring and degradation estimation of rollingelement bearings. Nectiux et al.(Nectoux et al., 2012) developed this dataset as part of the PRONOSTIA experimental\nplatform. The dataset comprises 16 complete run-to-failure\nexperiments performed under accelerated wear conditions\n\n\n\n**Algorithm 3** Time\u2013frequency Representation Algorithm\n\n\n**Require:** windowed signal _Iw_, critical frequency _fc_, operating\nfrequency _fo_, sampling period _Tsampling_, windowed physical\nconstraints ( _tw, Tw_ )\n_amin_ = _fmax\u00b7Tfsamplingc_ _[,]_ _amax_ = _f_ min _\u00b7Tsamplingfc_ [,]\n_ascale \u2208_ [ _amin, amax_ ]\n_IT F R \u2190{}_\n**for** ( _iw, tn, Tn_ ) in ( _Iw, tw, Tw_ ): **do**\n\nWavelets: \u0393 _iw_ ( _a, b_ ) = \ufffd _\u2212\u221e\u221e_ _[i][w][\u03c8][\u2217]_ [\ufffd] _[t][\u2212]_ _a_ _[b]_ \ufffd _dt._\n\nEnergy: _E_ = [\ufffd] _[M]_ _m_ =1 _[|]_ [\u0393] _[iw]_ [(] _[a, b]_ [)] _[|]_ [2]\n\nDominant frequency: _fd_ = _a_ scale [arg max( _E_ )].\nEntropy: _h_ = _\u2212_ [\ufffd] _[M]_ _i_ = _m_ _[P]_ [(] _[i][w]_ [(] _[t]_ [)) log] _[ P]_ [(] _[i][w]_ [(] _[t]_ [))][.]\n\nKurtosis: _K_ = [E][[(] _[i][w]_ [(] _\u03c3_ _[t]_ [4][)] _[\u2212][\u00b5]_ [)][4][]] .\n\nSkewness: _sk_ = [E][[(] _[i][w]_ [(] _\u03c3_ _[t]_ [3][)] _[\u2212][\u00b5]_ [)][3][]] .\n\n1 _M_\nmean: _\u00b5_ = _M_ \ufffd _m_ =1 _[i][w]_ [(] _[m]_ [)][.]\n\n\n1 _M_\n\nstandard deviation: _\u03c3_ = ~~\ufffd~~ _M_ ~~\ufffd~~ _i_ =1 [(] _[i][w]_ [(] _[m]_ [)] _[ \u2212]_ _[\u00b5]_ [)][2][.]\n\n_Xn \u2190_ [log( _E_ ) _, fd, h, K, sk, \u00b5, \u03c3_ ]\n**end for**\n**return** _IT F R_ = _Concat_ ( _X_ 1 _, X_ 2 _. . . XNs_ _, tn, Tn_ )\n\n\n\n16\n\n\n**Neuronal Attention Circuit (NAC) for Representation Learning**\n\n\n_Table 4._ Summary of Key Hyperparameters of All Experiments\n\n\n**Param.** **MNIST** **PAR** **CarRacing** **Udacity** **RUL EST.**\n\n\nConv layers 2\u00d7 **1D** (64@5) **1D** (64@5, 64@3) 3\u00d7TD- **2D** (10\u201330@3\u20135) 5\u00d7 **2D** (24\u201364@5\u20133, ELU) 2\u00d7 **1D** (32@3, 16@2)\nNAC 64-d, 8h 32-d, 4h 64-d, 16h 100-d, 16h 16-d, 8h\nDense 32\u201310(SM) 32\u201311(SM) 64\u20135(SM) 64\u20131(Lin) 1(Lin)\nDropout \u2013 \u2013 0.2 0.5 \u2013\nOpt. AdamW AdamW Adam AdamW AdamW\nLR 0.001 0.001 0.0001 0.001 0.001\n\nLoss SCE SCE SCE MSE MSE\n\nMetric Acc Acc Acc MAE Score\n\nBatch 32 20 32 \u2013 32\n\nEpochs 150 500 100 10 150\n\n\n**Note:** SCE = Sparse Categorical Crossentropy; Acc = Accuracy; MAE = Mean Absolute Error; MSE = Mean Squared Error; SM =\nsoftmax; Lin = Linear; TD = TimeDistributed; Conv1D/2D = Conv1D/2D; _d_ = model dimension; _h_ = attention heads.\n\n**Baselines Hyperparameters Clarification:** All (CT & DT) RNNs use the same number of hidden units as NAC\u2019s _d_ model, and all (DT &\nCT) Attention use the same _d_ model and _heads_ as NAC. The other layers, including 1D/2D, Dense, and the remaining hyperparameters, are\nthe same during our tests.\n\n\n_Table 5._ Data distributions of PRONOSTIA, XJTU-SY, and HUST datasets.\n\n\n**Dataset** **Condition** **Frequency** **Radial Load** **Speed** **Train** **Test**\n\n_Condition 1_ 100 Hz 4 kN 1800 rpm 4 _\u223c_ 7 1 _\u223c_ 3\n**PRONOSTIA** _Condition 2_ 100 Hz 4.2 kN 1650 rpm      - 1 _\u223c_ 7\n_Condition 3_ 100 Hz 5 kN 1500 rpm                - 1 _\u223c_ 3\n\n_Condition 1_ 35 Hz 4 kN 2100 rpm                - 1 _\u223c_ 5\n**XJTU-SY** _Condition 2_ 37.5 Hz 4.2 kN 2250 rpm       - 1 _\u223c_ 5\n_Condition 3_ 40 Hz 5 kN 2400 rpm                - 1 _\u223c_ 5\n\n_Condition 1_                - 0 W                -                - 1 _\u223c_ 5\n\n**HUST** _Condition 2_       - 200 W       -       - 1 _\u223c_ 5\n\n_Condition 3_                - 400 W                -                - 1 _\u223c_ 5\n\n\n**Note:** The PRONOSTIA dataset is utilized for training and generalization testing, while the XJTU-SY and HUST datasets are employed\nto evaluate cross-validation testing. (-) values are either not available or not utilized.\n\n\n\nacross three different operating settings: 1800 rpm with 4\nkN radial load, 1650 rpm with a 4.2 kN load, and 1500 rpm\nwith a 5 kN load, all at a frequency of 100 Hz. Vibration\ndata were recorded using accelerometers placed along the\nhorizontal and vertical axes, which were sampled at 25.6\nkHz. Additionally, temperature readings were collected at a\nsampling rate of 10 Hz. The data distributions for training\nand testing are provided in Table 5.\n_XJTU-SY Dataset_ is another widely recognized benchmark\ndataset developed through collaboration between Xi\u2019an\nJiaotong University and Changxing Sumyoung Technology\n(Wang et al., 2018). The dataset comprises 15 complete runto-failure experiments performed under accelerated degradation conditions with three distinct operational settings: 1200\nrpm (35 Hz) with a 12 kN radial load, 2250 rpm (37.5 Hz)\nwith an 11 kN radial load, and 2400 rpm (40 Hz) with a 10\nkN radial load. Vibrational signals were recorded using an\naccelerometer mounted on the horizontal and vertical axes\n\nand sampled at 25.6 kHz. This dataset is only used for the\n\n\n\ncross-validation test.\n\n_HUST Dataset_ is a practical dataset developed by Hanoi\nUniversity of Science and Technology to support research\non ball bearing fault diagnosis (Hong & Thuan, 2023). The\ndataset includes vibration data collected from five bearing\ntypes (6204, 6205, 6206, 6207, and 6208) under three different load conditions: 0 W, 200 W, and 400 W. Six fault\ncategories were introduced, consisting of single faults (inner race, outer race, and ball) and compound faults (inner\u2013outer, inner\u2013ball, and outer\u2013ball). Faults were created\nas early-stage defects in the form of 0.2 mm micro-cracks,\nsimulating real degradation scenarios. The vibration signals\nwere sampled at 51.2 kHz with approximately 10-second\nrecordings for each case. This dataset is only used for the\ncross-validation test.\n\n**Preprocess:** Condition monitoring data comprises 1D nonstationary vibrational signals collected from multiple sensors. To extract meaningful information, these signals must\nbe transformed into features that possess meaningful physi\n\n\n17\n\n\n**Neuronal Attention Circuit (NAC) for Representation Learning**\n\n\ncal interpretability. We utilized the preprocessing proposed\nin (Razzaq & Zhao, 2025a) and labels are generated according to (Razzaq & Zhao, 2025b). Initially, the signal\nis segmented into small, rectangularized vectors using a\nwindowing technique ( _w_ ), enabling better localization of\ntransient characteristics. The continuous wavelet transform\n\n(CWT) (Aguiar-Conraria & Soares, 2014) with the Morlet\nwavelet (Lin & Qu, 2000) as the mother wavelet is then applied to obtain a time-frequency representation (TFR). The\nCWT is defined as \u0393( _a, b_ ) = \ufffd _\u2212\u221e\u221e_ _[x][w]_ [(] _[t]_ [)] ~~_\u221a_~~ [1] ~~_a_~~ _\u03c8_ _[\u2217]_ [\ufffd] _[t][\u2212]_ _a_ _[b]_ \ufffd _dt_,\n\nwhere _a_ and _b_ denote the scale and translation parameters,\nrespectively, and _\u03c8_ is the Morlet wavelet function. From\nthe resulting TFR, a compact set of statistical and domainspecific features is extracted to characterize the operational\ncondition of the bearing. The complete feature extraction\nprocedure is described in Algorithm 3.\n**Neural Network Architecture:** The objective of this problem is to design a compact neural network that can effectively model degradation dynamics while remaining feasible\nfor deployment on resource-constrained devices, enabling\nlocalized and personalized prognostics for individual machines. To achieve this, we combine a compact convolutional network with NAC. The CNN component extracts\nspatial degradation features from the training data, while\nNAC performs temporal filtering to emphasize informative\nfeatures. This architecture maintains a small model size", "metadata": {"source": "neuronal_attention_circuits_raw_with_image_ids_with_captions.md"}, "chunk_index": 6, "token_size_config": 5000}
{"id": "neuronal_attention_circuits_raw_with_image_ids_with_captions_chunks_5k_7", "content": "**Preprocess:** Condition monitoring data comprises 1D nonstationary vibrational signals collected from multiple sensors. To extract meaningful information, these signals must\nbe transformed into features that possess meaningful physi\n\n\n17\n\n\n**Neuronal Attention Circuit (NAC) for Representation Learning**\n\n\ncal interpretability. We utilized the preprocessing proposed\nin (Razzaq & Zhao, 2025a) and labels are generated according to (Razzaq & Zhao, 2025b). Initially, the signal\nis segmented into small, rectangularized vectors using a\nwindowing technique ( _w_ ), enabling better localization of\ntransient characteristics. The continuous wavelet transform\n\n(CWT) (Aguiar-Conraria & Soares, 2014) with the Morlet\nwavelet (Lin & Qu, 2000) as the mother wavelet is then applied to obtain a time-frequency representation (TFR). The\nCWT is defined as \u0393( _a, b_ ) = \ufffd _\u2212\u221e\u221e_ _[x][w]_ [(] _[t]_ [)] ~~_\u221a_~~ [1] ~~_a_~~ _\u03c8_ _[\u2217]_ [\ufffd] _[t][\u2212]_ _a_ _[b]_ \ufffd _dt_,\n\nwhere _a_ and _b_ denote the scale and translation parameters,\nrespectively, and _\u03c8_ is the Morlet wavelet function. From\nthe resulting TFR, a compact set of statistical and domainspecific features is extracted to characterize the operational\ncondition of the bearing. The complete feature extraction\nprocedure is described in Algorithm 3.\n**Neural Network Architecture:** The objective of this problem is to design a compact neural network that can effectively model degradation dynamics while remaining feasible\nfor deployment on resource-constrained devices, enabling\nlocalized and personalized prognostics for individual machines. To achieve this, we combine a compact convolutional network with NAC. The CNN component extracts\nspatial degradation features from the training data, while\nNAC performs temporal filtering to emphasize informative\nfeatures. This architecture maintains a small model size\n\nwithout sacrificing representational capacity. Full hyperparameter configurations are reported in Table 4.\n**Evaluation Metric:** Score is a metric specifically designed\nfor RUL estimation in the IEEE PHM (Nectoux et al., 2012)\nto score the estimates. The scoring function is asymmetric and penalizes overestimations more heavily than early\npredictions. This reflects practical considerations, as late\nmaintenance prediction can lead to unexpected failures with\nmore severe consequences than early intervention can.\n\n\n\n_yi_ \u02c6 _\u2212yi_\n\n10 _\u2212_ 1\n\ufffd\n\n\n(48)\n\n\n\n_Score_ = \ufffd\n\n_i_ :\u02c6 _yi<yi_\n\n\n\n_e_ _[\u2212]_ _[yi]_ [\u02c6] 13 _[\u2212][yi]_ _\u2212_ 1 +\n\ufffd \ufffd \ufffd\n\n_i_ :\u02c6 _yi\u2265yi_\n\n\n\n_e_\n\ufffd\n\n\n\n18", "metadata": {"source": "neuronal_attention_circuits_raw_with_image_ids_with_captions.md"}, "chunk_index": 7, "token_size_config": 5000}
