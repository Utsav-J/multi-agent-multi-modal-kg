{"id": "attention_functional_roles_raw_chunks_5k_0", "content": "## - INVESTIGATING THE FUNCTIONAL ROLES OF ATTEN ### TION HEADS IN VISION LANGUAGE MODELS: EVI DENCE FOR REASONING MODULES\n\n**Yanbei Jiang** **[1]** _[\u2217]_ **Xueqi Ma** **[1]** _[\u2217]_ **Shu Liu** **[1]** **Sarah Monazam Erfani** **[1]**\n\n**Tongliang Liu** **[2]** **James Bailey** **[1]** **Jey Han Lau** **[1]** **Krista A. Ehinger** **[1]**\n\n1The University of Melbourne 2The University of Sydney\n_{_ yanbeij, xueqim, shu6 _}_ @student.unimelb.edu.au\n_{_ sarah.erfani, baileyj, jeyhan.lau, kris.ehinger _}_ @unimelb.edu.au\ntongliang.liu@sydney.edu.au\n\n\nABSTRACT\n\n\nDespite excelling on multimodal benchmarks, vision\u2013language models (VLMs)\nlargely remain a black box. In this paper, we propose a novel interpretability framework to systematically analyze the internal mechanisms of VLMs, focusing on the functional roles of attention heads in multimodal reasoning. To\nthis end, we introduce CogVision, a dataset that decomposes complex multimodal questions into step-by-step subquestions designed to simulate human reasoning through a chain-of-thought paradigm, with each subquestion associated\nwith specific receptive or cognitive functions such as high-level visual reception and inference. Using a probing-based methodology, we identify attention\nheads that specialize in these functions and characterize them as functional heads.\nOur analysis across diverse VLM families reveals that these functional heads\nare universally sparse, vary in number and distribution across functions, and\nmediate interactions and hierarchical organization. Furthermore, intervention\nexperiments demonstrate their critical role in multimodal reasoning: removing\nfunctional heads leads to performance degradation, while emphasizing them enhances accuracy. These findings provide new insights into the cognitive organization of VLMs and suggest promising directions for designing models with more\nhuman-aligned perceptual and reasoning abilities. Code and data are available at\nhttps://github.com/YanbeiJiang/CogVision.\n\n\n1 INTRODUCTION\n\n\nLarge Vision-Language Models (VLMs) (Zhu et al., 2023; Liu et al., 2023; Lu et al., 2024a) have\ndemonstrated remarkable success across diverse multimodal tasks, ranging from image captioning\nto visual question answering. Although VLMs can solve mathematical reasoning problems with\nvisual context (as shown in Fig. 1), their internal mechanisms remain poorly understood.\n\n\nFor humans, solving such complex problems (illustrated in Fig. 1) typically requires the collaboration of vision and language, engaging multiple brain regions (Barsalou, 2014): the occipital lobe\nfor visual reception, capturing and processing the content of the images; the temporal lobe supports\nlong-term memory and the recall of relevant factual knowledge, such as chemical concentration formulas (Wheeler et al., 1997); and the parietal and prefrontal cortices are involved in higher-order\nreasoning (Hubbard et al., 2005), to produce the correct answer.\n\n\nRecent research in interpretability has begun probing the internal organization of large language\nmodels (LLMs), revealing specialized attention heads for specific functions (Wu et al.; Li et al.,\n2023a; Zheng et al.). In the case of VLMs, several studies (Kang et al., 2025; Bi et al., 2025) have\nidentified sparse attention heads with special functional roles in tasks such as grounding. However,\nstudying VLMs in complex, multi-step reasoning scenarios remains underexplored. A deeper under\n\n_\u2217_ Both authors contributed equally to this research.\n\n\n1\n\n\n![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-1-0.png)\n\n\n\n![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-1-1.png)\n\n![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-1-2.png)\n\n\n\nDecision making\n\n\n\n![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-1-3.png)\n\nHigh-level Vision Reception\n\n\n\nLanguage Knowledge Recall\n\n\n\n![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-1-5.png)\n\n\n\n![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-1-6.png)\n\n\n\nMath Reasoning\n\n\n\n![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-1-7.png)\n\n\n\n![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-1-4.png)\n\nOccipital lobe Temporal lobe Parietal lobe Frontal lobe\n\n\n\n\n\n\n\n\n\nBy dividing the number of Solution A has a higher\n\nparticles by the solvent concentration of A, Solution A.\n\n\n\n\n\n\n\n\n\n![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-1-8.png)\n\n![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-1-11.png)\n\nFigure 1: To answer a complex question, the human brain engages multiple regions, each performing\ndistinct cognitive functions. We investigate whether specific attention heads in large vision language\nmodels play analogous functional roles in generating responses.\n\n\nstanding of whether such specialized components exist, how they are organized, and what functional\nroles they play in multimodal reasoning is therefore critical.\n\n\nIn this paper, we propose a novel interpretability framework for systematically analyzing the functional roles of attention heads-parallel units in transformer models that compute token-to-token\nattention-an important component in VLMs, with a focus on their contributions to reception (perceptual processing) and cognition. To facilitate this, we introduce CogVision, a dataset that bridges the\ngap between model analysis and human cognitive processes. CogVision decomposes multimodal\nqueries into step-by-step subquestions, each aligned with specific cognitive functions (such as math\nreasoning, decision-masking), thus enabling a fine-grained evaluation of reasoning aligned with the\nchain-of-thought (CoT) paradigm. Leveraging CogVision, we develop a probing method to identify and characterize attention heads responsible for distinct cognitive operations across vision and\nlanguage within the transformer architecture.\n\n\nWe conduct extensive experiments on three major VLM families, including Intern (Zhu et al., 2025),\nQwen (Yang et al., 2025), and Gemma (Team et al., 2025) with different model scales. Our results\nreveal the existence of cognitive heads that consistently exhibit **universal**, **sparse**, and **intrinsic**\nproperties across architectures. Further analysis of the correlations among these functional heads\nreveals **cross-function interactions**, where a single head may support multiple functions or modalities, and uncovers a **hierarchical structure** in which lower-level functional heads modulate higherlevel ones, showing the complexity of neural networks (Barsalou, 2014; Ono et al., 2022).\n\n\nFurthermore, we validate the functional importance of these heads by showing that their removal\ndegrades performance on complex tasks and leads to specific error patterns, while their enhancement improves reasoning capabilities. Our findings provide compelling evidence that these attention\nheads play a critical role in multimodal reasoning. This insight not only deepens our understanding of the internal organization of VLMs but also suggests potential avenues for designing more\ninterpretable and cognitive-inspired multimodal AI systems.\n\n\n2 COGVISION\n\n\nIn this section, we present our dataset Cognitive Vision (CogVision) that contains cognitive process\nin multimodal reasoning. CogVision contains 1,409 main questions and 5,744 subquestions. Each\n\n\n2\n\n\nexample comprises a main question and answer, its subquestions and subanswers and an annotation\nspecifying the receptive or cognitive function required for each subquestion.\n\n\n2.1 COGNITIVE FUNCTIONS\n\n\nTo systematically capture the cognitive processes involved in complex reasoning tasks, we consider eight functions related to complex multimodal reasoning. These functions are inspired by\nestablished frameworks in cognitive science (Anderson, 2014; Diamond, 2013), which highlight the\nimportance of perception, working memory, and reasoning in human cognition.\n\n\nThe functions related to vision include:\n\n\n    - **Low-level Visual Reception** : Recognizing basic visual features such as color, shape, size,\nposition, motion.\n\n    - **High-level Visual Reception** : Integrating visual information to recognize objects, patterns,\nand scene structure.\n\n    - **Visual Knowledge Recall** : Applying long-term visual knowledge related to visual concepts and their properties.\n\n\nThe functions related to language include:\n\n\n    - **Language Information Extraction and Understanding** : locating and understanding relevant information from an external source or prior context.\n\n    - **Language Knowledge Recall** : Accessing domain-specific knowledge without visual input.\n\n    - **Math Reasoning** : Performing counting, arithmetic, comparison, and logic-based operations.\n\n    - **Inference** : deriving implicit information that is not directly stated.\n\n    - **Decision-Making** : Selecting the best outcome among alternatives based on reasoning.\n\n\nThis categorization reflects a natural progression from basic information processing to complex cognitive integration. Both the human brain and VLMs encompass a wide range of functional modules.\nOur focus in this work is specifically on reasoning-related cognitive functions. By identifying and\norganizing these eight core reasoning functions, we can more clearly examine how VLMs handle\ndifferent types of thinking steps, in a way that is both systematic and easy to interpret. Detail descriptions of each function and examples in CogVision can be found in Appendix A.1.\n\n\n2.2 DATA COLLECTIONS\n\n\nBased on our categorization of reasoning functions, we sampled 2000 diverse questions from existing commonly used visual reasoning benchmarks, selecting 200 examples each from ClevrMath (Lindstr\u00a8om & Abraham, 2022), MathVision (Wang et al., 2024a), MathVista (Lu et al., 2024b),\nMMMU (Yue et al., 2024), ScienceQA (Saikh et al., 2022), OKVQA (Marino et al., 2019) (400\nexamples), VCR (Zellers et al., 2019), VisuLogic (Xu et al., 2025) and Super-Clevr (Li et al.,\n2023b) datasets. These datasets span a range of reasoning problems, including logical, mathematical, and commonsense reasoning. Using the chain-of-thought (CoT) paradigm, we prompted\nGPT-4.1 (Achiam et al., 2023) to decompose each main question (with its final answer) into subquestions, each targeting a subanswer and a specific function. The prompt encourages structured,\nstep-by-step reasoning, ensuring that each subquestion is clear, answerable, and sequentially dependent. This process yields a set of subquestion\u2013answer\u2013function (subQAF) triples for each QA pair:\nsubQAFs = _{_ ( _qi, ai, fi_ ) _}_ _[k]_ _i_ =1 [, where each contains a subquestion] _[ q][i]_ [, its concise answer] _[ a][i]_ [, and the]\ncorresponding function label _fi_ . The prompt for generating subquestions are list in Appendix A.6.\n\n\n2.3 DATA FILTERING AND ANNOTATION\n\n\nRecent advances enable leveraging large pre-trained models for dataset construction, thanks to their\nreasoning capabilities and ability to generate high-quality annotations at scale (Wang et al., 2024b).\nWhile our dataset is automatically generated using a VLM to minimize manual effort, we employ a\nrigorous two-stage human verification pipeline to ensure data quality and mitigate hallucination. In\nthe first stage, three expert annotator independently evaluate whether each subquestion is logically\nstructured and consistent with natural human reasoning. QA pairs with incoherent or inconsistent\n\n\n3\n\n\ndecompositions are moved out. In the second stage, annotators verify and, if necessary, relabel the\ncognitive function associated with each subquestion to ensure accurate alignment with the intended\nmental process. Subanswers are further cross-checked with GPT-o3 (OpenAI, 2024) and adjudicated\nby humans in cases of disagreement (details in Appendix A.5). This multi-step verification ensures\nthat each retained subQAF triplet represents a coherent, interpretable reasoning step grounded in\ncore cognitive functions. After this refinement, our final dataset comprises 1,409 main QA pairs\nand 5,744 validated subQAF triplets. The detailed statistics of the dataset including the number of\ntriplet in training and testing set and distributions of each function can be found in Appendix A.3.\nWe analyze in-group subquestion diversity, and the results in the Appendix A.2 show that the eight\nfunctions exhibit broad and overlapping distributions in phrasing patterns and token lengths, indicating no systematic surface-form differences.\n\n\n3 DETECTION OF COGNITIVE FUNCTIONS\n\n\nUsing the CogVision dataset, we adopt a probing-based framework (Alain & Bengio, 2016; Belinkov, 2022; Tenney et al., 2019) to identify which attention heads in VLMs are associated with\nspecific functions in reasoning process. Specifically, for each functional annotated subquestion, we\nextract head activations (see Subsection 3.1), train classifiers and compute accuracies to identify\ncontributing heads (see Subsection 3.2). Unlike prior work focusing on a single label, our formulation captures many-to-many relationships between heads and cognitive functions, enabling a more\nnuanced analysis of functional specialization and overlap within the model.\n\n\n3.1 HEAD FEATURE EXTRACTION\n\n\nGiven a large VLM _M_, we generate an answer _a_ _[M]_ _i_ for each subquestion _qi_ derived from a main\nquestion _Qi_ . To support coherent multi-step reasoning, we include preceding subquestions and\ntheir answers as contextual input, emulating the incremental reasoning process observed in human\ncognition.\n\n\nDuring inference, each input token is first mapped into an embedding and then propagated through\nthe transformer\u2019s layers. At each layer, attention and feedforward operations update the residual\nstream, which is ultimately decoded into token predictions. For each generated token _i_, we extract\nattention head outputs _Xi_ = _{x_ _[m]_ _l_ _| l \u2208_ 1 _, . . ., Nl, m \u2208_ 1 _, . . ., Nh}_ across all layers, where _x_ _[m]_ _l_\ndenotes the value vector from the _m_ -th head in layer _l_ projected into the residual stream, with _Nh_\nthe number of heads per layer and _Nl_ the total number of layers.\n\n\nLet _Nt_ denote the number of tokens in the generated answer _a_ _[M]_ _i_ [. To isolate semantically informative]\ncontent relevant to reasoning, we select the top- _k_ most important tokens, determined by prompting\nQwen3-30B LLM (Yang et al., 2025), yielding an index set _Ik_ with _|Ik|_ = _k_ . For each index\n_j \u2208Ik_, we extract the corresponding attention head activations _Xj_, and compute the averaged\nactivation feature for the _m_ -th head in layer _l_ as \u00af _x_ _[m]_ _l_ = _k_ [1] \ufffd _j \u2208Ikxml_ [. This results in a full set of]\n\nhead-level features _X_ [\u00af] _i_ = _{x_ \u00af _[m]_ _l_ _[|][ l][ \u2208]_ [1] _[, . . ., L, m][ \u2208]_ [1] _[, . . ., M]_ _[}]_ [.]\n\n\n3.2 FUNCTION PROBING\n\n\nFor the dataset with _N_ subQAF triplets, we collect all activations to construct the probing dataset:\n\n\n_D_ probe = _{_ (\u00af _x_ _[m]_ _l_ _[, c]_ [)] _[i][}][N]_ _i_ =1 _[, l][ \u2208{]_ [1] _[, . . ., L][}][, m][ \u2208{]_ [1] _[, . . ., M]_ _[}]_ (1)\n\n\nFor classification based on CogVision, the training set includes 1,124 main questions with 4,604\nsubQAF triplet, while the testing set has 285 main questions with 1,141 triplets. Our probe takes\nthe form _p\u03b8_ ( _x_ _[m]_ _l_ [) = sigmoid (] _[\u27e8][\u03b8, x][m]_ _l_ _[\u27e9]_ [)][. There is one probe per attention head per layer per function.]\nFor each target function, the probe is trained by treating the attention-head outputs that lead to\ncorrect answers for that function as the positive class, and those associated with correct answers\nfrom other functions as the negative class. To ensure data balance, we select an equal number of\nnegative samples to match the positive ones. Given prior findings suggesting that cognitive functions\nmay vary by layer depth (Zheng et al.), we incorporate layer-wise information by computing the\naverage activation \u00af _xl_ = _M_ 1 \ufffd _Mm_ =1 _[x]_ [\u00af] _l_ _[m]_ [for each layer. We then augment each head-level vector with]\n\n\n4\n\n\nits corresponding layer summary, resulting in enriched features _x_ _[m]_ _l_ _[\u2032]_ = [\u00af _x_ _[m]_ _l_ [; \u00af] _[x][l]_ []][ for probing. The]\nimportance for each head are then calculated based on the accuracy of predicting target function. The\neffectiveness of top-k tokens and layer information, as well as the sensitivity analysis with respect\nto the parameter _k_ and the choice of LLM fused for top-k token extraction, and prompt format, can\nbe found in Appendix A.8.\n\n\n4 EXPERIMENTS\n\n\nWe conduct a series of experiments on three VLM families across various model scales, including\nIntern (Zhu et al., 2025) (InternVL3-8B and InternVL3-2B), Qwen (Yang et al., 2025) (Qwen2.5VL-7B and Qwen2.5-VL-3B), and Gemma (Team et al., 2025) (Gemma3-4B and Gemma3-2B).\nWe analyze the commonalities and differences of functional heads (Subsection 4.1), validate their\ncontributions (Subsection 4.2), and examine correlations, including cross-function interactions and\nhierarchical organization (Subsection 4.3). We also assess their causal impact on downstream reasoning tasks (Subsection 4.4). Results confirm the existence of sparse, function-specific heads and\nhighlight their critical contribution to structured cognitive processing within VLMs.\n\n\n4.1 PROPERTIES OF COGNITIVE HEADS\n\n\n**Sparsity, Universality, and Intrinsic Organization:** Fig 2 shows the heatmap of attention head\naccuracy across eight functions in Qwen2.5-VL-7B on the CogVision test set, revealing a sparse\ndistribution. In total, fewer than 7% of all heads achieve accuracies above 0.9 across the eight functions (about 2% for high-level visual reception and math reasoning, and less than 1% for the others),\nsuggesting that only a small subset of heads meaningfully contributes to different reasoning tasks.\nThese results demonstrate that VLMs rely on highly specialized, localized components for distinct\ncognitive abilities. Pearson correlations between head-activation heatmaps across the eight functions\n(Fig. 3) are generally low, confirming that different functions tend to depend on partially separable\nsubsets of heads. Moreover, this sparse functional organization is consistent across architectures\nand scales: heatmaps for five additional models (Appendix A.4) confirm its universality, and the\nrelatively high Pearson correlation coefficients between models further verify this consistency (in\nAppendix A.9). Within the same model family (e.g., Qwen2.5-VL-7B in Fig 2 vs. Qwen2.5-VL-3B\nin Fig 8), we observe similar distributions, suggesting that such specialization is intrinsic to VLMs.", "metadata": {"source": "attention_functional_roles_raw.md"}, "chunk_index": 0, "token_size_config": 5000}
{"id": "attention_functional_roles_raw_chunks_5k_1", "content": "4.1 PROPERTIES OF COGNITIVE HEADS\n\n\n**Sparsity, Universality, and Intrinsic Organization:** Fig 2 shows the heatmap of attention head\naccuracy across eight functions in Qwen2.5-VL-7B on the CogVision test set, revealing a sparse\ndistribution. In total, fewer than 7% of all heads achieve accuracies above 0.9 across the eight functions (about 2% for high-level visual reception and math reasoning, and less than 1% for the others),\nsuggesting that only a small subset of heads meaningfully contributes to different reasoning tasks.\nThese results demonstrate that VLMs rely on highly specialized, localized components for distinct\ncognitive abilities. Pearson correlations between head-activation heatmaps across the eight functions\n(Fig. 3) are generally low, confirming that different functions tend to depend on partially separable\nsubsets of heads. Moreover, this sparse functional organization is consistent across architectures\nand scales: heatmaps for five additional models (Appendix A.4) confirm its universality, and the\nrelatively high Pearson correlation coefficients between models further verify this consistency (in\nAppendix A.9). Within the same model family (e.g., Qwen2.5-VL-7B in Fig 2 vs. Qwen2.5-VL-3B\nin Fig 8), we observe similar distributions, suggesting that such specialization is intrinsic to VLMs.\n\n\n**Functional Personalization:** Beyond sparsity, attention heads exhibit a structured distribution\nacross model layers. Math-related heads are dispersed throughout the network, whereas inferencerelated heads appear more frequently in higher layers. This task-dependent localization suggests an\nemergent modular organization in which different layers support distinct cognitive operations. We\nalso observe notable variation in head counts across functions. For example, in the Qwen family,\nmath reasoning and high-level visual reception heads are more prevalent than others, reflecting differences in representational and computational complexity. Smaller models contain fewer functional\nheads compared to their larger counterparts.\n\n\n4.2 FUNCTIONAL CONTRIBUTIONS OF COGNITIVE HEADS\n\n\nAfter identifying the cognitive heads associated with each function, we examine their functional\nroles by evaluating the model\u2019s behavior on the CogVision test set under targeted interventions. We\nperform head ablation by scaling the output of a specific attention head with a small factor _\u03f5_ (e.g.,\n0.001), effectively suppressing its contribution:\n\n\n\n\ufffd\n\n\n\n_x_ [mask] _i_ = Softmax\n\n\n\n_Wq_ _[i][W][ iT]_ _k_\n\ufffd ~~\ufffd~~ _dk/n_\n\n\n\n\n_\u00b7 \u03f5Wv_ _[i]_ (2)\n\n\n\nSpecifically, we compare model performance when masking identified cognitive heads versus masking an equal number of randomly-selected heads. To quantify the impact, we employ both an LLMbased judge and an integrated accuracy metric. For LLM-based judge, we use LLM (Qwen3-30B\nLLM (Yang et al., 2025)) to judge the correctness of the output. For the integrated accuracy metric,\nan output is considered unaffected if its BLEU score (Papineni et al., 2002) exceeds 0.8, or if either\n\n\n5\n\n\n![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-5-0.png)\n\nFigure 2: The existence of cognitive heads in Qwen2.5-VL-7B responsible for eight distinct functions in complex reasoning tasks. The x-axis represents the head index, while the y-axis indicates\nthe layer index. The values denote head importance scores, capped at a cutoff of 0.60.\n\n\nFigure 3: Pearson Correlation between different functions across two models.\n\n\nthe ROUGE score (Chin-Yew, 2004) or the semantic similarity score surpasses 0.6. This provides a\ncomprehensive evaluation of performance degradation.\n\n\nWe gradually mask out the number of cognitive heads and and observe how model behavior changes.\nAs shown in Fig 4, randomly masking up to 10% of heads has minimal impact on the overall performance of Qwen2.5-VL-3B. In contrast, masking a similar number of cognitive heads leads to a\nsubstantial drop across multiple functions. Notably, when more than 25% of heads are randomly\nmasked, performance also declines sharply, as this begins to include functionally critical heads.\nThese results further highlight the sparsity and importance of functional heads.\n\n\nFor each function, we select the top 10% of heads with the highest accuracy as cognitive heads. As\nshown in Table 1, masking cognitive heads lead to a substantial decline in performance, whereas\nmasking an equal number of random heads results in only minor degradation across all VLMs. In\nsome cases, masking identified cognitive heads reduces accuracy to zero, indicating that the model\ncannot perform the corresponding function without them. The t-test analysis (Appendix A.9) shows\nthat the difference between cognitive masking and random masking is statistically significant, with\n_p \u226a_ 0 _._ 05 in nearly all cases. To further validate their functional roles, we mask heads associated\nwith one function (e.g., language knowledge recall) while evaluating performance on a different\nfunction (e.g., vision knowledge recall). As shown in Fig 5, masking the relevant functional heads\nyields a significantly larger performance drop than masking unrelated heads, confirming their functional specialization.\n\n\nIn addition to masking, we also conduct activation patching, where the activations of cognitive heads\nassociated with one function are replaced by those from another function using two strategies: ran\n\n6\n\n\n\n![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-5-1.png)\n![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-6-0.png)\n\nFigure 4: The performance of Qwen2.5-VL-3B after masking out top K cognitive heads vs K random\nheads on inference, high-level visual reception, low-level visual reception, and decion-making.\n\n\nTable 1: Intervention results (mean accuracy over 5 runs %) of cognitive heads vs. random heads\nacross 8 functions: **Low-level** Visual Reception, **High-level** Visual Reception, Vision Knowledge\n**Recall**, Language **Info** rmation Extraction and Understanding, Language Knowledge **Recall**, **Math**\nReasoning, **Inference**, and **Decision** Making. For model names, Intern2B: InternVL3-2B, Intern8B:\nInternVL3-8B, gemma2B: gemma-3n-e2b-it, gemma4B: gemma-3n-e4b-it, Qwen3B: Qwen2.5VL-3B-Instruct, Qwen7B: Qwen2.5-VL-7B-Instruct. Lower values indicate more effective intervention outcomes, suggesting that the corresponding heads play a greater role in the cognitive function.\n\n\n\n\n\n\n\n|VisionmainlyFunctions LanguagemainlyFunctions<br>Model Inter Head<br>Low-Level High -Le vel Recall Info Recall Math Inference Decision<br>llm acc llm acc llm acc llm acc llm acc llm acc llm acc llm acc|Col2|\n|---|---|\n|Intern2B<br>random<br>cognitive|75.61 82.44<br>87.5<br>88.75 89.15<br>86.1<br>57.84 69.19 84.06 84.64 81.29 90.97 75.06 74.12 67.22 71.67<br>**60.24 62.68 75.71 76.61 73.05 64.58** 66.76 68.92** 44.93 46.38 61.29 64.52 71.76 64.71 48.06 52.22**|\n|Intern8B<br>random<br>cognitive|92.2<br>93.17 88.47 91.25 87.61 82.25 59.41 69.12 87.59 89.37 79.25 84.15 82.64 85.05 66.27 73.49<br>**68.78 78.05 56.94 65.97 71.69 70.56**<br>**8.82**<br>**19.12 74.68 77.22 20.75**<br>**56.6**<br>**43.96 42.86** 66.27** 66.27**|\n|gemma2B<br>random<br>cognitive|58.37 76.33 57.53 67.64 54.57 62.57 55.07 60.82 81.05 82.11 23.12<br>55.0<br>57.44 63.49<br>30.0<br>66.94<br>**48.98**<br>**55.1**<br>**55.06 63.48**<br>**2.86**<br>**8.57**<br>**30.27 38.49**<br>**50.0**<br>**47.37 11.25 36.88 36.98 52.09 19.44 54.17**|\n|gemma4B<br>random<br>cognitive|36.0<br>48.73 29.22 38.65 33.52 34.08 25.12 27.56 40.27 47.84 27.91 44.19<br>57.3<br>57.08 18.29 34.57<br>**29.09 41.82 10.88 21.24**<br>**5.63**<br>**14.08**<br>**22.2**<br>**36.34**<br>**9.46**<br>**13.51** 27.91** 53.49 36.18 32.81**<br>**4.29**<br>**15.71**|\n|Qwen3B<br>random<br>cognitive|70.97 82.58 82.42 86.06 88.48 86.36 52.22 58.89 85.14 86.57 65.85 89.76 85.32 90.13 63.44 71.88<br>**12.9**<br>**12.9**<br>**12.12 16.67 77.88 77.88** 55.56 62.96** 61.43 68.57**<br>**0.0**<br>**0.0**<br>**1.27**<br>**1.27**<br>**1.56**<br>**4.69**|\n|Qwen7B<br>random<br>cognitive|83.2<br>88.8<br>84.57 89.51 79.43 80.29 75.08 79.38 90.13<br>86.4<br>67.84 72.94 80.67 83.33 75.14 79.19<br>**30.0**<br>**38.0**<br>**73.21 73.83 21.43 22.86 15.38 33.85**<br>**84.0**<br>**78.67** 68.63 72.55 85.56 81.11** 25.68 27.03**|\n\n\ndom activation and mean activation. In the random activation setting, activations are substituted\nwith those from a randomly selected subquestion belonging to a different function. In the mean\nactivation setting, activations are replaced with the average activation computed over all subquestions associated with another function (details in Appendix A.10). As shown in Table 2, both types\nof activation patching result in substantial performance degradation for cognitive heads, consistent\nwith the effects observed under masking interventions.\n\n\nTable 2: Ablation study of different activation-masking methods on Qwen2.5-VL-3B. Random:\nrandom activation. Mean: mean activation. Scalar: masking.\n\n\n\n\n\n\n\n|VisionmainlyCognitiveFunctions LanguagemainlyCognitiveFunctions<br>Method Inter Head<br>Low-Level High -Level Recall Info Recall Math Inference Decision<br>llm acc llm acc llm acc llm acc llm acc llm acc llm acc llm acc|Col2|\n|---|---|\n|Random<br>cognitive|**0.00 0.00** 20.45 23.48** 62.12 65.15** 35.79 57.41 12.86 24.29 17.07 17.07 14.32 14.32 6.25 9.38|\n|Mean<br>cognitive|3.80 3.80 17.39 19.91 66.67 65.15** 35.19 55.56 10.00 21.43** 37.17 34.15<br>3.80<br>3.80<br>6.25 9.38|\n|Scalar<br>cognitive|6.45 6.45** 16.67 18.94** 75.76 75.76 62.96 81.48 57.14 62.86<br>**2.43**<br>**2.43**<br>**0.00**<br>**0.00**<br>**3.13 4.69**|\n\n\nWe further examine how vision-related and language-related heads attend to their respective modalities. For the top-30 heads of each function, we compute the average attention weight on visual tokens\nacross the test set. As shown in Appendix A.9, vision-related heads (e.g., high-level and low-level\n\n\n7\n\n\n![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-7-0.png)\n\nFigure 5: The performance of VLMs on target functions after masking out top K cognitive heads on\nintervention functions. The scores are based on LLM-Judge.\n\n\nTable 3: Study on the influence of low-level cognitive heads for high-order function on Qwen2.5VL-3B. The score is measured based on LLM-judge. We only evaluate subquestions that the model\noriginally answered correctly. This filtering ensures that any observed drop in performance is caused\nsolely by the intervention. Notably, the model\u2019s own generated outputs are used as inputs for subsequent subquestions.\n\n\nVision Recall Language Recall Info. Low-Level High Level Math Decision Inference\n\n\n\u2717 \u2713 \u2713 \u2713 \u2713 50 _._ 00 _\u2193_ 50.00 54 _._ 55 _\u2193_ 45.45 54 _._ 17 _\u2193_ 45.83\n\u2713 \u2717 \u2713 \u2713 \u2713 16 _._ 67 _\u2193_ 83.33 56 _._ 25 _\u2193_ 43.75 65 _._ 22 _\u2193_ 34.78\n\u2713 \u2713 \u2717 \u2713 \u2713 22 _._ 22 _\u2193_ 77.78 57 _._ 89 _\u2193_ 42.11 51 _._ 61 _\u2193_ 48.39\n\u2713 \u2713 \u2713 \u2717 \u2713 27 _._ 27 _\u2193_ 72.73 72 _._ 73 _\u2193_ 27.27 59 _._ 09 _\u2193_ 40.91\n\u2713 \u2713 \u2713 \u2713 \u2717 33 _._ 96 _\u2193_ 66.04 64 _._ 29 _\u2193_ 35.71 53 _._ 95 _\u2193_ 46.05\n\n\nvisual reception) predominantly focus on image tokens, capturing spatial and object-level information, whereas language-related heads (e.g., language knowledge recall) concentrate on text tokens.\nInterestingly, Qwen and Intern models allocate more attention to text, while Gemma emphasizes\nvision, revealing family-specific modality preferences. We also observe heads with cross-modal attention that respond to both visual and textual tokens, likely mediating interactions between visual\nperception and linguistic reasoning. These findings suggest that functional specialization in VLMs\nis complemented by selective cross-modal integration, enabling coherent multimodal reasoning.\n\n\n4.3 RELATIONSHIP AMONG COGNITIVE HEADS\n\n\nWhile cognitive heads are specialized for distinct functions, understanding their relationships is\ncrucial for revealing how complex reasoning emerges from their cooperation.\n\n\n**Heads Across Functions** The neural system is inherently complex, with individual neurons often\nparticipating in multiple functions (Mante et al., 2013). We observe a similar phenomenon in VLMs:\ncertain functional heads overlap, with a single head participating in multiple cognitive roles (e.g., in\nthe Qwen2.5-VL-7B model, 18% of cognitive heads across eight functions participate in more than\none function). In our probing-based method, we quantify and rank the accuracy of attention heads for\neach cognitive function. A head that ranks highly for one function may also exhibit non-negligible\nimportance for others, leading to the phenomenon of \u201dHeads Across Functions\u201d. Notably, even if\na head ranks in the top 10% for multiple cognitive functions, our ranking still reveals a primary\nfunction for which it is most diagnostic. In summary, functional heads serve not only as specialized\nunits but also as integrative components that bridge multiple reasoning processes.\n\n\n**Hierarchical Structure** Humans often solve complex tasks through step-by-step reasoning, where\nformer functions, such as low-level visual reception, support higher-level processes like inference\nand decision making. The CogVision dataset reflects this hierarchy: under CoT, early subquestions\nfocus on information extraction, while later ones require more complex reasoning. Leveraging this\n\n\n8\n\n\nstructure, we test whether VLMs exhibit similar functional dependencies by masking attention heads\nassociated with early-stage functions and observing the impact on subsequent reasoning steps. As\nshown in Table 3, masking vision or language knowledge recall heads significantly impairs laterstage performance, particularly in decision making. These results suggest that VLMs exhibit an\nemergent hierarchical organization, where early cognitive functions support more advanced reasoning. The prompt used for VLMs can be found in Appendix A.7.\n\n\n4.4 INFLUENCE OF FUNCTIONAL HEADS ON DOWNSTREAM TASKS\n\n\nIn this section, we investigate how functional heads influence downstream tasks through both negative interventions (masking out function heads) and positive interventions (shifting heads toward\nspecific functions).\n\n\n**Negative Intervention:** We randomly\nsample 200 question for two VQA bench- Table 4: Negative Intervention on Visual Question Anmarks, OK-VQA and Clevr, both reason- swering task (OK-VQA and Clevr-Math). The scores\ning tasks. We perform negative interven- are based on LLM-Judge.\ntion by masking high-level visual reception heads on OK-VQA and math reason- Dataset Inter ~~H~~ ead Model\n\nleads to a significant performance drop after **0.00** **59.00** 29.00 **13.00** 14.00 **13.00**\nacross all models. Further analysis in Ap\n|Dataset Inter Head|Model|\n|---|---|\n|Dataset<br>Inter~~ H~~ead|Qwen3B Qwen7B Intern2B Intern8B gemma2B gemma4B|\n|OK-VQA<br>before<br>after|54.00<br>55.00<br>46.00<br>51.00<br>21.00<br>20.00<br>**7.00**<br>55.00<br>**44.00**<br>53.00<br>**18.00**<br>21.00|\n|Clevr-Mathbefore<br>after|94.00<br>70.00<br>20.00<br>93.00<br>14.00<br>14.00<br>**0.00**<br>**59.00**<br>29.00<br>**13.00**<br>14.00<br>**13.00**|\n\npendix A.11 shows that masking the **math reasoning** heads leads to errors in arithmetic tasks, while\nvisual receptive functions remain largely unaffected. This confirms that these cognitive heads are\ncrucial for specific functions and highlights the robustness and generalizability of our method.\n\n\n**Positive Intervention:** We calculate the activation directions of different functions using the CogVision dataset. For each function, the activation direction of a head at layer _l_ and index _h_ is computed\n\nas:\ndir _[h]_ _l_ [=][ E] _[i][\u2208D]_ correct \ufffd _x_ _[h]_ _l_ [(] _[i]_ [)] \ufffd _\u2212_ E _i\u2208D_ incorrect \ufffd _x_ _[h]_ _l_ [(] _[i]_ [)] \ufffd (3)\n\nwhere _x_ _[h]_ _l_ [(] _[i]_ [)][ denotes the activation of head at layer] _[ l]_ [ and index] _[ h]_ [, and] _[ D]_ [correct][ and] _[ D]_ [incorrect][ represent]\nthe sets of samples answered correctly and incorrectly, respectively. Then we estimate the standard\ndeviation of activations (Li et al., 2023a) along the cognitive function direction to be _\u03c3l_ _[h]_ [, and shift]\noriginal head activation as _x_ _[h]_ _l_ [(] _[i]_ [)] _[ \u2190]_ _[x][h]_ _l_ [(] _[i]_ [) +] _[ \u03b1\u03c3]_ _l_ _[h]_ [dir] _l_ _[h]_ [, where] _[ \u03b1]_ [ is a parameter.]", "metadata": {"source": "attention_functional_roles_raw.md"}, "chunk_index": 1, "token_size_config": 5000}
{"id": "attention_functional_roles_raw_chunks_5k_2", "content": "pendix A.11 shows that masking the **math reasoning** heads leads to errors in arithmetic tasks, while\nvisual receptive functions remain largely unaffected. This confirms that these cognitive heads are\ncrucial for specific functions and highlights the robustness and generalizability of our method.\n\n\n**Positive Intervention:** We calculate the activation directions of different functions using the CogVision dataset. For each function, the activation direction of a head at layer _l_ and index _h_ is computed\n\nas:\ndir _[h]_ _l_ [=][ E] _[i][\u2208D]_ correct \ufffd _x_ _[h]_ _l_ [(] _[i]_ [)] \ufffd _\u2212_ E _i\u2208D_ incorrect \ufffd _x_ _[h]_ _l_ [(] _[i]_ [)] \ufffd (3)\n\nwhere _x_ _[h]_ _l_ [(] _[i]_ [)][ denotes the activation of head at layer] _[ l]_ [ and index] _[ h]_ [, and] _[ D]_ [correct][ and] _[ D]_ [incorrect][ represent]\nthe sets of samples answered correctly and incorrectly, respectively. Then we estimate the standard\ndeviation of activations (Li et al., 2023a) along the cognitive function direction to be _\u03c3l_ _[h]_ [, and shift]\noriginal head activation as _x_ _[h]_ _l_ [(] _[i]_ [)] _[ \u2190]_ _[x][h]_ _l_ [(] _[i]_ [) +] _[ \u03b1\u03c3]_ _l_ _[h]_ [dir] _l_ _[h]_ [, where] _[ \u03b1]_ [ is a parameter.]\n\n\nThe experimental results in Table 5 show that enhancing the activation of functional heads along\ntheir corresponding functional directions improves performance on the related tasks. For example,\npositive intervention on vision knowledge recall heads in InternVL3-8B increased accuracy on the\ncorresponding CogVision question-answering task from 88.54% to 91.67%. Similarly, enhancing\nfunction-specific heads can also boost performance on downstream tasks. Here, we set _\u03b1_ = 0 _._ 1\nfor all datasets, though tuning this parameter may further improve performance. Case analyses are\nprovided in Appendix A.11.\n\n\n5 RELATED WORKS\n\n\n**Neural Networks and the Brain.** Understanding the relationship between artificial neural networks (ANNs) and the biological brain has been a long-standing goal in both neuroscience and\nmachine learning. Early studies demonstrated that convolutional neural networks (CNNs) trained\non visual tasks develop hierarchical representations reminiscent of the ventral visual stream in primates (Yamins et al., 2014; Cadieu et al., 2014). Subsequent work extended this line of inquiry\nto recurrent and transformer-based architectures, showing that attention mechanisms can emulate\naspects of selective processing observed in cortical circuits (Tsividis et al., 2017). More recently,\nlarge language models (LLMs) have exhibited striking parallels with human brain activity during\nlanguage processing. In particular, transformer-based models such as GPT-2 produce internal representations that align with neural responses in language-selective brain regions (Caucheteux et al.,\n2022; Schrimpf et al., 2021). Some works (Schulze Buschoff et al., 2025; Li et al., 2024) have\nstudied how VLMs perform differently from humans from a cognitive perspective. Furthermore, the\n\n\n9\n\n\n\nTable 4: Negative Intervention on Visual Question Answering task (OK-VQA and Clevr-Math). The scores\nare based on LLM-Judge.\n\n\n\n\n\n\n\n\nTable 5: Positive Intervention on CogVision test set and other Visual Question Answering benchmarks (OK-VQA, MathVista and Visulogic). The scores are based on LLM-Judge. For OK-VQA,\nwe perform positive intervention by masking 10% high-level visual reception head, MathVista for\nmath reasoning heads, and Visulogic for decision-making heads.\n\n|In Domain Out of Domain<br>Model InterHead<br>Math Vision Recall Lang Recall Info Low-Level Inference High-Level Decision OK-VQA MathVista Visulogic|Col2|Col3|\n|---|---|---|\n|Qwen3B<br>before<br>after|46.40<br>82.29<br>81.37<br>48.03<br>78.26<br>65.87<br>77.38<br>29.73<br>46.40<br>**85.42**<br>**84.31**<br>44.74<br>78.26<br>**69.84**<br>**77.98**<br>**32.43**|62.50<br>60.00<br>26.50<br>62.00<br>60.00<br>**28.00**|\n|Qwen7B<br>before<br>after|52.00<br>84.38<br>86.27<br>43.42<br>77.18<br>69.84<br>82.44<br>36.94<br>**52.80**<br>**85.42**<br>86.27<br>**46.05**<br>**82.61**<br>**74.60**<br>82.44<br>36.94|66.00<br>63.00<br>24.00<br>**67.50**<br>63.00<br>**24.50**|\n|Intern2B<br>before<br>after|38.40<br>79.17<br>80.39<br>44.08<br>78.26<br>61.90<br>81.55<br>34.23<br>38.40<br>78.13<br>**84.31**<br>42.76<br>**80.43**<br>**64.29**<br>**82.44**<br>**35.14**|58.50<br>51.50<br>24.00<br>**61.50**<br>**52.00**<br>**26.00**|\n|Intern8B<br>before<br>after|52.00<br>88.54<br>82.35<br>45.39<br>88.04<br>73.81<br>86.61<br>42.34<br>**52.80**<br>**91.67**<br>**84.31**<br>**46.71**<br>86.96<br>73.81<br>**87.80**<br>**43.24**|67.00<br>66.00<br>24.00<br>**67.50**<br>66.00<br>**26.00**|\n|gemma2B<br>before<br>after|26.40<br>62.50<br>80.39<br>32.24<br>34.78<br>48.41<br>33.93<br>23.42<br>**28.00**<br>**64.58**<br>**84.31**<br>**38.16**<br>34.78<br>**50.00**<br>30.95<br>20.72|29.00<br>24.50<br>26.00<br>**29.50**<br>24.50<br>**29.00**|\n|gemma4B<br>before<br>after|32.80<br>62.50<br>83.33<br>32.89<br>27.17<br>52.38<br>33.63<br>18.92<br>**35.20**<br>**65.63**<br>**85.29**<br>**35.53**<br>27.17<br>**53.97**<br>**33.63**<br>**19.82**|31.50<br>24.00<br>27.00<br>**33.00**<br>**26.50**<br>**27.50**|\n\n\n\nchain-of-thought (CoT) paradigm has been argued to mirror step-by-step human reasoning, leading to improved problem-solving performance. These findings motivate the design of interpretable,\nfunctionally specialized modules in artificial networks, bridging insights from neuroscience with\nadvances in multimodal reasoning.\n\n\n**Attention Heads in Vision\u2013Language Models.** A growing body of interpretability research has\nrevealed that attention heads in LLMs exhibit functional specialization, such as pattern induction,\ntruthfulness, information retrieval, and safety alignment (Olsson et al., 2022; Li et al., 2023a; Wu\net al.; Zhou et al., 2024; Zheng et al.). Ma et al. further investigates the diverse cognitive roles that\nattention heads play in supporting LLM reasoning.\n\n\nIn the multimodal domain, recent works (Li et al., 2020) have begun to explore the internal mechanisms of Vision\u2013Language Models (VLMs). Studies have shown that certain sparse attention heads\nplay distinct roles in visual grounding, enabling alignment between textual tokens and image regions without additional fine-tuning (Kang et al., 2025; Bi et al., 2025). Similarly, probing studies\non multimodal pre-trained models (e.g., ViLBERT, LXMERT, UNITER) demonstrate that subsets\nof attention heads encode cross-modal interactions and semantic alignment between vision and language (Cao et al., 2020). These works highlight the existence of specialized heads in VLMs but\nlargely focus on perception-oriented tasks such as grounding or alignment. In contrast, we investigate functionally specialized heads under more complex reasoning settings by aligning attention\nhead behavior with human cognitive functions.\n\n\n6 CONCLUSION\n\n\nWe propose an interpretability framework that links attention heads in VLMs to human perceptual\nand cognitive functions involved in multimodal reasoning. To enable this, we introduce CogVision, a cognitively grounded dataset that decomposes complex multimodal questions into functional\nreasoning steps, and apply probing-based analyses to identify specialized heads supporting these\nfunctions. Our study across diverse VLM families reveals that functional heads are sparse, universal, and intrinsic properties of the models, while varying in number, distribution, and hierarchical\norganization. Moreover, we find that certain heads exhibit cross-modal interactions. Intervention\nexperiments further reveal their causal importance. Our insights into the functional organization\nof attention mechanisms provide a foundation for developing more interpretable, robust, and cognitively inspired vision-language models. While our work provides a first step toward exploring\npotential similarities between the cognitive processes of VLMs and those of the human brain, we do\nnot claim complete alignment, nor do we equate observations and analyses of attention heads with\nthe full scope of human reasoning.\n\n\n10\n\n\n**Limitations** While our study provides an initial framework for analyzing attention heads in VLMs,\nseveral limitations remain. We focus on eight predefined cognitive functions, which may not cover\nthe full spectrum of LLM capabilities; future work could expand this taxonomy to include finergrained or emergent functions. Additionally, we concentrate on attention heads, leaving other components such as MLPs unexplored. Further exploring advanced probing methods and extending the\nanalysis to other model components, could provide further understandings.\n\n\nREFERENCES\n\n\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical\nreport. _arXiv preprint arXiv:2303.08774_, 2023.\n\n\nGuillaume Alain and Yoshua Bengio. Understanding intermediate layers using linear classifier\nprobes. _arXiv preprint arXiv:1610.01644_, 2016.\n\n\nJohn R Anderson. _Rules of the mind_ . Psychology Press, 2014.\n\n\nLawrence W Barsalou. _Cognitive psychology: An overview for cognitive scientists_ . Psychology\nPress, 2014.\n\n\nYonatan Belinkov. Probing classifiers: Promises, shortcomings, and advances. _Computational_\n_Linguistics_, 48(1):207\u2013219, 2022.\n\n\nJing Bi, Junjia Guo, Yunlong Tang, Lianggong Bruce Wen, Zhang Liu, Bingjie Wang, and Chenliang\nXu. Unveiling visual perception in language models: An attention head analysis approach. In\n_Proceedings of the Computer Vision and Pattern Recognition Conference_, pp. 4135\u20134144, 2025.\n\n\nCharles F Cadieu, Ha Hong, Daniel LK Yamins, Nicolas Pinto, Diego Ardila, Ethan A Solomon,\nNajib J Majaj, and James J DiCarlo. Deep neural networks rival the representation of primate it\ncortex for core visual object recognition. _PLoS computational biology_, 10(12):e1003963, 2014.\n\n\nJize Cao, Zhe Gan, Yu Cheng, Licheng Yu, Yen-Chun Chen, and Jingjing Liu. Behind the scene:\nRevealing the secrets of pre-trained vision-and-language models. In _European Conference on_\n_Computer Vision_, pp. 565\u2013580. Springer, 2020.\n\n\nCharlotte Caucheteux, Alexandre Gramfort, and Jean-R\u00b4emi King. Deep language algorithms predict\nsemantic comprehension from brain activity. _Scientific reports_, 12(1):16327, 2022.\n\n\nLin Chin-Yew. Rouge: A package for automatic evaluation of summaries. In _Proceedings of the_\n_Workshop on Text Summarization Branches Out, 2004_, 2004.\n\n\nAdele Diamond. Executive functions. _Annual review of psychology_, 64(1):135\u2013168, 2013.\n\n\nEdward M Hubbard, Manuela Piazza, Philippe Pinel, and Stanislas Dehaene. Interactions between\nnumber and space in parietal cortex. _Nature reviews neuroscience_, 6(6):435\u2013448, 2005.\n\n\nSeil Kang, Jinyeong Kim, Junhyeok Kim, and Seong Jae Hwang. Your large vision-language model\nonly needs a few attention heads for visual grounding. In _Proceedings of the Computer Vision_\n_and Pattern Recognition Conference_, pp. 9339\u20139350, 2025.\n\n\nKenneth Li, Oam Patel, Fernanda Vi\u00b4egas, Hanspeter Pfister, and Martin Wattenberg. Inference-time\nintervention: Eliciting truthful answers from a language model. _Advances in Neural Information_\n_Processing Systems_, 36:41451\u201341530, 2023a.\n\n\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. What does bert with\nvision look at? In _Proceedings of the 58th annual meeting of the association for computational_\n_linguistics_, pp. 5265\u20135275, 2020.\n\n\nYijiang Li, Qingying Gao, Tianwei Zhao, Bingyang Wang, Haoran Sun, Haiyun Lyu, Robert D\nHawkins, Nuno Vasconcelos, Tal Golan, Dezhi Luo, et al. Core knowledge deficits in multimodal language models. _arXiv preprint arXiv:2410.10855_, 2024.\n\n\n11\n\n\nZhuowan Li, Xingrui Wang, Elias Stengel-Eskin, Adam Kortylewski, Wufei Ma, Benjamin\nVan Durme, and Alan L Yuille. Super-clevr: A virtual benchmark to diagnose domain robustness in visual reasoning. In _Proceedings of the IEEE/CVF conference on computer vision and_\n_pattern recognition_, pp. 14963\u201314973, 2023b.\n\n\nAdam Dahlgren Lindstr\u00a8om and Savitha Sam Abraham. Clevr-math: A dataset for compositional\nlanguage, visual and mathematical reasoning. _arXiv preprint arXiv:2208.05358_, 2022.\n\n\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. _Advances_\n_in neural information processing systems_, 36:34892\u201334916, 2023.\n\n\nHaoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren,\nZhuoshu Li, Hao Yang, et al. Deepseek-vl: towards real-world vision-language understanding.\n_arXiv preprint arXiv:2403.05525_, 2024a.\n\n\nPan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, KaiWei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of\nfoundation models in visual contexts. In _The Twelfth International Conference on Learning Repre-_\n_sentations_ [, January 2024b. URL https://openreview.net/forum?id=KUNzEQMWU7.](https://openreview.net/forum?id=KUNzEQMWU7)\n\n\nXueqi Ma, Jun Wang, Yanbei Jiang, Sarah Monazam Erfani, Tongliang Liu, and James Bailey.\nCognitive mirrors: Exploring the diverse functional roles of attention heads in llm reasoning. In\n_The Thirty-ninth Annual Conference on Neural Information Processing Systems_ .\n\n\nValerio Mante, David Sussillo, Krishna V Shenoy, and William T Newsome. Context-dependent\ncomputation by recurrent dynamics in prefrontal cortex. _nature_, 503(7474):78\u201384, 2013.\n\n\nKenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual\nquestion answering benchmark requiring external knowledge. In _Proceedings of the IEEE Con-_\n_ference on Computer Vision and Pattern Recognition_, pp. 3195\u20133204, 2019.\n\n\nCatherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan,\nBen Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction\nheads. _arXiv preprint arXiv:2209.11895_, 2022.\n\n\nYumie Ono, Xian Zhang, J Adam Noah, Swethasri Dravida, and Joy Hirsch. Bidirectional connectivity between broca\u2019s area and wernicke\u2019s area during interactive verbal communication. _Brain_\n_connectivity_, 12(3):210\u2013222, 2022.\n\n\nOpenAI. Introducing o3 and o4 mini. [https://openai.com/index/](https://openai.com/index/introducing-o3-and-o4-mini/)\n[introducing-o3-and-o4-mini/, 2024. Accessed: 2025-05-05.](https://openai.com/index/introducing-o3-and-o4-mini/)\n\n\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic\nevaluation of machine translation. In _Proceedings of the 40th annual meeting of the Association_\n_for Computational Linguistics_, pp. 311\u2013318, 2002.\n\n\nTanik Saikh, Tirthankar Ghosal, Amish Mittal, Asif Ekbal, and Pushpak Bhattacharyya. Scienceqa:\nA novel resource for question answering on scholarly articles. _International Journal on Digital_\n_Libraries_, 23(3):289\u2013301, 2022.\n\n\nMartin Schrimpf, Idan Asher Blank, Greta Tuckute, Carina Kauf, Eghbal A Hosseini, Nancy Kanwisher, Joshua B Tenenbaum, and Evelina Fedorenko. The neural architecture of language: Integrative modeling converges on predictive processing. _Proceedings of the National Academy of_\n_Sciences_, 118(45):e2105646118, 2021.\n\n\nLuca M Schulze Buschoff, Elif Akata, Matthias Bethge, and Eric Schulz. Visual cognition in multimodal large language models. _Nature Machine Intelligence_, 7(1):96\u2013106, 2025.\n\n\nGemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej,\nSarah Perrin, Tatiana Matejovicova, Alexandre Ram\u00b4e, Morgane Rivi`ere, et al. Gemma 3 technical\nreport. _arXiv preprint arXiv:2503.19786_, 2025.\n\n\nIan Tenney, Dipanjan Das, and Ellie Pavlick. Bert rediscovers the classical nlp pipeline. _arXiv_\n_preprint arXiv:1905.05950_, 2019.\n\n\n12\n\n\nPedro Tsividis, Thomas Pouncy, Jaqueline L Xu, Joshua B Tenenbaum, and Samuel J Gershman.\nHuman learning in atari. In _AAAI Spring Symposia_, volume 1, 2017.\n\n\nKe Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. _Advances in_\n_Neural Information Processing Systems_, 37:95095\u201395169, 2024a.", "metadata": {"source": "attention_functional_roles_raw.md"}, "chunk_index": 2, "token_size_config": 5000}
{"id": "attention_functional_roles_raw_chunks_5k_3", "content": "Tanik Saikh, Tirthankar Ghosal, Amish Mittal, Asif Ekbal, and Pushpak Bhattacharyya. Scienceqa:\nA novel resource for question answering on scholarly articles. _International Journal on Digital_\n_Libraries_, 23(3):289\u2013301, 2022.\n\n\nMartin Schrimpf, Idan Asher Blank, Greta Tuckute, Carina Kauf, Eghbal A Hosseini, Nancy Kanwisher, Joshua B Tenenbaum, and Evelina Fedorenko. The neural architecture of language: Integrative modeling converges on predictive processing. _Proceedings of the National Academy of_\n_Sciences_, 118(45):e2105646118, 2021.\n\n\nLuca M Schulze Buschoff, Elif Akata, Matthias Bethge, and Eric Schulz. Visual cognition in multimodal large language models. _Nature Machine Intelligence_, 7(1):96\u2013106, 2025.\n\n\nGemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej,\nSarah Perrin, Tatiana Matejovicova, Alexandre Ram\u00b4e, Morgane Rivi`ere, et al. Gemma 3 technical\nreport. _arXiv preprint arXiv:2503.19786_, 2025.\n\n\nIan Tenney, Dipanjan Das, and Ellie Pavlick. Bert rediscovers the classical nlp pipeline. _arXiv_\n_preprint arXiv:1905.05950_, 2019.\n\n\n12\n\n\nPedro Tsividis, Thomas Pouncy, Jaqueline L Xu, Joshua B Tenenbaum, and Samuel J Gershman.\nHuman learning in atari. In _AAAI Spring Symposia_, volume 1, 2017.\n\n\nKe Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. _Advances in_\n_Neural Information Processing Systems_, 37:95095\u201395169, 2024a.\n\n\nXinru Wang, Hannah Kim, Sajjadur Rahman, Kushan Mitra, and Zhengjie Miao. Human-llm collaborative annotation through effective verification of LLM labels. In Florian \u2019Floyd\u2019 Mueller,\nPenny Kyburz, Julie R. Williamson, Corina Sas, Max L. Wilson, Phoebe O. Toups Dugas, and\nIrina Shklovski (eds.), _Proceedings of the CHI Conference on Human Factors in Computing Sys-_\n_tems, CHI 2024, Honolulu, HI, USA, May 11-16, 2024_, pp. 303:1\u2013303:21. ACM, 2024b. doi:\n[10.1145/3613904.3641960. URL https://doi.org/10.1145/3613904.3641960.](https://doi.org/10.1145/3613904.3641960)\n\n\nMark A Wheeler, Donald T Stuss, and Endel Tulving. Toward a theory of episodic memory: the\nfrontal lobes and autonoetic consciousness. _Psychological bulletin_, 121(3):331, 1997.\n\n\nWenhao Wu, Yizhong Wang, Guangxuan Xiao, Hao Peng, and Yao Fu. Retrieval head mechanistically explains long-context factuality, 2024. _URL https://arxiv. org/abs/2404_, 15574.\n\n\nWeiye Xu, Jiahao Wang, Weiyun Wang, Zhe Chen, Wengang Zhou, Aijun Yang, Lewei Lu,\nHouqiang Li, Xiaohua Wang, Xizhou Zhu, et al. Visulogic: A benchmark for evaluating visual\nreasoning in multi-modal large language models. _arXiv preprint arXiv:2504.15279_, 2025.\n\n\nDaniel LK Yamins, Ha Hong, Charles F Cadieu, Ethan A Solomon, Darren Seibert, and James J\nDiCarlo. Performance-optimized hierarchical models predict neural responses in higher visual\ncortex. _Proceedings of the national academy of sciences_, 111(23):8619\u20138624, 2014.\n\n\nAn Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu,\nChang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. _arXiv preprint_\n_arXiv:2505.09388_, 2025.\n\n\nXiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens,\nDongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In _Proceedings of the IEEE/CVF_\n_Conference on Computer Vision and Pattern Recognition_, pp. 9556\u20139567, 2024.\n\n\nRowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition: Visual\ncommonsense reasoning. In _Proceedings of the IEEE Conference on Computer Vision and Pattern_\n_Recognition_, pp. 6720\u20136731, 2019.\n\n\nZ Zheng, Y Wang, Y Huang, S Song, M Yang, B Tang, F Xiong, and Z Li. Attention heads of large\nlanguage models: A survey. arxiv 2024. _arXiv preprint arXiv:2409.03752_ .\n\n\nZhenhong Zhou, Haiyang Yu, Xinghua Zhang, Rongwu Xu, Fei Huang, Kun Wang, Yang Liu,\nJunfeng Fang, and Yongbin Li. On the role of attention heads in large language model safety.\n_arXiv preprint arXiv:2410.13708_, 2024.\n\n\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. _arXiv preprint_\n_arXiv:2304.10592_, 2023.\n\n\nJinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen\nDuan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for\nopen-source multimodal models. _arXiv preprint arXiv:2504.10479_, 2025.\n\n\nA APPENDIX\n\n\nA.1 COGVISION FUNCTION DETAILS AND EXAMPLES\n\n\n**Language Information Extraction and Understanding:** The ability to comprehend and extract\nmeaning from only language, including understanding word relationships, sentence structures, context, and intent within a given textual input.\n\n\n13\n\n\n**Low-Level Vision Reception:** The perception and interpretation of visual content, including recognizing low-level visual features such as number, color, shape, size and position.\n\n\n**High-Level Vision Reception:** The perception and interpretation of visual content, including recognizing high-level visual features such as object recognition, the relationships, motion, spatial\narrangement, and scene-level understanding.\n\n\n**Vision Knowledge Recall:** The access and application of long-term visual knowledge, such as\nrecognizing familiar objects, understanding occlusion, symmetry, physical structure, and part-whole\nrelationships (e.g., \u201da cat has a tail\u201d).\n\n\n**Language Knowledge Recall:** The access and application of long-term domain-specific textual\nknowledge, such as factual knowledge from science, history, or everyday concepts stored in memory.\n\n\n**Math Reasoning:** The application of mathematical concepts and operations such as counting, comparison, arithmetic, and pattern-based quantitative reasoning.\n\n\n**Inference:** The logical derivation of conclusions from given information, including deductive (guaranteed) reasoning and abductive (plausible) reasoning.\n\n\n**Decision-Making:** The process of selecting the most appropriate option or answer based on prior\nreasoning, evaluation of evidence, or predefined objectives.\n\n\nTable 6 and Table 7 presents illustrative examples from the CogVision dataset. The main question\nand its corresponding answer are taken from the original dataset. Based on an analysis of the main\nquestion, a sequence of sub-questions, their answers, and associated cognitive function labels are\ngenerated in order.\n\n\nA.2 ANALYSES OF SURFACE-FORM VARIATION ACROSS COGNITIVE-FUNCTION GROUPS\n\n\nWe analyze the surface-form variation across cognitive-function groups. As shown in Figures 6\nand 7, the eight functions exhibit wide and overlapping distributions in phrasing patterns and token\nlengths, indicating no systematic surface-form differences. These results support that the cognitive\ngroups are not determined by trivial lexical or structural artifacts. About modality, as described\nin Section 2.1, some functions (Low-level Visual Reception, High-level Visual Reception, Visual\nKnowledge Recall) naturally involve vision, while others relate primarily to language. This modality\ntendency is intrinsic to the underlying cognitive processes rather than an artifact of the pipeline.\n\n\nFigure 6: Histogram of token length distribution for 8 functions.\n\n\n14\n\n\n\n![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-13-0.png)\n![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-14-0.png)\n\nFigure 7: Word cloud distribution for 8 functions.\n\n\nTable 6: One example from the CogVision dataset showing a main question, its final answer, and a\nbreakdown into subquestions with answers and their corresponding cognitive function labels.\n\n\n**Example 1:**\n\n\n**Main Question** In a case-control study, the results were shown in the table below. The OR was:\nChoose one option from the following: A: 18 B: 16 C: 20D: 10\n\n\n**Answer** B\n\n\n**Subquestion** **Answer** **Cognitive Label**\n\n\n\n1. What are the values in the 2x2\n\ntable for cases and controls with\nand without a history of exposure\nin the image?\n\n\n2. What is the standard formula\nto calculate the odds ratio (OR)\nin a case-control 2x2 table?\n\n\n3. What is the odds ratio (OR)\nwhen you substitute the identified values into the formula?\n\n\n4. Which option corresponds to\nthe calculated odds ratio?\n\n\n\nCases with exposure: 400, Cases without exposure: 100, Controls with exposure: 100,\nControls without exposure: 400\n\n\nOR = (a*d) / (b*c), where a = cases with exposure, b = controls with exposure, c = cases\nwithout exposure, d = controls without expo\nsure\n\n\n(400*400)/(100*100) = 160,000/10,000 =\n16\n\n\n\nB: 16 Decision-Making\n\n\n15\n\n\n\nHigh-Level Vision\nReception\n\n\nLanguage Knowledge\nRecall\n\n\nMath Reasoning\n\n\n\n![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-14-1.png)\nTable 7: One example from the CogQA dataset showing a main question, its final answer, and a\nbreakdown into subquestions with answers and their corresponding cognitive function labels.\n\n\n**Example 2:**\n\n\n**Main Question** How can you tell that this is a prokaryote or eukaryote cell? Choose one option\nfrom the following: A: It is a prokaryote because it doesn\u2019t have a nucleus B: It is a\neukaryotic cell because it has a cell wall and a nucleus C: It is eukaryotic because\nit does not a nucleus nor a cell membrane D: It is prokaryote because it has a cell\nwall\n\n\n**Answer** B.\n\n\n**Subquestion** **Answer** **Cognitive Label**\n\n\n\n1. What visible features can be\nobserved in the cell image, such\nas the presence of boundaries or\ninternal structures?\n\n\n2. What do the observed\nfeatures (rectangular shape, defined boundaries, distinct internal\nspots) correspond to in cellular\nbiology?\n\n\n3. What cellular structures differentiate prokaryotic cells from eukaryotic cells?\n\n\n4. Based on the cell\u2019s visible\nfeatures and the definitions of\nprokaryotic and eukaryotic cells,\nwhich type of cell is shown in the\nimage?\n\n\nA.3 COGVISION STATISTICS\n\n\n\nRectangular cells with defined boundaries\nand distinct internal spots are visible.\n\n\nDefined boundaries correspond to cell walls;\ninternal spots correspond to nuclei.\n\n\nEukaryotic cells have nuclei and may have\ncell walls; prokaryotic cells lack nuclei.\n\n\n\nLow-Level Vision Reception\n\n\nVision Knowledge\nRecall\n\n\nLanguage Knowledge\nRecall\n\n\n\nB: It is a eukaryotic cell. Decision-Making\n\n\n\n![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-15-0.png)\n\nThe statistics for the CogVision dataset is shown in Table 8\n\n\n16\n\n\nTable 8: Dataset Statistics\n\n\n**Metric** **Training** **Testing**\n\n\nMain Questions 1,124 285\nSub-questions 4,603 1,141\n\n\n**Cognitive Skills Distribution**\n\n\nHigh-Level Vision Reception 1,262 (27.42%) 336 (29.45%)\nMath Reasoning 570 (12.38%) 125 (10.96%)\nSemantic Understanding 545 (11.84%) 152 (13.32%)\nInference 544 (11.82%) 126 (11.04%)\nDecision-Making 454 (9.86%) 111 (9.73%)\nLanguage Knowledge Recall 424 (9.21%) 102 (8.94%)\nVision Knowledge Recall 403 (8.76%) 96 (8.41%)\nLow-Level Vision Reception 401 (8.71%) 92 (8.06%)\n\n\nA.4 THE COGNITIVE FUNCTION DISTRIBUTION OF OTHER MODELS\n\n\nWe present the heatmaps for the remaining five models in this subsection. The results reveal a\nnotable universality in the sparsity patterns of attention heads across different architectures.\n\n\nFigure 8: The existence of cognitive heads in Qwen2.5-VL-3B responsible for eight distinct functions in complex reasoning tasks. The x-axis represents the head index, while the y-axis indicates\nthe layer index.\n\n\n17\n\n\n\n![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-16-0.png)\n![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-17-0.png)\n\nFigure 9: InternVL3-2B Heatmap\n\n\nFigure 10: InternVL3-8B Heatmap\n\n\n18\n\n\n\n![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-17-1.png)\n![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-18-0.png)\n\nFigure 11: gemma-3n-e2b-it Heatmap\n\n\nFigure 12: gemma-3n-e4b-it Heatmap\n\n\nA.5 ANNOTATIONS\n\n\nTo ensure the quality and reliability of the decomposed subQAF triplets in the CogVision dataset,\nwe design a rigorous multi-stage annotation pipeline, combining expert review and model-based\nverification. The goal is to verify the logical validity of subquestions, the correctness of their associated cognitive function labels, and the accuracy of the answers. Notably, for each subquestion, we\naim to align it with a single primary cognitive function. However, certain queries\u2014such as \u201cWhat is\nthe solvent volume and how many particles in each solution?\u201d\u2014may involve multiple abilities (e.g.,\nobject recognition and counting). In such cases, we assign the subquestion to its dominant function\nin CogVision.\n\n\n19\n\n\n\n![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-18-1.png)\n**Stage 1: Validating Subquestion Decomposition** In the first stage, we evaluate whether the generated subquestions are logically sound and align with natural human reasoning. For each QA pair,\nthree expert annotators (with backgrounds in linguistics or cognitive science) independently assess\nthe validity of each subquestion. A subquestion is marked true if it meaningfully contributes to\nanswering the main question and follows a logical reasoning trajectory. Otherwise, it is marked\nfalse.\n\n\nWe apply the following filtering criteria:\n\n\n    - **AI-Human Agreement** : If any annotator considers fewer than 60% of the subquestions\nvalid, the entire QA decomposition is discarded.\n\n    - **Inter-Annotator Agreement** : A subquestion is deemed invalid if at least two annotators\nmark it as false. If over 40% of the subquestions in a QA pair are invalid under this rule,\nthe whole QA pair is removed.\n\n\nThis filtering ensures that the retained QA decompositions follow coherent, cognitively plausible\nreasoning chains.\n\n\n**Stage 2: Verifying Cognitive Function Labels** In the second stage, annotators evaluate the correctness of the function label _fi_ assigned to each subQAF triplet ( _qi, ai, fi_ ). Three annotators independently mark each label as true or false. When discrepancies occur, annotators collaboratively reassign the correct cognitive label to ensure alignment with the underlying mental operation.\n\n\nThis step ensures that the categorization of subquestions accurately reflects established distinctions\nbetween information retrieval, semantic understanding, logical reasoning, and other cognitive pro\ncesses.\n\n\n**Stage 3: Answer Verification via Model and Human Review** In the final stage, we verify the\ncorrectness of each answer _ai_ using both automated and manual procedures. We employ the GPTo3 model (OpenAI, 2024), known for its logical reasoning capabilities, to re-evaluate GPT-4.1generated answers, and approximately 38.78% were found to be in disagreement. If GPT-o3 disagrees with GPT-4.1, it provides an alternative answer. A human annotator then compares both\nanswers and resolves discrepancies by supplying the correct one when necessary. Given the generally objective nature of answers, only one annotator is required for this task.\n\n\n**Annotation Outcome** Following this multi-stage process, we retain 1,409 validated QA pairs,\nyielding a total of 5,744 high-quality subQAF triplets.\n\n\nA.6 PROMPT FOR GENERATING COGVISION\n\n\nWe decompose the main question into subquestions through a two-step process: first, we prompt\nGPT-4.1 to generate a chain-of-thought (CoT) for the main question; second, we use the main question together with the CoT to guide the model in generating subquestions.\n\n\nHere is the prompt for generating subquestions:\n\n\n20\n\n\n\n![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-19-0.png)\n![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-20-0.png)\n\n\n\n21\n\n\nA.7 PROMPT FOR QUESTION ASKING\n\n\n\n![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-21-0.png)\n\nA.8 ABLATION STUDY\n\n\nIn the main experiments, we use the top-k generated tokens and average their multi-head attention\nvectors. Here are some examples of selected top-k tokens by llm:\n\n\n\n![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-21-1.png)\n\n\n\n![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-21-2.png)\n\n\n\nWe also explore alternative strategies for extracting representations, including using the first meaningful token, first token, last token and with or without layerbias. The corresponding results are\nshown in Table 9.\n\n\nSensitivity to the choice of k: We vary _k \u2208{_ 1 _,_ 3 _,_ 5 _}_ and compute Pearson correlations of the\nresulting attention-head heatmaps. Figure 14 shows that the heatmaps remain highly correlated\nacross choices of k, demonstrating that our method is robust to the exact number of selected tokens.\nThis robustness arises because (1) the activation patterns associated with answering a subquestion\nare reflected across multiple output tokens, and (2) VLM outputs are short, reducing variance from\ntoken choice.\n\n\n22", "metadata": {"source": "attention_functional_roles_raw.md"}, "chunk_index": 3, "token_size_config": 5000}
{"id": "attention_functional_roles_raw_chunks_5k_4", "content": "Here is the prompt for generating subquestions:\n\n\n20\n\n\n\n![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-19-0.png)\n![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-20-0.png)\n\n\n\n21\n\n\nA.7 PROMPT FOR QUESTION ASKING\n\n\n\n![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-21-0.png)\n\nA.8 ABLATION STUDY\n\n\nIn the main experiments, we use the top-k generated tokens and average their multi-head attention\nvectors. Here are some examples of selected top-k tokens by llm:\n\n\n\n![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-21-1.png)\n\n\n\n![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-21-2.png)\n\n\n\nWe also explore alternative strategies for extracting representations, including using the first meaningful token, first token, last token and with or without layerbias. The corresponding results are\nshown in Table 9.\n\n\nSensitivity to the choice of k: We vary _k \u2208{_ 1 _,_ 3 _,_ 5 _}_ and compute Pearson correlations of the\nresulting attention-head heatmaps. Figure 14 shows that the heatmaps remain highly correlated\nacross choices of k, demonstrating that our method is robust to the exact number of selected tokens.\nThis robustness arises because (1) the activation patterns associated with answering a subquestion\nare reflected across multiple output tokens, and (2) VLM outputs are short, reducing variance from\ntoken choice.\n\n\n22\n\n\nTable 9: Ablation experiment of topK tokens, first meaningful token, first token and last token, and\nwith layerbias vs without layerbias on Qwen2.5-VL-3B-Instruct.\n\n\nVision mainly Cognitive Functions Language mainly Cognitive Functions\n\n\n\nToken LayerBias Inter ~~H~~ ead\n\n\n\nLow-Level High-Level Recall Info Recall Math Inference Decision\n\n\nllm acc llm acc llm acc llm acc llm acc llm acc llm acc llm acc\n\n\n\nrandom 80.65 90.32 87.88 93.18 92.42 90.91 59.26 75.93 87.14 85.71 65.85 87.80 81.01 87.34 40.63 73.44\nFirst without\ncognitive 48.39 54.84 45.24 48.23 80.30 83.33 50.00 72.22 **0.00** **0.01** 82.93 95.12 55.70 63.29 64.06 70.31\n\n\nrandom 51.61 67.74 81.82 88.64 92.42 95.45 64.81 81.48 88.57 85.71 78.05 92.68 83.54 87.34 65.63 75.00\nwith\ncognitive 45.16 48.39 56.06 66.67 86.36 86.36 48.15 62.96 74.29 81.43 78.05 90.24 51.90 56.96 68.75 75.00\n\n\nrandom 80.65 93.54 90.15 90.91 89.39 90.91 70.37 77.78 87.14 91.43 63.41 82.93 70.89 79.75 63.19 73.75\nLast without\ncognitive 12.90 12.90 66.94 67.12 86.36 86.36 38.89 64.81 15.71 18.57 82.93 95.12 6.33 7.59 48.44 50.00\n\n\nrandom 90.32 100.0 89.39 93.18 95.45 92.42 61.11 72.22 38.57 38.57 85.37 92.68 84.81 81.61 75.00 78.13\nwith\ncognitive 67.74 64.52 89.39 93.18 67.74 64.52 **0.06** **0.02** 68.57 64.29 82.93 95.12 43.04 46.84 65.63 70.31\n\n\nrandom 77.42 80.65 81.06 83.33 84.85 86.36 68.52 59.26 75.71 88.57 51.22 65.85 77.22 79.75 73.44 70.32\nMeaning ~~f~~ irst without cognitive 77.42 87.10 50.76 54.55 84.85 75.76 50.00 53.70 2.85 4.23 85.37 95.68 72.15 79.75 57.81 75.00\n\n\nrandom 90.32 93.55 86.36 87.88 84.85 86.36 77.78 83.33 81.43 87.14 65.85 82.93 70.89 62.03 68.75 67.19\nwith\ncognitive 67.74 64.52 68.18 72.73 84.85 75.76 59.26 62.96 2.85 4.23 7.31 9.76 72.15 79.75 48.44 57.82\n\n\nrandom 83.87 90.32 82.58 83.33 84.85 86.36 64.81 70.37 87.14 94.29 78.05 92.68 58.23 91.14 78.13 90.63\nTopK without cognitive 12.90 67.74 40.15 45.45 **27.27 28.79** 55.56 46.30 38.57 42.86 78.05 85.37 29.11 32.91 57.81 75.00\n\n\nrandom 87.10 96.77 82.58 83.33 86.36 84.85 59.26 55.56 85.71 85.71 82.93 87.80 91.14 86.08 81.25 82.81\nwith\ncognitive **6.45** **6.45** **16.67 18.94** 75.76 75.76 62.96 81.48 57.14 62.86 **2.43** **2.43** **0.00** **0.00** **3.13** **4.69**\n\n\nFigure 13: Pearson Correlation of different prompt types across eight functions for Qwen2.5-VL3B-Instruct. Full Prompt: The prompt used in the main results, with given CoT, Main Question,\nCurrent Question. w/o Main Question: Prompt without main question involved.\n\n\nSensitivity to the choice of LLM: We further experimented with alternative LLMs for token selection. As shown in the examples in Table 10, different LLMs consistently select highly similar\nsemantic tokens. The Pearson correlations of the resulting attention-head heatmaps (Fig. 15) are\nlikewise very high (almost 1), indicating that modern LLMs share a strong and consistent ability to\nidentify key semantic units.\n\n\n**Prompt format:** We also examined the influence of the main question and contextual input (preceding subquestions and their answers). Fig. 13 shows that head importance maps vary noticeably\nacross these changes, highlighting the importance of including both the main question and contextual information when identifying cognitive heads.\n\n\nA.9 MORE RESULTS\n\n\nWe conducted probing experiments using 3 random seeds. The results in Fig. 16 demonstrate that\nour probing method is highly stable across seeds, with Pearson correlations of the heatmaps reaching\n1 for all eight functions in Qwen2.5-VL-3B-Instruct.\n\n\n23\n\n\n\n![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-22-0.png)\n![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-23-0.png)\n\nFigure 14: Pearson Correlation of different K of TopK token across eight functions for Qwen2.5VL-3B-Instruct.\n\n\nFigure 15: Pearson Correlation of different LLM selection across eight functions for Qwen2.5-VL3B-Instruct.\n\n\n24\n\n\n\n![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-23-1.png)\nTable 10: Examples of topk token selection using different LLMs.\n\n\nQuestion: What are the notable visual features of the couch and love seat in the image, such\nas their shape, trim, and upholstery?\nAnswer: The couch and love seat in the image are patterned after a classic, elegant style with\nintrica te detailing and a neutral color palette.\nLLM top5 token selection:\nQwen3-30B: classic, elegant, patterned, detailing, neutral\nGPT5: classic, elegant, patterned, detailing, neutral\nLlama3.3-70B: classic, elegant, patterned, detailing, color\nQuestion: What familiar object matches the shape and features of the blue item shown in the\nimage?\nAnswer: The blue item is a fire hydrant.\n\nLLM top5 token selection:\nQwen3-30B: fire, hydrant\nGPT5: fire, hydrant\nLlama3.3-70B: fire, hydrant\n\n\nFigure 16: Pearson Correlation of different classification layer initialization seed across eight functions for Qwen2.5-VL-3B-Instruct.\n\n\n**Sparse consistency across models:** The relatively high Pearson correlation coefficients between\nmodels in Fig. 17 indicate that different models exhibit consistent sparsity patterns for different\nfunctions.\n\n\nTable 11 reports the t-test results comparing random heads and cognitive heads, showing that the\ndifferences are statistically significant, (p-value _\u226a_ 0.05).\n\n\nTable 11: Welch t-test between random heads and cognitive heads.\n\n\n\n![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-24-0.png)\n\n\n\n\n\n|Models|Vision mainly Cognitive Functions Language mainly Cognitive Functions|\n|---|---|\n|Models|Low-Level<br>High-Level<br>Recall<br>Info<br>Recall<br>Math<br>Inference<br>Decision|\n|Models|p-value (t-test)<br>p-value (t-test)<br>p-value (t-test)<br>p-value (t-test)<br>p-value (t-test)<br>p-value (t-test)<br>p-value (t-test)<br>p-value (t-test)|\n|Qwen2.5-VL-3B-Instruct<br>Qwen2.5-VL-3B-Instruct<br>InternVL3-2B<br>InternVL3-8B<br>Gemma3-2B<br>Gemma3-4B|1.83e-03<br>5.71e-05<br>0.43<br>0.66<br>2.84e-03<br>4.11e-03<br>8.47e-06<br>1.39e-04<br>8.27e-05<br>0.10<br>3.01e-03<br>2.91e-05<br>7.70e-03<br>0.76<br>0.46<br>7.71e-05<br>0.02<br>0.05<br>8.58e-03<br>0.88<br>4.97e-03<br>1.49e-03<br>0.18<br>3.78e-03<br>3.98e-07<br>4.86e-06<br>2.80e-04<br>3.15e-06<br>1.19e-03<br>1.86e-05<br>1.01e-03<br>0.19<br>1.84e-06<br>0.19<br>2.32e-03<br>3.87e-03<br>1.98e-04<br>1.86e-04<br>9.43e-03<br>6.65e-05<br>5.33e-05<br>0.04<br>0.01<br>0.07<br>2.28e-04<br>0.55<br>7.19e-03<br>4.15e-05|\n\n\n25\n\n\n![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-25-0.png)\n\nFigure 17: Pearson Correlation betweeen different models across eight functions.\n\n\nFigure 18: The importance (average attention weight) of visual modality for different functional\nheads (Top 30).\n\n\nA.10 DETAILS OF ACTIVATION PATCHING\n\n\nIn both the _random_ and _mean_ activation-patch methods, we replace the cognitive heads using patches\nfrom another function. To minimize correlation between functions, we use language-related functions when patching vision functions, and vice versa. Table 12 shows the corresponding function\nmappings.\n\n\nTable 12: Function-to-Function Mapping Used for Activation Patching\n\n\n**Original Function** **Replacement Function**\n\n\nVision Knowledge Recall Decision-Making\nLanguage Knowledge Recall High-Level Vision Reception\nSemantic Understanding High-Level Vision Reception\nMath Reasoning Vision Knowledge Recall\nLow-Level Vision Reception Inference\nInference Vision Knowledge Recall\nHigh-Level Vision Reception Semantic Understanding\nDecision-Making Vision Knowledge Recall\n\n\n26\n\n\n\n![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-25-1.png)\nA.11 NEGATIVE AND POSITIVE INTERVENTION CASES\n\n\nThe examples shown below illustrates a negative intervention case, where the model initially gives\nan correct answer but produces the incorrect one (The model fails to answer in this case) after\nintervention.\n\n\n\n![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-26-0.png)\n\n\n\nThe examples shown below illustrates the positive intervention success cases and failure case, where\nthe model initially gives an incorrect answer (marked in red) but produces the correct one (marked\nin green) after intervention. This shows that adjusting the activation of cognitive heads along their\ncorresponding functional directions enhances performance on visual reasoning tasks. However, in\ncomplex scenes, positive intervention may also have negative effects by assigning more weight to\nunimportant elements, for example, focusing on tools and materials while overlooking the toilet\nsettings in the example below.\n\n\n\n![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-26-2.png)\n\n\n\n![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-26-1.png)\n\n27\n\n\n![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-27-1.png)\n\n28\n\n\n\n![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-27-0.png)\n\n![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-27-2.png)\n\n![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-27-3.png)\n![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-28-1.png)\n\n29\n\n\n\n![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-28-0.png)\n\n![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-28-2.png)\n\n![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-28-3.png)", "metadata": {"source": "attention_functional_roles_raw.md"}, "chunk_index": 4, "token_size_config": 5000}
