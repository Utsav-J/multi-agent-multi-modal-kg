{"id": "sliding_window_attention_raw_with_image_ids_with_captions_chunks_5k_0", "content": "## **Sliding Window Attention Adaptation**\n\nYijiong Yu [a], Jiale Liu [b], Qingyun Wu [b], Huazheng Wang [a], and Ji Pei [c]\n\n\naOregon State University, {yuyiji, huazheng.wang}@oregonstate.edu\nbPenn State University, {jiale.liu, qingyun.wu}@psu.edu\ncDeepSolution, research@deepsolution.chat\n\n\n\n**Abstract**\n\n\nThe self-attention mechanism in Transformer\nbased Large Language Models (LLMs) scales\nquadratically with input length, making longcontext inference expensive. Sliding window attention (SWA) reduces this cost to linear complexity, but naively enabling complete\nSWA at inference-time for models pretrained\nwith full attention (FA) causes severe longcontext performance degradation due to training\u2013inference mismatch. This makes us wonder: _Can FA-pretrained LLMs be well adapted_\n_to SWA without pretraining?_ We investigate\nthis by proposing Sliding Window Attention\nAdaptation (SWAA), a set of practical recipes\nthat combine five methods for better adaptation: (i) applying SWA only during prefilling;\n(ii) preserving \u201csink\u201d tokens; (iii) interleaving\nFA/SWA layers; (iv) chain-of-thought (CoT);\nand (v) fine-tuning. Our experiments show that\nSWA adaptation is feasible while non-trivial:\nno single method suffices, yet specific synergistic combinations effectively recover the original\nlong-context performance. We further analyze\nthe performance-efficiency trade-offs of different SWAA configurations and provide recommended recipes for diverse scenarios. Our code\n[is available at github.](https://github.com/yuyijiong/sliding-window-attention-adaptation)\n\n\n**1** **Introduction**\n\n\nTransformer-based Large Language Models\n(LLMs) (Vaswani et al., 2017) demonstrate\nremarkable capabilities, but their self-attention\nscales quadratically with the input sequence length,\nmaking long context processing inefficient. Sliding\nWindow Attention (SWA), the most straightforward and widely adopted sparse attention\npattern, which restricts each token\u2019s attention to a\nfixed-size local window, reduces the computational\ncomplexity to linearity, along with some other\nbenefits (see Appendix A).\nTo apply SWA to LLMs, typical solutions involve training a model with SWA from scratch, but\n\n\n\nare prohibitively costly and cannot match the performance of state-of-the-art full-causal-attention\n\nmodels like Qwen3 (Team, 2025b), mainly due\nto the inability to reproduce pretraining data.\nTraining-free methods like streaming attention\n(Xiao et al., 2024) can stabilize LLM outputs by retaining \u201csink tokens\u201d while applying SWA, which\ngreatly improve efficiency but inevitably suffer\nfrom severe long-context performance degradation\npossibly due to the inaccessibility of distant tokens\u2019\ninformation (Xiao, 2025). This motivates a critical,\nunexplored question: _Can a full-attention model_\n_be adapted to sliding window attention at low cost_\n_while maintaining long-context performance?_\nWe answer Yes to this question by proposing\nSliding Window Attention Adaptation(SWAA), a\nset of recipes for adapting FA-pretrained models to\nSWA, which requires neither costly pretraining nor\nmodifications to the standard Transformer architecture. Specifically, it systematically combines five\npractical and composable methods:\n\n\n1. **Full Attention (FA) Decode** : applying SWA\nonly during the prefilling stage while switching back to full attention for decoding.\n\n\n2. **Keep First** _k_ **Tokens** : explicitly preserving\nattention to the first _k_ \u201csink\u201d tokens.\n\n\n3. **Interleaving FA/SWA layers** : mix fullattention and SWA layers (e.g., assigning\nSWA to half layers).\n\n\n4. **Chain-of-Thought (CoT)** : enforcing an explicit \"thinking\" process during decoding.\n\n\n5. **Fine-tuning with SWA** : lightweight SWAaware supervised fine-tuning on long-context\ndata.\n\n\nAmong these, FA Decode is a novel method we\nintroduce. Keep First _k_ Tokens and FA/SWA Interleaving have been proven effective in prior work\n\n\n\n1\n\n\n(Xiao et al., 2024; Team, 2024a; Zhang et al., 2024),\nwhile CoT and fine-tuning are common LLM techniques. However, how these methods should be\ncombined to be actually effective for SWA adaptation remains unexplored.\nTherefore, in our experiments, we evaluate\nSWAA on Qwen3 (Team, 2025b) and Llama3.1\n(Team, 2024b) across several long-context benchmarks, measuring both performance and efficiency\nunder a wide range of SWAA recipes. First, we\nfind that each method makes a distinct contribution,\nbut no single ingredient suffices to make SWA competitive with full attention. Second, we show that\nspecific synergistic combinations of methods can\nrecover a large fraction of the original long-context\nperformance. Third, we analyze the performanceefficiency trade-offs of different SWAA recipes and\nidentify some recommended configurations suitable for different deployment scenarios.\nRather than proposing a single globally optimal\nconfiguration, we view SWAA as a flexible toolkit\nof practical recipes: practitioners can select SWAA\nrecipes that match their accuracy and efficiency\nconstraints, or compose their own SWA adaptation\nstrategies by combining the available ingredients.\nOur key contributions are:\n\n\n1. We perform the first systematic study on adapting FA-pretrained LLMs to SWA without pretraining, revealing novel insights about how\nSWA impacts LLMs and providing a foundation for future research in efficient sparse\nattention.\n\n\n2. We propose SWAA, a set of practical\nSWA adaptation recipes that offer a robust\nperformance-efficiency balance for various\nuse cases, accelerating LLM inference from\nthe bottom level.\n\n\n3. We implement our methods with FlashAttention (Dao, 2024) and vLLM (Kwon\net al., 2023), making it plug-and-play and userfriendly for practical deployment.\n\n\n**2** **Related Works**\n\n\nThe _O_ ( _N_ [2] ) complexity of self-attention in Transformers (Vaswani et al., 2017) has spurred a wide\nfield of research about more efficient language\nmodel architectures. Among the two most popular technological routes are sparse attention and\nlinear attention.\n\n\n\n**2.1** **Sparse Attention**\n\n\nOur work falls in this category. Sliding Window\nAttention (SWA) represents the most basic form\nof local sparse attention, yet its performance is inherently limited. Therefore, model architectures\nsuch as Longformer (Beltagy et al., 2020), BigBird\n(Zaheer et al., 2020), and RATTENTION (Wang\net al., 2025) combine local SWA on most tokens\nwith special global attention on specific tokens to\ncreate a more powerful, albeit still sparse, pattern.\nPopular LLMs like Gemma2 (Team, 2024a) adopt\nSWA in half of their layers to balance the efficiency\nof SWA and peformance of FA. Sliding Window\nAttention Training (SWAT) (Fu et al., 2025b) introduces architectural changes, such as sigmoid\nactivation and balanced position embeddings, to\nstabilize SWA performance. More advanced methods like Deepseek-sparse-attention (Yuan et al.,\n2025; DeepSeek-AI, 2025b), although achieving\nexcellent quality, involve more complicated implementation and optimization due to semantic-aware\nattention operations (e.g., selecting the most important tokens based on attention weights). Regardless,\nalmost all of the above methods require pretraining with a specific sparse pattern, which is costly\nand fails to leverage the advantages of existing pretrained models.\n\n\nLightTransfer (Zhang et al., 2024) is a promising\nattempt at adapting existing models to SWA without pretraining, which has the same motivation as\nours. But it may generalize poorly across model\nfamilies (see Appendix G).\n\n\n**2.2** **Linear Attention**\n\n\nAn alternative approach involves reformulating\nthe attention mechanism entirely to achieve linear, _O_ ( _N_ ), complexity. This includes methods\nsuch as RNN-like linear attention transformers\n\n(Katharopoulos et al., 2020; Peng et al., 2023;\nSun et al., 2023) and structured state-space models\n(SSMs) like Mamba (Gu and Dao, 2023). Many\nworks such as Jamba and Nemotron-Flash(Lieber\net al., 2024; Linsong Chu et al., 2024; Team et al.,\n2025; Fu et al., 2025a) interleave linear attention\nlayers with traditional attention layers to create hybrid model structures. While promising, they represent a fundamental architectural departure from the\nstandard Transformer, and their performance is generally weaker than traditional Transformer-based\n\nLLMs.\n\n\n\n2\n\n\n**3** **Candidate Methods for SWA**\n\n**Adaptation**\n\n\nAs established, a naive application of SWA leads\nto severe long-context performance degradation.\nTherefore, we investigate five methods that can potentially facilitate SWA adaptation from distinct\nperspectives. However, every method except finetuning has some drawbacks to LLM inference efficiency, as discussed in Appendix A. Therefore,\nalthough these methods are not mutually exclusive,\nwe should not indiscriminately adopt all of them.\nInstead, we must evaluate various combinations to\nidentify the optimal trade-off between performance\nand efficiency.\n\n\n**3.1** **Full Attention Decode**\n\n\nThis method applies SWA **only** to the prefilling\nstage. During the decoding (auto-regressive generation) stage, each token still employs full attention,\nallowing access to all previous tokens in the context. The resulting attention mask is depicted in\nFigure 1a.\nThis novel approach, first proposed in our study,\nis inspired by human reading comprehension: humans typically scan a passage casually (prefilling)\nbefore thinking deeply to formulate an answer (decoding) for a specific problem. We term this strategy \"reading casually, thinking carefully.\" In our\ndesign, the SWA-constrained prefilling stage corresponds to casual reading, while the full-attention\ndecoding stage enables careful thinking. This analogy also suggests that Chain-of-Thought (CoT)\nduring decoding may be particularly beneficial, as\nextended generation (i.e. \"thinking\") could compensate for the insufficient contextual information\ngathered during the prefilling stage.\n\n\n(a) FA Decode (b) Keep First\n\n\nFigure 1: (a) Attention mask for FA Decode. SWA is\nused for prompt tokens (prefill), and full attention is\nused for generated tokens (decode). (b) Attention mask\nfor SWA combined with Keep First _k_ Tokens.\n\n\n\n**3.1.1** **Keep First k Tokens**\n\nXiao et al. demonstrate that models pretrained with\nfull attention allocate a disproportionate amount\nof attention to the initial tokens (\"attention sink\"),\nand removing the visibility of these tokens causes\nperformance collapse. Their solution, streaming\nattention, involves permanently retaining the attention to these \"sink\" tokens while using SWA, which\nsuccessfully maintains the stability of the attention\ndistribution and the model\u2019s output. Our method\nis basically the same: as shown in Figure 1b, any\nsubsequent token can attend to its local window\n**and** the initial _k_ tokens.\n\nNotably, however, our method extends beyond\nits original version. Streaming attention operates\nonly at the KV cache level; specifically, the KV\ncache of the sink tokens is retained during decoding, while the prefilling stage is not modified or accelerated at all. In contrast, we directly customize\nthe Flash-Attention-2 (Dao, 2024) kernel to implement such attention mask, thereby accelerating\nprefilling via SWA as well, and eliminating the\nneed to modify KV cache.\n\n\n**3.2** **Interleaving Layers**\n\n\nThis method retains full attention on a subset of\n\nlayers while applying SWA to the remainder, providing a simple hybrid mechanism to balance the\nperformance of full attention with the efficiency\nof pure SWA. A common strategy involves designating one in every _n_ layers to use full attention\n(e.g., layers 0, 2, 4, . . . retain full attention, while all\nothers use SWA). For example, Gemma-2 (Team,\n2024a) uses SWA only for layers [1, 3, 5, . . . ], and\nGemma-3 (Team, 2025a) uses SWA only for layers\n\n[5, 11, 17, ...].\nHowever, for an FA-pretrained model, layers\nmay exhibit distinct behaviors, suggesting it may\nnot be optimal to simply assign SWA to layers [1,\n3, 5, ...]. Instead, it might be preferable to use\nstatistical features to select \"lazy\" (mostly focusing\non just recent tokens) layers, as adopted by LightTransfer (Zhang et al., 2024). However, we find\nthat LightTransfer is not consistently superior in\npractice (see Appendix G). Therefore, for simplicity, we still choose the simplest fixed-interval layer\nselections in our experiments, such as [0, 2, 4, . . . ]\nand [1, 3, 5, ...].\n\n\n**3.3** **Chain-of-Thought**\n\n\nChain-of-Thought (CoT) (Wei et al., 2022) is a\nwidely used technique for improving model ac\n\n\n3\n\n\n\n\n```json\n\"img_sliding_window_attention_2_0\": {\n    \"path\": \"E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/sliding_window_attention.pdf-2-0.png\",\n    \"page\": 2,\n    \"section\": \"3.3 Chain-of-Thought\",\n    \"image_relevance\": \"high\",\n    \"image_type\": \"diagram\",\n    \"semantic_role\": \"explains\",\n    \"caption\": \"The diagram illustrates a computational workflow segmented into two distinct phases: 'Prefill' and 'Decode'. The upper triangular section, labeled 'Prefill', shows a pattern of green and white cells, representing an initial processing stage. The lower section, labeled 'Decode', consists of uniformly blue cells in a triangular arrangement, indicating a subsequent processing or generation stage.\",\n    \"depicted_concepts\": [\n      \"Prefill phase\",\n      \"Decode phase\",\n      \"computational workflow\",\n      \"triangular matrix structure\",\n      \"processing stages\"\n    ],\n    \"confidence\": \"high\"\n}\n```\n\n\n```json\n\"img_sliding_window_attention_2_1\": {\n  \"path\": \"E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/sliding_window_attention.pdf-2-1.png\",\n  \"page\": 2,\n  \"section\": \"3.3 Chain-of-Thought\",\n}\n```\ncuracy via reasoning. With the advent of \"thinking\" models, such as DeepSeek-R1 (DeepSeek-AI,\n2025a), CoT has evolved from a prompting strategy\nto an intrinsic LLM capability. However, whether\nCoT has a specific impact in SWA scenarios remains uninvestigated. We explore this by comparing a thinking model with its non-thinking variant,\ne.g., Qwen3-4B-Thinking and Qwen3-4B-Instruct,\nto verify the effect of CoT on SWA adaptation,\nwhich can produce more notable differences compared to simply adding CoT prompting to the same\nmodel.\n\n\n**3.4** **Fine-tuning**\n\n\nThis is the most natural way to mitigate traininginference mismatch. Apparently, the model should\nbe fine-tuned while SWA is applied, so that the\nmodel\u2019s parameters can be trained to better adapt\nto SWA. Meanwhile, the training data must be longcontext tasks, where SWA actually works.\nHowever, most available long-context datasets\nonly contain brief ground-truth answers, lacking the reasoning steps required for fine-tuning\na \"thinking\" model. Since our goal is to _restore_\nthe model\u2019s original capabilities under SWA rather\nthan teach it new ones, instead of directly using the\noriginal dataset, we adopt an approach similar to\nself-distillation (Yang et al., 2024). Specifically, we\nutilize the original full-attention model to generate\nnew answers for the dataset\u2019s questions, and these\ngenerated answers are then filtered for correctness\nusing GPT-5-Mini (OpenAI, 2025), to make up our\ntraining dataset. For each question, we sample 4\nanswers with temperature 1, because we find this\nstrategy is slightly better than generating only one\nanswer with temperature 0.\n\n\n**4** **Experiment Setup**\n\n\nWe organize our experiments around three research\nquestions:\n\n\n**RQ1: Is SWA adaptation feasible without any**\n**additional training?** We evaluate whether\nan FA LLM can be adapted to SWA using\nonly inference-time modifications, and which\ncombinations of techniques are necessary.\n\n\n**RQ2: How much does fine-tuning with SWA im-**\n**prove performance?** We study the effect of\nSWA-aware fine-tuning on long-context performance and identify which components of\nSWAA are still required.\n\n\n\n**RQ3: Which SWAA configurations achieves the**\n**optimal performance-efficiency trade-offs?**\nWe evaluate how different SWAA configurations trade off accuracy against inference la\ntency.\n\n\n**4.1** **Models**\n\n\nOur primary experiments use Qwen3-4B-Thinking\nand Qwen3-4B-Instruct (Team, 2025b). The Thinking variant enforces chain-of-thought (CoT) style\nreasoning, whereas the Instruct variant usually just\nanswers briefly. To ensure generality, we additionally evaluate Qwen3-30B-A3B-Thinking, Qwen330B-A3B-Instruct (Team, 2025b), and Llama3.18B-Instruct (Touvron et al., 2023).\n\nAll models are served with vLLM in float16\n\nprecision using a batch size of 64. We use greedy\ndecoding (temperature = 0) for all evaluations. In\npreliminary experiments, we observed that vLLM\nyields slightly lower (about 1% to 5%) scores\nthan HuggingFace Transformers due to precisionrelated discrepancies.\n\n\n**4.2** **Evaluation Dataset**\n\n\nSWA is identical to full attention when the context\n\nlength is within the window size. Even if the model\nis fine-tuned, we can pre-calculate the prompt\nlength and simply disable the LoRA adapters for\nshort prompts to get completely the same response\nas the original model. Therefore, our experiments\nfocus exclusively on long-context benchmarks with\ninputs exceeding 16k tokens, as re-evaluating models on standard short-context benchmarks (e.g.,\nMMLU (Hendrycks et al., 2021), GPQA (Rein\net al., 2023)) is completely unnecessary.\nSince we find other long-context benchmarks are\neither too easy or too difficult for 4B-level models (see Appendix B), we ultimately select LongMemEval (Wu et al., 2024), a benchmark consisting of various types of long-context QA tasks with\nmoderate difficulty, although it is originally designed for agent memory system evaluation. Its\ncontext length is controllable by selecting a specific number of chat sessions to concatenate as the\ncontext from a pool of hundreds of sessions (a session contains the chat history between user and\nassistant within a day). To create a moderately difficult and discriminative evaluation, we construct\n**LongMemEval_24k** by sampling 10 sessions, resulting in 500 samples ranging from 16k to 32k\nwith an average context length of 24k.\n\n\n\n4\n\n\nFor additional validation of generalizability, as\nshown in Appendix D, we also experiment on\nLongBench-V2 (Bai et al., 2024b), a more modern and challenging benchmark that requires deep\nreasoning across various real-world tasks.\n\n\n**4.3** **Training Details**\n\n\nFor the fine-tuning dataset, we initially considered LongAlign (Bai et al., 2024a), a widely used\nlong-context fine-tuning dataset for adapt a regularlength model to long-context tasks. However, since\n\n_\u223c_\nits sample count ( 10,000) is insufficient, we incorporate an additional 6,000 samples from Fusangv1-long (Pan, 2024), a more comprehensive corpus\nof over 40,000 long-context samples that includes\nLongAlign as a subset.\nWe perform SWA-aware fine-tuning using LoRA\n(Hu et al., 2022). Unless otherwise noted, we use\nrank _r_ = 16 and _\u03b1_ = 128, and apply LoRA only\nto the query, key, and value projection modules.\nWe adopt this parameter-efficient setting because\nfull-parameter fine-tuning often leads to overfitting\nand degradation of the model\u2019s original capabilities\nin our preliminary experiments. We use a learning\nrate of 1e-4 with a cosine decay schedule. Models\nare fine-tuned for a single epoch on the sampled\nlong-context dataset since we observe no meaningful gains from additional epochs (see Appendix F).\nOnce training takes approximately 12 hours on an\n8*H20 GPU server for Qwen3-4B and 30 hours for\nQwen3-30B-A3B.\n\n\n**5** **Experiment Results**", "metadata": {"source": "sliding_window_attention_raw_with_image_ids_with_captions.md"}, "chunk_index": 0, "token_size_config": 5000}
{"id": "sliding_window_attention_raw_with_image_ids_with_captions_chunks_5k_1", "content": "4\n\n\nFor additional validation of generalizability, as\nshown in Appendix D, we also experiment on\nLongBench-V2 (Bai et al., 2024b), a more modern and challenging benchmark that requires deep\nreasoning across various real-world tasks.\n\n\n**4.3** **Training Details**\n\n\nFor the fine-tuning dataset, we initially considered LongAlign (Bai et al., 2024a), a widely used\nlong-context fine-tuning dataset for adapt a regularlength model to long-context tasks. However, since\n\n_\u223c_\nits sample count ( 10,000) is insufficient, we incorporate an additional 6,000 samples from Fusangv1-long (Pan, 2024), a more comprehensive corpus\nof over 40,000 long-context samples that includes\nLongAlign as a subset.\nWe perform SWA-aware fine-tuning using LoRA\n(Hu et al., 2022). Unless otherwise noted, we use\nrank _r_ = 16 and _\u03b1_ = 128, and apply LoRA only\nto the query, key, and value projection modules.\nWe adopt this parameter-efficient setting because\nfull-parameter fine-tuning often leads to overfitting\nand degradation of the model\u2019s original capabilities\nin our preliminary experiments. We use a learning\nrate of 1e-4 with a cosine decay schedule. Models\nare fine-tuned for a single epoch on the sampled\nlong-context dataset since we observe no meaningful gains from additional epochs (see Appendix F).\nOnce training takes approximately 12 hours on an\n8*H20 GPU server for Qwen3-4B and 30 hours for\nQwen3-30B-A3B.\n\n\n**5** **Experiment Results**\n\n\n**5.1** **SWA Adaptation Without Fine-tuning**\n\n\nWe first study SWA adaptation without any additional training. Table 1 reports LongMemEval_24k\naccuracy for Qwen3-4B-Thinking (\"think\") and\nQwen3-4B-Instruct (\"non-think\") under different\ncombinations of SWAA components. In most settings, we use an aggressive 2k window to amplify\nthe impact of SWA. The configurations are ranked\nby the number of methods applied (0, 1, 2, or 3 of\nInterleaving Layers, Keep First and FA Decode).\nRows 1 (original model) and 2 (naive SWA) serve\nas upper and lower baselines, respectively. In the\ncolumn \"FA layers\", the value records which layers use full attention, and [] means all the layers\nuse SWA, i.e., this method is not enabled. In the\ncolumn \"keep first\", the value is _k_ in Keep First\n_k_ Tokens. When comparing results, an accuracy\ndifference of less than 5% is usually considered\n\n\n\nstatistically insignificant. From the results, we find\nthat:\n\n\n**Naive SWA is not viable.** Naively replacing FA\nwith a 2k sliding window attention (row 1) drops\naccuracy significantly to 3 _._ 2 and 11 _._ 0, respectively.\nEven with an 8k window (row 2), accuracy only recovers to 13 _._ 2 and 19 _._ 8, far below the FA baseline.\n\n\n**Single method helps, but cannot close the gap.**\nEach method\u2014Keep First, FA Decode, or Interleaving Layers\u2014improves over naive SWA (rows 3\u20136),\nyet each alone recovers only a small fraction of the\nFA gap and remains well below the baseline. In\nshort, no single method is sufficient.\n\n\n**Combinations exhibit strong synergy.** Recipes\nthat combine multiple methods deliver large gains:\n\n\n  - **FA Decode + Keep First** _k_ substantially improves over naive SWA (rows 7\u20139), recovering roughly half to two-thirds of the gap on\nthe thinking model as _k_ increases. However,\nincreasing _k_ from 100 to 1000 yields almost\nno improvement, indicating that _k_ does not\nneed to be exceedingly large.\n\n\n  - **Interleaving Layers + FA Decode** is\nmarkedly stronger (row 13), recovering most\nof the gap for the thinking model.\n\n\n  - **FA Decode + Interleaving Layers + Keep**\n**First** _k_ perform best (rows 18). The thinking\nmodel recovers close to 90% of the FA gap\neven at 2k window.\n\n\n**CoT synergizes with FA Decode.** Under recipes\nthat include FA Decode, the thinking model consistently benefits more than the non-thinking model\n(rows 13 and 18), suggesting that CoT synergizes\nwith FA Decode: preserving global attention at\ndecoding time enables longer reasoning traces to\ncapitalize on context processed by SWA, confirming our hypothesis in Section 3.1.\n\n\n**Sliding window size affects, but is not the deci-**\n**sive role.** With FA Decode + Keep First _k_, accuracy improves as the window increases (rows 7,\n14, 15), though benefits are modest until 8k. When\nadded with interleaving FA layers, moving from a\n2k to a 4k window is enough to close the remaining\ngap on the thinking model (row 22 matches the FA\nbaseline), indicating that FA Decode and Interleaving are the dominant drivers, and larger windows\nmainly smooth residual error.\n\n\n\n5\n\n\nTable 1: Qwen3-4B-Thinking and Qwen3-4B-Instruct results on LongMemEval without SFT\n\n\n**No.** **window size** **FA layers** **keep first** **FA decode** **Acc think** **Acc non-think**\n\n\n0 Full [] 0 False **73.0** **62.0**\n1 2k [] 0 False 3.2 11.0\n2 8k [] 0 False 13.2 19.8\n\n\n3 2k [] 10 False 16.0 15.6\n4 2k [] 0 True 11.8 14.2\n5 2k [1, 3, 5, ...] 0 False 13.4 18.4\n6 8k [] 0 True 26.2 25.0\n\n\n7 2k [] 10 True 38.2 20.6\n8 2k [] 100 True 50.0 17.8\n9 2k [] 1000 True 50.0 20.2\n10 2k [0, 2, 4, ...] 10 False 17.0 14.8\n11 2k [0, 2, 4, ...] 0 True 32.2 26.0\n12 2k [1, 3, 5, ...] 10 False 25.8 36.4\n13 2k [1, 3, 5, ...] 0 True 59.2 34.8\n14 4k [] 10 True 38.0 24.4\n15 8k [] 10 True 49.2 35.2\n\n\n16 2k [0, 2, 4, ...] 10 True 36.0 17.2\n17 2k [1, 3, 5, ...] 10 True 65.0 **53.6**\n18 2k [1, 3, 5, ...] 100 True **68.8** 50.6\n19 2k [1, 5, 9, ...] 10 True 53.2 31.4\n20 2k [1, 9, 17, ...] 10 True 36.4 18.8\n21 2k [3, 7, 11, ...] 10 True 54.2 34.6\n22 4k [1, 3, 5, ...] 100 True **73.0** **54.2**\n23 8k [1, 3, 5, ...] 100 True **71.6** **56.6**\n\n\n\n**SWA/FA layer selection has large impacts.** Selecting less layers for FA, e.g. only one fourth or\neighth (row 19, 20), though more efficient, will\nsignificantly decrease the improvement brought by\nInterleaving Layers. More importantly, for Qwen34B, configuring _odd-numbered_ layers with full attention is significantly better than _even-numbered_\nlayers (row 11, 13, 16, 17). However, surprisingly,\nthis result is reversed for Qwen3-30B-A3B and\nLlama3.1-8B-Instruct (row 10 and 11 in Table 4\nand 5). This suggests that layer functionalities differ across model families and sizes, necessitating\nmodel-specific layer selection strategies, as discussed in Section 3.2.\n\n\nTherefore, we answer RQ1: adapting an FA\nLLM to SWA is feasible even without any training. But it requires specific combinations of at\nleast 2 methods, which could be less efficient for\ninference.\n\n\n\n**5.2** **SWA Adaptation With Fine-tuning**\n\n\nWe next evaluate SWA-aware supervised finetuning, which is expected to provide higher improvement. Table 2 reports LongMemEval_24k\naccuracy after SFT under various SWAA configurations. The original full-attention model is also\nfine-tuned as a baseline (row 0). Since training is\nrelatively time-consuming, we only test a representative subset of configurations. Our findings are as\nfollows:\n\n\n**Fine-tuning substantially lifts all SWA config-**\n**urations.** Comparing all the fine-tuning results\nwith non-training ones, it is clear that fine-tuning\nconsistently provides great improvement. However,\nsimply fine-tuning with naive SWA remains insufficient, only achieving 18.8% and 23.8% accuracy\n(row 1).\n\n\n**FA Decode and Interleaving Layers emerge as**\n**dominant components.** After SFT, FA Decode\nand Interleaving Layers provide the largest gains.\n\n\n\n6\n\n\nTable 2: Qwen3-4B-Thinking and Qwen3-4B-Instruct results on LongMemEval with SFT\n\n\n**No.** **window size** **FA layers** **keep first** **FA decode** **Acc think** **Acc non-think**\n\n\n0 Full [] 0 True **74.6** **63.4**\n1 2k [] 0 False 18.8 23.8\n\n\n2 2k [] 10 False 15.6 /\n3 2k [] 0 True 57.9 42.0\n4 2k [1, 3, 5, ...] 0 False 63.6 54.6\n5 4k [] 0 True 62.6 /\n\n\n6 2k [] 10 True 56.7 /\n7 2k [] 100 True 62.2 42.6\n8 2k [1, 3, 5, ...] 0 True **73.2** **58.8**\n9 2k [0, 2, 4, ...] 0 True **66.0** /\n10 2k [1, 5, 9, ...] 0 True **68.8** 47.0\n\n\n11 2k [1, 3, 5, ...] 100 True **73.2** **61.4**\n\n\nTable 3: Recommended SWA adaptation recipes for different needs and scenarios. \ufffd means optional.\n\n\n**Training** ? **Thinking Model?** **Prefer?** **FA Decode** **Interleaving Layers** **Keep First**\n\n\nNo No Any \ufffd \ufffd \ufffd\nNo Yes Efficiency \ufffd \ufffd \ufffd\nNo Yes Accuracy \ufffd \ufffd \ufffd\n\n\nYes Any Efficiency \ufffd \ufffd \ufffd\nYes Any Accuracy \ufffd \ufffd \ufffd\n\n\n\nUsing FA Decode alone (row 3) or using Interleaving Layers alone (row 4) both get high accuracy.\nAnd combining both (row 8) further boosts performance to 73 _._ 2 (think) and 58 _._ 8 (non-think), nearly\nmatching the full-attention SFT baseline in row 0.\n\n\n**Keep First becomes optional rather than essen-**\n**tial.** Before fine-tuning, Keep First is crucial for\nstability under SWA. But after SFT, it only provides\nminor additional improvement. With FA Decode,\nadding _k_ = 100 (row 7) improves over _k_ = 0\n(row 3) only 4.5%, and if further combining FA\nlayers, it almost offers no improvements (row 11\nand row 8).\n\n\n**Effect of sliding window size.** Row 3 and 5\nshows that increasing the window from 2k to 4k\nwith FA Decode improves thinking-model accuracy from 57 _._ 9 to 62 _._ 6. This mirrors the non-SFT\ntrend that larger windows help, but the dominant\nimprovements still come from FA Decode and Interleaving Layers.\n\nSo, we answer RQ2: fine-tuning brings remarkably high performance restoration, provided we ap\n\n\nply **FA Decode**, **Interleaving Layers**, or a combination thereof, while **Keep First** becomes optional.\nAnd the improvement brought by SFT under each\nconfiguration varies significantly, meaning a nearoptimal training-free configuration need not remain\noptimal after SFT, and vice versa.\n\n\n**5.3** **Performance\u2013efficiency Trade-offs and**\n**Recommended Recipes**\n\n\nAlthough integrating more methods can typically\nachieve higher accuracy, it introduces more overhead, indicating that the efficiency of each recipe\nmust also be evaluated. To assess the performanceefficiency trade-off of different SWAA configurations, we evaluate time-to-first-token (TTFT), timeper-output-token (TPOT), total throughput, and average running time per request. Concretely, we\nbenchmark Qwen3-4B-Thinking on a single H100\nGPU using vLLM\u2019s bench_serve utility (Kwon\net al., 2023) with random input data and 100 total\nrequests. The prompt length and output length are\nset to 128k and 512 tokens, respectively, representing a typical long-context QA setting.\n\n\n\n7\n\n\n\n```json\n\"img_sliding_window_attention_7_0\": {\n    \"path\": \"E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/sliding_window_attention.pdf-7-0.png\",\n    \"page\": 7,\n    \"section\": \"5.3 Performance\u2013efficiency Trade-offs and\",\n    \"image_relevance\": \"high\",\n    \"image_type\": \"plot\",\n    \"semantic_role\": \"supports_result\",\n    \"caption\": \"A scatter plot illustrates the relationship between Accuracy (%) on the Y-axis and Time (s) on the X-axis for various model configurations. Data points are differentiated by shape, indicating results 'w/ SFT' (circles) or 'w/o SFT' (squares), and by color, representing combinations of 'Window' (Full, 2k), 'FA Layers' (0, 1/4, 1/2), and 'FA Decode' (False, True) parameters.\",\n    \"depicted_concepts\": [\n      \"Accuracy\",\n      \"Time\",\n      \"SFT\",\n      \"Window parameter\",\n      \"FA Layers parameter\",\n      \"FA Decode parameter\",\n      \"Scatter plot\"\n    ],\n    \"confidence\": \"high\"\n}\n```\n\n\n```json\n\"img_sliding_window_attention_7_1\": {\n  \"path\": \"E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/sliding_window_attention.pdf-7-1.png\",\n  \"page\": 7,\n  \"section\": \"5.3 Performance\u2013efficiency Trade-offs and\",\n}\n```\n\n(a) Qwen3-4B-Thinking (b) Qwen3-4B-Instruct\n\n\nFigure 2: Accuracy and inference time of each configuration of Qwen3-4B on LongMemEval\n\n\n\nTo visualize the performance-efficiency tradeoff, Figure 2 plots each configuration\u2019s accuracy\non LongMemEval_24k (Wu et al., 2024) against its\naverage running time, while detailed TTFT, TPOT,\nand throughput statistics for each configuration are\nprovided in Appendix E. We draw a line between\nthe full-attention point and the naive-SWA point\nas a baseline curve: configurations above this line\noffer a better accuracy-latency balance intuitively.\nFor configurations with nearly identical time costs,\nwe display only the one with the highest accuracy.\nSince **Keep First** _k_ has negligible impact on runtime (Appendix E), all plotted configurations fix\n_k_ = 10.\n\n\nFrom Figure 2, we observe that many configurations in Figure 2 achieve a clearly better\nperformance-efficiency ratio than baselines. And\nfor the thinking model, more points lie above the\nbaseline curve compared to non-thinking, indicating that **CoT** generally has a positive effect on improving the performance-efficiency ratio of SWAA.\n\n\nThus, we finally answer RQ3: many SWAA\nconfigurations all reach excellent performanceefficiency trade-off, but there is no single metric to\nquantify such trade-off to decide the globally optimal one. We therefore summarize **recommended**\n\n**SWA adaptation recipes** tailored to various deployment scenarios in Table 3. And we must note\nthat specific parameters should be flexibly set to\nmeet application-specific requirements, without the\nneed to follow our experimental parameters (e.g.,\n\n\n\na 2k window, _k_ = 10). For example, you can increase the window size to 4k or _k_ to 128 for higher\naccuracy and acceptable additional overhead.\n\n\n**6** **Conclusion**\n\n\nIn this work, we validate the feasibility of adapting\nfull-attention pretrained LLMs to Sliding Window\nAttention (SWA) for better efficiency, offering a\ncost-effective alternative that avoids training sparseattention models from scratch. By systematically\ndeconstructing the adaptation process, we identify that the catastrophic degradation observed in\nnaive implementations can be effectively mitigated\nthrough synergistic combinations of auxiliary methods. Our extensive experiments across the Qwen\nand Llama families demonstrate that while trade\noffs between computational overhead and model\nperformance are inevitable, optimized configurations can get an excellent performance-efficiency\nbalance.\n\n\n**7** **Limitations**\n\n\nWe speculate that the ideal reasoning trajectory of\nthe model adapted to SWA should be longer than\nthe original model, to compensate for the information loss caused by SWA. That means, using\nthe answers generated by the original model as\nfine-tuning data may not be the optimal training\nmethod. Rather, RL methods like GRPO (Shao\net al., 2024) might further help the model adapted\nto SWA learn a better reasoning trajectory. How\n\n\n8\n\n\never, we did not experiment with them since they\nare too time-consuming and unstable.\nWe have not yet implemented the KV cache eviction (or overwriting) mechanism when using SWA;\nthat is, although the speed is improved, memory\nusage is not effectively reduced.\nFurther experiments may be needed to confirm\nwhether our conclusions generalize to larger model\nsizes, such as 70B.\n\n\n**References**\n\n\nYushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei\nHou, Jie Tang, Yuxiao Dong, and Juanzi Li. 2024a.\n[Longalign: A recipe for long context alignment of](https://arxiv.org/abs/2401.18058)\n[large language models.](https://arxiv.org/abs/2401.18058)\n\n\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu,\nJiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao\nLiu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang,\n[and Juanzi Li. 2023. Longbench: A bilingual, multi-](https://arxiv.org/abs/2308.14508)\n[task benchmark for long context understanding.](https://arxiv.org/abs/2308.14508)\n\n\nYushi Bai, Shangqing Tu, Jiajie Zhang, Hao Peng, Xiaozhi Wang, Xin Lv, Shulin Cao, Jiazheng Xu, Lei\nHou, Yuxiao Dong, Jie Tang, and Juanzi Li. 2024b.\n[Longbench v2: Towards deeper understanding and](https://arxiv.org/abs/2412.15204)\n[reasoning on realistic long-context multitasks.](https://arxiv.org/abs/2412.15204)\n\n\nIz Beltagy, Matthew E. Peters, and Arman Cohan. 2020.\n\n[Longformer: The long-document transformer.](https://arxiv.org/abs/2004.05150)\n\n\n[Tri Dao. 2024. Flashattention-2: Faster attention with](https://openreview.net/forum?id=mZn2Xyh9Ec)\n[better parallelism and work partitioning.](https://openreview.net/forum?id=mZn2Xyh9Ec) In _The_\n_Twelfth International Conference on Learning Rep-_\n_resentations, ICLR 2024, Vienna, Austria, May 7-11,_\n_2024_ . OpenReview.net.\n\n\n[DeepSeek-AI. 2025a. Deepseek-r1: Incentivizing rea-](https://arxiv.org/abs/2501.12948)\n[soning capability in llms via reinforcement learning.](https://arxiv.org/abs/2501.12948)\n\n\n[DeepSeek-AI. 2025b. Deepseek-v3.2: Pushing the fron-](https://arxiv.org/abs/2512.02556)\n[tier of open large language models.](https://arxiv.org/abs/2512.02556)", "metadata": {"source": "sliding_window_attention_raw_with_image_ids_with_captions.md"}, "chunk_index": 1, "token_size_config": 5000}
{"id": "sliding_window_attention_raw_with_image_ids_with_captions_chunks_5k_2", "content": "Yushi Bai, Shangqing Tu, Jiajie Zhang, Hao Peng, Xiaozhi Wang, Xin Lv, Shulin Cao, Jiazheng Xu, Lei\nHou, Yuxiao Dong, Jie Tang, and Juanzi Li. 2024b.\n[Longbench v2: Towards deeper understanding and](https://arxiv.org/abs/2412.15204)\n[reasoning on realistic long-context multitasks.](https://arxiv.org/abs/2412.15204)\n\n\nIz Beltagy, Matthew E. Peters, and Arman Cohan. 2020.\n\n[Longformer: The long-document transformer.](https://arxiv.org/abs/2004.05150)\n\n\n[Tri Dao. 2024. Flashattention-2: Faster attention with](https://openreview.net/forum?id=mZn2Xyh9Ec)\n[better parallelism and work partitioning.](https://openreview.net/forum?id=mZn2Xyh9Ec) In _The_\n_Twelfth International Conference on Learning Rep-_\n_resentations, ICLR 2024, Vienna, Austria, May 7-11,_\n_2024_ . OpenReview.net.\n\n\n[DeepSeek-AI. 2025a. Deepseek-r1: Incentivizing rea-](https://arxiv.org/abs/2501.12948)\n[soning capability in llms via reinforcement learning.](https://arxiv.org/abs/2501.12948)\n\n\n[DeepSeek-AI. 2025b. Deepseek-v3.2: Pushing the fron-](https://arxiv.org/abs/2512.02556)\n[tier of open large language models.](https://arxiv.org/abs/2512.02556)\n\n\nYonggan Fu, Xin Dong, Shizhe Diao, Hanrong Ye, Wonmin Byeon, Yashaswi Karnati, Lucas Liebenwein,\nMaksim Khadkevich, Alexander Keller, Jan Kautz,\net al. 2025a. Nemotron-flash: Towards latencyoptimal hybrid small language models. In _The Thirty-_\n_ninth Annual Conference on Neural Information Pro-_\n_cessing Systems_ .\n\n\nZichuan Fu, Wentao Song, Yejing Wang, Xian Wu,\nYefeng Zheng, Yingying Zhang, Derong Xu, Xuetao\n[Wei, Tong Xu, and Xiangyu Zhao. 2025b. Sliding](https://arxiv.org/abs/2502.18845)\n[window attention training for efficient large language](https://arxiv.org/abs/2502.18845)\n[models.](https://arxiv.org/abs/2502.18845)\n\n\n[Albert Gu and Tri Dao. 2023. Mamba: Linear-time](https://arxiv.org/abs/2312.00752)\n[sequence modeling with selective state spaces.](https://arxiv.org/abs/2312.00752)\n\n\n\nDan Hendrycks, Collin Burns, Steven Basart, Andy\nZou, Mantas Mazeika, Dawn Song, and Jacob Stein[hardt. 2021. Measuring massive multitask language](https://openreview.net/forum?id=d7KBjmI3GmQ)\n[understanding. In](https://openreview.net/forum?id=d7KBjmI3GmQ) _9th International Conference on_\n_Learning Representations, ICLR 2021, Virtual Event,_\n_Austria, May 3-7, 2021_ . OpenReview.net.\n\n\nCheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang,\n[and Boris Ginsburg. 2024. Ruler: What\u2019s the real](https://arxiv.org/abs/2404.06654)\n[context size of your long-context language models?](https://arxiv.org/abs/2404.06654)\n\n\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\n[Weizhu Chen. 2022. Lora: Low-rank adaptation of](https://openreview.net/forum?id=nZeVKeeFYf9)\n[large language models. In](https://openreview.net/forum?id=nZeVKeeFYf9) _The Tenth International_\n_Conference on Learning Representations, ICLR 2022,_\n_Virtual Event, April 25-29, 2022_ . OpenReview.net.\n\n\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pap[pas, and Fran\u00e7ois Fleuret. 2020. Transformers are](http://proceedings.mlr.press/v119/katharopoulos20a.html)\n[rnns: Fast autoregressive transformers with linear](http://proceedings.mlr.press/v119/katharopoulos20a.html)\n[attention. In](http://proceedings.mlr.press/v119/katharopoulos20a.html) _Proceedings of the 37th International_\n_Conference on Machine Learning, ICML 2020, 13-18_\n_July 2020, Virtual Event_, volume 119 of _Proceedings_\n_of Machine Learning Research_, pages 5156\u20135165.\nPMLR.\n\n\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying\nSheng, Lianmin Zheng, Cody Hao Yu, Joseph E.\n[Gonzalez, Hao Zhang, and Ion Stoica. 2023. Effi-](https://arxiv.org/abs/2309.06180)\n[cient memory management for large language model](https://arxiv.org/abs/2309.06180)\n[serving with pagedattention.](https://arxiv.org/abs/2309.06180)\n\n\nOpher Lieber, Barak Lenz, Hofit Bata, Gal Cohen,\nJhonathan Osin, Itay Dalmedigos, Erez Safahi,\nShaked Meirom, Yonatan Belinkov, Shai ShalevShwartz, Omri Abend, Raz Alon, Tomer Asida,\nAmir Bergman, Roman Glozman, Michael Gokhman,\nAvashalom Manevich, Nir Ratner, Noam Rozen,\nErez Shwartz, Mor Zusman, and Yoav Shoham.\n[2024. Jamba: A hybrid transformer-mamba language](https://arxiv.org/abs/2403.19887)\n[model.](https://arxiv.org/abs/2403.19887)\n\n\nTri Dao Linsong Chu, Divya Kumari et al. 2024.\n\n[Bamba: Inference-efficient hybrid mamba2 model.](https://huggingface.co/blog/bamba)\n\n\n[OpenAI. 2025. ChatGPT.](https://chatgpt.com/)\n\n\nWenbo Pan. 2024. [Fusang-v1: A large curation of](https://huggingface.co/datasets/wenbopan/Fusang-v1)\n[instruction-tuning datasets for better bilingual and](https://huggingface.co/datasets/wenbopan/Fusang-v1)\n[long-range llms.](https://huggingface.co/datasets/wenbopan/Fusang-v1)\n\n\nBo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi\nCao, Xin Cheng, Michael Chung, Leon Derczynski,\nXingjian Du, Matteo Grella, Kranthi Gv, Xuzheng\nHe, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bart\u0142omiej Koptyra, Hayden\nLau, Jiaju Lin, Krishna Sri Ipsit Mantri, Ferdinand\nMom, Atsushi Saito, Guangyu Song, Xiangru Tang,\nJohan Wind, Stanis\u0142aw Wo\u00b4zniak, Zhenyuan Zhang,\nQinghua Zhou, Jian Zhu, and Rui-Jie Zhu. 2023.\n[RWKV: Reinventing RNNs for the transformer era.](https://doi.org/10.18653/v1/2023.findings-emnlp.936)\nIn _Findings of the Association for Computational_\n_Linguistics: EMNLP 2023_, pages 14048\u201314077, Singapore. Association for Computational Linguistics.\n\n\n\n9\n\n\nDavid Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Ju[lian Michael, and Samuel R. Bowman. 2023. Gpqa:](https://arxiv.org/abs/2311.12022)\n[A graduate-level google-proof q&a benchmark.](https://arxiv.org/abs/2311.12022)\n\n\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu,\nJunxiao Song, Xiao Bi, Haowei Zhang, Mingchuan\nZhang, Y. K. Li, Y. Wu, and Daya Guo. 2024.\n[Deepseekmath: Pushing the limits of mathematical](https://arxiv.org/abs/2402.03300)\n[reasoning in open language models.](https://arxiv.org/abs/2402.03300)\n\n\nYutao Sun, Li Dong, Shaohan Huang, Shuming Ma,\nYuqing Xia, Jilong Xue, Jianyong Wang, and Furu\n[Wei. 2023. Retentive network: A successor to trans-](https://arxiv.org/abs/2307.08621)\n[former for large language models.](https://arxiv.org/abs/2307.08621)\n\n\n[Gemma Team. 2024a. Gemma 2: Improving open lan-](https://arxiv.org/abs/2408.00118)\n[guage models at a practical size.](https://arxiv.org/abs/2408.00118)\n\n\n[Gemma Team. 2025a. Gemma 3 technical report.](https://arxiv.org/abs/2503.19786)\n\n\nLing Team, Bin Han, Caizhi Tang, Chen Liang, Donghao Zhang, Fan Yuan, Feng Zhu, Jie Gao, Jingyu Hu,\nLongfei Li, Meng Li, Mingyang Zhang, Peijie Jiang,\nPeng Jiao, Qian Zhao, Qingyuan Yang, Wenbo Shen,\nXinxing Yang, Yalin Zhang, Yankun Ren, Yao Zhao,\nYibo Cao, Yixuan Sun, Yue Zhang, Yuchen Fang,\n[Zibin Lin, Zixuan Cheng, and Jun Zhou. 2025. Every](https://arxiv.org/abs/2510.19338)\n[attention matters: An efficient hybrid architecture for](https://arxiv.org/abs/2510.19338)\n[long-context reasoning.](https://arxiv.org/abs/2510.19338)\n\n\n[Llama Team. 2024b. The llama 3 herd of models.](https://arxiv.org/abs/2407.21783)\n\n\n[Qwen3 Team. 2025b. Qwen3 technical report.](https://arxiv.org/abs/2505.09388)\n\n\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\n[Grave, and Guillaume Lample. 2023. Llama: Open](https://arxiv.org/abs/2302.13971)\n[and efficient foundation language models.](https://arxiv.org/abs/2302.13971)\n\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\n[Kaiser, and Illia Polosukhin. 2017. Attention is all](https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html)\n[you need. In](https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html) _Advances in Neural Information Pro-_\n_cessing Systems 30: Annual Conference on Neural_\n_Information Processing Systems 2017, December 4-9,_\n_2017, Long Beach, CA, USA_, pages 5998\u20136008.\n\n\nBailin Wang, Chang Lan, Chong Wang, and Ruoming\n[Pang. 2025. Rattention: Towards the minimal sliding](https://arxiv.org/abs/2506.15545)\n[window size in local-global attention models.](https://arxiv.org/abs/2506.15545)\n\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le,\n[and Denny Zhou. 2022. Chain-of-thought prompting](http://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html)\n[elicits reasoning in large language models. In](http://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html) _Ad-_\n_vances in Neural Information Processing Systems 35:_\n_Annual Conference on Neural Information Process-_\n_ing Systems 2022, NeurIPS 2022, New Orleans, LA,_\n_USA, November 28 - December 9, 2022_ .\n\n\nDi Wu, Hongwei Wang, Wenhao Yu, Yuwei Zhang,\n[Kai-Wei Chang, and Dong Yu. 2024. Longmemeval:](https://arxiv.org/abs/2410.10813)\n[Benchmarking chat assistants on long-term interac-](https://arxiv.org/abs/2410.10813)\n[tive memory.](https://arxiv.org/abs/2410.10813)\n\n\n\nGuangxuan Xiao. 2025. Why stacking sliding win[dows can\u2019t see very far. https://guangxuanx.com/](https://guangxuanx.com/blog/stacking-swa.html)\n[blog/stacking-swa.html.](https://guangxuanx.com/blog/stacking-swa.html)\n\n\nGuangxuan Xiao, Yuandong Tian, Beidi Chen, Song\n[Han, and Mike Lewis. 2024. Efficient streaming lan-](https://openreview.net/forum?id=NG7sS51zVF)\n[guage models with attention sinks. In](https://openreview.net/forum?id=NG7sS51zVF) _The Twelfth_\n_International Conference on Learning Representa-_\n_tions, ICLR 2024, Vienna, Austria, May 7-11, 2024_ .\nOpenReview.net.\n\n\nZhaorui Yang, Tianyu Pang, Haozhe Feng, Han Wang,\n[Wei Chen, Minfeng Zhu, and Qian Liu. 2024. Self-](https://arxiv.org/abs/2402.13669)\n[distillation bridges distribution gap in language](https://arxiv.org/abs/2402.13669)\n[model fine-tuning.](https://arxiv.org/abs/2402.13669)\n\n\nJingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo,\nLiang Zhao, Zhengyan Zhang, Zhenda Xie, Yuxing\nWei, Lean Wang, Zhiping Xiao, Yuqing Wang, Chong\nRuan, Ming Zhang, Wenfeng Liang, and Wangding\nZeng. 2025. [Native sparse attention: Hardware-](https://doi.org/10.18653/v1/2025.acl-long.1126)\n[aligned and natively trainable sparse attention. In](https://doi.org/10.18653/v1/2025.acl-long.1126)\n_Proceedings of the 63rd Annual Meeting of the As-_\n_sociation for Computational Linguistics (Volume 1:_\n_Long Papers)_, pages 23078\u201323097, Vienna, Austria.\nAssociation for Computational Linguistics.\n\n\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey, Joshua Ainslie, Chris Alberti, Santiago Onta\u00f1\u00f3n, Philip Pham, Anirudh Ravula, Qifan Wang,\n[Li Yang, and Amr Ahmed. 2020. Big bird: Trans-](https://proceedings.neurips.cc/paper/2020/hash/c8512d142a2d849725f31a9a7a361ab9-Abstract.html)\n[formers for longer sequences. In](https://proceedings.neurips.cc/paper/2020/hash/c8512d142a2d849725f31a9a7a361ab9-Abstract.html) _Advances in Neural_\n_Information Processing Systems 33: Annual Confer-_\n_ence on Neural Information Processing Systems 2020,_\n_NeurIPS 2020, December 6-12, 2020, virtual_ .\n\n\nXuan Zhang, Fengzhuo Zhang, Cunxiao Du, Chao Du,\n[Tianyu Pang, Wei Gao, and Min Lin. 2024. Light-](https://arxiv.org/abs/2410.13846)\n[transfer: Your long-context llm is secretly a hybrid](https://arxiv.org/abs/2410.13846)\n[model with effortless adaptation.](https://arxiv.org/abs/2410.13846)\n\n\n**A** **SWA\u2019s Benefits and Each Method\u2019s**\n**Drawbacks**\n\n\nSWA reduces the computational complexity to\n_O_ ( _N \u00b7 W_ ), where _W_ is the window size. The\nbenefits are threefold: (1) SWA reduces the computational load, (2) conserves GPU memory by limiting the required Key-Value (KV) cache, and (3)\nenhances KV cache reusability beyond traditional\nprefix caching, since a token\u2019s state is independent\nof tokens outside its local window. However, there\n\nis no free lunch\u2014each method of SWAA has some\n\ndrawbacks, impairing the benefits brought by SWA\nto varying degrees.\n**FA Decode** presents two primary drawbacks:\n(1) the benefits apply only to prefilling, while decoding speed is not accelerated as it utilizes full\nattention, and (2) the GPU memory required for\nthe KV cache is not reduced, as the KV cache for\nthe full context must be retained for decoding. In\n\n\n\n10\n\n\npractice, however, many distributed LLM services\nhave to recompute the KV cache of the entire chat\nhistory because storing and loading the KV cache\ncomplicates engineering systems, making prefilling occurs more frequently than expected, thereby\namplifying the advantage of this method.\n**Keep First** introduces very minor computational\noverhead, but it complicates efficient KV cache\nreuse. Due to positional encoding, a token\u2019s KV\nstate depends on its position relative to the initial\n_k_ tokens, hindering simple cache reuse across different requests. A position encoding separation or\noffsetting mechanism may be needed.\n**Interleaving Layers** introduces the most significant overhead, as only a subset of layers benefits from the computational savings of SWA. Furthermore, the GPU memory required for the KV\ncache is not reduced for the full-attention layers.\nAdditionally, this method negates the KV cache\nreusability advantage of SWA, as the existence of\nfull-attention layers violates the independence of\nthe KV cache beyond the local window.\n**CoT** will greatly increase the generation length,\nespecially for difficult tasks. So the decoding time\nwill be much longer.\n\n\n**B** **Other Long-context Benchmarks**\n\n\nWe find existing long-context benchmarks problematic for our specific needs. For example:\n\n\n1. LongBench (Bai et al., 2023) is classic and\nwidely used, but its average context length\n(most are under 16k) is relatively short for\nmodern models, i.e., it is already too easy.\nAnd its data source is too old, leading to a risk\nof test data leakage.\n\n\n2. Ruler(Hsieh et al., 2024) has controllable context length, but its tasks are almost all synthetic and most of them are needle-retrieval\n\ntasks, failing to reflect the model\u2019s overall\nlong-context capability in real-world scenarios.", "metadata": {"source": "sliding_window_attention_raw_with_image_ids_with_captions.md"}, "chunk_index": 2, "token_size_config": 5000}
{"id": "sliding_window_attention_raw_with_image_ids_with_captions_chunks_5k_3", "content": "10\n\n\npractice, however, many distributed LLM services\nhave to recompute the KV cache of the entire chat\nhistory because storing and loading the KV cache\ncomplicates engineering systems, making prefilling occurs more frequently than expected, thereby\namplifying the advantage of this method.\n**Keep First** introduces very minor computational\noverhead, but it complicates efficient KV cache\nreuse. Due to positional encoding, a token\u2019s KV\nstate depends on its position relative to the initial\n_k_ tokens, hindering simple cache reuse across different requests. A position encoding separation or\noffsetting mechanism may be needed.\n**Interleaving Layers** introduces the most significant overhead, as only a subset of layers benefits from the computational savings of SWA. Furthermore, the GPU memory required for the KV\ncache is not reduced for the full-attention layers.\nAdditionally, this method negates the KV cache\nreusability advantage of SWA, as the existence of\nfull-attention layers violates the independence of\nthe KV cache beyond the local window.\n**CoT** will greatly increase the generation length,\nespecially for difficult tasks. So the decoding time\nwill be much longer.\n\n\n**B** **Other Long-context Benchmarks**\n\n\nWe find existing long-context benchmarks problematic for our specific needs. For example:\n\n\n1. LongBench (Bai et al., 2023) is classic and\nwidely used, but its average context length\n(most are under 16k) is relatively short for\nmodern models, i.e., it is already too easy.\nAnd its data source is too old, leading to a risk\nof test data leakage.\n\n\n2. Ruler(Hsieh et al., 2024) has controllable context length, but its tasks are almost all synthetic and most of them are needle-retrieval\n\ntasks, failing to reflect the model\u2019s overall\nlong-context capability in real-world scenarios.\n\n\n3. LongBench-V2 (Bai et al., 2024b) is welldesigned to necessitate deep understanding\nand reasoning over very long context. But it\nis too challenging for 4B-level models (e.g.,\nQwen3-4B-Thinking only gets 35% accuracy,\nwhich is too close to the random guessing\nbaseline of 25%), making the improvement of\ndifferent methods less distinguishable. Moreover, since it is in a multiple-choice question\n\n\n\nformat, the results may not be sufficiently reliable because the model has a 25% chance of\n\nguessing the correct option.\n\n\nHowever, despite the extreme difficulty of\nLongBench-V2 (Bai et al., 2024b), it remains\na high-quality long-context benchmark after\nall. Thus we still elect to conduct our experiments on it to verify the generalizability of\nour conclusions, as shown in Appendix D.\n\n\n**C** **Results of Other Models**\n\n\nWe show the results of Qwen3-30B-A3BThinking and Qwen3-30B-A3B-Instruct on LongMemEval_24k in Table 4, and the results of\n\nLlama3.1-8B-Instruct in Table 5. The scores of\n\nQwen3-30B-A3B are generally higher and those\nof Llama3.1 are generally lower, but all results are\nconsistent with our previous conclusions, demonstrating their generalizability.\nDue to the time-intensive nature of training, we\nonly test a small set of configurations with finetuning.\n\n\n**D** **Results of LongBench V2**\n\n\nWe present the results of LongBench V2 (Bai et al.,\n\n2024b) in Tables 6, 7 and 8. We retain only the\nsamples whose context length is under 128k due to\nGPU memory limitations; thus, 384 of 500 samples\nare kept. However, due to the high difficulty, the\nperformance is generally poor. Some scores are\neven below the random guessing baseline (25%).\nFor Qwen3-4B and Qwen3-30B-A3B models,\n\nthe results show less noticeable differences be\ntween various methods. But fortunately, the trend\nof accuracy changes is generally consistent with\nthat of other datasets, so they do not conflict with\nall of our previous conclusions. For Llama3.1, due\nto its weaker long-context capability, accuracy consistently hovers around 30%.\n\n\n**E** **Inference Efficiency**\n\n\nThe TTFT, TPOT and total throughput when using vLLM are shown in Table 9. Since inference\nspeed is highly dependent on hardware, implementation details, and workload characteristics, these\nnumbers should be interpreted as reference values.\nHowever, from the results, we can still conclude\n\nthat:\n\n\n1. Interleaving Layers and FA Decode significantly slow down the speed compared to pure\n\n\n\n11\n\n\nSWA.\n\n\n2. Keep First _k_ Tokens has a negligible impact\non efficiency.\n\n\n3. Increasing the window size slightly increases\ninference time. For example, increasing from\n2k to 4k decreases throughput by only 10%,\nbut a 4k window generally achieves higher accuracy based on previous experiments. Therefore, in practice, a 4k window is a more common choice.\n\n\nIn theory, FA Decode should yield a decoding\nspeed identical to that of full attention. Yet, in this\ntable, we observe acceleration on TPOT. This is because vLLM-v1 typically mixes different requests\u2019\nprefilling and decoding tokens in one sequence\nto improve GPU utilization. Thus, the speeds of\nprefilling and decoding may affect each other. If\nprocessing only a single request, the situation differs. For example, when the generation length is\nset to 2000, we find decoding takes over 95% of the\ntotal time, rendering the acceleration of the prefilling stage negligible\u2014i.e., SWA with FA Decode is\nalmost unable to improve efficiency in such cases.\n\n\n**F** **Influence of Training Epochs**\n\n\nAs shown in Table 10, training for more than\n1 epoch yields no improvement. Therefore, we\nchoose to train for only 1 epoch.\n\n\n**G** **Results of LightTransfer**\n\n\nLightTransfer (Zhang et al., 2024) represents a\npromising attempt at SWA adaptation on fullattention models without pretraining. It proposes\na layer selection method for SWA adaptation that\ncalculates a \"lazy ratio,\" represented by the ratio\nof attention from tokens at the end of the sequence\n(from a calibration dataset) to recent tokens versus\nglobal tokens. Layers with a higher \"lazy ratio\" are\nselected to apply SWA, while the rest retain full\nattention. This method is intuitive and theoretically\nsound, but our experiments reveal some negative\nresults.\n\nSince the complete code of LightTransfer is not\nopen-source, we reproduce this method using LongAlign (Bai et al., 2024a) as the calibration dataset\nfor lazy layer detection, where the number of last\ntokens is set to 64, and the recent token window is\nset to 1024. From our experimental results shown\nin Table 11, we find that:\n\n\n\n1. For Qwen3-4B, LightTransfer even has a\ncounterproductive effect; allowing lazy layers to use FA yields higher scores, while following the original method (letting non-lazy\nlayers use FA) results in significantly lower\n\nscores.\n\n\n2. For Qwen3-30B, it provides nearly no improvement over fixed-interval selection.\n\n\n3. Only for Llama3.1-8B does LightTransfer\nshow advantages.\n\n\nTherefore, we conclude that LightTransfer does\nnot yield stable performance across various models.\nAlthough fine-grained layer selection methods are\ntheoretically superior, we believe they require further investigation before integration into our SWAA\nrecipes.\n\n\n\n12\n\n\nTable 4: Results of Qwen3-30B-A3B-Thinking and Qwen3-30B-A3B-Instruct on LongMemEval\n\n\n**No.** **SFT** **window size** **FA layers** **keep first** **FA decode** **Acc think** **Acc non-think**\n\n\n0 False Full [] 0 False **79.2** **71.6**\n1 False 2k [] 0 False 0.0 0.4\n2 False 8k [] 0 False 0.0 0.2\n\n\n3 False 2k [] 10 False 0.0 2.8\n4 False 2k [] 0 True 0.2 0.2\n5 False 2k [0, 2, 4, ...] 0 False 21.0 28.4\n\n\n6 False 2k [] 10 True 43.8 23.6\n7 False 2k [] 100 True 58.6 22.2\n8 False 2k [] 1000 True 59.0 25.4\n9 False 4k [] 10 True 49.8 26.6\n\n\n10 False 2k [0, 2, 4, ...] 10 True **74.8** **63.0**\n11 False 2k [1, 3, 5, ...] 10 True 51.6 24.0\n12 False 2k [0, 4, 8, ...] 10 True 48.8 23.8\n13 False 2k [2, 6, 10, ...] 10 True 64.8 44.2\n14 False 4k [0, 2, 4, ...] 100 True **74.6** **64.4**\n\n\n15 True _\\_ [] 0 False **79.6** _\\_\n\n\n16 True 2k [] 0 True 62.2 51.0\n\n\n17 True 2k [] 100 True 65.6 50.8\n18 True 2k [0, 2, 4, ...] 0 True **72.6** _\\_\n\n\n19 True 2k [0, 2, 4, ...] 100 True **77.8** **68.0**\n\n\nTable 5: Results of Llama3.1-8B-Instruct on LongMemEval\n\n\n**No.** **SFT** **window size** **FA layers** **keep first** **FA decode** **Acc non-think**\n\n\n0 False Full [] 0 False **61.0**\n1 False 2k [] 0 False 0.6\n2 False 8k [] 0 False 1.2\n\n\n3 False 2k [] 10 False 1.8\n4 False 2k [] 0 True 0.0\n5 False 2k [0, 2, 4, ...] 0 False 3.0\n\n\n6 False 2k [] 10 True 16.8\n7 False 2k [] 100 True 20.0\n8 False 2k [] 1k True 24.2\n9 False 4k [] 10 True 23.8\n\n\n10 False 2k [0, 2, 4, ...] 10 True **42.6**\n11 False 2k [1, 3, 5, ...] 10 True 21.0\n12 False 2k [0, 4, 8, ...] 10 True 17.8\n13 False 2k [2, 6, 10, ...] 10 True 24.4\n14 False 4k [0, 2, 4, ...] 100 True **44.0**\n\n\n13\n\n\nTable 6: Qwen3-4B-Thinking and Qwen3-4B-Instruct results on LongBench-V2\n\n\n**No.** **SFT** **window size** **FA layers** **keep first** **FA decode** **Acc think** **Acc non-think**\n\n\n0 False Full [] 0 False **34.6** **35.2**\n1 False 2k [] 0 False 9.4 25.8\n2 False 8k [] 0 False 15.1 22.1\n\n\n3 False 2k [] 10 False 7.7 25.8\n4 False 2k [] 0 True **26.2** 25.2\n5 False 2k [1, 3, 5, ...] 0 False 12.1 23.5\n6 False 8k [] 0 True 22.8 25.5\n\n\n7 False 2k [] 10 True 25.8 25.2\n8 False 2k [] 100 True 24.2 26.5\n9 False 2k [] 1000 True 23.8 25.2\n10 False 2k [0, 2, 4, ...] 10 False 19.8 **29.2**\n11 False 2k [0, 2, 4, ...] 0 True 23.8 **29.9**\n12 False 2k [1, 3, 5, ...] 10 False 21.1 **29.9**\n13 False 2k [1, 3, 5, ...] 0 True **28.5** 26.8\n14 False 4k [] 10 True **27.9** 27.5\n\n\n15 True Full [] 0 False **37.9** **34.9**\n16 True 2k [] 0 False 7.4 30.9\n\n\n17 True 2k [] 100 False 6.0 _\\_\n18 True 2k [] 0 True 29.2 30.2\n19 True 2k [1, 3, 5, ...] 0 False 29.5 31.9\n20 True 4k [] 0 True **32.9** _\\_\n\n\n21 True 2k [] 10 True 28.9 29.2\n22 True 2k [] 100 True 29.2 30.5\n23 True 2k [0, 2, 4, ...] 0 True 31.5 _\\_\n24 True 2k [0, 4, 8, ...] 0 True 30.9 _\\_\n25 True 2k [1, 3, 5, ...] 0 True **38.3** **34.6**\n26 True 2k [1, 5, 9, ...] 0 True 32.0 32.2\n\n\n27 True 2k [1, 3, 5, ...] 100 True **37.2** **33.9**\n\n\n14\n\n\nTable 7: Qwen3-30B-A3B-Thinking and Qwen3-30B-A3B-Instruct results on LongBench-V2\n\n\n**No.** **SFT** **window size** **FA layers** **keep first** **FA decode** **Acc think** **Acc non-think**\n\n\n0 False Full [] 0 False **49.7** **42.6**\n1 False 2k [] 0 False 0.0 0.0\n2 False 8k [] 0 False 0.0 0.0\n\n\n3 False 2k [] 10 False 9.1 32.2\n4 False 2k [] 0 True 0.0 0.0\n5 False 2k [0, 2, 4, ...] 0 False 20.1 25.8\n\n\n6 False 2k [] 10 True 9.1 32.2\n7 False 2k [] 100 True 10.4 28.2\n8 False 2k [] 1k True 11.7 29.5\n9 False 4k [] 10 True 26.8 30.9\n\n\n10 False 2k [0, 2, 4, ...] 10 True 22.1 **33.6**\n11 False 2k [0, 4, 8, ...] 10 True 12.4 29.5\n12 False 2k [1, 3, 5, ...] 10 True **30.2** 28.9\n13 False 2k [2, 6, 10, ...] 10 True 21.1 35.6\n14 False 4k [0, 2, 4, ...] 100 True **29.5** **35.9**\n\n\n15 True Full [] 0 False **43.6** _\\_\n\n\n16 True 2k [] 0 True 35.9 33.9\n\n\n17 True 2k [] 100 True **36.6** 32.9\n18 True 2k [0, 2, 4, ...] 0 True **41.3** _\\_\n\n\n19 True 2k [0, 2, 4, ...] 100 True **48.0** **37.9**\n\n\nTable 8: Llama3.1-8B-Instruct results on LongBench-V2\n\n\n**No.** **SFT** **window size** **FA layers** **keep first** **FA decode** **Acc non-think**\n\n\n0 False Full [] 0 False **33.2**\n1 False 2k [] 0 False 0.0\n2 False 8k [] 0 False 0.0\n\n\n3 False 2k [] 10 False 28.9\n4 False 2k [] 0 True 0.0\n5 False 2k [0, 2, 4, ...] 0 False 0.0\n\n\n6 False 2k [] 10 True 28.9\n7 False 2k [] 100 True **30.2**\n8 False 2k [] 1000 True **30.2**\n9 False 4k [] 10 True 27.5\n\n\n10 False 2k [0, 2, 4, ...] 10 True 28.2\n11 False 2k [0, 4, 8, ...] 10 True **32.6**\n12 False 2k [1, 3, 5, ...] 10 True 26.5\n13 False 2k [2, 6, 10, ...] 10 True 26.8\n14 False 4k [0, 2, 4, ...] 100 True **30.9**\n\n\n15\n\n\nTable 9: Efficiency metrics of different SWAA configurations on vLLM. \"FA layers = 1/4\" means one fourth of total\nlayers use full attention while the others use SWA.\n\n\n**window** **keep first** **FA decode** **FA layers** **TTFT (s)** **TPOT (s)** **Throughput (k tks/s)**\n\n\nFull 0 False None 1681.44 0.16 3.74\n\n\n2k 0 False None 203.20 0.02 30.72\n\n2k 100 False None 207.74 0.02 30.65\n\n2k 0 False 1/2 938.00 0.09 6.70\n\n2k 0 True None 963.39 0.11 6.39\n\n2k 0 True 1/2 1321.39 0.14 4.72\n\n2k 0 True 1/4 1141.66 0.12 5.43\n\n\n4k 0 False None 233.07 0.02 27.03\n\n4k 100 False None 237.87 0.02 26.74\n\n4k 0 False 1/2 949.02 0.09 6.64\n\n4k 0 True None 990.00 0.11 6.23\n\n4k 0 True 1/2 1340.91 0.14 4.64\n\n4k 0 True 1/4 1166.69 0.13 5.32\n\n\nTable 10: Results of different training epochs of Qwen3-4B-Thinking on LongMemEval\n\n\n**SFT (epochs)** **window size** **FA layers** **keep first** **FA decode** **Acc**\n\n\n1 2k [] 0 True 58.0\n2 2k [] 0 True 57.6\n3 2k [] 0 True 56.0\n\n\nTable 11: Results of LightTransfer on LongMemEval. \"lazy\" represents the half layers with higher lazy ratio, i.e.\nthose which should apply SWA in theory. \"non-lazy\" represents the other part, i.e. those which should keep full\nattention.\n\n\n**SFT** **window size** **FA layers** **keep first** **FA decode** **Acc think** **Acc non-think**\n\n\n**Model Group: Qwen3-4B**\n\n\nFalse 2k [0, 2, 4, ...] 100 True 48.8 18.4\nFalse 2k [1, 3, 5, ...] 100 True **70.8** **50.4**\nFalse 2k lazy 100 True 70.2 47.8\nFalse 2k non-lazy 100 True 54.0 19.6\n\n\n**Model Group: Qwen3-30B-A3B**\n\n\nFalse 2k [0, 2, 4, ...] 100 True **75.8** **64.2**\nFalse 2k [1, 3, 5, ...] 100 True 60.2 25.8\nFalse 2k lazy 100 True 61.8 25.2\nFalse 2k non-lazy 100 True 74.8 59.2\n\n\n**Model Group: Llama3.1-8B-Instruct**\n\n\nFalse 2k [0, 2, 4, ...] 100 True _\\_ 39.8\nFalse 2k [1, 3, 5, ...] 100 True _\\_ 24.2\nFalse 2k lazy 100 True _\\_ 20.2\nFalse 2k non-lazy 100 True _\\_ **49.8**\n\n\n16", "metadata": {"source": "sliding_window_attention_raw_with_image_ids_with_captions.md"}, "chunk_index": 3, "token_size_config": 5000}
