2025-12-18 16:49:06,357 - __main__ - INFO - Initializing resources...
2025-12-18 16:49:06,357 - __main__ - INFO - Loading EmbeddingGemma model...
2025-12-18 16:49:06,360 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2025-12-18 16:49:06,360 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: google/embeddinggemma-300m
2025-12-18 16:49:12,593 - sentence_transformers.SentenceTransformer - INFO - 14 prompts are loaded, with the keys: ['query', 'document', 'BitextMining', 'Clustering', 'Classification', 'InstructionRetrieval', 'MultilabelClassification', 'PairClassification', 'Reranking', 'Retrieval', 'Retrieval-query', 'Retrieval-document', 'STS', 'Summarization']
2025-12-18 16:49:12,595 - __main__ - INFO - Loading vector store from E:\Python Stuff\MAS-for-multimodal-knowledge-graph\vector_store_outputs\index...
2025-12-18 16:49:12,607 - faiss.loader - INFO - Loading faiss with AVX2 support.
2025-12-18 16:49:12,628 - faiss.loader - INFO - Successfully loaded faiss with AVX2 support.
2025-12-18 16:49:12,659 - __main__ - INFO - User query received: what is attention?
2025-12-18 16:49:12,659 - __main__ - INFO - Starting Query Agent with query: 'what is attention?'
2025-12-18 16:49:15,015 - __main__ - INFO - RAG Tool invoked with user query: what is attention?
2025-12-18 16:49:18,231 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-18 16:49:18,243 - __main__ - INFO - Generated 4 RAG subqueries:
2025-12-18 16:49:18,243 - __main__ - INFO -   Subquery 1: attention mechanism
2025-12-18 16:49:18,243 - __main__ - INFO -   Subquery 2: attention in deep learning
2025-12-18 16:49:18,243 - __main__ - INFO -   Subquery 3: transformer attention
2025-12-18 16:49:18,243 - __main__ - INFO -   Subquery 4: self-attention
2025-12-18 16:49:18,244 - __main__ - INFO - RAG internal retrieval query: attention mechanism
2025-12-18 16:49:18,384 - __main__ - INFO - RAG retrieved 3 documents for subquery 'attention mechanism'
2025-12-18 16:49:18,384 - __main__ - INFO - RAG internal retrieval query: attention in deep learning
2025-12-18 16:49:18,465 - __main__ - INFO - RAG retrieved 3 documents for subquery 'attention in deep learning'
2025-12-18 16:49:18,465 - __main__ - INFO - RAG internal retrieval query: transformer attention
2025-12-18 16:49:18,546 - __main__ - INFO - RAG retrieved 3 documents for subquery 'transformer attention'
2025-12-18 16:49:18,547 - __main__ - INFO - RAG internal retrieval query: self-attention
2025-12-18 16:49:18,625 - __main__ - INFO - RAG retrieved 3 documents for subquery 'self-attention'
2025-12-18 16:49:18,625 - __main__ - INFO - Total unique documents after de-duplication across all subqueries: 8
2025-12-18 16:49:18,626 - __main__ - INFO - RAG Unique Doc 1:
  Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
  Chunk ID: neuronal_attention_circuits_raw_chunks_2k_0
  Metadata: {'source': 'neuronal_attention_circuits_raw.md'}
  Content (truncated): ## **Neuronal Attention Circuit (NAC) for Representation Learning**

**Waleed Razzaq** [1] **Izis Kankaraway** [1] **Yun-Bo Zhao** [1 2]



**Abstract**

Attention improves representation learning over
RNNs, but its discrete nature limits continuoustime (CT) modeling. We introduce Neuronal Attention Circuit (NAC), a novel, biologically plausible CT-Attention mechanism that reformulates

attention logits computation as the solution to a
linear first-order ODE with nonlinear interlinked
gates deri...
2025-12-18 16:49:18,626 - __main__ - INFO - RAG Unique Doc 2:
  Source: attention_is_all_you_need_raw_chunks_2k.jsonl
  Chunk ID: attention_is_all_you_need_raw_chunks_2k_1
  Metadata: {'source': 'attention_is_all_you_need_raw.md'}
  Content (truncated): [10], consuming the previously generated symbols as additional input when generating the next.


2


![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_is_all_you_need.pdf-2-0.png)

Figure 1: The Transformer - model architecture.


The Transformer follows this overall architecture using stacked self-attention and point-wise, fully
connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,
respectively.


**3.1** **Enco...
2025-12-18 16:49:18,626 - __main__ - INFO - RAG Unique Doc 3:
  Source: attention_functional_roles_raw_chunks_2k.jsonl
  Chunk ID: attention_functional_roles_raw_chunks_2k_6
  Metadata: {'source': 'attention_functional_roles_raw.md'}
  Content (truncated): chain-of-thought (CoT) paradigm has been argued to mirror step-by-step human reasoning, leading to improved problem-solving performance. These findings motivate the design of interpretable,
functionally specialized modules in artificial networks, bridging insights from neuroscience with
advances in multimodal reasoning.


**Attention Heads in Vision–Language Models.** A growing body of interpretability research has
revealed that attention heads in LLMs exhibit functional specialization, such as ...
2025-12-18 16:49:18,627 - __main__ - INFO - RAG Unique Doc 4:
  Source: attention_is_all_you_need_raw_chunks_2k.jsonl
  Chunk ID: attention_is_all_you_need_raw_chunks_2k_2
  Metadata: {'source': 'attention_is_all_you_need_raw.md'}
  Content (truncated): [38, 2, 9].


    - The encoder contains self-attention layers. In a self-attention layer all of the keys, values
and queries come from the same place, in this case, the output of the previous layer in the
encoder. Each position in the encoder can attend to all positions in the previous layer of the
encoder.


    - Similarly, self-attention layers in the decoder allow each position in the decoder to attend to
all positions in the decoder up to and including that position. We need to prevent lef...
2025-12-18 16:49:18,627 - __main__ - INFO - RAG Unique Doc 5:
  Source: attention_is_all_you_need_raw_chunks_2k.jsonl
  Chunk ID: attention_is_all_you_need_raw_chunks_2k_5
  Metadata: {'source': 'attention_is_all_you_need_raw.md'}
  Content (truncated): We are excited about the future of attention-based models and plan to apply them to other tasks. We
plan to extend the Transformer to problems involving input and output modalities other than text and
to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs
such as images, audio and video. Making generation less sequential is another research goals of ours.


The code we used to train and evaluate our models is available at `[https://github.com/](https...
2025-12-18 16:49:18,627 - __main__ - INFO - RAG Unique Doc 6:
  Source: attention_is_all_you_need_raw_chunks_2k.jsonl
  Chunk ID: attention_is_all_you_need_raw_chunks_2k_0
  Metadata: {'source': 'attention_is_all_you_need_raw.md'}
  Content (truncated): Provided proper attribution is provided, Google hereby grants permission to
reproduce the tables and figures in this paper solely for use in journalistic or
scholarly works.

## **Attention Is All You Need**



**Niki Parmar** _[∗]_
Google Research
```
nikip@google.com

```


**Ashish Vaswani** _[∗]_
Google Brain
```
avaswani@google.com

```

**Llion Jones** _[∗]_
Google Research
```
 llion@google.com

```


**Noam Shazeer** _[∗]_
Google Brain
```
noam@google.com

```


**Jakob Uszkoreit** _[∗]_...
2025-12-18 16:49:18,627 - __main__ - INFO - RAG Unique Doc 7:
  Source: attention_is_all_you_need_raw_chunks_2k.jsonl
  Chunk ID: attention_is_all_you_need_raw_chunks_2k_4
  Metadata: {'source': 'attention_is_all_you_need_raw.md'}
  Content (truncated): **6.2** **Model Variations**


To evaluate the importance of different components of the Transformer, we varied our base model
in different ways, measuring the change in performance on English-to-German translation on the


5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.


8


Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base
model. All metrics are on the English-to-German translation development set,...
2025-12-18 16:49:18,628 - __main__ - INFO - RAG Unique Doc 8:
  Source: sliding_window_attention_annotated_chunks_2k.jsonl
  Chunk ID: sliding_window_attention_annotated_chunks_2k_0
  Metadata: {'source': 'sliding_window_attention_annotated.md'}
  Content (truncated): ## **Sliding Window Attention Adaptation**

Yijiong Yu [a], Jiale Liu [b], Qingyun Wu [b], Huazheng Wang [a], and Ji Pei [c]


aOregon State University, {yuyiji, huazheng.wang}@oregonstate.edu
bPenn State University, {jiale.liu, qingyun.wu}@psu.edu
cDeepSolution, research@deepsolution.chat



**Abstract**


The self-attention mechanism in Transformer
based Large Language Models (LLMs) scales
quadratically with input length, making longcontext inference expensive. Sliding window attention (SWA) r...
2025-12-18 16:49:18,628 - __main__ - INFO - RAG context returned to LLM (multi-subquery, k=3, deduplicated):
Content: ## **Neuronal Attention Circuit (NAC) for Representation Learning**

**Waleed Razzaq** [1] **Izis Kankaraway** [1] **Yun-Bo Zhao** [1 2]



**Abstract**

Attention improves representation learning over
RNNs, but its discrete nature limits continuoustime (CT) modeling. We introduce Neuronal Attention Circuit (NAC), a novel, biologically plausible CT-Attention mechanism that reformulates

attention logits computation as the solution to a
linear first-order ODE with nonlinear interlinked
gates derived from repurposing _C. elegans_ Neuronal Circuit Policies (NCPs) wiring mechanism.
NAC replaces dense projections with sparse sensory gates for key-query projections and a sparse
backbone network with two heads for computing
_content-target_ and _learnable time-constant_ gates,
enabling efficient adaptive dynamics. NAC supports three attention logit computation modes: (i)
explicit Euler integration, (ii) exact closed-form
solution, and (iii) steady-state approximation. To
improve memory intensity, we implemented a
sparse Top- _K_ pairwise concatenation scheme that
selectively curates key-query interactions. We
provide rigorous theoretical guarantees, including
state stability, bounded approximation errors, and
universal approximation. Empirically, we implemented NAC in diverse domains, including irregular time-series classification, lane-keeping for
autonomous vehicles, and industrial prognostics.
We observed that NAC matches or outperforms
competing baselines in accuracy and occupies an
intermediate position in runtime and memory efficiency compared with several CT baselines.


**1. Introduction**


Learning representations of sequential data in temporal or
spatio-temporal domains is essential for capturing patterns
and enabling accurate forecasting. Discrete-time Recurrent neural networks (DT-RNNs) such as RNN (Rumelhart et al., 1985; Jordan, 1997), Long-short term memory


1Department of Automation, University of Science & Technology of China, Hefei, China [2] Institute of Artificial Intelligence,
Hefei Comprehensive National Science Center. Correspondence to:
Yun-Bo Zhao _<_ ybzhao@ustc.edu.cn _>_, Waleed Razzaq _<_ waleedrazzaq@mail.ustc.edu.cn _>_ .


_Preprint. December 12, 2025._



(LSTM) (Hochreiter & Schmidhuber, 1997), and Gated Recurrent Unit (GRU) (Cho et al., 2014) model sequential dependencies by iteratively updating hidden states to represent
or predict future elements in a sequence. While effective
for regularly sampled sequences, DT-RNNs face challenges
with irregularly sampled data because they assume uniform
time intervals. In addition, vanishing gradients can make
it difficult to capture long-term dependencies (Hochreiter,
1998).
Continuous-time RNNs (CT-RNNs) (Rubanova et al., 2019)
model hidden states as ordinary differential equations
(ODEs), allowing them to process inputs that arrive at arbitrary or irregular time intervals. Mixed-memory RNNs
(mmRNNs) (Lechner & Hasani, 2022) build on this idea
by separating memory compartments from time-continuous
states, helping maintain stable error propagation while capturing continuous-time dynamics. Liquid neural networks
(LNNs) (Hasani et al., 2021; 2022) take a biologically inspired approach by assigning variable time-constants to hidden states, improving adaptability and robustness, though
vanishing gradients can still pose challenges during training.
The attention mechanisms (Vaswani et al., 2017) mitigate
this limitation by treating all time steps equally and allowing models to focus on the most relevant observations. It
computes the similarity between queries ( _q_ ) and keys ( _k_ ),
scaling by the key dimension to keep gradients stable. MultiHead Attention (MHA) (Vaswani et al., 2017) extends this
by allowing the model to attend to different representation
subspaces in parallel. Variants like Sparse Attention (Tay
et al., 2020; Roy et al., 2021), BigBird (Zaheer et al., 2020),
and Longformer (Beltagy et al., 2020) modify the attention
pattern to reduce computational cost, particularly for long sequences, by attending only to selected positions rather than
all pairs. Even with these improvements, attention-based
methods still rely on discrete scaled dot-product operations,
limiting their ability to model continuous trajectories often
captured by CT counterparts.
Recent work has explored bridging this gap through NeuralODE (Chen et al., 2018) formulation. mTAN (Shukla &
Marlin, 2021) learns CT embeddings and uses time-based
attention to interpolate irregular observations into a fixedlength representation for downstream encoder-decoder modeling. ODEFormer (d’Ascoli et al., 2023) trains a sequenceto-sequence transformer on synthetic trajectories to directly
infer symbolic ODE systems from noisy, irregular data,
though it struggles with chaotic systems and generalization



1


**Neuronal Attention Circuit (NAC) for Representation Learning**



beyond observed conditions. Continuous-time Attention
(CTA) (Chien & Chen, 2021) embeds a continuous-time
attention mechanism within a Neural ODE, allowing attention weights and hidden states to evolve jointly over time.
Still, it remains computationally intensive and sensitive to
the accuracy of the ODE solver. ContiFormer (Chen et al.,
2023) builds a CT-transformer by pairing ODE-defined latent trajectories with a time-aware attention mechanism to
model dynamic relationships in data.
Despite these innovations, a persistent and underexplored
gap remains in developing a biologically plausible attention
mechanism that seamlessly integrates CT dynamics with the
abstraction of the brain’s connectome to handle irregular sequences without prohibitive computational costs. Building
on this, we propose a novel attention mechanism called the
_Neuronal Attention Circuit_ (NAC), in which attention logits
are computed as the solution to a first-order ODE modulated
by nonlinear, interlinked gates derived from repurposing
Neuronal Circuit Policies (NCPs) from the nervous system
of _C. elegans_ nematode (refer to Appendix A.2 for more
information). Unlike standard attention, which projects keyquery pairs through a dense layer, NAC employs a sensory
gate to transform input features and a backbone to model
nonlinear interactions, with multiple heads producing outputs structured for attention logits computation. Based on
the solutions to ODE, we define three computation modes:
(i) Exact, using the closed-form ODE solution; (ii) Euler,
approximating the solution via _explicit Euler_ integration;
and (iii) Steady, using only the steady-state solution, analogous to standard attention scores. To reduce computational
complexity, we implemented a sparse Top- _K_ pairwise concatenation algorithm that selectively curates key-query inputs. We evaluate NAC across multiple domains, including
irregularly sampled time series, autonomous vehicle lanekeeping, and Industry 4.0, comparing it to state-of-the-art
baselines. NAC consistently matches or outperforms these
models, while runtime and peak memory benchmarks place
it between CT-RNNs in terms of speed and CT-Attentions
in terms of memory requirements.


**2. Neuronal Attention Circuit (NAC)**


We propose a simple alternative formulation of the attention
logits _a_ (refer to Appendix A.1 for more information), interpreting them as the solution to a first-order linear ODE
modulated by nonlinear, interlinked gates:



from repurposing NCPs. We refer to this formulation as
the Neuronal Attention Circuit (NAC). It enables the logits
_at_ to evolve dynamically with input-dependent, variable
time constants, mirroring the adaptive temporal dynamics
found in _C. elegans_ nervous systems while improving
computational efficiency and expressiveness. Moreover, it
introduces continuous depth into the attention mechanism,
bridging discrete-layer computation with dynamic temporal
evolution.

**Motivation behind this formulation:** The proposed
formulation is loosely motivated by the input-dependent
time-constant mechanism of Liquid Neural Networks
(LNNs), a class of CT-RNNs inspired by biological nervous
systems and synaptic transmission. In this framework, the
dynamics of non-spiking neurons are described by a linear
ODE with nonlinear, interlinked gates: _ddt_ **xt** = **[x]** _τ_ **[t]** [+] **[ S][t]** _[,]_
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl

Content: [10], consuming the previously generated symbols as additional input when generating the next.


2


![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_is_all_you_need.pdf-2-0.png)

Figure 1: The Transformer - model architecture.


The Transformer follows this overall architecture using stacked self-attention and point-wise, fully
connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,
respectively.


**3.1** **Encoder and Decoder Stacks**


**Encoder:** The encoder is composed of a stack of _N_ = 6 identical layers. Each layer has two
sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network. We employ a residual connection [11] around each of
the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is
LayerNorm( _x_ + Sublayer( _x_ )), where Sublayer( _x_ ) is the function implemented by the sub-layer
itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding
layers, produce outputs of dimension _d_ model = 512.


**Decoder:** The decoder is also composed of a stack of _N_ = 6 identical layers. In addition to the two
sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head
attention over the output of the encoder stack. Similar to the encoder, we employ residual connections
around each of the sub-layers, followed by layer normalization. We also modify the self-attention
sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This
masking, combined with fact that the output embeddings are offset by one position, ensures that the
predictions for position _i_ can depend only on the known outputs at positions less than _i_ .


**3.2** **Attention**


An attention function can be described as mapping a query and a set of key-value pairs to an output,
where the query, keys, values, and output are all vectors. The output is computed as a weighted sum


3


Scaled Dot-Product Attention Multi-Head Attention


Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several
attention layers running in parallel.


of the values, where the weight assigned to each value is computed by a compatibility function of the
query with the corresponding key.


**3.2.1** **Scaled Dot-Product Attention**


We call our particular attention "Scaled Dot-Product Attention" (Figure 2). The input consists of
queries and keys of dimension _dk_, and values of dimension _dv_ . We compute the dot products of the
query with all keys, divide each by _[√]_ _dk_, and apply a softmax function to obtain the weights on the
values.


In practice, we compute the attention function on a set of queries simultaneously, packed together
into a matrix _Q_ . The keys and values are also packed together into matrices _K_ and _V_ . We compute
the matrix of outputs as:


Attention( _Q, K, V_ ) = softmax( _[Q][K]_ _[T]_ ) _V_ (1)

~~_√_~~ _dk_


The two most commonly used attention functions are additive attention [2], and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor
of ~~_√_~~ 1 _dk_ . Additive attention computes the compatibility function using a feed-forward network with
a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is
much faster and more space-efficient in practice, since it can be implemented using highly optimized
matrix multiplication code.


While for small values of _dk_ the two mechanisms perform similarly, additive attention outperforms
dot product attention without scaling for larger values of _dk_ [3]. We suspect that for large values of
_dk_, the dot products grow large in magnitude, pushing the softmax function into regions where it has
extremely small gradients [4] . To counteract this effect, we scale the dot products by ~~_√_~~ 1 _dk_ .


**3.2.2** **Multi-Head Attention**


Instead of performing a single attention function with _d_ model-dimensional keys, values and queries,
we found it beneficial to linearly project the queries, keys and values _h_ times with different, learned
linear projections to _dk_, _dk_ and _dv_ dimensions, respectively. On each of these projected versions of
queries, keys and values we then perform the attention function in parallel, yielding _dv_ -dimensional


4To illustrate why the dot products get large, assume that the components of _q_ and _k_ are independent random
variables with mean 0 and variance 1. Then their dot product, _q · k_ = [�] _[d]_ _i_ =1 _[k]_ _[q][i][k][i]_ [, has mean][ 0][ and variance] _[ d][k]_ [.]


4



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_is_all_you_need.pdf-3-0.png)

![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_is_all_you_need.pdf-3-1.png)
output values. These are concatenated and once again projected, resulting in the final values, as
depicted in Figure 2.


Multi-head attention allows the model to jointly attend to information from different representation
subspaces at different positions. With a single attention head, averaging inhibits this.


MultiHead( _Q, K, V_ ) = Concat(head1 _, ...,_ headh) _W_ _[O]_

where headi = Attention( _QWi_ _[Q][, KW][ K]_ _i_ _[, V W][ V]_ _i_ [)]


Where the projections are parameter matrices _Wi_ _[Q]_ _∈_ R _[d]_ [model] _[×][d][k]_, _Wi_ _[K]_ _∈_ R _[d]_ [model] _[×][d][k]_, _Wi_ _[V]_ _∈_ R _[d]_ [model] _[×][d][v]_
and _W_ _[O]_ _∈_ R _[hd][v][×][d]_ [model] .


In this work we employ _h_ = 8 parallel attention layers, or heads. For each of these we use
_dk_ = _dv_ = _d_ model _/h_ = 64. Due to the reduced dimension of each head, the total computational cost
is similar to that of single-head attention with full dimensionality.


**3.2.3** **Applications of Attention in our Model**


The Transformer uses multi-head attention in three different ways:


    - In "encoder-decoder attention" layers, the queries come from the previous decoder layer,
and the memory keys and values come from the output of the encoder. This allows every
position in the decoder to attend over all positions in the input sequence. This mimics the
typical encoder-decoder attention mechanisms in sequence-to-sequence models such as

[38, 2, 9].


    - The encoder contains self-attention layers. In a self-attention layer all of the keys, values
and queries come from the same place, in this case, the output of the previous layer in the
encoder. Each position in the encoder can attend to all positions in the previous layer of the
encoder.


    - Similarly, self-attention layers in the decoder allow each position in the decoder to attend to
all positions in the decoder up to and including that position. We need to prevent leftward
information flow in the decoder to preserve the auto-regressive property. We implement this
inside of scaled dot-product attention by masking out (setting to _−∞_ ) all values in the input
of the softmax which correspond to illegal connections. See Figure 2.
Source: attention_is_all_you_need_raw_chunks_2k.jsonl

Content: chain-of-thought (CoT) paradigm has been argued to mirror step-by-step human reasoning, leading to improved problem-solving performance. These findings motivate the design of interpretable,
functionally specialized modules in artificial networks, bridging insights from neuroscience with
advances in multimodal reasoning.


**Attention Heads in Vision–Language Models.** A growing body of interpretability research has
revealed that attention heads in LLMs exhibit functional specialization, such as pattern induction,
truthfulness, information retrieval, and safety alignment (Olsson et al., 2022; Li et al., 2023a; Wu
et al.; Zhou et al., 2024; Zheng et al.). Ma et al. further investigates the diverse cognitive roles that
attention heads play in supporting LLM reasoning.


In the multimodal domain, recent works (Li et al., 2020) have begun to explore the internal mechanisms of Vision–Language Models (VLMs). Studies have shown that certain sparse attention heads
play distinct roles in visual grounding, enabling alignment between textual tokens and image regions without additional fine-tuning (Kang et al., 2025; Bi et al., 2025). Similarly, probing studies
on multimodal pre-trained models (e.g., ViLBERT, LXMERT, UNITER) demonstrate that subsets
of attention heads encode cross-modal interactions and semantic alignment between vision and language (Cao et al., 2020). These works highlight the existence of specialized heads in VLMs but
largely focus on perception-oriented tasks such as grounding or alignment. In contrast, we investigate functionally specialized heads under more complex reasoning settings by aligning attention
head behavior with human cognitive functions.


6 CONCLUSION


We propose an interpretability framework that links attention heads in VLMs to human perceptual
and cognitive functions involved in multimodal reasoning. To enable this, we introduce CogVision, a cognitively grounded dataset that decomposes complex multimodal questions into functional
reasoning steps, and apply probing-based analyses to identify specialized heads supporting these
functions. Our study across diverse VLM families reveals that functional heads are sparse, universal, and intrinsic properties of the models, while varying in number, distribution, and hierarchical
organization. Moreover, we find that certain heads exhibit cross-modal interactions. Intervention
experiments further reveal their causal importance. Our insights into the functional organization
of attention mechanisms provide a foundation for developing more interpretable, robust, and cognitively inspired vision-language models. While our work provides a first step toward exploring
potential similarities between the cognitive processes of VLMs and those of the human brain, we do
not claim complete alignment, nor do we equate observations and analyses of attention heads with
the full scope of human reasoning.


10


**Limitations** While our study provides an initial framework for analyzing attention heads in VLMs,
several limitations remain. We focus on eight predefined cognitive functions, which may not cover
the full spectrum of LLM capabilities; future work could expand this taxonomy to include finergrained or emergent functions. Additionally, we concentrate on attention heads, leaving other components such as MLPs unexplored. Further exploring advanced probing methods and extending the
analysis to other model components, could provide further understandings.


REFERENCES


Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical
report. _arXiv preprint arXiv:2303.08774_, 2023.


Guillaume Alain and Yoshua Bengio. Understanding intermediate layers using linear classifier
probes. _arXiv preprint arXiv:1610.01644_, 2016.


John R Anderson. _Rules of the mind_ . Psychology Press, 2014.


Lawrence W Barsalou. _Cognitive psychology: An overview for cognitive scientists_ . Psychology
Press, 2014.


Yonatan Belinkov. Probing classifiers: Promises, shortcomings, and advances. _Computational_
_Linguistics_, 48(1):207–219, 2022.


Jing Bi, Junjia Guo, Yunlong Tang, Lianggong Bruce Wen, Zhang Liu, Bingjie Wang, and Chenliang
Xu. Unveiling visual perception in language models: An attention head analysis approach. In
_Proceedings of the Computer Vision and Pattern Recognition Conference_, pp. 4135–4144, 2025.


Charles F Cadieu, Ha Hong, Daniel LK Yamins, Nicolas Pinto, Diego Ardila, Ethan A Solomon,
Najib J Majaj, and James J DiCarlo. Deep neural networks rival the representation of primate it
cortex for core visual object recognition. _PLoS computational biology_, 10(12):e1003963, 2014.


Jize Cao, Zhe Gan, Yu Cheng, Licheng Yu, Yen-Chun Chen, and Jingjing Liu. Behind the scene:
Revealing the secrets of pre-trained vision-and-language models. In _European Conference on_
_Computer Vision_, pp. 565–580. Springer, 2020.


Charlotte Caucheteux, Alexandre Gramfort, and Jean-R´emi King. Deep language algorithms predict
semantic comprehension from brain activity. _Scientific reports_, 12(1):16327, 2022.


Lin Chin-Yew. Rouge: A package for automatic evaluation of summaries. In _Proceedings of the_
_Workshop on Text Summarization Branches Out, 2004_, 2004.


Adele Diamond. Executive functions. _Annual review of psychology_, 64(1):135–168, 2013.


Edward M Hubbard, Manuela Piazza, Philippe Pinel, and Stanislas Dehaene. Interactions between
number and space in parietal cortex. _Nature reviews neuroscience_, 6(6):435–448, 2005.


Seil Kang, Jinyeong Kim, Junhyeok Kim, and Seong Jae Hwang. Your large vision-language model
only needs a few attention heads for visual grounding. In _Proceedings of the Computer Vision_
_and Pattern Recognition Conference_, pp. 9339–9350, 2025.


Kenneth Li, Oam Patel, Fernanda Vi´egas, Hanspeter Pfister, and Martin Wattenberg. Inference-time
intervention: Eliciting truthful answers from a language model. _Advances in Neural Information_
_Processing Systems_, 36:41451–41530, 2023a.


Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. What does bert with
vision look at? In _Proceedings of the 58th annual meeting of the association for computational_
_linguistics_, pp. 5265–5275, 2020.


Yijiang Li, Qingying Gao, Tianwei Zhao, Bingyang Wang, Haoran Sun, Haiyun Lyu, Robert D
Hawkins, Nuno Vasconcelos, Tal Golan, Dezhi Luo, et al. Core knowledge deficits in multimodal language models. _arXiv preprint arXiv:2410.10855_, 2024.


11


Zhuowan Li, Xingrui Wang, Elias Stengel-Eskin, Adam Kortylewski, Wufei Ma, Benjamin
Van Durme, and Alan L Yuille. Super-clevr: A virtual benchmark to diagnose domain robustness in visual reasoning. In _Proceedings of the IEEE/CVF conference on computer vision and_
_pattern recognition_, pp. 14963–14973, 2023b.


Adam Dahlgren Lindstr¨om and Savitha Sam Abraham. Clevr-math: A dataset for compositional
language, visual and mathematical reasoning. _arXiv preprint arXiv:2208.05358_, 2022.


Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. _Advances_
_in neural information processing systems_, 36:34892–34916, 2023.


Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren,
Zhuoshu Li, Hao Yang, et al. Deepseek-vl: towards real-world vision-language understanding.
_arXiv preprint arXiv:2403.05525_, 2024a.
Source: attention_functional_roles_raw_chunks_2k.jsonl

Content: [38, 2, 9].


    - The encoder contains self-attention layers. In a self-attention layer all of the keys, values
and queries come from the same place, in this case, the output of the previous layer in the
encoder. Each position in the encoder can attend to all positions in the previous layer of the
encoder.


    - Similarly, self-attention layers in the decoder allow each position in the decoder to attend to
all positions in the decoder up to and including that position. We need to prevent leftward
information flow in the decoder to preserve the auto-regressive property. We implement this
inside of scaled dot-product attention by masking out (setting to _−∞_ ) all values in the input
of the softmax which correspond to illegal connections. See Figure 2.


**3.3** **Position-wise Feed-Forward Networks**


In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully
connected feed-forward network, which is applied to each position separately and identically. This
consists of two linear transformations with a ReLU activation in between.


FFN( _x_ ) = max(0 _, xW_ 1 + _b_ 1) _W_ 2 + _b_ 2 (2)


While the linear transformations are the same across different positions, they use different parameters
from layer to layer. Another way of describing this is as two convolutions with kernel size 1.
The dimensionality of input and output is _d_ model = 512, and the inner-layer has dimensionality
_dff_ = 2048.


**3.4** **Embeddings and Softmax**


Similarly to other sequence transduction models, we use learned embeddings to convert the input
tokens and output tokens to vectors of dimension _d_ model. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In
our model, we share the same weight matrix between the two embedding layers and the pre-softmax
linear transformation, similar to [30]. In the embedding layers, we multiply those weights by _[√]_ _d_ model.


5


Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations
for different layer types. _n_ is the sequence length, _d_ is the representation dimension, _k_ is the kernel
size of convolutions and _r_ the size of the neighborhood in restricted self-attention.


Layer Type Complexity per Layer Sequential Maximum Path Length
Operations
Self-Attention _O_ ( _n_ [2] _· d_ ) _O_ (1) _O_ (1)
Recurrent _O_ ( _n · d_ [2] ) _O_ ( _n_ ) _O_ ( _n_ )
Convolutional _O_ ( _k · n · d_ [2] ) _O_ (1) _O_ ( _logk_ ( _n_ ))
Self-Attention (restricted) _O_ ( _r · n · d_ ) _O_ (1) _O_ ( _n/r_ )


**3.5** **Positional Encoding**


Since our model contains no recurrence and no convolution, in order for the model to make use of the
order of the sequence, we must inject some information about the relative or absolute position of the
tokens in the sequence. To this end, we add "positional encodings" to the input embeddings at the
bottoms of the encoder and decoder stacks. The positional encodings have the same dimension _d_ model
as the embeddings, so that the two can be summed. There are many choices of positional encodings,
learned and fixed [9].


In this work, we use sine and cosine functions of different frequencies:


_PE_ ( _pos,_ 2 _i_ ) = _sin_ ( _pos/_ 10000 [2] _[i/d]_ [model] )

_PE_ ( _pos,_ 2 _i_ +1) = _cos_ ( _pos/_ 10000 [2] _[i/d]_ [model] )


where _pos_ is the position and _i_ is the dimension. That is, each dimension of the positional encoding
corresponds to a sinusoid. The wavelengths form a geometric progression from 2 _π_ to 10000 _·_ 2 _π_ . We
chose this function because we hypothesized it would allow the model to easily learn to attend by
relative positions, since for any fixed offset _k_, _PEpos_ + _k_ can be represented as a linear function of
_PEpos_ .


We also experimented with using learned positional embeddings [9] instead, and found that the two
versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version
because it may allow the model to extrapolate to sequence lengths longer than the ones encountered
during training.


**4** **Why Self-Attention**


In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations
( _x_ 1 _, ..., xn_ ) to another sequence of equal length ( _z_ 1 _, ..., zn_ ), with _xi, zi ∈_ R _[d]_, such as a hidden
layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we
consider three desiderata.


One is the total computational complexity per layer. Another is the amount of computation that can
be parallelized, as measured by the minimum number of sequential operations required.


The third is the path length between long-range dependencies in the network. Learning long-range
dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the
ability to learn such dependencies is the length of the paths forward and backward signals have to
traverse in the network. The shorter these paths between any combination of positions in the input
and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare
the maximum path length between any two input and output positions in networks composed of the
different layer types.


As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially
executed operations, whereas a recurrent layer requires _O_ ( _n_ ) sequential operations. In terms of
computational complexity, self-attention layers are faster than recurrent layers when the sequence


6


length _n_ is smaller than the representation dimensionality _d_, which is most often the case with
sentence representations used by state-of-the-art models in machine translations, such as word-piece

[38] and byte-pair [31] representations. To improve computational performance for tasks involving
very long sequences, self-attention could be restricted to considering only a neighborhood of size _r_ in
the input sequence centered around the respective output position. This would increase the maximum
path length to _O_ ( _n/r_ ). We plan to investigate this approach further in future work.


A single convolutional layer with kernel width _k < n_ does not connect all pairs of input and output
positions. Doing so requires a stack of _O_ ( _n/k_ ) convolutional layers in the case of contiguous kernels,
or _O_ ( _logk_ ( _n_ )) in the case of dilated convolutions [18], increasing the length of the longest paths
between any two positions in the network. Convolutional layers are generally more expensive than
recurrent layers, by a factor of _k_ . Separable convolutions [6], however, decrease the complexity
considerably, to _O_ ( _k · n · d_ + _n · d_ [2] ). Even with _k_ = _n_, however, the complexity of a separable
convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,
the approach we take in our model.


As side benefit, self-attention could yield more interpretable models. We inspect attention distributions
from our models and present and discuss examples in the appendix. Not only do individual attention
heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic
and semantic structure of the sentences.


**5** **Training**


This section describes the training regime for our models.


**5.1** **Training Data and Batching**
Source: attention_is_all_you_need_raw_chunks_2k.jsonl

Content: We are excited about the future of attention-based models and plan to apply them to other tasks. We
plan to extend the Transformer to problems involving input and output modalities other than text and
to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs
such as images, audio and video. Making generation less sequential is another research goals of ours.


The code we used to train and evaluate our models is available at `[https://github.com/](https://github.com/tensorflow/tensor2tensor)`
`[tensorflow/tensor2tensor](https://github.com/tensorflow/tensor2tensor)` .


**Acknowledgements** We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful
comments, corrections and inspiration.


**References**


[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. _arXiv preprint_
_[arXiv:1607.06450](http://arxiv.org/abs/1607.06450)_, 2016.


[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. _CoRR_, abs/1409.0473, 2014.


[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural
machine translation architectures. _CoRR_, abs/1703.03906, 2017.


[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine
reading. _[arXiv preprint arXiv:1601.06733](http://arxiv.org/abs/1601.06733)_, 2016.


10


[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,
and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical
machine translation. _CoRR_, abs/1406.1078, 2014.


[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. _arXiv_
_[preprint arXiv:1610.02357](http://arxiv.org/abs/1610.02357)_, 2016.


[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation
of gated recurrent neural networks on sequence modeling. _CoRR_, abs/1412.3555, 2014.


[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural
network grammars. In _Proc. of NAACL_, 2016.


[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional sequence to sequence learning. _[arXiv preprint arXiv:1705.03122v2](http://arxiv.org/abs/1705.03122)_, 2017.


[10] Alex Graves. Generating sequences with recurrent neural networks. _arXiv preprint_
_[arXiv:1308.0850](http://arxiv.org/abs/1308.0850)_, 2013.


[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE Conference on Computer Vision and Pattern_
_Recognition_, pages 770–778, 2016.


[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in
recurrent nets: the difficulty of learning long-term dependencies, 2001.


[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. _Neural computation_,
9(8):1735–1780, 1997.


[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations
across languages. In _Proceedings of the 2009 Conference on Empirical Methods in Natural_
_Language Processing_, pages 832–841. ACL, August 2009.


[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring
the limits of language modeling. _[arXiv preprint arXiv:1602.02410](http://arxiv.org/abs/1602.02410)_, 2016.


[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In _Advances in Neural_
_Information Processing Systems, (NIPS)_, 2016.


[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In _International Conference_
_on Learning Representations (ICLR)_, 2016.


[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Koray Kavukcuoglu. Neural machine translation in linear time. _[arXiv preprint arXiv:1610.10099v2](http://arxiv.org/abs/1610.10099)_,
2017.


[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.
In _International Conference on Learning Representations_, 2017.


[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In _ICLR_, 2015.


[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. _arXiv preprint_
_[arXiv:1703.10722](http://arxiv.org/abs/1703.10722)_, 2017.


[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen
Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. _arXiv preprint_
_[arXiv:1703.03130](http://arxiv.org/abs/1703.03130)_, 2017.


[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task
sequence to sequence learning. _[arXiv preprint arXiv:1511.06114](http://arxiv.org/abs/1511.06114)_, 2015.


[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attentionbased neural machine translation. _[arXiv preprint arXiv:1508.04025](http://arxiv.org/abs/1508.04025)_, 2015.


11


[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated
corpus of english: The penn treebank. _Computational linguistics_, 19(2):313–330, 1993.


[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In
_Proceedings of the Human Language Technology Conference of the NAACL, Main Conference_,
pages 152–159. ACL, June 2006.


[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention
model. In _Empirical Methods in Natural Language Processing_, 2016.


[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive
summarization. _[arXiv preprint arXiv:1705.04304](http://arxiv.org/abs/1705.04304)_, 2017.
Source: attention_is_all_you_need_raw_chunks_2k.jsonl

Content: Provided proper attribution is provided, Google hereby grants permission to
reproduce the tables and figures in this paper solely for use in journalistic or
scholarly works.

## **Attention Is All You Need**



**Niki Parmar** _[∗]_
Google Research
```
nikip@google.com

```


**Ashish Vaswani** _[∗]_
Google Brain
```
avaswani@google.com

```

**Llion Jones** _[∗]_
Google Research
```
 llion@google.com

```


**Noam Shazeer** _[∗]_
Google Brain
```
noam@google.com

```


**Jakob Uszkoreit** _[∗]_
Google Research
```
usz@google.com

```


**Aidan N. Gomez** _[∗†]_
University of Toronto
```
aidan@cs.toronto.edu

```


**Łukasz Kaiser** _[∗]_
Google Brain
```
lukaszkaiser@google.com

```


**Illia Polosukhin** _[∗‡]_

```
             illia.polosukhin@gmail.com

```

**Abstract**


The dominant sequence transduction models are based on complex recurrent or
convolutional neural networks that include an encoder and a decoder. The best
performing models also connect the encoder and decoder through an attention
mechanism. We propose a new simple network architecture, the Transformer,
based solely on attention mechanisms, dispensing with recurrence and convolutions
entirely. Experiments on two machine translation tasks show these models to
be superior in quality while being more parallelizable and requiring significantly
less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including
ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,
our model establishes a new single-model state-of-the-art BLEU score of 41.8 after
training for 3.5 days on eight GPUs, a small fraction of the training costs of the
best models from the literature. We show that the Transformer generalizes well to
other tasks by applying it successfully to English constituency parsing both with
large and limited training data.


_∗_ Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started
the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and
has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head
attention and the parameter-free position representation and became the other person involved in nearly every
detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and
tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and
efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and
implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating
our research.

_†_ Work performed while at Google Brain.

_‡_ Work performed while at Google Research.


31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.


**1** **Introduction**


Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks
in particular, have been firmly established as state of the art approaches in sequence modeling and
transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous
efforts have since continued to push the boundaries of recurrent language models and encoder-decoder
architectures [38, 24, 15].


Recurrent models typically factor computation along the symbol positions of the input and output
sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden
states _ht_, as a function of the previous hidden state _ht−_ 1 and the input for position _t_ . This inherently
sequential nature precludes parallelization within training examples, which becomes critical at longer
sequence lengths, as memory constraints limit batching across examples. Recent work has achieved
significant improvements in computational efficiency through factorization tricks [21] and conditional
computation [32], while also improving model performance in case of the latter. The fundamental
constraint of sequential computation, however, remains.


Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in
the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms
are used in conjunction with a recurrent network.


In this work we propose the Transformer, a model architecture eschewing recurrence and instead
relying entirely on an attention mechanism to draw global dependencies between input and output.
The Transformer allows for significantly more parallelization and can reach a new state of the art in
translation quality after being trained for as little as twelve hours on eight P100 GPUs.


**2** **Background**


The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU

[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building
block, computing hidden representations in parallel for all input and output positions. In these models,
the number of operations required to relate signals from two arbitrary input or output positions grows
in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes
it more difficult to learn dependencies between distant positions [12]. In the Transformer this is
reduced to a constant number of operations, albeit at the cost of reduced effective resolution due
to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as
described in section 3.2.


Self-attention, sometimes called intra-attention is an attention mechanism relating different positions
of a single sequence in order to compute a representation of the sequence. Self-attention has been
used successfully in a variety of tasks including reading comprehension, abstractive summarization,
textual entailment and learning task-independent sentence representations [4, 27, 28, 22].


End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering and
language modeling tasks [34].


To the best of our knowledge, however, the Transformer is the first transduction model relying
entirely on self-attention to compute representations of its input and output without using sequencealigned RNNs or convolution. In the following sections, we will describe the Transformer, motivate
self-attention and discuss its advantages over models such as [17, 18] and [9].


**3** **Model Architecture**


Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].
Here, the encoder maps an input sequence of symbol representations ( _x_ 1 _, ..., xn_ ) to a sequence
of continuous representations **z** = ( _z_ 1 _, ..., zn_ ). Given **z**, the decoder then generates an output
sequence ( _y_ 1 _, ..., ym_ ) of symbols one element at a time. At each step the model is auto-regressive

[10], consuming the previously generated symbols as additional input when generating the next.


2


![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_is_all_you_need.pdf-2-0.png)

Figure 1: The Transformer - model architecture.


The Transformer follows this overall architecture using stacked self-attention and point-wise, fully
connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,
respectively.


**3.1** **Encoder and Decoder Stacks**
Source: attention_is_all_you_need_raw_chunks_2k.jsonl

Content: **6.2** **Model Variations**


To evaluate the importance of different components of the Transformer, we varied our base model
in different ways, measuring the change in performance on English-to-German translation on the


5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.


8


Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base
model. All metrics are on the English-to-German translation development set, newstest2013. Listed
perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to
per-word perplexities.













|Col1|train<br>N d model d ff h d k d v P drop ϵls steps|PPL BLEU params<br>(dev) (dev) ×106|
|---|---|---|
|base|6<br>512<br>2048<br>8<br>64<br>64<br>0.1<br>0.1<br>100K|4.92<br>25.8<br>65|
|(A)|1<br>512<br>512<br>4<br>128<br>128<br>16<br>32<br>32<br>32<br>16<br>16|5.29<br>24.9<br>5.00<br>25.5<br>4.91<br>25.8<br>5.01<br>25.4|
|(B)|16<br>32|5.16<br>25.1<br>58<br>5.01<br>25.4<br>60|
|(C)|2<br>4<br>8<br>256<br>32<br>32<br>1024<br>128<br>128<br>1024<br>4096|6.11<br>23.7<br>36<br>5.19<br>25.3<br>50<br>4.88<br>25.5<br>80<br>5.75<br>24.5<br>28<br>4.66<br>26.0<br>168<br>5.12<br>25.4<br>53<br>4.75<br>26.2<br>90|
|(D)|0.0<br>0.2<br>0.0<br>0.2|5.77<br>24.6<br>4.95<br>25.5<br>4.67<br>25.3<br>5.47<br>25.7|
|(E)|positional embedding instead of sinusoids|4.92<br>25.7|
|big|6<br>1024<br>4096<br>16<br>0.3<br>300K|**4.33**<br>**26.4**<br>213|


development set, newstest2013. We used beam search as described in the previous section, but no
checkpoint averaging. We present these results in Table 3.


In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,
keeping the amount of computation constant, as described in Section 3.2.2. While single-head
attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.


In Table 3 rows (B), we observe that reducing the attention key size _dk_ hurts model quality. This
suggests that determining compatibility is not easy and that a more sophisticated compatibility
function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,
bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our
sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical
results to the base model.


**6.3** **English Constituency Parsing**


To evaluate if the Transformer can generalize to other tasks we performed experiments on English
constituency parsing. This task presents specific challenges: the output is subject to strong structural
constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence
models have not been able to attain state-of-the-art results in small-data regimes [37].


We trained a 4-layer transformer with _dmodel_ = 1024 on the Wall Street Journal (WSJ) portion of the
Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,
using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences

[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens
for the semi-supervised setting.


We performed only a small number of experiments to select the dropout, both attention and residual
(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters
remained unchanged from the English-to-German base translation model. During inference, we


9


Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23
of WSJ)

|Parser|Training|WSJ23F1|
|---|---|---|
|Vinyals & Kaiser el al. (2014) [37]<br>Petrov et al. (2006) [29]<br>Zhu et al. (2013) [40]<br>Dyer et al.(2016) [8]|WSJ only, discriminative<br>WSJ only, discriminative<br>WSJ only, discriminative<br>WSJ only, discriminative|88.3<br>90.4<br>90.4<br>91.7|
|Transformer(4 layers)|WSJ only, discriminative|91.3|
|Zhu et al. (2013) [40]<br>Huang & Harper (2009) [14]<br>McClosky et al. (2006) [26]<br>Vinyals & Kaiser el al.(2014) [37]|semi-supervised<br>semi-supervised<br>semi-supervised<br>semi-supervised|91.3<br>91.3<br>92.1<br>92.1|
|Transformer(4 layers)|semi-supervised|92.7|
|Luong et al. (2015) [23]<br>Dyer et al.(2016) [8]|multi-task<br>generative|93.0<br>93.3|



increased the maximum output length to input length + 300. We used a beam size of 21 and _α_ = 0 _._ 3
for both WSJ only and the semi-supervised setting.


Our results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the
Recurrent Neural Network Grammar [8].


In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the BerkeleyParser [29] even when training only on the WSJ training set of 40K sentences.


**7** **Conclusion**


In this work, we presented the Transformer, the first sequence transduction model based entirely on
attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with
multi-headed self-attention.


For translation tasks, the Transformer can be trained significantly faster than architectures based
on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014
English-to-French translation tasks, we achieve a new state of the art. In the former task our best
model outperforms even all previously reported ensembles.


We are excited about the future of attention-based models and plan to apply them to other tasks. We
plan to extend the Transformer to problems involving input and output modalities other than text and
to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs
such as images, audio and video. Making generation less sequential is another research goals of ours.


The code we used to train and evaluate our models is available at `[https://github.com/](https://github.com/tensorflow/tensor2tensor)`
`[tensorflow/tensor2tensor](https://github.com/tensorflow/tensor2tensor)` .
Source: attention_is_all_you_need_raw_chunks_2k.jsonl

Content: ## **Sliding Window Attention Adaptation**

Yijiong Yu [a], Jiale Liu [b], Qingyun Wu [b], Huazheng Wang [a], and Ji Pei [c]


aOregon State University, {yuyiji, huazheng.wang}@oregonstate.edu
bPenn State University, {jiale.liu, qingyun.wu}@psu.edu
cDeepSolution, research@deepsolution.chat



**Abstract**


The self-attention mechanism in Transformer
based Large Language Models (LLMs) scales
quadratically with input length, making longcontext inference expensive. Sliding window attention (SWA) reduces this cost to linear complexity, but naively enabling complete
SWA at inference-time for models pretrained
with full attention (FA) causes severe longcontext performance degradation due to training–inference mismatch. This makes us wonder: _Can FA-pretrained LLMs be well adapted_
_to SWA without pretraining?_ We investigate
this by proposing Sliding Window Attention
Adaptation (SWAA), a set of practical recipes
that combine five methods for better adaptation: (i) applying SWA only during prefilling;
(ii) preserving “sink” tokens; (iii) interleaving
FA/SWA layers; (iv) chain-of-thought (CoT);
and (v) fine-tuning. Our experiments show that
SWA adaptation is feasible while non-trivial:
no single method suffices, yet specific synergistic combinations effectively recover the original
long-context performance. We further analyze
the performance-efficiency trade-offs of different SWAA configurations and provide recommended recipes for diverse scenarios. Our code
[is available at github.](https://github.com/yuyijiong/sliding-window-attention-adaptation)


**1** **Introduction**


Transformer-based Large Language Models
(LLMs) (Vaswani et al., 2017) demonstrate
remarkable capabilities, but their self-attention
scales quadratically with the input sequence length,
making long context processing inefficient. Sliding
Window Attention (SWA), the most straightforward and widely adopted sparse attention
pattern, which restricts each token’s attention to a
fixed-size local window, reduces the computational
complexity to linearity, along with some other
benefits (see Appendix A).
To apply SWA to LLMs, typical solutions involve training a model with SWA from scratch, but



are prohibitively costly and cannot match the performance of state-of-the-art full-causal-attention

models like Qwen3 (Team, 2025b), mainly due
to the inability to reproduce pretraining data.
Training-free methods like streaming attention
(Xiao et al., 2024) can stabilize LLM outputs by retaining “sink tokens” while applying SWA, which
greatly improve efficiency but inevitably suffer
from severe long-context performance degradation
possibly due to the inaccessibility of distant tokens’
information (Xiao, 2025). This motivates a critical,
unexplored question: _Can a full-attention model_
_be adapted to sliding window attention at low cost_
_while maintaining long-context performance?_
We answer Yes to this question by proposing
Sliding Window Attention Adaptation(SWAA), a
set of recipes for adapting FA-pretrained models to
SWA, which requires neither costly pretraining nor
modifications to the standard Transformer architecture. Specifically, it systematically combines five
practical and composable methods:


1. **Full Attention (FA) Decode** : applying SWA
only during the prefilling stage while switching back to full attention for decoding.


2. **Keep First** _k_ **Tokens** : explicitly preserving
attention to the first _k_ “sink” tokens.


3. **Interleaving FA/SWA layers** : mix fullattention and SWA layers (e.g., assigning
SWA to half layers).


4. **Chain-of-Thought (CoT)** : enforcing an explicit "thinking" process during decoding.


5. **Fine-tuning with SWA** : lightweight SWAaware supervised fine-tuning on long-context
data.


Among these, FA Decode is a novel method we
introduce. Keep First _k_ Tokens and FA/SWA Interleaving have been proven effective in prior work



1


(Xiao et al., 2024; Team, 2024a; Zhang et al., 2024),
while CoT and fine-tuning are common LLM techniques. However, how these methods should be
combined to be actually effective for SWA adaptation remains unexplored.
Therefore, in our experiments, we evaluate
SWAA on Qwen3 (Team, 2025b) and Llama3.1
(Team, 2024b) across several long-context benchmarks, measuring both performance and efficiency
under a wide range of SWAA recipes. First, we
find that each method makes a distinct contribution,
but no single ingredient suffices to make SWA competitive with full attention. Second, we show that
specific synergistic combinations of methods can
recover a large fraction of the original long-context
performance. Third, we analyze the performanceefficiency trade-offs of different SWAA recipes and
identify some recommended configurations suitable for different deployment scenarios.
Rather than proposing a single globally optimal
configuration, we view SWAA as a flexible toolkit
of practical recipes: practitioners can select SWAA
recipes that match their accuracy and efficiency
constraints, or compose their own SWA adaptation
strategies by combining the available ingredients.
Our key contributions are:


1. We perform the first systematic study on adapting FA-pretrained LLMs to SWA without pretraining, revealing novel insights about how
SWA impacts LLMs and providing a foundation for future research in efficient sparse
attention.


2. We propose SWAA, a set of practical
SWA adaptation recipes that offer a robust
performance-efficiency balance for various
use cases, accelerating LLM inference from
the bottom level.


3. We implement our methods with FlashAttention (Dao, 2024) and vLLM (Kwon
et al., 2023), making it plug-and-play and userfriendly for practical deployment.


**2** **Related Works**


The _O_ ( _N_ [2] ) complexity of self-attention in Transformers (Vaswani et al., 2017) has spurred a wide
field of research about more efficient language
model architectures. Among the two most popular technological routes are sparse attention and
linear attention.



**2.1** **Sparse Attention**


Our work falls in this category. Sliding Window
Attention (SWA) represents the most basic form
of local sparse attention, yet its performance is inherently limited. Therefore, model architectures
such as Longformer (Beltagy et al., 2020), BigBird
(Zaheer et al., 2020), and RATTENTION (Wang
et al., 2025) combine local SWA on most tokens
with special global attention on specific tokens to
create a more powerful, albeit still sparse, pattern.
Popular LLMs like Gemma2 (Team, 2024a) adopt
SWA in half of their layers to balance the efficiency
of SWA and peformance of FA. Sliding Window
Attention Training (SWAT) (Fu et al., 2025b) introduces architectural changes, such as sigmoid
activation and balanced position embeddings, to
stabilize SWA performance. More advanced methods like Deepseek-sparse-attention (Yuan et al.,
2025; DeepSeek-AI, 2025b), although achieving
excellent quality, involve more complicated implementation and optimization due to semantic-aware
attention operations (e.g., selecting the most important tokens based on attention weights). Regardless,
almost all of the above methods require pretraining with a specific sparse pattern, which is costly
and fails to leverage the advantages of existing pretrained models.


LightTransfer (Zhang et al., 2024) is a promising
attempt at adapting existing models to SWA without pretraining, which has the same motivation as
ours. But it may generalize poorly across model
families (see Appendix G).


**2.2** **Linear Attention**


An alternative approach involves reformulating
the attention mechanism entirely to achieve linear, _O_ ( _N_ ), complexity. This includes methods
such as RNN-like linear attention transformers
Source: sliding_window_attention_annotated_chunks_2k.jsonl
2025-12-18 16:49:39,353 - __main__ - INFO - Final Answer: [{'type': 'text', 'text': 'Attention, in the context of neural networks and large language models, is a mechanism that allows a model to focus on the most relevant parts of its input when processing information. It addresses limitations of traditional recurrent neural networks (RNNs) by enabling the modeling of dependencies regardless of their distance in a sequence, thereby improving representation learning and the ability to capture long-term dependencies (according to "Neuronal Attention Circuit (NAC) for Representation Learning" and "Attention Is All You Need" documents).\n\nMore specifically, an attention function works by mapping a query and a set of key-value pairs to an output. This output is calculated as a weighted sum of the values, where the weight assigned to each value is determined by a compatibility function between the query and its corresponding key (according to "Attention Is All You Need" document).\n\nThere are several types and applications of attention mechanisms:\n\n*   **Scaled Dot-Product Attention** computes dot products between a query and all keys, scales them, and then applies a softmax function to derive the weights for the values.\n*   **Multi-Head Attention (MHA)** extends this by allowing the model to attend to different representation subspaces in parallel. It achieves this by projecting queries, keys, and values multiple times, performing attention functions in parallel, and then concatenating and projecting the resulting outputs.\n*   **Self-Attention (Intra-attention)** is a specific type where all keys, values, and queries originate from the same source, such as the output of a previous layer in the network. This allows the model to relate different positions within a single sequence to compute a representation of that sequence.\n*   **Neuronal Attention Circuit (NAC)** is a biologically inspired continuous-time attention mechanism that redefines attention logits computation as the solution to a linear first-order ordinary differential equation (ODE) with nonlinear, interlinked gates.\n*   **Sliding Window Attention (SWA)** is a sparse attention pattern that limits each token\'s attention to a fixed-size local window, which helps reduce computational complexity, especially for long input sequences.\n\nAttention mechanisms offer several benefits, including improved representation learning, the ability to focus on relevant observations, and the capacity to model dependencies irrespective of their distance in input or output sequences. In models like the Transformer, attention allows for greater parallelization and faster training compared to architectures based on recurrent or convolutional layers. Furthermore, attention can lead to more interpretable models, as individual attention heads can specialize in different tasks related to the syntactic and semantic structure of sentences (according to "Attention Is All You Need" document).\n\nAttention is widely applied in various domains, including sequence transduction models, machine translation, reading comprehension, abstractive summarization, textual entailment, and learning task-independent sentence representations. It is also used in irregular time-series classification, lane-keeping for autonomous vehicles, and industrial prognostics. In Vision-Language Models (VLMs), attention heads demonstrate functional specialization for tasks such as visual grounding and cross-modal interactions (according to "attention_functional_roles_raw_chunks_2k.jsonl" document).', 'extras': {'signature': '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'}}]
2025-12-18 16:56:13,969 - __main__ - INFO - Initializing resources...
2025-12-18 16:56:13,970 - __main__ - INFO - Loading EmbeddingGemma model...
2025-12-18 16:56:13,972 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2025-12-18 16:56:13,972 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: google/embeddinggemma-300m
2025-12-18 16:56:20,440 - sentence_transformers.SentenceTransformer - INFO - 14 prompts are loaded, with the keys: ['query', 'document', 'BitextMining', 'Clustering', 'Classification', 'InstructionRetrieval', 'MultilabelClassification', 'PairClassification', 'Reranking', 'Retrieval', 'Retrieval-query', 'Retrieval-document', 'STS', 'Summarization']
2025-12-18 16:56:20,441 - __main__ - INFO - Loading vector store from E:\Python Stuff\MAS-for-multimodal-knowledge-graph\vector_store_outputs\index...
2025-12-18 16:56:20,447 - faiss.loader - INFO - Loading faiss with AVX2 support.
2025-12-18 16:56:20,474 - faiss.loader - INFO - Successfully loaded faiss with AVX2 support.
2025-12-18 16:56:20,486 - __main__ - INFO - User query received: what is attention?
2025-12-18 16:56:20,487 - __main__ - INFO - Starting Query Agent with query: 'what is attention?'
2025-12-18 16:56:20,487 - __main__ - INFO - Invoking RAG retrieval tool...
2025-12-18 16:56:20,488 - __main__ - INFO - RAG Tool invoked with user query: what is attention?
2025-12-18 16:56:20,774 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-18 16:56:20,793 - __main__ - INFO - Generated 3 RAG subqueries:
2025-12-18 16:56:20,794 - __main__ - INFO -   Subquery 1: attention mechanism
2025-12-18 16:56:20,795 - __main__ - INFO -   Subquery 2: definition of attention in deep learning
2025-12-18 16:56:20,795 - __main__ - INFO -   Subquery 3: what is attention in neural networks
2025-12-18 16:56:20,796 - __main__ - INFO - RAG internal retrieval query: attention mechanism
2025-12-18 16:56:20,957 - __main__ - INFO - RAG retrieved 3 documents for subquery 'attention mechanism'
2025-12-18 16:56:20,957 - __main__ - INFO - RAG internal retrieval query: definition of attention in deep learning
2025-12-18 16:56:21,043 - __main__ - INFO - RAG retrieved 3 documents for subquery 'definition of attention in deep learning'
2025-12-18 16:56:21,043 - __main__ - INFO - RAG internal retrieval query: what is attention in neural networks
2025-12-18 16:56:21,136 - __main__ - INFO - RAG retrieved 3 documents for subquery 'what is attention in neural networks'
2025-12-18 16:56:21,136 - __main__ - INFO - Total unique documents after de-duplication across all subqueries: 7
2025-12-18 16:56:21,136 - __main__ - INFO - RAG Unique Doc 1:
  Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
  Chunk ID: neuronal_attention_circuits_raw_chunks_2k_0
  Metadata: {'source': 'neuronal_attention_circuits_raw.md'}
2025-12-18 16:56:21,136 - __main__ - INFO - RAG Unique Doc 2:
  Source: attention_is_all_you_need_raw_chunks_2k.jsonl
  Chunk ID: attention_is_all_you_need_raw_chunks_2k_1
  Metadata: {'source': 'attention_is_all_you_need_raw.md'}
2025-12-18 16:56:21,136 - __main__ - INFO - RAG Unique Doc 3:
  Source: attention_functional_roles_raw_chunks_2k.jsonl
  Chunk ID: attention_functional_roles_raw_chunks_2k_6
  Metadata: {'source': 'attention_functional_roles_raw.md'}
2025-12-18 16:56:21,136 - __main__ - INFO - RAG Unique Doc 4:
  Source: attention_is_all_you_need_raw_chunks_2k.jsonl
  Chunk ID: attention_is_all_you_need_raw_chunks_2k_2
  Metadata: {'source': 'attention_is_all_you_need_raw.md'}
2025-12-18 16:56:21,137 - __main__ - INFO - RAG Unique Doc 5:
  Source: attention_is_all_you_need_raw_chunks_2k.jsonl
  Chunk ID: attention_is_all_you_need_raw_chunks_2k_5
  Metadata: {'source': 'attention_is_all_you_need_raw.md'}
2025-12-18 16:56:21,137 - __main__ - INFO - RAG Unique Doc 6:
  Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
  Chunk ID: neuronal_attention_circuits_raw_chunks_2k_3
  Metadata: {'source': 'neuronal_attention_circuits_raw.md'}
2025-12-18 16:56:21,137 - __main__ - INFO - RAG Unique Doc 7:
  Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
  Chunk ID: neuronal_attention_circuits_raw_chunks_2k_12
  Metadata: {'source': 'neuronal_attention_circuits_raw.md'}
2025-12-18 16:56:21,137 - __main__ - INFO - RAG context returned to LLM (multi-subquery, k=3, deduplicated): num_chunks=7, total_chars=47824
2025-12-18 16:56:21,137 - __main__ - INFO - Invoking Graph retrieval tool...
2025-12-18 16:56:21,138 - __main__ - INFO - Graph Tool invoked with user query: what is attention?
2025-12-18 16:56:21,138 - __main__ - INFO - Connecting to Neo4j at neo4j+s://d15b3f68.databases.neo4j.io...
2025-12-18 16:56:23,321 - __main__ - INFO - Resolving entities from graph for query: what is attention?
2025-12-18 16:56:23,519 - __main__ - WARNING - No matching entities found.
2025-12-18 16:56:23,521 - __main__ - INFO - Synthesizing final answer from RAG and Graph contexts...
2025-12-18 16:56:33,528 - __main__ - INFO - Final Answer: Attention is a mechanism that allows models to focus on the most relevant observations within sequential data, thereby improving representation learning. It addresses limitations of traditional recurrent neural networks (RNNs), such as vanishing gradients and difficulties with irregularly sampled data, by treating all time steps equally.

Fundamentally, an attention function maps a query and a set of key-value pairs to an output. This output is calculated as a weighted sum of the values, where the weight assigned to each value is determined by a compatibility function between the query and its corresponding key.

Key types and concepts of attention include:

*   **Scaled Dot-Product Attention:** This common form computes the dot products of queries with all keys, scales them by the key dimension, and then applies a softmax function to obtain the weights for the values. The formula is `Attention(Q, K, V) = softmax(QK^T / sqrt(dk))V`.
*   **Multi-Head Attention (MHA):** This extends single attention by allowing the model to attend to different representation subspaces in parallel. It achieves this by linearly projecting queries, keys, and values multiple times, performing attention functions in parallel on these projected versions, concatenating the outputs, and then projecting them again.
*   **Self-Attention:** In this variant, all queries, keys, and values originate from the same source (e.g., the output of a previous layer), enabling each position in a sequence to attend to all other positions within that sequence.
*   **Continuous-time Attention (CTA) and Neuronal Attention Circuit (NAC):** These are more recent advancements that integrate continuous-time dynamics, often using Ordinary Differential Equations (ODEs), to model attention. NAC, for instance, computes attention logits as the solution to a first-order ODE, modulated by nonlinear, interlinked gates inspired by biological neuronal circuits.

Attention mechanisms are integral to models like the Transformer, and their individual "attention heads" have been shown to exhibit functional specialization in Large Language Models (LLMs) and Vision-Language Models (VLMs), performing tasks such as pattern induction, information retrieval, and visual grounding.
