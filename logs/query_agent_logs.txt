2025-12-18 16:56:13,969 - __main__ - INFO - Initializing resources...
2025-12-18 16:56:13,970 - __main__ - INFO - Loading EmbeddingGemma model...
2025-12-18 16:56:13,972 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2025-12-18 16:56:13,972 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: google/embeddinggemma-300m
2025-12-18 16:56:20,440 - sentence_transformers.SentenceTransformer - INFO - 14 prompts are loaded, with the keys: ['query', 'document', 'BitextMining', 'Clustering', 'Classification', 'InstructionRetrieval', 'MultilabelClassification', 'PairClassification', 'Reranking', 'Retrieval', 'Retrieval-query', 'Retrieval-document', 'STS', 'Summarization']
2025-12-18 16:56:20,441 - __main__ - INFO - Loading vector store from E:\Python Stuff\MAS-for-multimodal-knowledge-graph\vector_store_outputs\index...
2025-12-18 16:56:20,447 - faiss.loader - INFO - Loading faiss with AVX2 support.
2025-12-18 16:56:20,474 - faiss.loader - INFO - Successfully loaded faiss with AVX2 support.
2025-12-18 16:56:20,486 - __main__ - INFO - User query received: what is attention?
2025-12-18 16:56:20,487 - __main__ - INFO - Starting Query Agent with query: 'what is attention?'
2025-12-18 16:56:20,487 - __main__ - INFO - Invoking RAG retrieval tool...
2025-12-18 16:56:20,488 - __main__ - INFO - RAG Tool invoked with user query: what is attention?
2025-12-18 16:56:20,774 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-18 16:56:20,793 - __main__ - INFO - Generated 3 RAG subqueries:
2025-12-18 16:56:20,794 - __main__ - INFO -   Subquery 1: attention mechanism
2025-12-18 16:56:20,795 - __main__ - INFO -   Subquery 2: definition of attention in deep learning
2025-12-18 16:56:20,795 - __main__ - INFO -   Subquery 3: what is attention in neural networks
2025-12-18 16:56:20,796 - __main__ - INFO - RAG internal retrieval query: attention mechanism
2025-12-18 16:56:20,957 - __main__ - INFO - RAG retrieved 3 documents for subquery 'attention mechanism'
2025-12-18 16:56:20,957 - __main__ - INFO - RAG internal retrieval query: definition of attention in deep learning
2025-12-18 16:56:21,043 - __main__ - INFO - RAG retrieved 3 documents for subquery 'definition of attention in deep learning'
2025-12-18 16:56:21,043 - __main__ - INFO - RAG internal retrieval query: what is attention in neural networks
2025-12-18 16:56:21,136 - __main__ - INFO - RAG retrieved 3 documents for subquery 'what is attention in neural networks'
2025-12-18 16:56:21,136 - __main__ - INFO - Total unique documents after de-duplication across all subqueries: 7
2025-12-18 16:56:21,136 - __main__ - INFO - RAG Unique Doc 1:
  Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
  Chunk ID: neuronal_attention_circuits_raw_chunks_2k_0
  Metadata: {'source': 'neuronal_attention_circuits_raw.md'}
2025-12-18 16:56:21,136 - __main__ - INFO - RAG Unique Doc 2:
  Source: attention_is_all_you_need_raw_chunks_2k.jsonl
  Chunk ID: attention_is_all_you_need_raw_chunks_2k_1
  Metadata: {'source': 'attention_is_all_you_need_raw.md'}
2025-12-18 16:56:21,136 - __main__ - INFO - RAG Unique Doc 3:
  Source: attention_functional_roles_raw_chunks_2k.jsonl
  Chunk ID: attention_functional_roles_raw_chunks_2k_6
  Metadata: {'source': 'attention_functional_roles_raw.md'}
2025-12-18 16:56:21,136 - __main__ - INFO - RAG Unique Doc 4:
  Source: attention_is_all_you_need_raw_chunks_2k.jsonl
  Chunk ID: attention_is_all_you_need_raw_chunks_2k_2
  Metadata: {'source': 'attention_is_all_you_need_raw.md'}
2025-12-18 16:56:21,137 - __main__ - INFO - RAG Unique Doc 5:
  Source: attention_is_all_you_need_raw_chunks_2k.jsonl
  Chunk ID: attention_is_all_you_need_raw_chunks_2k_5
  Metadata: {'source': 'attention_is_all_you_need_raw.md'}
2025-12-18 16:56:21,137 - __main__ - INFO - RAG Unique Doc 6:
  Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
  Chunk ID: neuronal_attention_circuits_raw_chunks_2k_3
  Metadata: {'source': 'neuronal_attention_circuits_raw.md'}
2025-12-18 16:56:21,137 - __main__ - INFO - RAG Unique Doc 7:
  Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
  Chunk ID: neuronal_attention_circuits_raw_chunks_2k_12
  Metadata: {'source': 'neuronal_attention_circuits_raw.md'}
2025-12-18 16:56:21,137 - __main__ - INFO - RAG context returned to LLM (multi-subquery, k=3, deduplicated): num_chunks=7, total_chars=47824
2025-12-18 16:56:21,137 - __main__ - INFO - Invoking Graph retrieval tool...
2025-12-18 16:56:21,138 - __main__ - INFO - Graph Tool invoked with user query: what is attention?
2025-12-18 16:56:21,138 - __main__ - INFO - Connecting to Neo4j at neo4j+s://d15b3f68.databases.neo4j.io...
2025-12-18 16:56:23,321 - __main__ - INFO - Resolving entities from graph for query: what is attention?
2025-12-18 16:56:23,519 - __main__ - WARNING - No matching entities found.
2025-12-18 16:56:23,521 - __main__ - INFO - Synthesizing final answer from RAG and Graph contexts...
2025-12-18 16:56:33,528 - __main__ - INFO - Final Answer: Attention is a mechanism that allows models to focus on the most relevant observations within sequential data, thereby improving representation learning. It addresses limitations of traditional recurrent neural networks (RNNs), such as vanishing gradients and difficulties with irregularly sampled data, by treating all time steps equally.

Fundamentally, an attention function maps a query and a set of key-value pairs to an output. This output is calculated as a weighted sum of the values, where the weight assigned to each value is determined by a compatibility function between the query and its corresponding key.

Key types and concepts of attention include:

*   **Scaled Dot-Product Attention:** This common form computes the dot products of queries with all keys, scales them by the key dimension, and then applies a softmax function to obtain the weights for the values. The formula is `Attention(Q, K, V) = softmax(QK^T / sqrt(dk))V`.
*   **Multi-Head Attention (MHA):** This extends single attention by allowing the model to attend to different representation subspaces in parallel. It achieves this by linearly projecting queries, keys, and values multiple times, performing attention functions in parallel on these projected versions, concatenating the outputs, and then projecting them again.
*   **Self-Attention:** In this variant, all queries, keys, and values originate from the same source (e.g., the output of a previous layer), enabling each position in a sequence to attend to all other positions within that sequence.
*   **Continuous-time Attention (CTA) and Neuronal Attention Circuit (NAC):** These are more recent advancements that integrate continuous-time dynamics, often using Ordinary Differential Equations (ODEs), to model attention. NAC, for instance, computes attention logits as the solution to a first-order ODE, modulated by nonlinear, interlinked gates inspired by biological neuronal circuits.

Attention mechanisms are integral to models like the Transformer, and their individual "attention heads" have been shown to exhibit functional specialization in Large Language Models (LLMs) and Vision-Language Models (VLMs), performing tasks such as pattern induction, information retrieval, and visual grounding.
