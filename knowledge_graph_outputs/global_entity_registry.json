[
  "Accuracy",
  "Add & Norm layer",
  "Albert Gu",
  "Angelos Katharopoulos",
  "Ashish Vaswani",
  "Attention map",
  "Attention mechanism",
  "Attention weights",
  "Average running time per request",
  "Bai et al., 2023",
  "Bai et al., 2024a",
  "Bai et al., 2024b",
  "Bailin Wang",
  "Bamba",
  "Beltagy et al., 2020",
  "BigBird",
  "Bo Peng",
  "Catastrophic degradation",
  "Chain-of-Thought",
  "ChatGPT",
  "Cheng-Ping Hsieh",
  "Concatenation",
  "Dan Hendrycks",
  "Dao, 2024",
  "David Rein",
  "Decode phase",
  "Decoder",
  "Deep reasoning tasks",
  "DeepSeek-AI",
  "DeepSeek-AI, 2025a",
  "DeepSeek-AI, 2025b",
  "DeepSeek-R1",
  "DeepSeek-v3.2",
  "DeepSolution",
  "Deepseek-sparse-attention",
  "DeepseekMath",
  "Di Wu",
  "Edward J. Hu",
  "Efficient memory management",
  "Encoder",
  "Encoder attention head",
  "FA Decode",
  "FA Decode parameter",
  "FA Layers parameter",
  "Feed Forward Network",
  "Fine-tuning",
  "Flash-Attention-2",
  "FlashAttention",
  "Fu et al., 2025a",
  "Fu et al., 2025b",
  "Full Attention",
  "Full Attention Decode",
  "Full Attention models",
  "Fusang-v1-long",
  "GPQA",
  "GPT-5-Mini",
  "GPU memory",
  "GPU utilization",
  "GRPO",
  "Gemma Team",
  "Gemma Team, 2024a",
  "Gemma Team, 2025a",
  "Gemma2",
  "Gemma3",
  "Gu and Dao, 2023",
  "Guangxuan Xiao",
  "Hendrycks et al., 2021",
  "Hsieh et al., 2024",
  "Hu et al., 2022",
  "Huazheng Wang",
  "HuggingFace Transformers",
  "Hugo Touvron",
  "Hybrid Attention Architecture",
  "Input Embedding",
  "Interleaving FA/SWA layers",
  "Interleaving Layers",
  "Jamba",
  "Jason Wei",
  "Ji Pei",
  "Jiale Liu",
  "Jingyang Yuan",
  "K (key)",
  "KV cache",
  "KV cache eviction",
  "KV cache reusability",
  "Katharopoulos et al., 2020",
  "Keep First k Tokens",
  "Key (K)",
  "Kwon et al., 2023",
  "Large Language Models",
  "Lieber et al., 2024",
  "LightTransfer",
  "Linear attention",
  "Linear layer",
  "Ling Team",
  "Linsong Chu",
  "Linsong Chu et al., 2024",
  "Llama",
  "Llama Team",
  "Llama Team, 2024b",
  "Llama3",
  "Llama3.1",
  "Llama3.1-8B",
  "Llama3.1-8B-Instruct",
  "LoRA",
  "Long context processing",
  "Long-context benchmarks",
  "Long-context fine-tuning",
  "Long-context tasks",
  "LongAlign",
  "LongBench",
  "LongBench-V2",
  "LongMemEval",
  "LongMemEval_24k",
  "Longformer",
  "MMLU",
  "Mamba",
  "Manzil Zaheer",
  "Masked Multi-Head Attention",
  "Masking layer",
  "Matrix Multiplication",
  "Multi-Head Attention",
  "Multi-head attention",
  "Naive SWA",
  "Native Sparse Attention",
  "Natural Language Processing",
  "Nemotron-Flash",
  "OpenAI",
  "OpenAI, 2025",
  "Opher Lieber",
  "Oregon State University",
  "Output Embedding",
  "Output Probabilities",
  "PagedAttention",
  "Pan, 2024",
  "Peng et al., 2023",
  "Penn State University",
  "Performance–efficiency Trade-offs",
  "Positional Encoding",
  "Prefill phase",
  "Q (query)",
  "Qingyun Wu",
  "Query (Q)",
  "Qwen",
  "Qwen3",
  "Qwen3 Team",
  "Qwen3 Team, 2025b",
  "Qwen3-30B",
  "Qwen3-30B-A3B",
  "Qwen3-30B-A3B-Instruct",
  "Qwen3-30B-A3B-Thinking",
  "Qwen3-4B",
  "Qwen3-4B-Instruct",
  "Qwen3-4B-Thinking",
  "RATTENTION",
  "RNN-like linear attention transformers",
  "RWKV",
  "Rattention",
  "Real-world tasks",
  "Recurrent Neural Networks",
  "Rein et al., 2023",
  "Reinforcement Learning",
  "Retentive Network",
  "Ruler",
  "SFT",
  "SWA adaptation",
  "SWA-aware fine-tuning",
  "Scale operation",
  "Scaled Dot-Product Attention",
  "Scatter plot",
  "Self-attention mechanism",
  "Self-distillation",
  "Sentence structure",
  "Sentence visualization",
  "Sequence Modeling",
  "Shao et al., 2024",
  "Sliding Window Attention",
  "Sliding Window Attention Adaptation",
  "Sliding Window Attention Adaptation Code Repository",
  "Sliding Window Attention Training",
  "SoftMax function",
  "Softmax layer",
  "Sparse attention",
  "Streaming Attention",
  "Sun et al., 2023",
  "Team et al., 2025",
  "Team, 2024a",
  "Team, 2025a",
  "Team, 2025b",
  "Throughput",
  "Time",
  "Time-per-output-token",
  "Time-to-first-token",
  "Token relationships",
  "Tokens",
  "Touvron et al., 2023",
  "Transformer architecture",
  "Tri Dao",
  "V (value)",
  "Value (V)",
  "Vaswani et al., 2017",
  "Wang et al., 2025",
  "Wei et al., 2022",
  "Wenbo Pan",
  "Window parameter",
  "Woosuk Kwon",
  "Word embeddings",
  "Wu et al., 2024",
  "Xiao et al., 2024",
  "Xiao, 2025",
  "Xuan Zhang",
  "Yang et al., 2024",
  "Yijiong Yu",
  "Yonggan Fu",
  "Yuan et al., 2025",
  "Yushi Bai",
  "Yutao Sun",
  "Zaheer et al., 2020",
  "Zhang et al., 2024",
  "Zhaorui Yang",
  "Zhihong Shao",
  "Zichuan Fu",
  "anaphora resolution",
  "attention heads",
  "attention mask",
  "attention patterns",
  "attention sinks",
  "attention weights",
  "attention_is_all_you_need_raw_with_image_ids_with_captions",
  "attention_is_all_you_need_raw_with_image_ids_with_captions::section::3 Model Architecture",
  "attention_is_all_you_need_raw_with_image_ids_with_captions::section::3.2 Attention",
  "attention_is_all_you_need_raw_with_image_ids_with_captions::section::References",
  "catastrophic degradation",
  "computational complexity",
  "computational workflow",
  "decoding speed",
  "deep reasoning tasks",
  "deep understanding and reasoning",
  "distributed LLM services",
  "fixed-interval selection",
  "generation length",
  "human reading comprehension",
  "img_attention_is_all_you_need_12_0",
  "img_attention_is_all_you_need_13_0",
  "img_attention_is_all_you_need_14_0",
  "img_attention_is_all_you_need_2_0",
  "img_attention_is_all_you_need_3_0",
  "img_attention_is_all_you_need_3_1",
  "img_sliding_window_attention_2_0",
  "img_sliding_window_attention_7_0",
  "instruction-tuning",
  "key",
  "language model fine-tuning",
  "linear attention",
  "long context processing",
  "long-context QA tasks",
  "long-context fine-tuning",
  "long-context performance",
  "long-context tasks",
  "long-term interactive memory",
  "mathematical reasoning",
  "needle-retrieval tasks",
  "neural network layers",
  "performance restoration",
  "performance-efficiency trade-off",
  "performance-efficiency trade-offs",
  "prefilling",
  "processing stages",
  "query",
  "real-world tasks",
  "reasoning trajectory",
  "self-distillation",
  "sink tokens",
  "sliding_window_attention_raw_with_image_ids_with_captions",
  "sliding_window_attention_raw_with_image_ids_with_captions::section::3.3 Chain-of-Thought",
  "sliding_window_attention_raw_with_image_ids_with_captions::section::5.3 Performance–efficiency Trade-offs and",
  "sparse attention",
  "stacking sliding windows",
  "structured state-space models",
  "synthetic tasks",
  "training–inference mismatch",
  "triangular matrix structure",
  "vLLM",
  "value",
  "word tokens"
]