[
  "3D rendering",
  "4B-level Models",
  "A Massive Multi-Discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI",
  "AAAI Spring Symposia",
  "ACM",
  "Aaron van den Oord",
  "Abductive Reasoning",
  "Ablation study",
  "Abstractive summarization",
  "Accuracy",
  "Achiam et al., 2023",
  "Activation function",
  "Activation patching",
  "Activation patterns",
  "Activation-masking methods",
  "Active group",
  "Active memory",
  "Adam optimizer",
  "Adaptive checkpoint adjoint method",
  "Add & Norm layer",
  "Adhiguna Kuncoro",
  "Adjoint sensitivity method",
  "Advances in Neural Information Processing Systems",
  "Aguiar-Conraria and Soares, 2014",
  "Aidan N. Gomez",
  "Aijun Yang",
  "Aishwarya Kamath",
  "Alain & Bengio, 2016",
  "Albert Gu",
  "Alex Graves",
  "Alex Krizhevsky",
  "Alexander Keller",
  "Alexander M. Rush",
  "Alexandra Birch",
  "Alexandre Ram´e",
  "Ali Farhadi",
  "Alon Albalak",
  "Amir Bergman",
  "Amish Mittal",
  "Amr Ahmed",
  "An Yang",
  "Anderson, 2014",
  "Andy Davis",
  "Andy Zou",
  "Anfeng Li",
  "Angelos Katharopoulos",
  "Anirudh Ravula",
  "Ankur Parikh",
  "Anna Goldie",
  "Answer format",
  "Aojun Zhou",
  "Apoorv Vyas",
  "Arithmetic",
  "Arman Cohan",
  "Armand Joulin",
  "Arthur Szlam",
  "Asa Cooper Stickland",
  "Ashish Vaswani",
  "Asif Ekbal",
  "Association for Computational Linguistics",
  "Atari",
  "Atsushi Saito",
  "Attention Heads of Large Language Models: A Survey",
  "Attention Score",
  "Attention heads",
  "Attention is all you need.",
  "Attention map",
  "Attention mechanism",
  "Attention sinks",
  "Attention weights",
  "Aurelien Rodriguez",
  "Autonoetic Consciousness",
  "Autonomous Vehicles",
  "Avashalom Manevich",
  "Average running time per request",
  "Azalia Mirhoseini",
  "B Tang",
  "BERT",
  "BLEU score",
  "BPTT",
  "Ba et al., 2016",
  "Backbone",
  "Backbone circuit",
  "Backpropagation through time (BPTT)",
  "Bahdanau et al., 2014",
  "Bai et al., 2023",
  "Bai et al., 2024a",
  "Bai et al., 2024b",
  "Bailin Wang",
  "Ball Bearing Fault Diagnosis",
  "Bamba",
  "Bamba: Inference-efficient hybrid mamba2 model.",
  "Baosong Yang",
  "Baptiste Rozière",
  "Barak Lenz",
  "Barry Haddow",
  "Barsalou, 2014",
  "Bart van Merrienboer",
  "Bartłomiej Koptyra",
  "Bathroom renovation",
  "Beam search",
  "Beatrice Santorini",
  "Beichen Zhang",
  "Beidi Chen",
  "Belinkov, 2022",
  "Beltagy et al., 2020",
  "BerkeleyParser",
  "Bert rediscovers the classical nlp pipeline",
  "Betty Li Hou",
  "Bi et al., 2025",
  "Big bird: Transformers for longer sequences.",
  "BigBird",
  "Bin Han",
  "Bing Xiang",
  "Binyuan Hui",
  "Biologically Inspired Models",
  "Blue sphere",
  "Bo Peng",
  "Bo Zheng",
  "Bojarski et al., 2016",
  "Boris Ginsburg",
  "Bowen Yu",
  "Bowen Zhou",
  "Brian Ichter",
  "Brockman et al., 2016",
  "Byte-pair encoding",
  "ByteNet",
  "CT-RNN",
  "CTA",
  "Cadieu et al., 2014",
  "Caglar Gulcehre",
  "Caiming Xiong",
  "Caizhi Tang",
  "Cao et al., 2003",
  "Cao et al., 2020",
  "Car behavioral cloning",
  "Carina Kauf",
  "Carl Denton",
  "Carle",
  "Case Study",
  "Case group",
  "Catastrophic degradation",
  "Caucheteux et al., 2022",
  "Cell wall",
  "CfC",
  "Chain of Thought (CoT) format",
  "Chain of Thought (CoT) prompting",
  "Chain-of-Thought",
  "Chain-of-of-thought",
  "Chain-of-thought (CoT) format",
  "Chain-of-thought prompting elicits reasoning in large language models.",
  "Chang Gao",
  "Chang Lan",
  "Changxing Sumyoung Technology",
  "Chao Du",
  "Charles F Cadieu",
  "ChatGPT",
  "ChatGPT.",
  "Chemical concentration formulas",
  "Chemical solution",
  "Chen Liang",
  "Chen et al., 2018",
  "Chen et al., 2023",
  "Cheng et al., 2016",
  "Cheng-Ping Hsieh",
  "Chengen Huang",
  "Chenxu Lv",
  "Chien & Chen, 2021",
  "Chin-Yew, 2004",
  "Cho et al., 2014",
  "Chong Ruan",
  "Chong Wang",
  "Chris Alberti",
  "Chris Dyer",
  "Christian Szegedy",
  "Christopher D Manning",
  "Chung et al., 2014",
  "Cicero Nogueira dos Santos",
  "Clevr-Math",
  "Clevr-Math dataset",
  "Clock Design",
  "Closed form continuous-time neural networks",
  "Closed-form solution",
  "Cody Hao Yu",
  "CogVisioin Dataset",
  "CogVision",
  "CogVision Dataset",
  "Cognition",
  "Cognitive Task",
  "Cognitive Tasks",
  "Cognitive ability",
  "Cognitive functions",
  "Cognitive heads",
  "Cognitive organization of VLMs",
  "Cognitive science",
  "Cognitive skills",
  "Collin Burns",
  "Color",
  "Color gradient",
  "Command neuron",
  "Comparison",
  "Complex Reasoning Tasks",
  "Computational complexity",
  "Computational overhead",
  "Computational requirements",
  "Computational units",
  "Compute logits",
  "Computer vision",
  "Concatenation",
  "Concentration",
  "Condition monitoring",
  "Context Understanding",
  "ContiFormer",
  "Contingency table",
  "Continuous wavelet transform (CWT)",
  "Continuous-Time Attention (CTA)",
  "Continuous-Time Modeling",
  "Continuous-time Attention (CTA)",
  "Continuous-time Neural Network",
  "Control group",
  "ConvS2S",
  "Convolutional neural networks",
  "Corina Sas",
  "Correlation coefficient",
  "Correlation matrix",
  "Counting",
  "Counts",
  "Cross-domain adaptation",
  "Cross-function interactions",
  "Cross-modal integration",
  "Cunxiao Du",
  "Dale Schuurmans",
  "Damai Dai",
  "Dan Hendrycks",
  "Dan Klein",
  "Daniel LK Yamins",
  "Dao, 2024",
  "Darren Seibert",
  "Data points",
  "David Grangier",
  "David McClosky",
  "David Rein",
  "Dawn Song",
  "Daya Guo",
  "De Brouwer et al., 2019",
  "Decision function",
  "Decision skill",
  "Decision-Making",
  "Decision-Making task",
  "Decode phase",
  "Decoder",
  "Decoding speed",
  "Decomposable attention model",
  "Deductive Reasoning",
  "Deep reasoning tasks",
  "Deep recurrent models",
  "Deep reinforced model",
  "Deep residual learning",
  "Deep-Att + PosUnk",
  "DeepSeek-AI",
  "DeepSeek-AI, 2025a",
  "DeepSeek-AI, 2025b",
  "DeepSeek-R1",
  "DeepSeek-v3.2",
  "DeepSolution",
  "Deepseek-r1",
  "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.",
  "Deepseek-sparse-attention",
  "Deepseek-v3.2",
  "Deepseek-v3.2: Pushing the frontier of open large language models.",
  "DeepseekMath",
  "Deepseekmath: Pushing the limits of mathematical reasoning in open language models.",
  "Degradation Estimation",
  "Deng, 2012",
  "Denis Yarats",
  "Denny Britz",
  "Denny Zhou",
  "Depthwise separable convolutions",
  "Derong Xu",
  "Deyao Zhu",
  "Di Wu",
  "Diamond, 2013",
  "Diederik Kingma",
  "Dilated Convolutions",
  "Dima Rekesh",
  "Ding et al., 2021",
  "Dipanjan Das",
  "Disabled group",
  "Distributed LLM services",
  "Divya Kumari",
  "Donald T Stuss",
  "Dong Yu",
  "Dongfu Jiang",
  "Donghao Zhang",
  "Downstream tasks",
  "Drill",
  "Dropout",
  "Dyer",
  "Dyer et al.(2016)",
  "Dzmitry Bahdanau",
  "Ed H. Chi",
  "Edouard Grave",
  "Edward J. Hu",
  "Efficient memory management",
  "Efficient memory management for large language model serving with pagedattention.",
  "Efficient streaming language models with attention sinks.",
  "Eghbal A Hosseini",
  "Elif Akata",
  "Ellie Pavlick",
  "Encoder",
  "Encoder attention head",
  "Encoder-decoder architectures",
  "End-to-end memory networks",
  "Endel Tulving",
  "English constituency parsing",
  "Episodic Memory",
  "Equilibrium solution",
  "Erez Safahi",
  "Erez Shwartz",
  "Eric Alcaide",
  "Eric Hambro",
  "Eric Schulz",
  "Error propagation",
  "Ethan A Solomon",
  "Eugene Charniak",
  "Euler mode",
  "Evaluation of Evidence",
  "Evelina Fedorenko",
  "Event-based MNIST",
  "Every attention matters: An efficient hybrid architecture for long-context reasoning.",
  "Everyday Concepts",
  "Exact mode",
  "Expected degradation",
  "Expert AGI",
  "Exposure history",
  "Extended Neural GPU",
  "F Xiong",
  "FA Decode",
  "FA Decode parameter",
  "FA Layers parameter",
  "FA-pretrained models",
  "Factual Knowledge",
  "Failure Case",
  "Faisal Azhar",
  "Fan Yuan",
  "Fast-forward connections",
  "Feed Forward Network",
  "Feedback loop",
  "Fei Huang",
  "Fei Jia",
  "Fei Xia",
  "Feng Zhu",
  "Fengzhuo Zhang",
  "Ferdinand Mom",
  "Fethi Bougares",
  "Findings of the Association for Computational Linguistics: EMNLP 2023",
  "Fine-tuning",
  "Fine-tuning with SWA",
  "First token",
  "Flash-Attention-2",
  "FlashAttention",
  "Flashattention-2",
  "Flashattention-2: Faster attention with better parallelism and work partitioning.",
  "Flask",
  "Florian ’Floyd’ Mueller",
  "Francois Chollet",
  "François Fleuret",
  "Frequency",
  "From recognition to cognition: Visual commonsense reasoning",
  "Frontal lobe",
  "Frozen-coefficient approximation",
  "Fu et al., 2025a",
  "Fu et al., 2025b",
  "Full Attention",
  "Full Attention Decode",
  "Full Attention models",
  "Full Prompt",
  "Full connectivity",
  "Full-Attention Pretrained LLMs",
  "Function Graph",
  "Functional specialization",
  "Furu Wei",
  "Fusang-v1-long",
  "Fusang-v1: A large curation of instruction-tuning datasets for better bilingual and long-range llms.",
  "GNMT + RL",
  "GPQA",
  "GPT-2",
  "GPT-3",
  "GPT-4",
  "GPT-5-Mini",
  "GPTS",
  "GPU memory",
  "GPU utilization",
  "GRPO",
  "GRU",
  "GRU-ODE",
  "Gal Cohen",
  "Gated recurrent neural networks",
  "Gautier Izacard",
  "Ge Zhang",
  "Gemma",
  "Gemma 2: Improving open language models at a practical size.",
  "Gemma 3 technical report",
  "Gemma 3 technical report.",
  "Gemma Team",
  "Gemma Team, 2024a",
  "Gemma Team, 2025a",
  "Gemma-3",
  "Gemma2",
  "Gemma3",
  "Gemma3-2B",
  "Gemma3-4B",
  "Generated Answer",
  "Generated Full Answer",
  "Generating sequences",
  "Geoffrey E Hinton",
  "Geometric shapes",
  "GitHub",
  "Google",
  "Google Brain",
  "Google Colab",
  "Google Research",
  "Gpqa: A graduate-level google-proof q&a benchmark.",
  "Gradient estimation",
  "Gradient flow",
  "Gradient-Based Optimization",
  "Graves, 2013",
  "Green sphere",
  "Greta Tuckute",
  "Ground Truth",
  "Ground Truth Answer",
  "Ground truth",
  "Grounding",
  "Gu and Dao, 2023",
  "Guangxuan Xiao",
  "Guangyu Song",
  "Guillaume Lample",
  "Guru Guruganesh",
  "HUST dataset",
  "Ha Hong",
  "Haiyang Yu",
  "Hammer",
  "Han Wang",
  "Handyman",
  "Hannah Kim",
  "Hanoi University of Science and Technology",
  "Hanrong Ye",
  "Hao Peng",
  "Hao Tian",
  "Hao Zhang",
  "Haowei Zhang",
  "Haowen Hou",
  "Haozhe Feng",
  "Harper",
  "Hasani et al., 2021",
  "Hasani et al., 2022",
  "Hayden Lau",
  "He et al., 2016",
  "Head Ablation",
  "Head Number",
  "Head importance",
  "Head number",
  "Heatmap",
  "Hendrycks et al., 2021",
  "Hierarchical structure",
  "Hieu Pham",
  "High-Level Vision Reception",
  "High-Level Vision Reception task",
  "High-Level Visual Features",
  "High-Level Visual Reception",
  "High-Level Visual Reception task",
  "High-Level function",
  "High-Level skill",
  "High-level Visual Reception",
  "Higher Visual Cortex",
  "Higher-order reasoning",
  "Histogram",
  "History",
  "Hochreiter & Schmidhuber, 1997",
  "Hochreiter et al., 2001",
  "Hochreiter, 1998",
  "Hofit Bata",
  "Holger Schwenk",
  "Hong & Thuan, 2023",
  "Hongsheng Li",
  "Hongwei Wang",
  "Houqiang Li",
  "Houxing Ren",
  "Hsieh et al., 2024",
  "Hu et al., 2022",
  "Huang",
  "Huang & Harper (2009)",
  "Huanqi Cao",
  "Huazheng Wang",
  "Huazuo Gao",
  "Hubbard et al., 2005",
  "HuggingFace Transformers",
  "Hugo Touvron",
  "Human Learning",
  "Human brain",
  "Human brain anatomy",
  "Human cognition",
  "Human cognitive processes",
  "Human learning in atari",
  "Human reasoning",
  "Human-LLM Collaborative Annotation",
  "Human-LLM Collaborative Annotation Through Effective Verification of LLM Labels",
  "Human-aligned perceptual and reasoning abilities",
  "Hybrid Attention Architecture",
  "IEEE PHM",
  "Ian Tenney",
  "Idan Asher Blank",
  "Illia Polosukhin",
  "Ilya Sutskever",
  "Image captioning",
  "Image classification",
  "Image recognition",
  "In-Domain Data",
  "Inception architecture",
  "Industrial prognostics",
  "Industry 4.0",
  "Industry 4.0 Prognostics",
  "Inference",
  "Inference function",
  "Inference skill",
  "Inference task",
  "Info Under. & Extra",
  "Info function",
  "Information Loss",
  "Information Understanding",
  "Information Understanding & Extraction",
  "Information Understanding & Extraction task",
  "Information Understanding and Extraction",
  "Information skill",
  "Input Embedding",
  "Institute of Artificial Intelligence, Hefei Comprehensive National Science Center",
  "Instruction-tuning",
  "Instructions",
  "Interleaving FA/SWA layers",
  "Interleaving Layers",
  "Intern",
  "Intern3VL-2B model",
  "Intern3VL-8B model",
  "InternVL3",
  "InternVL3-2B",
  "InternVL3-2B model",
  "InternVL3-8B",
  "InternVL3-8B model",
  "Internal organization of large language models (LLMs)",
  "International Journal on Digital Libraries",
  "Interneuron",
  "Internvl3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models",
  "Interpretability",
  "Interpretability framework",
  "Intervention",
  "Intervention Function",
  "Invariant interval",
  "Ion Stoica",
  "Irina Shklovski",
  "Irregular Time-series Modeling",
  "Irregularly Sampled Time Series",
  "Itay Dalmedigos",
  "Iz Beltagy",
  "Izis Kankaraway",
  "JSON output format",
  "Jackson Petty",
  "Jacob Steinhardt",
  "Jakob Uszkoreit",
  "Jamba",
  "Jamba: A hybrid transformer-mamba language model.",
  "James Bailey",
  "James J DiCarlo",
  "Jamie Ryan Kiros",
  "Jan Kautz",
  "Jan Kocon",
  "Jaqueline L Xu",
  "Jason Wei",
  "Jason Weston",
  "Jeff Dean",
  "Jey Han Lau",
  "Jhonathan Osin",
  "Ji Pei",
  "Jiahao Wang",
  "Jiajie Zhang",
  "Jiaju Lin",
  "Jiale Liu",
  "Jiaming Kong",
  "Jian Sun",
  "Jian Zhu",
  "Jianpeng Cheng",
  "Jianyong Wang",
  "Jiazheng Xu",
  "Jie Gao",
  "Jie Shao",
  "Jie Tang",
  "Jie Zhou",
  "Jilong Xue",
  "Jimmy Lei Ba",
  "Jingbo Zhu",
  "Jinguo Zhu",
  "Jingyang Yuan",
  "Jingyu Hu",
  "Johan Ferret",
  "Johan Wind",
  "John, 1952",
  "Jonas Gehring",
  "Jonathon Shlens",
  "Jordan, 1997",
  "Joseph E. Gonzalez",
  "Joshua Ainslie",
  "Joshua B Tenenbaum",
  "Jozefowicz et al., 2016",
  "Juanzi Li",
  "Julian Michael",
  "Julie R. Williamson",
  "Julien Dirani",
  "Jun Chen",
  "Jun Zhou",
  "Junfeng Fang",
  "Junting Pan",
  "Junxiao Song",
  "Junyoung Chung",
  "Junyu Luo",
  "Jürgen Schmidhuber",
  "K (key)",
  "KV cache",
  "KV cache eviction",
  "KV cache reusability",
  "KV cache reuse",
  "Kai Zhang",
  "Kai-Wei Chang",
  "Kaiming He",
  "Kaiser",
  "Kaiser & Bengio, 2016",
  "Kaiser & Sutskever, 2016",
  "Kang et al., 2025",
  "Karen Simonyan",
  "Katharopoulos et al., 2020",
  "Ke Wang",
  "Keep First",
  "Keep First k Tokens",
  "Key",
  "Key (K)",
  "Kim et al., 2017",
  "Klaus Macherey",
  "Koray Kavukcuoglu",
  "Kranthi Gv",
  "Krishna Sri Ipsit Mantri",
  "Krista A. Ehinger",
  "Krzysztof Maziarz",
  "Kuchaiev & Ginsburg, 2017",
  "Kumar Avinava Dubey",
  "Kun Wang",
  "Kushan Mitra",
  "Kwon et al., 2023",
  "Kyunghyun Cho",
  "LLM Labels",
  "LLM technique",
  "LLM-Judge",
  "LSTM",
  "LTC",
  "LXMERT",
  "Label Smoothing",
  "Lane Keeping",
  "Lane marking detection",
  "Language Knowledge Recall",
  "Language Knowledge Recall task",
  "Language Recall function",
  "Language Recall skill",
  "Language model fine-tuning",
  "Language modeling",
  "Language models",
  "Large Language Model Safety",
  "Large Language Models",
  "Lasse Espeholt",
  "Last token",
  "Latent ordinary differential equations",
  "Layer Number",
  "Layer normalization",
  "Layer number",
  "LayerBias",
  "Layered network",
  "LeCun et al., 1988",
  "Lean Wang",
  "Lechner & Hasani, 2022",
  "Lechner et al., 2018",
  "Lechner et al., 2020",
  "Lei Hou",
  "Leon Barrett",
  "Leon Derczynski",
  "Lewei Lu",
  "Li Dong",
  "Li Yang",
  "Li et al., 2020",
  "Li et al., 2023a",
  "Li et al., 2023b",
  "Li et al., 2024",
  "Liang Zhao",
  "Lianmin Zheng",
  "Lieber et al., 2024",
  "Light blue cube",
  "Light-transfer: Your long-context llm is secretly a hybrid model with effortless adaptation.",
  "LightTransfer",
  "Lin & Qu, 2000",
  "Lin et al., 2017",
  "Lindstr¨om & Abraham, 2022",
  "Line plot",
  "Linear attention",
  "Linear layer",
  "Ling Team",
  "Linsong Chu",
  "Linsong Chu et al., 2024",
  "Lior Wolf",
  "Liquid neural networks (LNNs)",
  "Liu et al., 2023",
  "Lixin Gu",
  "Llama",
  "Llama Family",
  "Llama Team",
  "Llama Team, 2024b",
  "Llama3",
  "Llama3.1",
  "Llama3.1-8B",
  "Llama3.1-8B-Instruct",
  "Llama3.3-70B",
  "Llama: Open and efficient foundation language models.",
  "Llion Jones",
  "LoRA",
  "Localized Person Activity Recognition dataset",
  "Localized Person Activity dataset",
  "Long context processing",
  "Long short-term memory",
  "Long-Context Factuality",
  "Long-Term Domain-Specific Textual Knowledge",
  "Long-Term Visual Knowledge",
  "Long-context benchmarks",
  "Long-context fine-tuning",
  "Long-context reasoning",
  "Long-context tasks",
  "Long-range dependencies",
  "Long-term dependencies",
  "Long-term interactive memory",
  "Long-term memory",
  "LongAlign",
  "LongBench",
  "LongBench-V2",
  "LongMemEval",
  "LongMemEval_24k",
  "Longbench",
  "Longbench v2: Towards deeper understanding and reasoning on realistic long-context multitasks.",
  "Longfei Li",
  "Longformer",
  "Longformer: The long-document transformer.",
  "Longmemeval: Benchmarking chat assistants on long-term interactive memory.",
  "Lora: Low-rank adaptation of large language models.",
  "Low-Level Vision Reception",
  "Low-Level Vision Reception task",
  "Low-Level Visual Features",
  "Low-Level Visual Reception",
  "Low-Level Visual Reception task",
  "Low-Level function",
  "Low-Level skill",
  "Low-level Visual Reception",
  "Lu Wang",
  "Lu et al., 2024a",
  "Lu et al., 2024b",
  "Luca M Schulze Buschoff",
  "Lucas Liebenwein",
  "Luong",
  "Luong Hoang",
  "Luong et al. (2015)",
  "Luong, Pham, and Manning, 2015",
  "M Yang",
  "MHA",
  "MHA (Multi-Head Attention)",
  "MMLU",
  "MMMU",
  "MNIST dataset",
  "Ma et al.",
  "Maarten Bosma",
  "Machine Translation",
  "Machine reading",
  "Maksim Khadkevich",
  "Mamba",
  "Mamba: Linear-time sequence modeling with selective state spaces.",
  "Mantas Mazeika",
  "Mante et al., 2013",
  "Manzil Zaheer",
  "Marie-Anne Lachaux",
  "Marino et al., 2019",
  "Mark A Wheeler",
  "Mark Johnson",
  "Martin Schrimpf",
  "Mary Ann Marcinkiewicz",
  "Mary Harper",
  "Masked Heads Ratio",
  "Masked Multi-Head Attention",
  "Masking",
  "Masking layer",
  "Math Reasoning",
  "Math Reasoning task",
  "Math function",
  "Math reasoning",
  "Math skill",
  "MathVision",
  "MathVision Dataset",
  "MathVista Dataset",
  "Mathematical Concepts",
  "Mathematical Reasoning",
  "Matrix Multiplication",
  "Matteo Grella",
  "Matthew E. Peters",
  "Matthias Bethge",
  "Max L. Wilson",
  "Maxim Krikun",
  "Maximum Value",
  "McClosky",
  "McClosky et al. (2006)",
  "Mean activation",
  "Meaningful first token",
  "Measuring massive multitask language understanding.",
  "Measuring multimodal mathematical reasoning with math-vision dataset",
  "Memory consumption",
  "Meng Li",
  "Michael Auli",
  "Michael Chung",
  "Michael Gokhman",
  "Microscopy",
  "Miguel Ballesteros",
  "Mike Lewis",
  "Mike Schuster",
  "Min Lin",
  "Min Zhang",
  "Minfeng Zhu",
  "Ming Zhang",
  "Mingchuan Zhang",
  "Mingjie Zhan",
  "Mingyang Zhang",
  "Minh-Thang Luong",
  "MiniGPT-4",
  "Minigpt-4: Enhancing Vision-Language Understanding With Advanced Large Language Models",
  "Minwei Feng",
  "Mirella Lapata",
  "Mitchell P Marcus",
  "Mixed-memory RNNs",
  "Mo Yu",
  "MoE",
  "Modality",
  "Model Architectures",
  "Model Family",
  "Model Output",
  "Model Performance",
  "Mohamed Elhoseiny",
  "Mohammad Norouzi",
  "Monotonic convergence",
  "Mor Zusman",
  "Morgane Rivi`ere",
  "Morlet wavelet",
  "Motor neuron",
  "Muhua Zhu",
  "Multi-Head Attention",
  "Multi-Head Neuronal Attention Circuit",
  "Multi-head attention",
  "Multi-task sequence to sequence learning",
  "Multi-time attention networks",
  "Multimodal Large Language Models",
  "Multimodal Mathematical Reasoning",
  "Multimodal Models",
  "Multimodal Understanding and Reasoning",
  "Multimodal reasoning",
  "Multimodal tasks",
  "NAC",
  "NAC degradation",
  "NAC-02s",
  "NAC-09s",
  "NAC-2k",
  "NAC-32k",
  "NAC-Euler",
  "NAC-Exact",
  "NAC-Exact/05s/8k",
  "NAC-FC",
  "NAC-PW",
  "NAC-Steady",
  "NCPCell",
  "NLP Pipeline",
  "Naive SWA",
  "Nal Kalchbrenner",
  "Naman Goyal",
  "Nancy Kanwisher",
  "Native Sparse Attention",
  "Native sparse attention: Hardware-aligned and natively trainable sparse attention.",
  "Natural Language Processing",
  "Nature Machine Intelligence",
  "Nectoux et al., 2012",
  "Needle-retrieval tasks",
  "Negative Intervention",
  "Negative Intervention Case",
  "Neil et al., 2016",
  "Nemotron-Flash",
  "Nemotron-flash: Towards latencyoptimal hybrid small language models.",
  "Neural Accumulator (NAC)",
  "Neural Architecture of Language",
  "Neural Machine Translation System",
  "Neural Responses",
  "Neural circuits",
  "Neural control agents",
  "Neural machine translation",
  "Neural network architecture",
  "Neural networks",
  "NeuralODE",
  "Neuronal Attention Circuit (NAC)",
  "Neuronal Circuit Policies (NCPs)",
  "Neuronal circuit",
  "Niki Parmar",
  "Nikolaos Pappas",
  "Nino Vieillard",
  "Nir Ratner",
  "Nishijima, 2021",
  "Nitish Srivastava",
  "Noah A. Smith",
  "Noam Rozen",
  "Noam Shazeer",
  "Normalized Degradation",
  "Nucleus",
  "Number",
  "Number of particles",
  "Numerical score",
  "ODE solver",
  "ODE-based models",
  "ODEFormer",
  "OK-VQA",
  "OK-VQA Dataset",
  "Object Recognition",
  "Occipital lobe",
  "Occlusion",
  "Ofir Press",
  "Oleksii Kuchaiev",
  "Olsson et al., 2022",
  "Omri Abend",
  "On the Role of Attention Heads in Large Language Model Safety",
  "Onion epidermal cell",
  "Ono et al., 2022",
  "OpenAI",
  "OpenAI CarRacing",
  "OpenAI Gym",
  "OpenAI, 2024",
  "OpenAI, 2025",
  "OpenReview.net",
  "Opher Lieber",
  "Ordinary differential equations (ODEs)",
  "Oregon State University",
  "Original Model Output",
  "Original Output",
  "Oriol Vinyals",
  "Oscar Täckström",
  "Out-of-Domain",
  "Out-of-Domain Query",
  "Output Embedding",
  "Output Probabilities",
  "Over-fitting",
  "PMLR",
  "PRONOSTIA dataset",
  "PagedAttention",
  "Pan, 2024",
  "Paolo Frasconi",
  "Papineni et al., 2002",
  "Parabola",
  "Parietal lobe",
  "Parikh et al., 2016",
  "Park et al., 2021",
  "Parsing",
  "Part-Whole Relationships",
  "Particle concentration",
  "Particle concentration calculation",
  "Particle count",
  "Particles",
  "Pattern-Based Quantitative Reasoning",
  "Paulus et al., 2017",
  "Pearson Correlation",
  "Pedro Tsividis",
  "Peijie Jiang",
  "Peiyi Wang",
  "Peng Jiao",
  "Peng Li",
  "Peng et al., 2023",
  "Penn State University",
  "Penn Treebank",
  "Penny Kyburz",
  "Perception",
  "Performance-Efficiency Balance",
  "Performance-Optimized Hierarchical Models",
  "Performance-optimized hierarchical models predict neural responses in higher visual cortex",
  "Performance–efficiency Trade-offs",
  "Periodic function",
  "Person Activity Recognition (PAR)",
  "Petrov",
  "Petrov et al. (2006)",
  "PhasedLSTM",
  "Philip Pham",
  "Phillip Wallis",
  "Phoebe O. Toups Dugas",
  "Physical Structure",
  "Plant cell",
  "Plant-based art",
  "Plumber",
  "Position",
  "Positional Encoding",
  "Positive Intervention",
  "Positive Intervention Case",
  "Positive Intervention Output",
  "Predictive Processing",
  "Prefill phase",
  "Prefilling",
  "Prefilling stage",
  "Press & Wolf, 2016",
  "Prior knowledge",
  "Probing-based framework",
  "Proceedings of the CHI Conference on Human Factors in Computing Systems, CHI 2024",
  "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
  "Proceedings of the National Academy of Sciences",
  "Processing flow",
  "Prompt",
  "Prompt engineering",
  "Prompt template",
  "Proximal Policy Optimization (PPO)",
  "Przemyslaw Kazienko",
  "Psychological bulletin",
  "Publication [18]",
  "Publication [20]",
  "Publication [25]",
  "Publication [32]",
  "Publication [33]",
  "Publication [36]",
  "Publication [37]",
  "Publication [38]",
  "Publication [39]",
  "Publication [3]",
  "Publication [6]",
  "Publication [9]",
  "Purple matte cylinders",
  "Pushpak Bhattacharyya",
  "Q (query)",
  "Qian Liu",
  "Qian Zhao",
  "Qifan Wang",
  "Qihao Zhu",
  "Qin Gao",
  "Qinghua Zhou",
  "Qingyuan Yang",
  "Qingyun Wu",
  "Quentin Anthony",
  "Query",
  "Query (Q)",
  "Question",
  "Question Answering",
  "Question decomposition",
  "Question placeholder",
  "Quiz answer",
  "Quiz question",
  "Quoc V. Le",
  "Qwen",
  "Qwen Family",
  "Qwen2.5-VL-3B",
  "Qwen2.5-VL-3B-Instruct model",
  "Qwen2.5-VL-7B-Instruct",
  "Qwen2.5-VL-7B-Instruct model",
  "Qwen2.5VL-3B",
  "Qwen2.5VL-3B model",
  "Qwen2.5VL-3B-Instruct",
  "Qwen2.5VL-7B",
  "Qwen2.5VL-7B model",
  "Qwen3",
  "Qwen3 Team",
  "Qwen3 Team, 2025b",
  "Qwen3 technical report",
  "Qwen3 technical report.",
  "Qwen3-30B",
  "Qwen3-30B-A3B",
  "Qwen3-30B-A3B-Instruct",
  "Qwen3-30B-A3B-Thinking",
  "Qwen3-4B",
  "Qwen3-4B-Instruct",
  "Qwen3-4B-Thinking",
  "RATTENTION",
  "RNN",
  "RNN encoder-decoder",
  "RNN sequence-to-sequence models",
  "RNN-like linear attention transformers",
  "ROUGE score",
  "RWKV",
  "RWKV: Reinventing RNNs for the transformer era.",
  "Rafal Jozefowicz",
  "Ramona Merhej",
  "Random Seed",
  "Random activation",
  "RandomK ACC",
  "RandomK LLM",
  "Rattention",
  "Rattention: Towards the minimal sliding window size in local-global attention models.",
  "Raz Alon",
  "Razzaq & Hongwei, 2023",
  "Razzaq & Zhao, 2025a",
  "Razzaq & Zhao, 2025b",
  "ReLU Activation",
  "Real-world tasks",
  "Reasoning",
  "Reception (perceptual processing)",
  "Recurrent Neural Network",
  "Recurrent Neural Network Grammar",
  "Recurrent Neural Network Grammars",
  "Recurrent Neural Networks",
  "Regularization",
  "Rein et al., 2023",
  "Reinforcement Learning",
  "Remaining Useful Life (RUL) estimation",
  "Representation Learning",
  "Retentive Network",
  "Retentive network: A successor to transformer for large language models.",
  "Retrieval Head",
  "Retrieval head mechanistically explains long-context factuality",
  "Richard Socher",
  "Richard Yuanzhe Pang",
  "Rico Sennrich",
  "Rob Fergus",
  "Romain Paulus",
  "Romain Thibaux",
  "Roman Glozman",
  "Rongwu Xu",
  "Rowan Zellers",
  "Roy et al., 2021",
  "Rubanova et al., 2019",
  "Rui-Jie Zhu",
  "Ruler",
  "Ruler: What’s the real context size of your long-context language models?",
  "Rumelhart et al., 1985",
  "Runtime",
  "Runxin Xu",
  "Ruoming Pang",
  "Ruoqi Liu",
  "Ruslan Salakhutdinov",
  "S Song",
  "SFT",
  "SWA",
  "SWA adaptation",
  "SWA-aware fine-tuning",
  "Saikh et al., 2022",
  "Sainbayar Sukhbaatar",
  "Sajjadur Rahman",
  "Saliency map",
  "Samuel Arcadinho",
  "Samuel J Gershman",
  "Samuel Kriman",
  "Samuel R. Bowman",
  "Samuel Stevens",
  "Samy Bengio",
  "Sandwich",
  "Santiago Ontañón",
  "Sarah Monazam Erfani",
  "Sarah Perrin",
  "Scale operation",
  "Scaled Dot-Product Attention",
  "Scatter plot",
  "Scene-Level Understanding",
  "Schrimpf et al., 2021",
  "Schulze Buschoff et al., 2025",
  "Science",
  "ScienceQA",
  "Score",
  "Screwdriver",
  "Self-attention mechanism",
  "Self-distillation",
  "Self-distillation bridges distribution gap in language model fine-tuning.",
  "Self-driving",
  "Self-training",
  "Semantic Understanding",
  "Sennrich et al., 2015",
  "Sensitivity analysis",
  "Sensory Gate",
  "Sensory gate",
  "Sensory neuron",
  "Sentence Structures",
  "Sentence structure",
  "Sentence visualization",
  "Separable Convolutions",
  "Sepp Hochreiter",
  "Sequence Modeling",
  "Sequence to sequence learning",
  "Sequential architecture",
  "Sergey Ioffe",
  "Shai ShalevShwartz",
  "Shaked Meirom",
  "Shangqing Tu",
  "Shantanu Acharya",
  "Shao et al., 2024",
  "Shaohan Huang",
  "Shaoqing Ren",
  "Shape",
  "Shean Wang",
  "Shenglong Ye",
  "Shibuya",
  "Shibuya, 2017",
  "Shift-reduce constituent parsing",
  "Shizhe Diao",
  "Shreya Pathak",
  "Shu Liu",
  "Shukla & Marlin, 2021",
  "Shulin Cao",
  "Shuming Ma",
  "Simeng Sun",
  "Sinusoidal curve",
  "Siyuan Zhuang",
  "Size",
  "Skip connections",
  "Slav Petrov",
  "Sliding Window Attention",
  "Sliding Window Attention Adaptation",
  "Sliding Window Attention Adaptation (SWAA)",
  "Sliding Window Attention Adaptation Code Repository",
  "Sliding Window Attention Training",
  "Sliding window attention training for efficient large language models.",
  "Sliding window size",
  "SoftMax function",
  "Softmax layer",
  "Solution",
  "Solution A",
  "Solution B",
  "Solution concentration",
  "Solvent volume",
  "Song Han",
  "Soup",
  "Sparse Top-K Pairwise Concatenation",
  "Sparse Topk-Pairwise",
  "Sparse attention",
  "Sparse sinkhorn attention",
  "Sparsity",
  "Sparsity Patterns",
  "Spatial Arrangement",
  "Spiking neural networks",
  "Staining",
  "Stanisław Wo´zniak",
  "Statistical machine translation",
  "Steady mode",
  "Stella Biderman",
  "Step-by-step reasoning",
  "Stephan Gouws",
  "Steven Basart",
  "Stinchcomb, 1989",
  "Stochastic optimization",
  "Streaming Attention",
  "Structured attention networks",
  "Structured self-attentive sentence embedding",
  "Subquestion",
  "Subquestion generation",
  "Subword units",
  "Success Case",
  "Sukhbaatar et al., 2015",
  "Sun et al., 2023",
  "Super-Clevr",
  "Surface-Form Variation",
  "Sutskever et al., 2014",
  "Symmetry",
  "Tanik Saikh",
  "Target Function",
  "Tatiana Matejovicova",
  "Tay et al., 2020",
  "Team et al., 2025",
  "Team, 2024a",
  "Team, 2024b",
  "Team, 2025a",
  "Team, 2025b",
  "Temporal convolution",
  "Temporal dynamics",
  "Temporal lobe",
  "Tenney et al., 2019",
  "Terry Koo",
  "The University of Melbourne",
  "The University of Sydney",
  "The llama 3 herd of models.",
  "The neural architecture of language: Integrative modeling converges on predictive processing",
  "Thibaut Lavril",
  "Thomas Pouncy",
  "Throughput",
  "Thuan & Hong, 2023",
  "Tiang et al., 2018",
  "Tianyu Pang",
  "Tianyu Zheng",
  "Time",
  "Time-frequency Representation Algorithm",
  "Time-frequency representation (TFR)",
  "Time-per-output-token",
  "Time-to-first-token",
  "Timothée Lacroix",
  "Tirthankar Ghosal",
  "Toilet repair",
  "Token Length",
  "Token Selection",
  "Token relationships",
  "Tokens",
  "Tomer Asida",
  "Tong Xu",
  "Tongliang Liu",
  "Top1",
  "Top3",
  "Top5",
  "TopK",
  "TopK ACC",
  "TopK LLM",
  "Topk Tokens Selection",
  "Totals",
  "Touvron et al., 2023",
  "Toward a theory of episodic memory: the frontal lobes and autonoetic consciousness",
  "Transformer",
  "Transformer architecture",
  "Transformers are rnns: Fast autoregressive transformers with linear attention.",
  "Translation tasks",
  "Tri Dao",
  "Tsividis et al., 2017",
  "UC Irvine",
  "UCI Machine Learning Repository",
  "UNITER",
  "Udacity Simulator",
  "Universal Approximation Theorem (UAT)",
  "University of Science & Technology of China",
  "University of Toronto",
  "V (value)",
  "VCR",
  "VLMs",
  "Value",
  "Value (V)",
  "Value Distribution",
  "Value distribution",
  "Vaswani et al., 2017",
  "Vertex",
  "ViLBERT",
  "Vidulin et al., 2010",
  "Vincent Vanhoucke",
  "Vinyals",
  "Vinyals & Kaiser el al. (2014)",
  "Vinyals & Kaiser el al.(2014)",
  "Vision",
  "Vision Knowledge Recall",
  "Vision Knowledge Recall task",
  "Vision Recall function",
  "Vision Recall skill",
  "Vision-Language Models (VLMs)",
  "Vision-Language Understanding",
  "VisuLogic",
  "VisuLogic: A Benchmark for Evaluating Visual Reasoning in Multi-modal Large Language Models",
  "Visual Cognition",
  "Visual Knowledge Recall",
  "Visual Question Answering (VQA)",
  "Visual Question Answering (VQA) Query",
  "Visual Reasoning",
  "Visual cognition in multimodal large language models",
  "Visual commonsense reasoning",
  "Visual counting",
  "Visual reception",
  "Volume",
  "WMT 2014 English-to-French translation task",
  "WMT 2014 English-to-German translation task",
  "Waleed Razzaq",
  "Wang et al., 2018",
  "Wang et al., 2024a",
  "Wang et al., 2024b",
  "Wang et al., 2025",
  "Wangding Zeng",
  "Wei Chen",
  "Wei Gao",
  "Wei Xu",
  "Wei et al., 2022",
  "Weight",
  "Weight Estimation",
  "Weijie Su",
  "Weikang Shi",
  "Weiming Ren",
  "Weiye Xu",
  "Weiyun Wang",
  "Weizhu Chen",
  "Wenbo Pan",
  "Wenbo Shen",
  "Wenfeng Liang",
  "Wengang Zhou",
  "Wenhao Wu",
  "Wenhao Yu",
  "Wenliang Chen",
  "Wentao Song",
  "Wheeler et al., 1997",
  "Why stacking sliding windows can’t see very far.",
  "Window parameter",
  "Wolfgang Macherey",
  "Wonmin Byeon",
  "Woosuk Kwon",
  "Word Relationships",
  "Word embeddings",
  "Word-piece",
  "Working memory",
  "Wu et al., 2024",
  "X-axis",
  "XJTU-SY dataset",
  "Xavier Martinet",
  "Xception",
  "Xi'an Jiaotong University",
  "Xian Wu",
  "Xiang Li",
  "Xiang Yue",
  "Xiangru Tang",
  "Xiangyu Zhang",
  "Xiangyu Zhao",
  "Xiao Bi",
  "Xiao et al., 2024",
  "Xiao, 2025",
  "Xiaohua Wang",
  "Xiaoqian Shen",
  "Xiaozhi Wang",
  "Xin Cheng",
  "Xin Dong",
  "Xin Lv",
  "Xinghua Zhang",
  "Xingjian Du",
  "Xinru Wang",
  "Xinxing Yang",
  "Xizhou Zhu",
  "Xu et al., 2025",
  "Xuan Zhang",
  "Xueqi Ma",
  "Xuetao Wei",
  "Xuezhi Wang",
  "Xuguang Wang",
  "Xuzheng He",
  "Y Huang",
  "Y Wang",
  "Y-axis",
  "Y-value",
  "Y. K. Li",
  "Y. Wu",
  "Yalin Zhang",
  "Yamins et al., 2014",
  "Yanbei Jiang",
  "Yang Liu",
  "Yang Zhang",
  "Yang et al., 2024",
  "Yang et al., 2025",
  "Yankun Ren",
  "Yann N. Dauphin",
  "Yao Fu",
  "Yao Zhao",
  "Yashaswi Karnati",
  "Yefeng Zheng",
  "Yejin Choi",
  "Yejing Wang",
  "Yelong Shen",
  "Yibo Cao",
  "Yijiong Yu",
  "Ying Cao",
  "Ying Sheng",
  "Yingying Zhang",
  "Yixuan Sun",
  "Yizhong Wang",
  "Yoav Shoham",
  "Yonatan Belinkov",
  "Yonatan Bisk",
  "Yongbin Li",
  "Yonggan Fu",
  "Yonghui Wu",
  "Yoon Kim",
  "Yoshua Bengio",
  "Yuan Cao",
  "Yuan et al., 2025",
  "Yuandong Tian",
  "Yuansheng Ni",
  "Yuanzhi Li",
  "Yuchen Duan",
  "Yuchen Fang",
  "Yue Zhang",
  "Yue et al., 2024",
  "Yun-Bo Zhao",
  "Yuqing Wang",
  "Yuqing Xia",
  "Yushi Bai",
  "Yutao Sun",
  "Yuwei Zhang",
  "Yuxiao Dong",
  "Yuxing Wei",
  "Yuxuan Sun",
  "Z Li",
  "Z Zheng",
  "Zaheer et al., 2020",
  "Zbigniew Wojna",
  "Zellers et al., 2019",
  "Zeyuan Allen-Zhu",
  "Zhang et al., 2024",
  "Zhaorui Yang",
  "Zhaoyang Liu",
  "Zhe Chen",
  "Zhenda Xie",
  "Zhengjie Miao",
  "Zhengyan Zhang",
  "Zhenhong Zhou",
  "Zhenyuan Zhang",
  "Zhifeng Chen",
  "Zhihong Shao",
  "Zhiping Xiao",
  "Zhongqiang Huang",
  "Zhou et al., 2024",
  "Zhouhan Lin",
  "Zhu",
  "Zhu et al. (2013)",
  "Zhu et al., 2023",
  "Zhu et al., 2025",
  "Zhuang et al., 2020",
  "Zhuang et al., 2021",
  "Zhuohan Li",
  "Zibin Lin",
  "Zichuan Fu",
  "Zimu Lu",
  "Zixuan Cheng",
  "agent memory system evaluation",
  "alignment",
  "anaphora resolution",
  "annotation pipeline",
  "arXiv preprint arXiv:1905.05950",
  "arXiv preprint arXiv:2101.10318",
  "arXiv preprint arXiv:2304.10592",
  "arXiv preprint arXiv:2409.03752",
  "arXiv preprint arXiv:2410.13708",
  "arXiv preprint arXiv:2503.19786",
  "arXiv preprint arXiv:2504.10479",
  "arXiv preprint arXiv:2504.15279",
  "arXiv preprint arXiv:2505.09388",
  "attention heads",
  "attention mask",
  "attention patterns",
  "attention sinks",
  "attention weights",
  "attention_functional_roles_raw_with_image_ids_with_captions",
  "attention_functional_roles_raw_with_image_ids_with_captions::section::- INVESTIGATING THE FUNCTIONAL ROLES OF ATTEN ### TION HEADS IN VISION LANGUAGE MODELS: EVI DENCE FOR REASONING MODULES",
  "attention_is_all_you_need_raw_with_image_ids_with_captions",
  "attention_is_all_you_need_raw_with_image_ids_with_captions::section::3 Model Architecture",
  "attention_is_all_you_need_raw_with_image_ids_with_captions::section::3.2 Attention",
  "attention_is_all_you_need_raw_with_image_ids_with_captions::section::References",
  "catastrophic degradation",
  "cognitive heads",
  "cognitive processes",
  "cognitive science",
  "cognitive skills",
  "computational complexity",
  "computational overhead",
  "computational workflow",
  "cross-modal interactions",
  "cube",
  "cylinder",
  "decoding speed",
  "decoding stage",
  "deep reasoning",
  "deep reasoning tasks",
  "deep understanding and reasoning",
  "diffuse material",
  "distributed LLM services",
  "d’Ascoli et al., 2023",
  "efficiency",
  "emergent modular organization",
  "exact closed-form solution",
  "explicit Euler solver",
  "fixed-interval selection",
  "flask",
  "functional specialization",
  "gemma-3n-e2b-it",
  "gemma-3n-e2b-it model",
  "gemma-3n-e4b-it",
  "generation length",
  "geometric primitives",
  "global attention",
  "grounding",
  "human reading comprehension",
  "img_attention_functional_roles_13_0",
  "img_attention_functional_roles_14_0",
  "img_attention_functional_roles_14_1",
  "img_attention_functional_roles_15_0",
  "img_attention_functional_roles_16_0",
  "img_attention_functional_roles_17_0",
  "img_attention_functional_roles_17_1",
  "img_attention_functional_roles_18_0",
  "img_attention_functional_roles_18_1",
  "img_attention_functional_roles_19_0",
  "img_attention_functional_roles_1_0",
  "img_attention_functional_roles_1_1",
  "img_attention_functional_roles_1_11",
  "img_attention_functional_roles_1_2",
  "img_attention_functional_roles_1_3",
  "img_attention_functional_roles_1_4",
  "img_attention_functional_roles_1_5",
  "img_attention_functional_roles_1_6",
  "img_attention_functional_roles_1_7",
  "img_attention_functional_roles_1_8",
  "img_attention_functional_roles_20_0",
  "img_attention_functional_roles_21_0",
  "img_attention_functional_roles_21_1",
  "img_attention_functional_roles_21_2",
  "img_attention_functional_roles_22_0",
  "img_attention_functional_roles_23_0",
  "img_attention_functional_roles_23_1",
  "img_attention_functional_roles_24_0",
  "img_attention_functional_roles_25_0",
  "img_attention_functional_roles_25_1",
  "img_attention_functional_roles_26_0",
  "img_attention_functional_roles_26_1",
  "img_attention_functional_roles_26_2",
  "img_attention_functional_roles_27_0",
  "img_attention_functional_roles_27_1",
  "img_attention_functional_roles_27_2",
  "img_attention_functional_roles_27_3",
  "img_attention_functional_roles_28_0",
  "img_attention_functional_roles_28_1",
  "img_attention_functional_roles_28_2",
  "img_attention_functional_roles_28_3",
  "img_attention_functional_roles_5_0",
  "img_attention_functional_roles_5_1",
  "img_attention_functional_roles_6_0",
  "img_attention_functional_roles_7_0",
  "img_attention_is_all_you_need_12_0",
  "img_attention_is_all_you_need_13_0",
  "img_attention_is_all_you_need_14_0",
  "img_attention_is_all_you_need_2_0",
  "img_attention_is_all_you_need_3_0",
  "img_attention_is_all_you_need_3_1",
  "img_neuronal_attention_circuits_13_0",
  "img_neuronal_attention_circuits_15_0",
  "img_neuronal_attention_circuits_15_1",
  "img_neuronal_attention_circuits_2_0",
  "img_neuronal_attention_circuits_5_0",
  "img_neuronal_attention_circuits_6_0",
  "img_sliding_window_attention_2_0",
  "img_sliding_window_attention_7_0",
  "industrial prognostics",
  "information retrieval",
  "instruction-tuning",
  "irregular time-series classification",
  "key",
  "lane-keeping for autonomous vehicles",
  "language model fine-tuning",
  "layer functionalities",
  "layer selection method",
  "layers",
  "lazy layers",
  "lazy ratio",
  "linear attention",
  "linguistics",
  "long context processing",
  "long-context QA tasks",
  "long-context fine-tuning",
  "long-context inference",
  "long-context performance",
  "long-context performance degradation",
  "long-context tasks",
  "long-term interactive memory",
  "mTAN",
  "machine learning",
  "mathematical reasoning",
  "memory usage",
  "metallic material",
  "mmRNN",
  "mmRNNs",
  "model accuracy",
  "model sizes",
  "model-specific layer selection strategies",
  "modern models",
  "multimodal reasoning",
  "multiple-choice question format",
  "needle-retrieval tasks",
  "neural network layers",
  "neuronal_attention_circuits_raw_with_image_ids_with_captions",
  "neuronal_attention_circuits_raw_with_image_ids_with_captions::section::Abstract",
  "neuronal_attention_circuits_raw_with_image_ids_with_captions::section::References",
  "neuroscience",
  "okvqa Dataset",
  "ordinary differential equations (ODEs)",
  "particles",
  "particulate matter",
  "pattern induction",
  "performance restoration",
  "performance-efficiency trade-off",
  "performance-efficiency trade-offs",
  "prefilling",
  "prefilling stage",
  "problem analysis",
  "processing stages",
  "prompt generation",
  "query",
  "question answering",
  "random heads",
  "real-world scenarios",
  "real-world tasks",
  "reasoning",
  "reasoning trajectory",
  "round-bottom flask",
  "safety alignment",
  "scene composition",
  "self-attention mechanism",
  "self-distillation",
  "semantic alignment",
  "sink tokens",
  "sliding_window_attention_raw_with_image_ids_with_captions",
  "sliding_window_attention_raw_with_image_ids_with_captions::section::3.3 Chain-of-Thought",
  "sliding_window_attention_raw_with_image_ids_with_captions::section::5.3 Performance–efficiency Trade-offs and",
  "softmax",
  "solution",
  "sparse Top-K pairwise concatenation scheme",
  "sparse attention",
  "sparse-attention models",
  "sparsity patterns",
  "sphere",
  "stacking sliding windows",
  "statistical features",
  "steady-state approximation",
  "step-by-step reasoning",
  "streaming attention",
  "structured state-space models",
  "synthetic tasks",
  "t-test analysis",
  "task-dependent localization",
  "term frequency",
  "training–inference mismatch",
  "triangular matrix structure",
  "truthfulness",
  "vLLM",
  "value",
  "visual grounding",
  "visual reasoning assistant",
  "volume",
  "w/o CoT",
  "w/o Main Question",
  "window size",
  "word cloud",
  "word tokens",
  "Çaglar Gülçehre",
  "Łukasz Kaiser",
  "δt (delta t)"
]