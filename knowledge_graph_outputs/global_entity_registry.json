[
  "Aaron van den Oord",
  "Abstractive summarization",
  "Accuracy",
  "Active memory",
  "Adam optimizer",
  "Add & Norm layer",
  "Adhiguna Kuncoro",
  "Aidan N. Gomez",
  "Albert Gu",
  "Alex Graves",
  "Alex Krizhevsky",
  "Alexander M. Rush",
  "Alexandra Birch",
  "Andy Davis",
  "Angelos Katharopoulos",
  "Ankur Parikh",
  "Anna Goldie",
  "Arthur Szlam",
  "Ashish Vaswani",
  "Attention map",
  "Attention mechanism",
  "Attention weights",
  "Average running time per request",
  "Azalia Mirhoseini",
  "BLEU score",
  "Ba et al., 2016",
  "Bahdanau et al., 2014",
  "Bai et al., 2023",
  "Bai et al., 2024a",
  "Bai et al., 2024b",
  "Bailin Wang",
  "Bamba",
  "Barry Haddow",
  "Bart van Merrienboer",
  "Beam search",
  "Beatrice Santorini",
  "Beltagy et al., 2020",
  "BerkeleyParser",
  "BigBird",
  "Bing Xiang",
  "Bo Peng",
  "Boris Ginsburg",
  "Bowen Zhou",
  "Byte-pair encoding",
  "ByteNet",
  "Caglar Gulcehre",
  "Caiming Xiong",
  "Carl Denton",
  "Catastrophic degradation",
  "Chain-of-Thought",
  "ChatGPT",
  "Cheng et al., 2016",
  "Cheng-Ping Hsieh",
  "Cho et al., 2014",
  "Chris Dyer",
  "Christian Szegedy",
  "Christopher D Manning",
  "Chung et al., 2014",
  "Cicero Nogueira dos Santos",
  "Computer vision",
  "Concatenation",
  "ConvS2S",
  "Convolutional neural networks",
  "Dan Hendrycks",
  "Dan Klein",
  "Dao, 2024",
  "David Grangier",
  "David McClosky",
  "David Rein",
  "Decode phase",
  "Decoder",
  "Decomposable attention model",
  "Deep reasoning tasks",
  "Deep recurrent models",
  "Deep reinforced model",
  "Deep residual learning",
  "Deep-Att + PosUnk",
  "DeepSeek-AI",
  "DeepSeek-AI, 2025a",
  "DeepSeek-AI, 2025b",
  "DeepSeek-R1",
  "DeepSeek-v3.2",
  "DeepSolution",
  "Deepseek-sparse-attention",
  "DeepseekMath",
  "Denis Yarats",
  "Denny Britz",
  "Depthwise separable convolutions",
  "Di Wu",
  "Diederik Kingma",
  "Dilated Convolutions",
  "Dipanjan Das",
  "Dropout",
  "Dyer",
  "Dyer et al.(2016)",
  "Dzmitry Bahdanau",
  "Edward J. Hu",
  "Efficient memory management",
  "Encoder",
  "Encoder attention head",
  "Encoder-decoder architectures",
  "End-to-end memory networks",
  "English constituency parsing",
  "Eugene Charniak",
  "Extended Neural GPU",
  "FA Decode",
  "FA Decode parameter",
  "FA Layers parameter",
  "Fast-forward connections",
  "Feed Forward Network",
  "Fethi Bougares",
  "Fine-tuning",
  "Flash-Attention-2",
  "FlashAttention",
  "Francois Chollet",
  "Fu et al., 2025a",
  "Fu et al., 2025b",
  "Full Attention",
  "Full Attention Decode",
  "Full Attention models",
  "Fusang-v1-long",
  "GNMT + RL",
  "GPQA",
  "GPT-5-Mini",
  "GPU memory",
  "GPU utilization",
  "GRPO",
  "Gated recurrent neural networks",
  "Gemma Team",
  "Gemma Team, 2024a",
  "Gemma Team, 2025a",
  "Gemma2",
  "Gemma3",
  "Generating sequences",
  "Geoffrey E Hinton",
  "Google",
  "Google Brain",
  "Google Research",
  "Gradient flow",
  "Graves, 2013",
  "Gu and Dao, 2023",
  "Guangxuan Xiao",
  "Harper",
  "He et al., 2016",
  "Hendrycks et al., 2021",
  "Hieu Pham",
  "Hochreiter & Schmidhuber, 1997",
  "Hochreiter et al., 2001",
  "Holger Schwenk",
  "Hsieh et al., 2024",
  "Hu et al., 2022",
  "Huang",
  "Huang & Harper (2009)",
  "Huazheng Wang",
  "HuggingFace Transformers",
  "Hugo Touvron",
  "Hybrid Attention Architecture",
  "Illia Polosukhin",
  "Ilya Sutskever",
  "Image recognition",
  "Inception architecture",
  "Input Embedding",
  "Interleaving FA/SWA layers",
  "Interleaving Layers",
  "Jakob Uszkoreit",
  "Jamba",
  "Jamie Ryan Kiros",
  "Jason Wei",
  "Jason Weston",
  "Jeff Dean",
  "Ji Pei",
  "Jiale Liu",
  "Jian Sun",
  "Jianpeng Cheng",
  "Jie Zhou",
  "Jimmy Lei Ba",
  "Jingbo Zhu",
  "Jingyang Yuan",
  "Jonas Gehring",
  "Jonathon Shlens",
  "Jozefowicz et al., 2016",
  "Junyoung Chung",
  "Jürgen Schmidhuber",
  "K (key)",
  "KV cache",
  "KV cache eviction",
  "KV cache reusability",
  "Kaiming He",
  "Kaiser",
  "Kaiser & Bengio, 2016",
  "Kaiser & Sutskever, 2016",
  "Karen Simonyan",
  "Katharopoulos et al., 2020",
  "Keep First k Tokens",
  "Key (K)",
  "Kim et al., 2017",
  "Klaus Macherey",
  "Koray Kavukcuoglu",
  "Krzysztof Maziarz",
  "Kuchaiev & Ginsburg, 2017",
  "Kwon et al., 2023",
  "Kyunghyun Cho",
  "Label Smoothing",
  "Language modeling",
  "Language models",
  "Large Language Models",
  "Lasse Espeholt",
  "Layer normalization",
  "Leon Barrett",
  "Li Dong",
  "Lieber et al., 2024",
  "LightTransfer",
  "Lin et al., 2017",
  "Linear attention",
  "Linear layer",
  "Ling Team",
  "Linsong Chu",
  "Linsong Chu et al., 2024",
  "Lior Wolf",
  "Llama",
  "Llama Team",
  "Llama Team, 2024b",
  "Llama3",
  "Llama3.1",
  "Llama3.1-8B",
  "Llama3.1-8B-Instruct",
  "Llion Jones",
  "LoRA",
  "Long context processing",
  "Long short-term memory",
  "Long-context benchmarks",
  "Long-context fine-tuning",
  "Long-context tasks",
  "Long-range dependencies",
  "Long-term dependencies",
  "LongAlign",
  "LongBench",
  "LongBench-V2",
  "LongMemEval",
  "LongMemEval_24k",
  "Longformer",
  "Luong",
  "Luong Hoang",
  "Luong et al. (2015)",
  "Luong, Pham, and Manning, 2015",
  "MMLU",
  "Machine Translation",
  "Machine reading",
  "Mamba",
  "Manzil Zaheer",
  "Mark Johnson",
  "Mary Ann Marcinkiewicz",
  "Mary Harper",
  "Masked Multi-Head Attention",
  "Masking layer",
  "Matrix Multiplication",
  "Maxim Krikun",
  "McClosky",
  "McClosky et al. (2006)",
  "Michael Auli",
  "Miguel Ballesteros",
  "Mike Schuster",
  "Min Zhang",
  "Minh-Thang Luong",
  "Minwei Feng",
  "Mirella Lapata",
  "Mitchell P Marcus",
  "Mo Yu",
  "MoE",
  "Mohammad Norouzi",
  "Muhua Zhu",
  "Multi-Head Attention",
  "Multi-head attention",
  "Multi-task sequence to sequence learning",
  "Naive SWA",
  "Nal Kalchbrenner",
  "Native Sparse Attention",
  "Natural Language Processing",
  "Nemotron-Flash",
  "Neural Machine Translation System",
  "Neural machine translation",
  "Neural networks",
  "Niki Parmar",
  "Nitish Srivastava",
  "Noah A. Smith",
  "Noam Shazeer",
  "Ofir Press",
  "Oleksii Kuchaiev",
  "OpenAI",
  "OpenAI, 2025",
  "Opher Lieber",
  "Oregon State University",
  "Oriol Vinyals",
  "Oscar Täckström",
  "Output Embedding",
  "Output Probabilities",
  "Over-fitting",
  "PagedAttention",
  "Pan, 2024",
  "Paolo Frasconi",
  "Parikh et al., 2016",
  "Parsing",
  "Paulus et al., 2017",
  "Peng Li",
  "Peng et al., 2023",
  "Penn State University",
  "Penn Treebank",
  "Performance–efficiency Trade-offs",
  "Petrov",
  "Petrov et al. (2006)",
  "Positional Encoding",
  "Prefill phase",
  "Press & Wolf, 2016",
  "Publication [18]",
  "Publication [20]",
  "Publication [25]",
  "Publication [32]",
  "Publication [33]",
  "Publication [36]",
  "Publication [37]",
  "Publication [38]",
  "Publication [39]",
  "Publication [3]",
  "Publication [6]",
  "Publication [9]",
  "Q (query)",
  "Qin Gao",
  "Qingyun Wu",
  "Query (Q)",
  "Quoc V. Le",
  "Qwen",
  "Qwen3",
  "Qwen3 Team",
  "Qwen3 Team, 2025b",
  "Qwen3-30B",
  "Qwen3-30B-A3B",
  "Qwen3-30B-A3B-Instruct",
  "Qwen3-30B-A3B-Thinking",
  "Qwen3-4B",
  "Qwen3-4B-Instruct",
  "Qwen3-4B-Thinking",
  "RATTENTION",
  "RNN encoder-decoder",
  "RNN sequence-to-sequence models",
  "RNN-like linear attention transformers",
  "RWKV",
  "Rafal Jozefowicz",
  "Rattention",
  "ReLU Activation",
  "Real-world tasks",
  "Recurrent Neural Network Grammar",
  "Recurrent Neural Network Grammars",
  "Recurrent Neural Networks",
  "Regularization",
  "Rein et al., 2023",
  "Reinforcement Learning",
  "Retentive Network",
  "Richard Socher",
  "Rico Sennrich",
  "Rob Fergus",
  "Romain Paulus",
  "Romain Thibaux",
  "Ruler",
  "Ruslan Salakhutdinov",
  "SFT",
  "SWA adaptation",
  "SWA-aware fine-tuning",
  "Sainbayar Sukhbaatar",
  "Samy Bengio",
  "Scale operation",
  "Scaled Dot-Product Attention",
  "Scatter plot",
  "Self-attention mechanism",
  "Self-distillation",
  "Self-training",
  "Sennrich et al., 2015",
  "Sentence structure",
  "Sentence visualization",
  "Separable Convolutions",
  "Sepp Hochreiter",
  "Sequence Modeling",
  "Sequence to sequence learning",
  "Sergey Ioffe",
  "Shao et al., 2024",
  "Shaoqing Ren",
  "Shift-reduce constituent parsing",
  "Slav Petrov",
  "Sliding Window Attention",
  "Sliding Window Attention Adaptation",
  "Sliding Window Attention Adaptation Code Repository",
  "Sliding Window Attention Training",
  "SoftMax function",
  "Softmax layer",
  "Sparse attention",
  "Statistical machine translation",
  "Stephan Gouws",
  "Stochastic optimization",
  "Streaming Attention",
  "Structured attention networks",
  "Structured self-attentive sentence embedding",
  "Subword units",
  "Sukhbaatar et al., 2015",
  "Sun et al., 2023",
  "Sutskever et al., 2014",
  "Team et al., 2025",
  "Team, 2024a",
  "Team, 2025a",
  "Team, 2025b",
  "Terry Koo",
  "Throughput",
  "Time",
  "Time-per-output-token",
  "Time-to-first-token",
  "Token relationships",
  "Tokens",
  "Touvron et al., 2023",
  "Transformer",
  "Transformer architecture",
  "Translation tasks",
  "Tri Dao",
  "University of Toronto",
  "V (value)",
  "Value (V)",
  "Vaswani et al., 2017",
  "Vincent Vanhoucke",
  "Vinyals",
  "Vinyals & Kaiser el al. (2014)",
  "Vinyals & Kaiser el al.(2014)",
  "WMT 2014 English-to-French translation task",
  "WMT 2014 English-to-German translation task",
  "Wang et al., 2025",
  "Wei Xu",
  "Wei et al., 2022",
  "Wenbo Pan",
  "Wenliang Chen",
  "Window parameter",
  "Wolfgang Macherey",
  "Woosuk Kwon",
  "Word embeddings",
  "Word-piece",
  "Wu et al., 2024",
  "Xception",
  "Xiangyu Zhang",
  "Xiao et al., 2024",
  "Xiao, 2025",
  "Xuan Zhang",
  "Xuguang Wang",
  "Yang et al., 2024",
  "Yann N. Dauphin",
  "Yijiong Yu",
  "Ying Cao",
  "Yonggan Fu",
  "Yonghui Wu",
  "Yoon Kim",
  "Yoshua Bengio",
  "Yuan Cao",
  "Yuan et al., 2025",
  "Yue Zhang",
  "Yushi Bai",
  "Yutao Sun",
  "Zaheer et al., 2020",
  "Zbigniew Wojna",
  "Zhang et al., 2024",
  "Zhaorui Yang",
  "Zhifeng Chen",
  "Zhihong Shao",
  "Zhongqiang Huang",
  "Zhouhan Lin",
  "Zhu",
  "Zhu et al. (2013)",
  "Zichuan Fu",
  "anaphora resolution",
  "attention heads",
  "attention mask",
  "attention patterns",
  "attention sinks",
  "attention weights",
  "attention_is_all_you_need_raw_with_image_ids_with_captions",
  "attention_is_all_you_need_raw_with_image_ids_with_captions::section::3 Model Architecture",
  "attention_is_all_you_need_raw_with_image_ids_with_captions::section::3.2 Attention",
  "attention_is_all_you_need_raw_with_image_ids_with_captions::section::References",
  "catastrophic degradation",
  "computational complexity",
  "computational workflow",
  "decoding speed",
  "deep reasoning tasks",
  "deep understanding and reasoning",
  "distributed LLM services",
  "fixed-interval selection",
  "generation length",
  "human reading comprehension",
  "img_attention_is_all_you_need_12_0",
  "img_attention_is_all_you_need_13_0",
  "img_attention_is_all_you_need_14_0",
  "img_attention_is_all_you_need_2_0",
  "img_attention_is_all_you_need_3_0",
  "img_attention_is_all_you_need_3_1",
  "img_sliding_window_attention_2_0",
  "img_sliding_window_attention_7_0",
  "instruction-tuning",
  "key",
  "language model fine-tuning",
  "linear attention",
  "long context processing",
  "long-context QA tasks",
  "long-context fine-tuning",
  "long-context performance",
  "long-context tasks",
  "long-term interactive memory",
  "mathematical reasoning",
  "needle-retrieval tasks",
  "neural network layers",
  "performance restoration",
  "performance-efficiency trade-off",
  "performance-efficiency trade-offs",
  "prefilling",
  "processing stages",
  "query",
  "real-world tasks",
  "reasoning trajectory",
  "self-distillation",
  "sink tokens",
  "sliding_window_attention_raw_with_image_ids_with_captions",
  "sliding_window_attention_raw_with_image_ids_with_captions::section::3.3 Chain-of-Thought",
  "sliding_window_attention_raw_with_image_ids_with_captions::section::5.3 Performance–efficiency Trade-offs and",
  "sparse attention",
  "stacking sliding windows",
  "structured state-space models",
  "synthetic tasks",
  "training–inference mismatch",
  "triangular matrix structure",
  "vLLM",
  "value",
  "word tokens",
  "Çaglar Gülçehre",
  "Łukasz Kaiser"
]