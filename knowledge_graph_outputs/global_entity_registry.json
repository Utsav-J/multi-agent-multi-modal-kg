[
  "#1 Ensemble - nlnet",
  "#1 Single - MIR-MRC (F-Net)",
  "#2 Ensemble - QANet",
  "#2 Single - nlnet",
  "+ BiLSTM",
  "-ϕ/ωτ",
  "0 W",
  "0.0001 Learning Rate",
  "0.001 Learning Rate",
  "0.2 Dropout Value",
  "0.2 mm micro-cracks",
  "0.5 Dropout Value",
  "1 ~ 3 Testing Samples",
  "1 ~ 5 Testing Samples",
  "1 ~ 7 Testing Samples",
  "1(Lin) Dense Configuration",
  "10 Epochs",
  "10 Hz Sampling Rate",
  "10 kN",
  "10% testing",
  "10,000 testing samples",
  "10-second recordings",
  "100 Epochs",
  "100 Hz",
  "100-d, 16h NAC Configuration",
  "11 kN",
  "12 kN",
  "15 complete run-to-failure experiments",
  "150 Epochs",
  "1500 rpm",
  "15647 RGB images",
  "16 run-to-failure experiments",
  "16-d, 8h NAC Configuration",
  "1650 rpm",
  "1800 rpm",
  "1D (64@5, 64@3) Conv Configuration",
  "1D non-stationary vibrational signals",
  "1D/2D Layer Type",
  "20 Batch Size",
  "20 episodes",
  "20% Sparsity",
  "20% testing",
  "200 W",
  "2100 rpm",
  "2250 rpm",
  "2400 rpm",
  "25 recordings",
  "25.6 kHz Sampling Rate",
  "28x28 image",
  "28x28 pixels",
  "2x 1D (32@3, 16@2) Conv Configuration",
  "2x 1D (64@5) Conv Configuration",
  "32 Batch Size",
  "32-d, 4h NAC Configuration",
  "320x160x3",
  "32–10(SM) Dense Configuration",
  "32–11(SM) Dense Configuration",
  "35 Hz",
  "37.5 Hz",
  "3xTD- 2D (10–30@3–5) Conv Configuration",
  "4 kN",
  "4 ~ 7 Training Samples",
  "4.2 kN",
  "40 Hz",
  "400 W",
  "48,174 RGB images",
  "5 kN",
  "5-fold Cross-Validation",
  "50 minutes",
  "50% Sparsity",
  "500 Epochs",
  "51.2 kHz Sampling Rate",
  "53 time steps",
  "5M timesteps",
  "5x 2D (24–64@5–3, ELU) Conv Configuration",
  "60,000 training samples",
  "6204",
  "6205",
  "6206",
  "6207",
  "6208",
  "64-d, 16h NAC Configuration",
  "64-d, 8h NAC Configuration",
  "64–1(Lin) Dense Configuration",
  "64–5(SM) Dense Configuration",
  "66x120x3",
  "70,000 grayscale images",
  "784 time steps",
  "8-bit pixel values",
  "80% training",
  "90% Sparsity",
  "90% training",
  "90:10 ratio",
  "92x92x3",
  "A",
  "A Embedding",
  "A broad-coverage challenge corpus for sentence understanding through inference",
  "A embedding",
  "A theoretical framework for back-propagation",
  "A.1 Illustration of the Pre-training Tasks",
  "A.2 Pre-training Procedure",
  "A.3 Fine-tuning Procedure",
  "A.4 Comparison of BERT, ELMo, and OpenAI GPT",
  "A.5 Illustrations of Fine-tuning on Different Tasks",
  "ACM",
  "AI",
  "AI Models",
  "A_max",
  "A_min",
  "Ablation Studies",
  "Ablation for Different Masking Procedures",
  "Ablation over Pre-training Tasks",
  "Acc",
  "Accelerates Convergence During Training",
  "Accuracy",
  "Accuracy Level",
  "Accuracy Scores",
  "Accurate RUL Estimation",
  "Adam",
  "AdamW",
  "Adams Wei Yu",
  "Adaptive Remapping",
  "Adaptive Temporal Dynamics",
  "Adaptive Top-K Selection",
  "Adaptive checkpoint adjoint method",
  "Adina Williams",
  "Adjacency Matrices",
  "Adjoint sensitivity analysis for differential-algebraic equations",
  "Advances in Neural Information Processing Systems",
  "Advances in neural information processing systems",
  "Advances in psychology",
  "Aggregate Representation",
  "Aguiar-Conraria & Soares, 2014",
  "Aguiar-Conraria, L.",
  "Aidan N Gomez",
  "Aj",
  "Al-Rfou et al., 2018",
  "Alex Perelygin",
  "Alex Wang",
  "Alex Warstadt",
  "Algorithm 1",
  "Algorithm 2",
  "Algorithm 3",
  "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
  "Amanpreet Singh",
  "Andrew Ng",
  "Answer Text Span Prediction",
  "Antonio Torralba",
  "Appendix A",
  "Appendix A. Preliminaries",
  "Appendix A.5",
  "Appendix B",
  "Appendix B.1",
  "Appendix C",
  "Appendix D.3.3",
  "Architectural improvement",
  "Ashish Vaswani",
  "Association for Computational Linguistics",
  "Attention",
  "Attention Heads",
  "Attention Logits",
  "Attention Logits Computation",
  "Attention Masking",
  "Attention Mechanisms",
  "Attention Output",
  "Attention Trajectory",
  "Attention Weights",
  "Attention Weights and output",
  "Attention is all you need",
  "Auditable Autonomy",
  "AutoNCP",
  "Autonomous Systems",
  "Autonomous Vehicles (AVs)",
  "B Detailed Experimental Setup",
  "B Embedding",
  "B embedding",
  "B.1 Detailed Descriptions for the GLUE Benchmark Experiments",
  "BERT",
  "BERT SQuAD Model",
  "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
  "BERTBASE",
  "BERTBASE Architecture",
  "BERTLARGE",
  "BERTLARGE Ensemble",
  "BERTLARGE Ensemble + TriviaQA",
  "BERTLARGE Single",
  "BERTLARGE Single + TriviaQA",
  "BMC research notes",
  "BPTT",
  "Backbone Network",
  "Batch Size",
  "Bearing 1",
  "Beltagy et al., 2020",
  "Beltagy, I.",
  "Benchmark Datasets",
  "Benefits",
  "Bentivogli et al. (2009)",
  "Bi-directionality",
  "BiDAF+ELMo (Single)",
  "BiLSTM",
  "Bidirectional Cross Attention",
  "BigBird",
  "Billion Word Benchmark",
  "Binary classification",
  "Binary single-sentence classification",
  "Biological Nervous Systems",
  "Biologically Inspired Approach",
  "Biologically Inspired Attention Mechanism",
  "Biologically Plausible Attention Mechanisms",
  "Biologically Plausible CT-Attention Mechanism",
  "Bojarski et al., 2016",
  "Bojarski, M.",
  "BooksCorpus",
  "Bounded Representation of Time",
  "Boundedness of Attention Logit State Trajectory",
  "Brockman et al., 2016",
  "Brockman, G.",
  "C Additional Ablation Studies",
  "C. elegans",
  "C. elegans Neuronal Circuit Policies (NCPs)",
  "C.1 Effect of Number of Training Steps",
  "C.2 Ablation for Different Masking Procedures",
  "CASE 1: Single Connection (M=1)",
  "CASE 2: Multiple Connections (M>1)",
  "CNN component",
  "CRF Layer",
  "CSE",
  "CT",
  "CT Baselines",
  "CT-Attention Models",
  "CT-RNN",
  "CT-RNN Models",
  "CT-RNNs",
  "CT-transformer",
  "CTA",
  "Camera Fusion",
  "Cao et al., 2003",
  "Cao, Y.",
  "Car behavioral cloning",
  "CarRacing",
  "CarRacing Benchmark",
  "Carle",
  "Causal Structure",
  "Cer et al. (2017)",
  "CfC",
  "Changxing Sumyoung Technology",
  "Chelba et al., 2013",
  "Chen Wu",
  "Chen et al. (2018)",
  "Chen et al., 2018",
  "Chen et al., 2023",
  "Chen, R. T.",
  "Chen, Y.",
  "Chen, Y.-H.",
  "Chien & Chen, 2021",
  "Chien, J.-T.",
  "Cho et al., 2014",
  "Cho, K.",
  "Christopher D Manning",
  "Christopher Potts",
  "Clark and Gardner, 2018",
  "Clark et al., 2018",
  "Classification Layer Weights W",
  "Classification Loss",
  "Classification Task",
  "Classifier Layer Initialization",
  "Classifying Steering Actions",
  "Closed form continuous-time neural networks",
  "Cloud TPU",
  "Cloud TPUs",
  "Cloze procedure: A new tool for measuring readability",
  "CoLA",
  "CoNLL",
  "CoNLL-2003 Named Entity Recognition",
  "Code for reproducibility",
  "Cohan, A.",
  "Communications on Pure and Applied Mathematics",
  "Compact Architectures",
  "Components",
  "Computation of phi(h) and omega_tau(h)",
  "Computational Requirements",
  "Condition 1",
  "Condition 2",
  "Condition 3",
  "Condition monitoring data",
  "Content Alignments",
  "Content-Target Gate",
  "Context",
  "Contextual representation of token i",
  "ContiFormer",
  "Continuous Analogue of Standard Weighted Sums",
  "Continuous modeling of sporadicallyobserved time series",
  "Continuous-depth Models",
  "Continuous-time (CT) modeling",
  "Continuous-time Attention (CTA)",
  "Continuous-time RNNs (CT-RNNs)",
  "Continuous-time attention for sequential learning",
  "Continuous-time transformer for irregular time series modeling",
  "Conv layers",
  "Conv1D/2D layer",
  "Convergence",
  "Convergence Rate",
  "Corollary 1",
  "Corollary 2",
  "Corollary 3",
  "Corpus",
  "Cost",
  "Cross-Validation",
  "Cross-Validation Capability",
  "DT",
  "DT Attention",
  "Data Augmentation",
  "Data distributions",
  "Dataset",
  "David Dohan",
  "De Brouwer et al., 2019",
  "De Brouwer, E.",
  "Decoder",
  "Degradation Features",
  "Deng, 2012",
  "Deng, L.",
  "Dense",
  "Department of Automation, University of Science & Technology of China",
  "Deriving Closed-form (Exact) Solution",
  "Dev F1 Score",
  "Dev Set",
  "Dev Set Accuracy",
  "Dev Set accuracy",
  "Dev results",
  "Different Datasets",
  "Ding et al., 2021",
  "Ding, Y.",
  "Discrete-time Recurrent Neural Networks (DT-RNNs)",
  "Document-level Corpus",
  "Dolan and Brockett (2005)",
  "Dominant frequency",
  "Downstream Tasks",
  "Dropout",
  "Dropout probability",
  "Dynamic Compatibilities",
  "E",
  "ELMo",
  "ESIM+ELMo",
  "ESIM+GloVe",
  "Early Works",
  "Effect of Model Size",
  "Effect of Number of Training Steps",
  "Efficiency",
  "Efficient content-based sparse attention with routing transformers",
  "Electronics",
  "Elsevier",
  "Empirical evaluations",
  "Encoder",
  "Encoder Outputs",
  "End Vector E",
  "End to end learning for self-driving cars",
  "End-Task Model Parameters",
  "Energy",
  "English Wikipedia",
  "Entailment",
  "Entailment classification",
  "Entropy",
  "Epochs",
  "Equation 1",
  "Equation 18",
  "Equation 24",
  "Equation 25",
  "Equation 26",
  "Equation 27",
  "Equation 28",
  "Equation 29",
  "Equation 30",
  "Equation 31",
  "Equation 32",
  "Equation 33",
  "Equation 34",
  "Equation 35",
  "Equation 37",
  "Equation 38",
  "Equation 39",
  "Equation 40",
  "Equation 41",
  "Equation 42",
  "Equation 43",
  "Equation 44",
  "Equation 45",
  "Erik F Tjong Kim Sang",
  "Euler Mode",
  "Evaluation",
  "Evaluation Metric type",
  "Event-based Format",
  "Event-based MNIST",
  "Exact Closed-Form Solution",
  "Exact Mode",
  "Expected Degradation",
  "Experiment",
  "Experimental Videos",
  "Experiments",
  "Explicit Euler Integration",
  "Explicit Euler Solver",
  "Exponential Convergence",
  "Exponential Decay Bound",
  "Expression Extent",
  "Expression Speed",
  "Expressive Similarities",
  "Expressiveness",
  "Extracting and composing robust features with denoising autoencoders",
  "F1 Score",
  "Fastest",
  "Feature extraction based on morlet wavelet",
  "Feature-based Approach",
  "Feature-based approach",
  "Felix Hill",
  "Fien De Meulder",
  "Figure 1",
  "Figure 1(a)",
  "Figure 2",
  "Figure 3",
  "Figure 4",
  "Figure 5",
  "Figure 6",
  "Final Hidden Vector C",
  "Fine-tuning",
  "Fine-tuning BERT",
  "Fine-tuning Data Shuffling",
  "Fine-tuning Task Accuracy",
  "First Place",
  "Fixed-Length Sequences",
  "Flexibility",
  "Flexibility of NAC",
  "Flexible Recurrent Architecture",
  "Fu Sun",
  "Full Pairwise Concatenation",
  "Future Work",
  "GLUE",
  "GLUE Benchmark",
  "GLUE Benchmark FAQ",
  "GLUE Leaderboard",
  "GLUE Tasks",
  "GLUE Test Results",
  "GPU",
  "GRU",
  "GRU-ODE",
  "Gated Recurrent Unit (GRU)",
  "Generalization",
  "GitHub Repository",
  "Glue: A multi-task benchmark and analysis platform for natural language understanding",
  "Google",
  "Google Colab T4-GPU",
  "Google’s neural machine translation system: Bridging the gap between human and machine translation",
  "Gradient Characterization",
  "Grosu, R.",
  "Grounded Commonsense Inference",
  "Group-Wise Masking",
  "Gru-ode-bayes",
  "H",
  "H heads",
  "H units",
  "HUST",
  "HUST Bearing",
  "Hanoi University of Science and Technology",
  "Hasani et al., 2021",
  "Hasani et al., 2022",
  "Hasani, R.",
  "Hasani, R. M.",
  "Head Layers",
  "Head Splitting and Sparse Top-K Pairwise Computation",
  "Head-specific Attention Outputs",
  "Heads",
  "Hefei",
  "Hefei Comprehensive National Science Center",
  "Hendrycks and Gimpel (2016)",
  "Hidden Units",
  "Hochreiter & Schmidhuber, 1997",
  "Hochreiter, S.",
  "Hod Lipson",
  "Hong & Thuan, 2023",
  "Hong, H. S.",
  "Hongwei, M.",
  "How transferable are features in deep neural networks?",
  "Hu et al., 2018",
  "Hugo Larochelle",
  "Human",
  "Hyperparameter",
  "ICLR",
  "IEEE",
  "IEEE International Conference on Prognostics and Health Management, PHM’12",
  "IEEE PHM",
  "IEEE signal processing magazine",
  "ITFR",
  "Illia Polosukhin",
  "Images",
  "In",
  "Independent Subspaces",
  "Industrial Prognostics",
  "Industry 4.0",
  "Inertial Measurement Sensors",
  "Input Curation",
  "Input Embeddings",
  "Input Projections",
  "Input Question",
  "Input Representation",
  "Input Sequence",
  "Input embedding E",
  "Institute of Artificial Intelligence, Hefei Comprehensive National Science Center",
  "Inter-to-Motor Pathways",
  "International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems",
  "International conference on machine learning",
  "Interneuron",
  "Introduction to self-driving cars",
  "Introduction to the conll-2003 shared task: Language-independent named entity recognition",
  "Irregular Sampling",
  "Irregular Time-Series Classification",
  "Irregularly Sampled Data",
  "Irregularly Sampled Series",
  "Iw",
  "Izis Kankaraway",
  "Jakob Uszkoreit",
  "Jason Chuang",
  "Jason Yosinski",
  "Jean Wu",
  "Jeff Clune",
  "Jernite et al. (2017)",
  "John, F.",
  "John, F. (1952)",
  "Jordan, 1997",
  "Jordan, M. I.",
  "Joseph Turian",
  "Joshi et al., 2017",
  "Journal of economic surveys",
  "Journal of sound and vibration",
  "Journalism Bulletin",
  "Julian Michael",
  "K",
  "K_eff",
  "Kai Chen",
  "Key Hyperparameters",
  "Key Tensors",
  "Klaus Macherey",
  "Kurtosis",
  "L2 weight decay",
  "LM Perplexity",
  "LNN (ODEsolve)",
  "LNN layers",
  "LNNs",
  "LSTM",
  "LSTM architecture",
  "LSTMs",
  "LTC",
  "LTC dynamics",
  "LTR & No NSP",
  "LTR Model",
  "LTR Models",
  "LTR model",
  "LTR models",
  "LTR pre-training",
  "Lane marking detection",
  "Lane-Keeping for Autonomous Vehicles",
  "Language Model Pre-training",
  "Language Modeling",
  "Language understanding capability",
  "Large Models",
  "Latent ordinary differential equations for irregularly-sampled time series",
  "Layers",
  "LeCun et al., 1988",
  "LeCun, Y.",
  "Learnable Time-Constant Gate",
  "Learnable Time-Constant Gate Head",
  "Learning Rate",
  "Learning internal representations by error propagation",
  "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
  "Learning rate warmup",
  "Lechner & Hasani, 2022",
  "Lechner et al., 2018",
  "Lechner et al., 2020",
  "Lechner, M.",
  "Left-to-Right (LTR) LM",
  "Lev Ratinov",
  "Levesque et al. (2011)",
  "Lidar",
  "Limitations",
  "Lin & Qu, 2000",
  "Lin, J.",
  "Linear First-Order ODE",
  "Linear Projection",
  "Linear activation function",
  "Linyang Li",
  "Liquid Neural Networks (LNNs)",
  "Llion Jones",
  "Localized Person Activity Dataset",
  "Localized Person Activity Recognition dataset",
  "Localized Safety",
  "Log-likelihoods",
  "Logeswaran and Lee (2018)",
  "Long Sequences",
  "Long-short term memory (LSTM)",
  "Longformer",
  "Loss Function type",
  "Lower bound",
  "Lukasz Kaiser",
  "MAE",
  "MASK strategy",
  "MLM Model",
  "MLM model",
  "MLM pre-training",
  "MNIST",
  "MNIST Dataset",
  "MNLI",
  "MNLI Dev accuracy",
  "MRPC",
  "MSE",
  "Machine Failures",
  "Machine Translation",
  "Manufacturing",
  "Marlin, B. M.",
  "Masked LM (MLM)",
  "Masked LM and the Masking Procedure",
  "Masked LM likelihood",
  "Masked Language Model (MLM) objective",
  "Masking Procedures",
  "Masking Rate",
  "Maxim Krikun",
  "Mean Absolute Error",
  "Mean Squared Error (Loss)",
  "Mean Squared Error (Metric)",
  "Melamud et al., 2016",
  "Memory",
  "Memory Consumption",
  "Memory Requirements",
  "Mendeley Data",
  "Mike Schuster",
  "Ming Yan",
  "Minh-Thang Luong",
  "Mismatch between pre-training and fine-tuning",
  "Mixed strategy for masking",
  "Mixed-memory RNNs (mmRNNs)",
  "Mn",
  "Model",
  "Model Architecture",
  "Model Dimension",
  "Mohammad Norouzi",
  "Morlet wavelet",
  "Morlet wavelet function",
  "Motor Neurons",
  "Mp",
  "Multi-Head Attention",
  "Multi-Head Attention (MHA)",
  "Multi-Head Extension",
  "Multi-granularity hierarchical attention fusion networks for reading comprehension and question answering",
  "Multi-time attention networks for irregularly sampled time series",
  "Multilayered feedforward networks",
  "NAACL",
  "NAC",
  "NAC Ablation Configurations",
  "NAC Layer",
  "NAC Sparsity",
  "NAC layer",
  "NAC-02s",
  "NAC-09s",
  "NAC-2k",
  "NAC-32k",
  "NAC-Euler",
  "NAC-Exact",
  "NAC-Exact/05s/8k",
  "NAC-FC",
  "NAC-PW",
  "NAC-Steady",
  "NAC’s ability to understand task",
  "NCP architecture",
  "NCP sparsity s",
  "NCP-based inter-to-motor projection",
  "NCP-based sensory projections",
  "NER label set",
  "NLP Tasks",
  "NN sensory",
  "Named Entity Recognition",
  "Natural Language Inference (NLI)",
  "Natural Language Inference dataset",
  "Natural Language Understanding Tasks",
  "Nature Machine Intelligence",
  "Nc",
  "Nectoux et al., 2012",
  "Nectoux, P.",
  "Neil et al., 2016",
  "Neil, D.",
  "Nervous System of C. elegans",
  "Neural Network Architecture",
  "Neural Network Layer Design",
  "Neural Networks",
  "Neural ODEs",
  "Neural Processing Letters",
  "Neural circuit policies enabling auditable autonomy",
  "Neural circuit policies imposing visual perceptual autonomy",
  "Neural computation",
  "Neural network acceptability judgments",
  "NeuralODE",
  "Neuronal Attention Circuit (NAC)",
  "Neuronal Circuit Policies",
  "Neuronal Circuit Policies (NCPs)",
  "Next Sentence Prediction",
  "Next Sentence Prediction (NSP)",
  "Next Sentence Prediction likelihood",
  "Ni",
  "Niki Parmar",
  "Nikita Nangia",
  "Nishijima, 2021",
  "Nishijima, T.",
  "Nm",
  "No NSP",
  "No-answer Span",
  "Noam Shazeer",
  "Non-null Span",
  "Nonlinear Activations",
  "Nonlinear Backbone",
  "Nonlinear Content-Target Head",
  "Nonlinear Interlinked Gates",
  "Ns",
  "Number of Epochs",
  "Number of epochs",
  "O(k * S)",
  "O(k)",
  "O(n^2k)",
  "O(nk * S)",
  "O(nk)",
  "ODE solvers",
  "ODE-based models",
  "ODE-defined latent trajectories",
  "ODEFormer",
  "ODEs",
  "Objective",
  "Omer Levy",
  "OpenAI CarRacing",
  "OpenAI CarRacing environment",
  "OpenAI GPT",
  "OpenAI Gym",
  "OpenAI-CarRacing dataset",
  "Operating Condition",
  "Optimizer type",
  "Ordinary Differential Equations (ODEs)",
  "Output Layer",
  "Output Projection",
  "Output layer",
  "PAR",
  "PMLR",
  "PPO-trained agent",
  "PRONOSTIA",
  "PRONOSTIA experimental platform",
  "Pairwise Logits",
  "Paper",
  "Parameters",
  "Paraphrasing",
  "Parikh et al. (2016)",
  "Park et al., 2021",
  "Park, M.",
  "Pascal Vincent",
  "Passage",
  "Peak Memory Usage",
  "Performance",
  "Person Activity Recognition (PAR)",
  "Peters et al., 2018a",
  "Peters et al., 2018b",
  "Peters, M. E.",
  "Phased LSTM",
  "PhasedLSTM",
  "Pierre-Antoine Manzagol",
  "Position Embeddings",
  "Positional embeddings",
  "Pre-trained Checkpoint",
  "Pre-training",
  "Pre-training Data",
  "Pre-training Task",
  "Pre-training Tasks",
  "Predicting Steering Values",
  "Probability Distribution",
  "Proceedings of the 1988 connectionist models summer school",
  "Proceedings of the 2013 conference on empirical methods in natural language processing",
  "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
  "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP",
  "Proceedings of the 25th international conference on Machine learning",
  "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics",
  "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  "Proceedings of the AAAI Conference on Artificial Intelligence",
  "Proceedings of the AAAI conference on artificial intelligence",
  "Proceedings of the IEEE international conference on computer vision",
  "Prognostic Health Management (PHM) Systems",
  "Proof for Theorem 2",
  "Proof of Theorem 1",
  "Proofs",
  "Pseudo-Time Step",
  "Pseudo-Time Vector",
  "Q",
  "QA",
  "QANet",
  "QANet: Combining local convolution with global self-attention for reading comprehension",
  "QK[⊤] matrix",
  "QNLI",
  "QQP",
  "Qin Gao",
  "Qu, L.",
  "Query Tensors",
  "Query-Key Pair",
  "Question Answering",
  "Question/Answer Pairs",
  "Quoc V Le",
  "R.M. Reader (Ensemble)",
  "RGB color space",
  "RND strategy",
  "RNN",
  "RNNs",
  "RTE",
  "RTL Models",
  "RTL models",
  "R^(2d)",
  "R^(d_model)",
  "R^H",
  "R^d",
  "R^m",
  "R^n",
  "Rajpurkar et al. (2016)",
  "Rajpurkar et al., 2016",
  "Random Restarts",
  "Raquel Urtasun",
  "Rate of Convergence",
  "Razzaq & Hongwei, 2023",
  "Razzaq & Zhao, 2025a",
  "Razzaq & Zhao, 2025b",
  "Recent Research",
  "Recurrent Connections",
  "Recurrent Layer",
  "Recursive deep models for semantic compositionality over a sentiment treebank",
  "Reliability Engineering & System Safety",
  "Remaining Useful Life (RUL) Estimation",
  "Remaining useful life estimation using deep metric transfer learning for kernel regression",
  "Remark 1",
  "Representation Learning",
  "Representation Learning Objectives",
  "Repurposed NCPCell",
  "Research",
  "Resource-Constrained Devices",
  "Results",
  "Rich Zemel",
  "Richard Socher",
  "Riemann-Style Approach",
  "Road's Horizon",
  "Robotics",
  "Robustness",
  "Rolling Element Bearings (REB)",
  "Rowan Zellers",
  "Roy Schwartz",
  "Roy et al., 2021",
  "Roy, A.",
  "Rubanova et al., 2019",
  "Rubanova, Y.",
  "Rui Zhao",
  "Rumelhart et al., 1985",
  "Rumelhart, D. E.",
  "Runge-Kutta",
  "Runtime",
  "Ruslan Salakhutdinov",
  "Ryan Kiros",
  "S",
  "SAME strategy",
  "SCE",
  "SIAM journal on scientific computing",
  "SLQA+ (Single)",
  "SQuAD",
  "SQuAD 2.0",
  "SQuAD 2.0 Results",
  "SQuAD Leaderboard",
  "SQuAD v1.1",
  "SQuAD v1.1 BERT Model",
  "SST-2",
  "STS-B",
  "SWAG",
  "SWAG Dev and Test Accuracies",
  "SWAG Paper",
  "Saliency Maps",
  "Sample Complexity to δ-accuracy",
  "Samuel R Bowman",
  "Sanja Fidler",
  "Saturation of Preferences",
  "Scaled-Dot Attention",
  "Schmidhuber, J.",
  "Score",
  "Score Metric",
  "Second Place",
  "Section 3",
  "Section 4",
  "Section 5.1",
  "Segmentation Embeddings",
  "Self-attention Mechanism",
  "Semantic Textual Similarity Benchmark",
  "Semantic equivalence classification",
  "Sensory Gate",
  "Sensory Neurons",
  "Sentence A",
  "Sentence A/B embeddings",
  "Sentence B",
  "Sentence Embeddings",
  "Sentence-pair Completion Examples",
  "Sentiment Analysis",
  "Seo et al. (2017)",
  "Sequence Length",
  "Sequence Tagging",
  "Sequence-level tasks",
  "Sequential Data",
  "Serial order: A parallel distributed processing approach",
  "Shared Representations",
  "Shibuya, 2017",
  "Shibuya, N.",
  "Shuffled Sentence-level Corpus",
  "Shukla & Marlin, 2021",
  "Shukla, S. N.",
  "Sigmoid Function",
  "Sigmoidal Transformation",
  "Single Packed Sequence",
  "Single-model, Single-task",
  "Six fault categories",
  "Skewness",
  "Small Datasets",
  "Soares, M. J.",
  "Socher et al. (2013)",
  "Soft Computing",
  "Softmax Layer",
  "Softmax Normalization",
  "Sparse Attention",
  "Sparse Backbone Network",
  "Sparse Categorical Crossentropy",
  "Sparse Input",
  "Sparse Sensory Gates",
  "Sparse Top-K Pairwise Concatenation",
  "Sparse Top-K Pairwise Concatenation Scheme",
  "Sparse Top-K Selection",
  "Sparse sinkhorn attention",
  "Sparsity",
  "Sparsity of NAC Layer",
  "Sparsity s",
  "Spearman Correlations",
  "Speed of Preferences",
  "Standard Attention",
  "Stanford Question Answering Dataset",
  "Start Vector S",
  "State Stability",
  "Steady Mode",
  "Steady State",
  "Steady-State Amplitude",
  "Steady-State Approximation",
  "Steering Actions",
  "Steering Commands",
  "Steering Values",
  "Stinchcomb, M.",
  "Stinchcomb, M. (1989)",
  "Structural Dependencies",
  "Sun et al., 2018",
  "Surveillance",
  "Swag: A large-scale adversarial dataset for grounded commonsense inference",
  "Symbolic regression of dynamical systems with transformers",
  "Synaptic Transmission",
  "TPU chips",
  "TRAJECTORY SENSITIVITIES FOR CLOSED-FORM FORMULATION",
  "Table 1",
  "Table 2",
  "Table 3",
  "Table 4",
  "Table 5",
  "Table 6",
  "Table 7",
  "Target Signal Strength",
  "Target Token",
  "Task-specific Inputs",
  "Task-specific Outputs",
  "Task-specific fine-tuning learning rate",
  "Task-specific models",
  "Tay et al., 2020",
  "Tay, Y.",
  "Technical report",
  "Temporal Dependencies",
  "Temporal convolution-based transferable cross-domain adaptation approach",
  "Text Classification",
  "Text Pairs",
  "The continuous wavelet transform",
  "The long-document transformer",
  "Theorem 1",
  "Theorem 2",
  "Third Place",
  "Threshold τ",
  "Throughput",
  "Thuan & Hong, 2023",
  "Thuan, N.",
  "Thuan, N. D.",
  "Tiang et al., 2018",
  "Tiang, Y.",
  "Time Series",
  "Time Vector Construction",
  "Time-Based Attention",
  "Time-Varying Datasets",
  "TimeDistributed layer",
  "Timestamp",
  "Tjong Kim Sang and De Meulder, 2003",
  "Tn",
  "Token Embeddings",
  "Token Predictions",
  "Token Representations",
  "Token-level tasks",
  "Top-K Interactions",
  "Top-K Selection",
  "Top-K=2",
  "Top-K=32",
  "Top-K=8",
  "Training",
  "Training Input Sequence",
  "Training Loss",
  "Training Objective",
  "Training, Gradients and Complexity",
  "Transactions of the Association for Computational Linguistics",
  "Transformer",
  "Transformer (Al-Rfou et al., 2018)",
  "Transformer (Vaswani et al., 2017)",
  "Transformer encoder",
  "Transformer encoder architecture",
  "Transformers for longer sequences",
  "TriviaQA",
  "Tsampling",
  "Tw",
  "Two-Layer 768-Dimensional BiLSTM",
  "U",
  "U topk",
  "U-net: Machine reading comprehension with unanswerable questions",
  "UAT proofs",
  "UC Irvine",
  "UCI Machine Learning Repository",
  "Udacity",
  "Udacity Benchmark",
  "Udacity Self-Driving Car Simulator",
  "Udacity Simulator",
  "Udacity simulator",
  "Uniform Initialization Bound",
  "Universal Approximation Theorem (UAT)",
  "Universal Approximation by NAC",
  "Universal Approximator",
  "University of Science & Technology of China",
  "Unseen Conditions",
  "Upper bound",
  "V",
  "Value Matrix",
  "Value Tensors",
  "Vanishing Gradients",
  "Vaswani et al. (2017)",
  "Vaswani et al., 2017",
  "Vaswani, A.",
  "Vibration data",
  "Vidulin et al., 2010",
  "Vidulin, V.",
  "WNLI",
  "WNLI Set",
  "Waleed Razzaq",
  "Wang et al. (2018a)",
  "Wang et al., 2018",
  "Wang et al., 2018a",
  "Wang et al., 2018b",
  "Wang, B.",
  "Warstadt et al. (2018)",
  "Wavelets",
  "Wei Wang",
  "Wikipedia",
  "Williams et al. (2018)",
  "Wilson L Taylor",
  "Wolfgang Macherey",
  "Word representations: A simple and general method for semi-supervised learning",
  "WordPiece model",
  "X1",
  "X2",
  "XJTU-SY",
  "XNs",
  "Xipeng Qiu",
  "Xi’an Jiaotong University",
  "Xn",
  "Y",
  "YUV color space",
  "Yang Liu",
  "Yejin Choi",
  "Yonatan Bisk",
  "Yonghui Wu",
  "Yoshua Bengio",
  "Yu et al. (2018)",
  "Yuan Cao",
  "Yukun Zhu",
  "Yun-Bo Zhao",
  "Zaheer et al., 2020",
  "Zaheer, M.",
  "Zellers et al., 2018",
  "Zhifeng Chen",
  "Zhu et al., 2015",
  "Zhuang et al., 2020",
  "Zhuang et al., 2021",
  "Zhuang, J.",
  "[CLS] Representation",
  "[CLS] Token",
  "[CLS] Token Representation C",
  "[CLS] as classification output symbol",
  "[CLS] token",
  "[MASK] token",
  "[SEP] as separator symbol",
  "[SEP] token",
  "_C_",
  "a",
  "a(h)",
  "a*",
  "a0",
  "ablation",
  "abstraction of neural circuits",
  "accelerated degradation conditions",
  "accelerated wear conditions",
  "accelerometer",
  "accelerometers",
  "accuracy",
  "accuracy epsilon",
  "accurate visual maps",
  "action labels",
  "activation potential",
  "activations",
  "adaptive temporal processing",
  "adjoint sensitivity method",
  "alpha(h)",
  "amax",
  "amin",
  "approximation assumption",
  "arXiv preprint arXiv:1406.1078",
  "arXiv preprint arXiv:1604.07316",
  "arXiv preprint arXiv:1606.01540",
  "arXiv preprint arXiv:1609.08144",
  "arXiv preprint arXiv:1803.08554",
  "arXiv preprint arXiv:1805.12471",
  "arXiv preprint arXiv:1810.06638",
  "arXiv preprint arXiv:2004.05150",
  "arXiv preprint arXiv:2101.10318",
  "arXiv preprint arXiv:2102.10993",
  "arXiv preprint arXiv:2310.05573",
  "architectural size",
  "architectural specifications",
  "architecture",
  "ascale",
  "asymmetric",
  "asynchronous time-series",
  "at",
  "attention",
  "attention scores",
  "attention weights",
  "b",
  "b(h)",
  "back-propagation",
  "backbone units",
  "ball",
  "ball bearing fault diagnosis",
  "bearings accelerated degradation tests",
  "benchmark dataset",
  "beta 1",
  "beta 2",
  "bidirectional model",
  "binary time series",
  "binary values",
  "biological sparsity",
  "biologically plausible range",
  "blurry maps",
  "bo",
  "brightness variations",
  "center camera stream",
  "class imbalance",
  "classification layer",
  "closed-form derivatives",
  "closed-form equilibrium solution",
  "closed-form exact solution",
  "closed-form solution",
  "compact CNN layers",
  "compact K",
  "compact convolutional layers",
  "compact convolutional network",
  "compact neural network",
  "comparable alternatives",
  "comparable performance",
  "complex temporal patterns",
  "complex time-series modeling",
  "compound faults",
  "computational complexity",
  "computational modes",
  "computer vision",
  "concatenated pair u(h)",
  "condition monitoring",
  "connectome of NCPs",
  "constant",
  "content-based sparse attention",
  "context vectors",
  "contextual embeddings",
  "continuous function f",
  "continuous scalar function phi~",
  "continuous steering values",
  "continuous time",
  "continuous wavelet transform (CWT)",
  "continuous-time Transformer",
  "continuous-time attention state",
  "continuous-time data",
  "continuous-time dynamics",
  "continuous-time embeddings",
  "controllable variable y",
  "convex combination",
  "convolutional heads",
  "convolutional neural network-based end-to-end self-driving",
  "counterparts",
  "counterparts baselines",
  "critical frequency",
  "cross-validation test",
  "current observation",
  "data",
  "data augmentation techniques",
  "data collection methodology",
  "dataset",
  "decisions",
  "deep bidirectional architectures",
  "deep bidirectional model",
  "deep unidirectional architectures",
  "degradation dynamics",
  "degradation estimation",
  "derivative",
  "destabilize optimization",
  "destabilizing the softmax",
  "differentiable computational graphs",
  "differential equations",
  "discrete attention",
  "discrete-time attention",
  "distance-aware uncertainty quantification methods",
  "diverse lighting conditions",
  "diverse tasks",
  "document context",
  "domain-specific features",
  "downstream task data",
  "downstream task impact",
  "dynamic relationships",
  "dynamics",
  "d’Ascoli et al., 2023",
  "d’Ascoli, S.",
  "e^(-ωτ t)",
  "early intervention",
  "early predictions",
  "early-stage defects",
  "effective equilibrium",
  "efficient architectures",
  "efficient continuous-time modeling",
  "elapsed time",
  "emulation",
  "encoder-decoder modeling",
  "encoding",
  "end-to-end hybrid neural network",
  "epsilon",
  "equilibrium",
  "equilibrium states",
  "error δ",
  "ethical concerns",
  "event-based format",
  "exact computation mode",
  "exact formulation",
  "excess samples",
  "experiment",
  "experimental platform",
  "explicit Euler integration method",
  "exploding gradients",
  "exponential error bounds",
  "exponential term",
  "falling",
  "fast-changing patterns",
  "fc",
  "fd",
  "feature extraction procedure",
  "features",
  "feedforward connections",
  "final dense layer",
  "five bearing types",
  "five discrete actions",
  "fixed interval of 211 ms",
  "fixed length of 256",
  "fixed-length representation",
  "fj",
  "fmax",
  "fmin",
  "fo",
  "focus",
  "forget gate",
  "forward",
  "forward-invariant",
  "four inertial measurement units",
  "four subcategories",
  "frozen-coefficient approximation",
  "full connectivity",
  "full pairwise concatenation",
  "full trajectory",
  "fully connected layer",
  "function ϕ",
  "g(x)",
  "gelu activation",
  "generalization testing",
  "gradient",
  "gradient estimation in neural ode",
  "gradient-based optimization",
  "gradients",
  "h",
  "handwritten digits",
  "hardware-aware optimization",
  "head h",
  "head output y(h)",
  "held-out training data",
  "hidden dimension k",
  "hidden dimension size",
  "hidden state",
  "hidden states",
  "hidden units",
  "horizontal axes",
  "https://arxiv.org/abs/2512.08499",
  "https://github.com/itxwaleedrazzaq/neuronal_attention_circuit",
  "https://github.com/naokishibuya/car-behavioral-cloning",
  "https://www.udacity.com/course/intro-to-self-driving-cars--nd113",
  "human participants",
  "hybrid deep-shallow learning framework",
  "hyperparameter configurations",
  "identity function",
  "image",
  "image classification tasks",
  "improved key scoring",
  "individual machines",
  "inertial sensors",
  "informative features",
  "inner race",
  "inner–ball",
  "inner–outer",
  "input curation",
  "input data",
  "input gate",
  "input u",
  "input variations",
  "input x",
  "input-dependent time constants",
  "integrating factor",
  "integrating factor µ",
  "integration",
  "integration of parabolic equations by difference methods",
  "interval [m, M]",
  "interval length",
  "irregular observations",
  "irregular sequences",
  "irregularly sampled time series",
  "irregularly sampled time-series",
  "irregularly sampled time-series benchmarks",
  "irregularly sampled time-series data",
  "irregularly-sampled time series",
  "irrelevant regions",
  "k",
  "k proj",
  "k(h)",
  "keys",
  "labels",
  "language models",
  "language understanding systems",
  "large steady-state gradients",
  "large-scale tasks",
  "late maintenance prediction",
  "latent MLP layers",
  "lateral shifts",
  "learnable parameters",
  "learnable time constants",
  "learning",
  "left camera stream",
  "left context",
  "length 784",
  "length n",
  "linear first-order ODE",
  "linearly",
  "localization of transient characteristics",
  "localized prognostics",
  "log(E)",
  "logits",
  "long or event-based sequences",
  "long-term dependencies",
  "long-term memory",
  "long-term memory units",
  "low-resource tasks",
  "lying",
  "lying down",
  "mTAN",
  "mean",
  "meaningful information",
  "mechanical fault diagnosis",
  "memory intensity",
  "mmRNNs",
  "model",
  "model size",
  "modes",
  "mother wavelet",
  "move left",
  "move right",
  "multi-connection case M>1",
  "multi-head",
  "multi-layer networks",
  "multiple sensors",
  "multiplicative factor S",
  "n",
  "natural wiring",
  "network",
  "network input",
  "neural network architecture",
  "neural networks",
  "neurons",
  "no-act",
  "noisy time-series data",
  "non-uniform time intervals",
  "nonlinear activations",
  "nonlinear functions",
  "nonlinear, interlinked gates",
  "number of layers",
  "number of units",
  "numerical ODE solvers",
  "numerical errors",
  "omega_tau(h)",
  "on all fours",
  "one side of the road",
  "one unit of time",
  "one-dimensional time series",
  "operating frequency",
  "operational condition of the bearing",
  "optimization",
  "other models",
  "outer race",
  "outer–ball",
  "output gate",
  "output motor neurons",
  "output projection Wo",
  "overestimations",
  "overrepresented classes",
  "parallel scaled dot-product attention mechanisms",
  "parameterizations",
  "parameters",
  "participant's activity",
  "participant's recordings",
  "participants' arms",
  "participants' feet",
  "patch proposal network (ppn)",
  "per-connection equilibria",
  "per-sequence classification problem",
  "per-time-step classification problem",
  "personalized prognostics",
  "phase shifts",
  "physical activities",
  "physical interpretability",
  "physics-guided neural networks",
  "practical considerations",
  "pre-trained Transformer",
  "pre-trained bi-LM",
  "pre-trained model",
  "pre-trained representations",
  "predetermined wiring",
  "preprocessing",
  "preprocessing pipeline",
  "preprocessing steps",
  "previous state",
  "prior works",
  "pseudo-time integration interval",
  "q",
  "q proj",
  "q(h)",
  "quadratically",
  "queries",
  "query–key pair",
  "random flips",
  "randomized wiring",
  "real degradation scenarios",
  "reasonable saliency maps",
  "recordings",
  "rectangularized vectors",
  "recurrent connections",
  "recurrent network training",
  "regions of the input",
  "related works",
  "relaxation rate ωτ",
  "relevant regions",
  "reliable bearing health prediction",
  "representational capacity",
  "resource-constrained devices",
  "resource-limited settings",
  "resting potential",
  "results",
  "rhythmic schedule",
  "right camera stream",
  "right context",
  "road's horizon",
  "robust AI",
  "robust and explainable RUL estimation",
  "robust temporal modeling",
  "robustness",
  "rolling-element bearings",
  "routing transformers",
  "saliency map",
  "saliency maps",
  "sampling period",
  "scalar phi(h)",
  "scaling factor",
  "scene",
  "scoring function",
  "segmented",
  "self-attention",
  "sensitivity",
  "sensitivity analysis",
  "sensor readings",
  "sequence",
  "sequence identity",
  "sequence length T",
  "sequence models",
  "sequence prediction",
  "sequence-to-sequence transformer",
  "sequences",
  "sequential data",
  "sequential dependencies",
  "severe consequences",
  "shadow overlays",
  "short-term memory units",
  "sides",
  "sigma(z)",
  "sigmoid",
  "sigmoid hidden unit",
  "signal",
  "single faults",
  "single-connection case M=1",
  "single-hidden-layer feedforward neural network",
  "single-time-step prediction",
  "single-token input x",
  "sitting",
  "sitting down",
  "sitting on the ground",
  "sk",
  "slowing learning",
  "slowly evolving patterns",
  "small model size",
  "small scale tasks",
  "small ωτ",
  "smallest class",
  "softmax",
  "softmax function",
  "softplus",
  "solver steps",
  "sparse NCP gating mechanism",
  "sparse top-k pairs",
  "sparse, adaptive networks",
  "spatial degradation features",
  "spatial feature extraction",
  "speed of dynamics",
  "stacked layers",
  "standard deviation",
  "standard scaler",
  "standing up from lying",
  "standing up from sitting",
  "standing up from sitting on the ground",
  "state trajectory",
  "state-of-the-art methods",
  "state-of-the-art performance",
  "statistical features",
  "steady mode",
  "steady state",
  "steady-mode logit",
  "steady-state contribution",
  "steady-state solution",
  "stop",
  "symbolic ODE system",
  "synthetic trajectories",
  "system",
  "t",
  "ta",
  "tagging task",
  "target complexity",
  "task",
  "task-specific model architecture",
  "task-specific models",
  "tb",
  "te^(-ωτ t)",
  "temperature readings",
  "temporal dimension",
  "temporal dynamics",
  "temporal filtering",
  "testing",
  "three camera streams",
  "three different load conditions",
  "three different operating settings",
  "threshold",
  "time dimension",
  "time gate",
  "time-aware attention mechanism",
  "time-based attention",
  "time-frequency representation (TFR)",
  "time-series data",
  "time-series sequences",
  "time-step contributions",
  "tn",
  "token representations",
  "token-level classifier",
  "top four hidden layers",
  "traditional scaled dot-product attention",
  "training",
  "training data",
  "training procedure",
  "trajectory",
  "transfer learning",
  "transient term",
  "translations",
  "tw",
  "unet (Ensemble)",
  "unexpected failures",
  "universal approximator",
  "unsupervised pre-training",
  "update gate",
  "user-defined NCPs configurations",
  "v",
  "v proj",
  "v(h)",
  "vanishing gradients",
  "vanishing or exploding gradients",
  "variants",
  "vehicle",
  "vertical axes",
  "visual perceptual autonomy",
  "w(h)",
  "walking",
  "weights",
  "weights fj",
  "well-defined interval",
  "windowed physical constraints",
  "windowed signal",
  "windowing technique (w)",
  "wiring",
  "y",
  "µ",
  "Γ iw(a, b)",
  "δt",
  "δt=1.0",
  "σ",
  "ψ",
  "ψ*",
  "ωτ",
  "ϕ",
  "ϕj"
]