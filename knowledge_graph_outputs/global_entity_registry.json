[
  "Add & Norm layer",
  "Attention map",
  "Attention mechanism",
  "Attention weights",
  "Concatenation",
  "Decoder",
  "Encoder",
  "Encoder attention head",
  "Feed Forward Network",
  "Input Embedding",
  "K (key)",
  "Key (K)",
  "Linear layer",
  "Masked Multi-Head Attention",
  "Masking layer",
  "Matrix Multiplication",
  "Multi-Head Attention",
  "Multi-head attention",
  "Natural Language Processing",
  "Output Embedding",
  "Output Probabilities",
  "Positional Encoding",
  "Q (query)",
  "Query (Q)",
  "Scale operation",
  "Scaled Dot-Product Attention",
  "Self-attention mechanism",
  "Sentence structure",
  "Sentence visualization",
  "SoftMax function",
  "Softmax layer",
  "Token relationships",
  "Tokens",
  "Transformer architecture",
  "V (value)",
  "Value (V)",
  "Word embeddings",
  "anaphora resolution",
  "attention heads",
  "attention patterns",
  "attention weights",
  "attention_is_all_you_need_raw_with_image_ids_with_captions",
  "attention_is_all_you_need_raw_with_image_ids_with_captions::section::3 Model Architecture",
  "attention_is_all_you_need_raw_with_image_ids_with_captions::section::3.2 Attention",
  "attention_is_all_you_need_raw_with_image_ids_with_captions::section::References",
  "img_attention_is_all_you_need_12_0",
  "img_attention_is_all_you_need_13_0",
  "img_attention_is_all_you_need_14_0",
  "img_attention_is_all_you_need_2_0",
  "img_attention_is_all_you_need_3_0",
  "img_attention_is_all_you_need_3_1",
  "neural network layers",
  "word tokens"
]