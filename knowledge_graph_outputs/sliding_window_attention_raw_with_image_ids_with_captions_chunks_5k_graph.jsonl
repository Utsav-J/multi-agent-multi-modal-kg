{"nodes": [{"id": "Yijiong Yu", "type": "Person", "metadata": []}, {"id": "Oregon State University", "type": "Organization", "metadata": []}, {"id": "Jiale Liu", "type": "Person", "metadata": []}, {"id": "Penn State University", "type": "Organization", "metadata": []}, {"id": "Qingyun Wu", "type": "Person", "metadata": []}, {"id": "Huazheng Wang", "type": "Person", "metadata": []}, {"id": "Ji Pei", "type": "Person", "metadata": []}, {"id": "DeepSolution", "type": "Organization", "metadata": []}, {"id": "self-attention mechanism", "type": "Concept", "metadata": []}, {"id": "Transformer", "type": "Model", "metadata": []}, {"id": "Large Language Models", "type": "Model", "metadata": []}, {"id": "long-context inference", "type": "Task", "metadata": []}, {"id": "Sliding Window Attention", "type": "Model", "metadata": []}, {"id": "Full Attention", "type": "Concept", "metadata": []}, {"id": "training\u2013inference mismatch", "type": "Concept", "metadata": []}, {"id": "Sliding Window Attention Adaptation", "type": "Algorithm", "metadata": []}, {"id": "Full Attention Decode", "type": "Algorithm", "metadata": []}, {"id": "Keep First k Tokens", "type": "Algorithm", "metadata": []}, {"id": "Interleaving FA/SWA layers", "type": "Algorithm", "metadata": []}, {"id": "Chain-of-Thought", "type": "Concept", "metadata": []}, {"id": "Fine-tuning with SWA", "type": "Algorithm", "metadata": []}, {"id": "long-context performance degradation", "type": "Concept", "metadata": []}, {"id": "long context processing", "type": "Task", "metadata": []}, {"id": "Vaswani et al., 2017", "type": "Publication", "metadata": []}, {"id": "Qwen3", "type": "Model", "metadata": []}, {"id": "Team, 2025b", "type": "Publication", "metadata": []}, {"id": "streaming attention", "type": "Algorithm", "metadata": []}, {"id": "Xiao et al., 2024", "type": "Publication", "metadata": []}, {"id": "sink tokens", "type": "Concept", "metadata": []}, {"id": "Xiao, 2025", "type": "Publication", "metadata": []}, {"id": "FA-pretrained models", "type": "Model", "metadata": []}, {"id": "Team, 2024a", "type": "Publication", "metadata": []}, {"id": "Zhang et al., 2024", "type": "Publication", "metadata": []}, {"id": "LLM technique", "type": "Concept", "metadata": []}, {"id": "Llama3.1", "type": "Model", "metadata": []}, {"id": "Team, 2024b", "type": "Publication", "metadata": []}, {"id": "FlashAttention", "type": "Algorithm", "metadata": []}, {"id": "Dao, 2024", "type": "Publication", "metadata": []}, {"id": "vLLM", "type": "Model", "metadata": []}, {"id": "Kwon et al., 2023", "type": "Publication", "metadata": []}, {"id": "sparse attention", "type": "Concept", "metadata": []}, {"id": "linear attention", "type": "Concept", "metadata": []}, {"id": "Longformer", "type": "Model", "metadata": []}, {"id": "Beltagy et al., 2020", "type": "Publication", "metadata": []}, {"id": "BigBird", "type": "Model", "metadata": []}, {"id": "Zaheer et al., 2020", "type": "Publication", "metadata": []}, {"id": "RATTENTION", "type": "Model", "metadata": []}, {"id": "Wang et al., 2025", "type": "Publication", "metadata": []}, {"id": "Gemma2", "type": "Model", "metadata": []}, {"id": "Sliding Window Attention Training", "type": "Algorithm", "metadata": []}, {"id": "Fu et al., 2025b", "type": "Publication", "metadata": []}, {"id": "Deepseek-sparse-attention", "type": "Model", "metadata": []}, {"id": "Yuan et al., 2025", "type": "Publication", "metadata": []}, {"id": "DeepSeek-AI, 2025b", "type": "Publication", "metadata": []}, {"id": "LightTransfer", "type": "Algorithm", "metadata": []}, {"id": "RNN-like linear attention transformers", "type": "Model", "metadata": []}, {"id": "Katharopoulos et al., 2020", "type": "Publication", "metadata": []}, {"id": "Peng et al., 2023", "type": "Publication", "metadata": []}, {"id": "Sun et al., 2023", "type": "Publication", "metadata": []}, {"id": "structured state-space models", "type": "Model", "metadata": []}, {"id": "Mamba", "type": "Model", "metadata": []}, {"id": "Gu and Dao, 2023", "type": "Publication", "metadata": []}, {"id": "Jamba", "type": "Model", "metadata": []}, {"id": "Nemotron-Flash", "type": "Model", "metadata": []}, {"id": "Lieber et al., 2024", "type": "Publication", "metadata": []}, {"id": "Linsong Chu et al., 2024", "type": "Publication", "metadata": []}, {"id": "Team et al., 2025", "type": "Publication", "metadata": []}, {"id": "Fu et al., 2025a", "type": "Publication", "metadata": []}, {"id": "prefilling stage", "type": "Concept", "metadata": []}, {"id": "decoding stage", "type": "Concept", "metadata": []}, {"id": "human reading comprehension", "type": "Concept", "metadata": []}, {"id": "Flash-Attention-2", "type": "Algorithm", "metadata": []}, {"id": "layers", "type": "Concept", "metadata": []}, {"id": "Gemma-3", "type": "Model", "metadata": []}, {"id": "Team, 2025a", "type": "Publication", "metadata": []}, {"id": "statistical features", "type": "Concept", "metadata": []}, {"id": "lazy layers", "type": "Concept", "metadata": []}, {"id": "model accuracy", "type": "Concept", "metadata": []}, {"id": "reasoning", "type": "Concept", "metadata": []}, {"id": "Wei et al., 2022", "type": "Publication", "metadata": []}, {"id": "DeepSeek-R1", "type": "Model", "metadata": []}, {"id": "DeepSeek-AI, 2025a", "type": "Publication", "metadata": []}, {"id": "Qwen3-4B-Thinking", "type": "Model", "metadata": []}, {"id": "Qwen3-4B-Instruct", "type": "Model", "metadata": []}, {"id": "self-distillation", "type": "Concept", "metadata": []}, {"id": "Yang et al., 2024", "type": "Publication", "metadata": []}, {"id": "GPT-5-Mini", "type": "Model", "metadata": []}, {"id": "OpenAI, 2025", "type": "Publication", "metadata": []}, {"id": "Qwen3-30B-A3B-Thinking", "type": "Model", "metadata": []}, {"id": "Qwen3-30B-A3B-Instruct", "type": "Model", "metadata": []}, {"id": "Llama3.1-8B-Instruct", "type": "Model", "metadata": []}, {"id": "Touvron et al., 2023", "type": "Publication", "metadata": []}, {"id": "HuggingFace Transformers", "type": "Model", "metadata": []}, {"id": "LongMemEval", "type": "Benchmark", "metadata": []}, {"id": "Wu et al., 2024", "type": "Publication", "metadata": []}, {"id": "agent memory system evaluation", "type": "Task", "metadata": []}, {"id": "LongMemEval_24k", "type": "Benchmark", "metadata": []}, {"id": "LongBench-V2", "type": "Benchmark", "metadata": []}, {"id": "Bai et al., 2024b", "type": "Publication", "metadata": []}, {"id": "deep reasoning tasks", "type": "Task", "metadata": []}, {"id": "real-world tasks", "type": "Task", "metadata": []}, {"id": "LongAlign", "type": "Dataset", "metadata": []}, {"id": "Bai et al., 2024a", "type": "Publication", "metadata": []}, {"id": "Fusang-v1-long", "type": "Dataset", "metadata": []}, {"id": "Pan, 2024", "type": "Publication", "metadata": []}, {"id": "LoRA", "type": "Algorithm", "metadata": []}, {"id": "Hu et al., 2022", "type": "Publication", "metadata": []}, {"id": "long-context tasks", "type": "Task", "metadata": []}], "relationships": [{"source": {"id": "Yijiong Yu", "type": "Person", "metadata": []}, "target": {"id": "Oregon State University", "type": "Organization", "metadata": []}, "type": "AFFILIATED_WITH"}, {"source": {"id": "Jiale Liu", "type": "Person", "metadata": []}, "target": {"id": "Penn State University", "type": "Organization", "metadata": []}, "type": "AFFILIATED_WITH"}, {"source": {"id": "Qingyun Wu", "type": "Person", "metadata": []}, "target": {"id": "Penn State University", "type": "Organization", "metadata": []}, "type": "AFFILIATED_WITH"}, {"source": {"id": "Huazheng Wang", "type": "Person", "metadata": []}, "target": {"id": "Oregon State University", "type": "Organization", "metadata": []}, "type": "AFFILIATED_WITH"}, {"source": {"id": "Ji Pei", "type": "Person", "metadata": []}, "target": {"id": "DeepSolution", "type": "Organization", "metadata": []}, "type": "AFFILIATED_WITH"}, {"source": {"id": "self-attention mechanism", "type": "Concept", "metadata": []}, "target": {"id": "Transformer", "type": "Model", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "Sliding Window Attention", "type": "Model", "metadata": []}, "target": {"id": "long-context inference", "type": "Task", "metadata": []}, "type": "ADDRESSES"}, {"source": {"id": "Sliding Window Attention Adaptation", "type": "Algorithm", "metadata": []}, "target": {"id": "long-context performance degradation", "type": "Concept", "metadata": []}, "type": "ADDRESSES"}, {"source": {"id": "Sliding Window Attention Adaptation", "type": "Algorithm", "metadata": []}, "target": {"id": "training\u2013inference mismatch", "type": "Concept", "metadata": []}, "type": "MITIGATES"}, {"source": {"id": "Sliding Window Attention Adaptation", "type": "Algorithm", "metadata": []}, "target": {"id": "Full Attention Decode", "type": "Algorithm", "metadata": []}, "type": "SUPPORTS"}, {"source": {"id": "Sliding Window Attention Adaptation", "type": "Algorithm", "metadata": []}, "target": {"id": "Keep First k Tokens", "type": "Algorithm", "metadata": []}, "type": "SUPPORTS"}, {"source": {"id": "Sliding Window Attention Adaptation", "type": "Algorithm", "metadata": []}, "target": {"id": "Interleaving FA/SWA layers", "type": "Algorithm", "metadata": []}, "type": "SUPPORTS"}, {"source": {"id": "Sliding Window Attention Adaptation", "type": "Algorithm", "metadata": []}, "target": {"id": "Chain-of-Thought", "type": "Concept", "metadata": []}, "type": "SUPPORTS"}, {"source": {"id": "Sliding Window Attention Adaptation", "type": "Algorithm", "metadata": []}, "target": {"id": "Fine-tuning with SWA", "type": "Algorithm", "metadata": []}, "type": "SUPPORTS"}, {"source": {"id": "Large Language Models", "type": "Model", "metadata": []}, "target": {"id": "Vaswani et al., 2017", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "self-attention mechanism", "type": "Concept", "metadata": []}, "target": {"id": "long context processing", "type": "Task", "metadata": []}, "type": "ADDRESSES"}, {"source": {"id": "Sliding Window Attention", "type": "Model", "metadata": []}, "target": {"id": "sparse attention", "type": "Concept", "metadata": []}, "type": "BUILDS_ON"}, {"source": {"id": "Qwen3", "type": "Model", "metadata": []}, "target": {"id": "Team, 2025b", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "streaming attention", "type": "Algorithm", "metadata": []}, "target": {"id": "Xiao et al., 2024", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "streaming attention", "type": "Algorithm", "metadata": []}, "target": {"id": "long-context performance degradation", "type": "Concept", "metadata": []}, "type": "ADDRESSES"}, {"source": {"id": "streaming attention", "type": "Algorithm", "metadata": []}, "target": {"id": "sink tokens", "type": "Concept", "metadata": []}, "type": "PRESERVES"}, {"source": {"id": "long-context performance degradation", "type": "Concept", "metadata": []}, "target": {"id": "Xiao, 2025", "type": "Publication", "metadata": []}, "type": "EXPLAINED_BY"}, {"source": {"id": "Sliding Window Attention Adaptation", "type": "Algorithm", "metadata": []}, "target": {"id": "FA-pretrained models", "type": "Model", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "FA-pretrained models", "type": "Model", "metadata": []}, "target": {"id": "Sliding Window Attention", "type": "Model", "metadata": []}, "type": "ADAPTED_TO"}, {"source": {"id": "Keep First k Tokens", "type": "Algorithm", "metadata": []}, "target": {"id": "Xiao et al., 2024", "type": "Publication", "metadata": []}, "type": "BUILDS_ON"}, {"source": {"id": "Interleaving FA/SWA layers", "type": "Algorithm", "metadata": []}, "target": {"id": "Xiao et al., 2024", "type": "Publication", "metadata": []}, "type": "BUILDS_ON"}, {"source": {"id": "Interleaving FA/SWA layers", "type": "Algorithm", "metadata": []}, "target": {"id": "Team, 2024a", "type": "Publication", "metadata": []}, "type": "BUILDS_ON"}, {"source": {"id": "Interleaving FA/SWA layers", "type": "Algorithm", "metadata": []}, "target": {"id": "Zhang et al., 2024", "type": "Publication", "metadata": []}, "type": "BUILDS_ON"}, {"source": {"id": "Chain-of-Thought", "type": "Concept", "metadata": []}, "target": {"id": "LLM technique", "type": "Concept", "metadata": []}, "type": "IS_A"}, {"source": {"id": "Fine-tuning with SWA", "type": "Algorithm", "metadata": []}, "target": {"id": "LLM technique", "type": "Concept", "metadata": []}, "type": "IS_A"}, {"source": {"id": "Sliding Window Attention Adaptation", "type": "Algorithm", "metadata": []}, "target": {"id": "Qwen3", "type": "Model", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "Sliding Window Attention Adaptation", "type": "Algorithm", "metadata": []}, "target": {"id": "Llama3.1", "type": "Model", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "FlashAttention", "type": "Algorithm", "metadata": []}, "target": {"id": "Dao, 2024", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "vLLM", "type": "Model", "metadata": []}, "target": {"id": "Kwon et al., 2023", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "self-attention mechanism", "type": "Concept", "metadata": []}, "target": {"id": "Transformer", "type": "Model", "metadata": []}, "type": "BUILDS_ON"}, {"source": {"id": "self-attention mechanism", "type": "Concept", "metadata": []}, "target": {"id": "Vaswani et al., 2017", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Longformer", "type": "Model", "metadata": []}, "target": {"id": "sparse attention", "type": "Concept", "metadata": []}, "type": "BUILDS_ON"}, {"source": {"id": "Longformer", "type": "Model", "metadata": []}, "target": {"id": "Beltagy et al., 2020", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "BigBird", "type": "Model", "metadata": []}, "target": {"id": "sparse attention", "type": "Concept", "metadata": []}, "type": "BUILDS_ON"}, {"source": {"id": "BigBird", "type": "Model", "metadata": []}, "target": {"id": "Zaheer et al., 2020", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "RATTENTION", "type": "Model", "metadata": []}, "target": {"id": "sparse attention", "type": "Concept", "metadata": []}, "type": "BUILDS_ON"}, {"source": {"id": "RATTENTION", "type": "Model", "metadata": []}, "target": {"id": "Wang et al., 2025", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Gemma2", "type": "Model", "metadata": []}, "target": {"id": "Sliding Window Attention", "type": "Model", "metadata": []}, "type": "ADOPTS"}, {"source": {"id": "Gemma2", "type": "Model", "metadata": []}, "target": {"id": "Team, 2024a", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Sliding Window Attention Training", "type": "Algorithm", "metadata": []}, "target": {"id": "Fu et al., 2025b", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Deepseek-sparse-attention", "type": "Model", "metadata": []}, "target": {"id": "Yuan et al., 2025", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Deepseek-sparse-attention", "type": "Model", "metadata": []}, "target": {"id": "DeepSeek-AI, 2025b", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "LightTransfer", "type": "Algorithm", "metadata": []}, "target": {"id": "Zhang et al., 2024", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "LightTransfer", "type": "Algorithm", "metadata": []}, "target": {"id": "Large Language Models", "type": "Model", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "LightTransfer", "type": "Algorithm", "metadata": []}, "target": {"id": "Sliding Window Attention", "type": "Model", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "RNN-like linear attention transformers", "type": "Model", "metadata": []}, "target": {"id": "Katharopoulos et al., 2020", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "RNN-like linear attention transformers", "type": "Model", "metadata": []}, "target": {"id": "Peng et al., 2023", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "RNN-like linear attention transformers", "type": "Model", "metadata": []}, "target": {"id": "Sun et al., 2023", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Mamba", "type": "Model", "metadata": []}, "target": {"id": "structured state-space models", "type": "Model", "metadata": []}, "type": "IS_A"}, {"source": {"id": "Mamba", "type": "Model", "metadata": []}, "target": {"id": "Gu and Dao, 2023", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Jamba", "type": "Model", "metadata": []}, "target": {"id": "linear attention", "type": "Concept", "metadata": []}, "type": "BUILDS_ON"}, {"source": {"id": "Jamba", "type": "Model", "metadata": []}, "target": {"id": "Transformer", "type": "Model", "metadata": []}, "type": "BUILDS_ON"}, {"source": {"id": "Jamba", "type": "Model", "metadata": []}, "target": {"id": "Lieber et al., 2024", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Jamba", "type": "Model", "metadata": []}, "target": {"id": "Linsong Chu et al., 2024", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Jamba", "type": "Model", "metadata": []}, "target": {"id": "Team et al., 2025", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Jamba", "type": "Model", "metadata": []}, "target": {"id": "Fu et al., 2025a", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Nemotron-Flash", "type": "Model", "metadata": []}, "target": {"id": "linear attention", "type": "Concept", "metadata": []}, "type": "BUILDS_ON"}, {"source": {"id": "Nemotron-Flash", "type": "Model", "metadata": []}, "target": {"id": "Transformer", "type": "Model", "metadata": []}, "type": "BUILDS_ON"}, {"source": {"id": "Nemotron-Flash", "type": "Model", "metadata": []}, "target": {"id": "Lieber et al., 2024", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Nemotron-Flash", "type": "Model", "metadata": []}, "target": {"id": "Linsong Chu et al., 2024", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Nemotron-Flash", "type": "Model", "metadata": []}, "target": {"id": "Team et al., 2025", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Nemotron-Flash", "type": "Model", "metadata": []}, "target": {"id": "Fu et al., 2025a", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Full Attention Decode", "type": "Algorithm", "metadata": []}, "target": {"id": "prefilling stage", "type": "Concept", "metadata": []}, "type": "APPLIES"}, {"source": {"id": "Full Attention Decode", "type": "Algorithm", "metadata": []}, "target": {"id": "decoding stage", "type": "Concept", "metadata": []}, "type": "APPLIES"}, {"source": {"id": "Full Attention Decode", "type": "Algorithm", "metadata": []}, "target": {"id": "human reading comprehension", "type": "Concept", "metadata": []}, "type": "INSPIRED_BY"}, {"source": {"id": "Keep First k Tokens", "type": "Algorithm", "metadata": []}, "target": {"id": "Xiao et al., 2024", "type": "Publication", "metadata": []}, "type": "BUILDS_ON"}, {"source": {"id": "Keep First k Tokens", "type": "Algorithm", "metadata": []}, "target": {"id": "sink tokens", "type": "Concept", "metadata": []}, "type": "PRESERVES"}, {"source": {"id": "Keep First k Tokens", "type": "Algorithm", "metadata": []}, "target": {"id": "Flash-Attention-2", "type": "Algorithm", "metadata": []}, "type": "CUSTOMIZES"}, {"source": {"id": "Interleaving FA/SWA layers", "type": "Algorithm", "metadata": []}, "target": {"id": "Full Attention", "type": "Concept", "metadata": []}, "type": "APPLIES"}, {"source": {"id": "Interleaving FA/SWA layers", "type": "Algorithm", "metadata": []}, "target": {"id": "Sliding Window Attention", "type": "Model", "metadata": []}, "type": "APPLIES"}, {"source": {"id": "Gemma2", "type": "Model", "metadata": []}, "target": {"id": "Sliding Window Attention", "type": "Model", "metadata": []}, "type": "USES"}, {"source": {"id": "Gemma2", "type": "Model", "metadata": []}, "target": {"id": "layers", "type": "Concept", "metadata": []}, "type": "USES"}, {"source": {"id": "Gemma2", "type": "Model", "metadata": []}, "target": {"id": "Team, 2024a", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Gemma-3", "type": "Model", "metadata": []}, "target": {"id": "Sliding Window Attention", "type": "Model", "metadata": []}, "type": "USES"}, {"source": {"id": "Gemma-3", "type": "Model", "metadata": []}, "target": {"id": "layers", "type": "Concept", "metadata": []}, "type": "USES"}, {"source": {"id": "Gemma-3", "type": "Model", "metadata": []}, "target": {"id": "Team, 2025a", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "LightTransfer", "type": "Algorithm", "metadata": []}, "target": {"id": "statistical features", "type": "Concept", "metadata": []}, "type": "USES"}, {"source": {"id": "LightTransfer", "type": "Algorithm", "metadata": []}, "target": {"id": "lazy layers", "type": "Concept", "metadata": []}, "type": "SELECTS"}, {"source": {"id": "Chain-of-Thought", "type": "Concept", "metadata": []}, "target": {"id": "model accuracy", "type": "Concept", "metadata": []}, "type": "IMPROVES"}, {"source": {"id": "Chain-of-Thought", "type": "Concept", "metadata": []}, "target": {"id": "reasoning", "type": "Concept", "metadata": []}, "type": "IMPROVES_VIA"}, {"source": {"id": "Chain-of-Thought", "type": "Concept", "metadata": []}, "target": {"id": "Wei et al., 2022", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "DeepSeek-R1", "type": "Model", "metadata": []}, "target": {"id": "DeepSeek-AI, 2025a", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Qwen3-4B-Thinking", "type": "Model", "metadata": []}, "target": {"id": "DeepSeek-R1", "type": "Model", "metadata": []}, "type": "COMPARED_WITH"}, {"source": {"id": "Qwen3-4B-Thinking", "type": "Model", "metadata": []}, "target": {"id": "Qwen3-4B-Instruct", "type": "Model", "metadata": []}, "type": "COMPARED_WITH"}, {"source": {"id": "Fine-tuning with SWA", "type": "Algorithm", "metadata": []}, "target": {"id": "training\u2013inference mismatch", "type": "Concept", "metadata": []}, "type": "MITIGATES"}, {"source": {"id": "self-distillation", "type": "Concept", "metadata": []}, "target": {"id": "Yang et al., 2024", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Fine-tuning with SWA", "type": "Algorithm", "metadata": []}, "target": {"id": "self-distillation", "type": "Concept", "metadata": []}, "type": "INSPIRED_BY"}, {"source": {"id": "GPT-5-Mini", "type": "Model", "metadata": []}, "target": {"id": "OpenAI, 2025", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Qwen3-4B-Thinking", "type": "Model", "metadata": []}, "target": {"id": "Team, 2025b", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Qwen3-4B-Instruct", "type": "Model", "metadata": []}, "target": {"id": "Team, 2025b", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Qwen3-30B-A3B-Thinking", "type": "Model", "metadata": []}, "target": {"id": "Team, 2025b", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Qwen3-30B-A3B-Instruct", "type": "Model", "metadata": []}, "target": {"id": "Team, 2025b", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Llama3.1-8B-Instruct", "type": "Model", "metadata": []}, "target": {"id": "Touvron et al., 2023", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "LongMemEval", "type": "Benchmark", "metadata": []}, "target": {"id": "Wu et al., 2024", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "LongMemEval", "type": "Benchmark", "metadata": []}, "target": {"id": "agent memory system evaluation", "type": "Task", "metadata": []}, "type": "DESIGNED_FOR"}, {"source": {"id": "LongMemEval_24k", "type": "Benchmark", "metadata": []}, "target": {"id": "LongMemEval", "type": "Benchmark", "metadata": []}, "type": "EXTENDS"}, {"source": {"id": "LongBench-V2", "type": "Benchmark", "metadata": []}, "target": {"id": "Bai et al., 2024b", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "LongBench-V2", "type": "Benchmark", "metadata": []}, "target": {"id": "deep reasoning tasks", "type": "Task", "metadata": []}, "type": "REQUIRES"}, {"source": {"id": "LongBench-V2", "type": "Benchmark", "metadata": []}, "target": {"id": "real-world tasks", "type": "Task", "metadata": []}, "type": "REQUIRES"}, {"source": {"id": "LongAlign", "type": "Dataset", "metadata": []}, "target": {"id": "Bai et al., 2024a", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Fusang-v1-long", "type": "Dataset", "metadata": []}, "target": {"id": "Pan, 2024", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Fusang-v1-long", "type": "Dataset", "metadata": []}, "target": {"id": "LongAlign", "type": "Dataset", "metadata": []}, "type": "EXTENDS"}, {"source": {"id": "LoRA", "type": "Algorithm", "metadata": []}, "target": {"id": "Hu et al., 2022", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Fine-tuning with SWA", "type": "Algorithm", "metadata": []}, "target": {"id": "LoRA", "type": "Algorithm", "metadata": []}, "type": "USES"}, {"source": {"id": "Fine-tuning with SWA", "type": "Algorithm", "metadata": []}, "target": {"id": "long-context tasks", "type": "Task", "metadata": []}, "type": "APPLIED_TO"}], "source": {"source_id": "sliding_window_attention_raw_with_image_ids_with_captions_chunks_5k.jsonl::sliding_window_attention_raw_with_image_ids_with_captions_chunks_5k_0", "source_type": "chunk", "metadata": [{"key": "chunk_file", "value": "sliding_window_attention_raw_with_image_ids_with_captions_chunks_5k.jsonl"}, {"key": "chunk_id", "value": "sliding_window_attention_raw_with_image_ids_with_captions_chunks_5k_0"}, {"key": "chunk_index", "value": "0"}]}}
{"nodes": [{"id": "LongBench-V2", "type": "Benchmark", "metadata": []}, {"id": "deep reasoning", "type": "Concept", "metadata": []}, {"id": "real-world tasks", "type": "Task", "metadata": []}, {"id": "LongAlign", "type": "Publication", "metadata": []}, {"id": "long-context fine-tuning", "type": "Task", "metadata": []}, {"id": "long-context tasks", "type": "Task", "metadata": []}, {"id": "Fusang-v1-long", "type": "Model", "metadata": []}, {"id": "Pan, 2024", "type": "Publication", "metadata": []}, {"id": "SWA-aware fine-tuning", "type": "Algorithm", "metadata": []}, {"id": "LoRA", "type": "Algorithm", "metadata": []}, {"id": "Hu et al., 2022", "type": "Publication", "metadata": []}, {"id": "Over-fitting", "type": "Concept", "metadata": []}, {"id": "Qwen3-4B", "type": "Model", "metadata": []}, {"id": "Qwen3-30B-A3B", "type": "Model", "metadata": []}, {"id": "LongMemEval_24k", "type": "Benchmark", "metadata": []}, {"id": "Qwen3-4B-Thinking", "type": "Model", "metadata": []}, {"id": "Qwen3-4B-Instruct", "type": "Model", "metadata": []}, {"id": "Sliding Window Attention Adaptation (SWAA)", "type": "Method", "metadata": []}, {"id": "Interleaving Layers", "type": "Method", "metadata": []}, {"id": "Keep First", "type": "Method", "metadata": []}, {"id": "FA Decode", "type": "Method", "metadata": []}, {"id": "Naive SWA", "type": "Method", "metadata": []}, {"id": "Full Attention", "type": "Model", "metadata": []}, {"id": "Chain of Thought (CoT) prompting", "type": "Concept", "metadata": []}, {"id": "global attention", "type": "Concept", "metadata": []}, {"id": "Sliding window size", "type": "Concept", "metadata": []}, {"id": "Llama3.1-8B-Instruct", "type": "Model", "metadata": []}, {"id": "layer functionalities", "type": "Concept", "metadata": []}, {"id": "Model Family", "type": "Concept", "metadata": []}, {"id": "model sizes", "type": "Concept", "metadata": []}, {"id": "model-specific layer selection strategies", "type": "Concept", "metadata": []}, {"id": "SWA", "type": "Model", "metadata": []}, {"id": "Inference", "type": "Task", "metadata": []}, {"id": "SFT", "type": "Task", "metadata": []}, {"id": "performance-efficiency trade-off", "type": "Concept", "metadata": []}, {"id": "vLLM", "type": "Model", "metadata": []}, {"id": "Kwon et al., 2023", "type": "Publication", "metadata": []}, {"id": "long-context QA tasks", "type": "Task", "metadata": []}, {"id": "Wu et al., 2024", "type": "Publication", "metadata": []}, {"id": "Qwen Family", "type": "Model", "metadata": []}, {"id": "Llama Family", "type": "Model", "metadata": []}, {"id": "computational overhead", "type": "Concept", "metadata": []}, {"id": "Model Performance", "type": "Concept", "metadata": []}, {"id": "Performance-Efficiency Balance", "type": "Concept", "metadata": []}, {"id": "reasoning trajectory", "type": "Concept", "metadata": []}, {"id": "Information Loss", "type": "Concept", "metadata": []}, {"id": "Reinforcement Learning", "type": "Concept", "metadata": []}, {"id": "GRPO", "type": "Algorithm", "metadata": []}, {"id": "Shao et al., 2024", "type": "Publication", "metadata": []}, {"id": "KV cache eviction", "type": "Mechanism", "metadata": []}, {"id": "memory usage", "type": "Concept", "metadata": []}, {"id": "sparse-attention models", "type": "Model", "metadata": []}, {"id": "Full-Attention Pretrained LLMs", "type": "Model", "metadata": []}, {"id": "Bai et al., 2024a", "type": "Publication", "metadata": []}, {"id": "Bai et al., 2023", "type": "Publication", "metadata": []}, {"id": "Longbench", "type": "Benchmark", "metadata": []}, {"id": "Bai et al., 2024b", "type": "Publication", "metadata": []}, {"id": "Beltagy et al., 2020", "type": "Publication", "metadata": []}, {"id": "Longformer", "type": "Model", "metadata": []}, {"id": "Dao, 2024", "type": "Publication", "metadata": []}, {"id": "Flashattention-2", "type": "Model", "metadata": []}, {"id": "DeepSeek-AI, 2025a", "type": "Publication", "metadata": []}, {"id": "Deepseek-r1", "type": "Model", "metadata": []}, {"id": "DeepSeek-AI, 2025b", "type": "Publication", "metadata": []}, {"id": "Deepseek-v3.2", "type": "Model", "metadata": []}], "relationships": [{"source": {"id": "LongBench-V2", "type": "Benchmark", "metadata": []}, "target": {"id": "deep reasoning", "type": "Concept", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "LongBench-V2", "type": "Benchmark", "metadata": []}, "target": {"id": "real-world tasks", "type": "Task", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "LongBench-V2", "type": "Benchmark", "metadata": []}, "target": {"id": "Bai et al., 2024b", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "LongAlign", "type": "Publication", "metadata": []}, "target": {"id": "long-context fine-tuning", "type": "Task", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "LongAlign", "type": "Publication", "metadata": []}, "target": {"id": "long-context tasks", "type": "Task", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "Fusang-v1-long", "type": "Model", "metadata": []}, "target": {"id": "Pan, 2024", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "SWA-aware fine-tuning", "type": "Algorithm", "metadata": []}, "target": {"id": "LoRA", "type": "Algorithm", "metadata": []}, "type": "BUILDS_ON"}, {"source": {"id": "LoRA", "type": "Algorithm", "metadata": []}, "target": {"id": "Hu et al., 2022", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "SWA-aware fine-tuning", "type": "Algorithm", "metadata": []}, "target": {"id": "Over-fitting", "type": "Concept", "metadata": []}, "type": "MITIGATES"}, {"source": {"id": "LongMemEval_24k", "type": "Benchmark", "metadata": []}, "target": {"id": "Qwen3-4B-Thinking", "type": "Model", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "LongMemEval_24k", "type": "Benchmark", "metadata": []}, "target": {"id": "Qwen3-4B-Instruct", "type": "Model", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "Naive SWA", "type": "Method", "metadata": []}, "target": {"id": "Full Attention", "type": "Model", "metadata": []}, "type": "COMPARED_WITH"}, {"source": {"id": "Keep First", "type": "Method", "metadata": []}, "target": {"id": "Naive SWA", "type": "Method", "metadata": []}, "type": "EXTENDS"}, {"source": {"id": "FA Decode", "type": "Method", "metadata": []}, "target": {"id": "Naive SWA", "type": "Method", "metadata": []}, "type": "EXTENDS"}, {"source": {"id": "Interleaving Layers", "type": "Method", "metadata": []}, "target": {"id": "Naive SWA", "type": "Method", "metadata": []}, "type": "EXTENDS"}, {"source": {"id": "Chain of Thought (CoT) prompting", "type": "Concept", "metadata": []}, "target": {"id": "FA Decode", "type": "Method", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "FA Decode", "type": "Method", "metadata": []}, "target": {"id": "global attention", "type": "Concept", "metadata": []}, "type": "ADDRESSES"}, {"source": {"id": "Chain of Thought (CoT) prompting", "type": "Concept", "metadata": []}, "target": {"id": "SWA", "type": "Model", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "Llama3.1-8B-Instruct", "type": "Model", "metadata": []}, "target": {"id": "Qwen3-4B", "type": "Model", "metadata": []}, "type": "COMPARED_WITH"}, {"source": {"id": "Llama3.1-8B-Instruct", "type": "Model", "metadata": []}, "target": {"id": "Qwen3-30B-A3B", "type": "Model", "metadata": []}, "type": "COMPARED_WITH"}, {"source": {"id": "model-specific layer selection strategies", "type": "Concept", "metadata": []}, "target": {"id": "layer functionalities", "type": "Concept", "metadata": []}, "type": "ADDRESSES"}, {"source": {"id": "model-specific layer selection strategies", "type": "Concept", "metadata": []}, "target": {"id": "model sizes", "type": "Concept", "metadata": []}, "type": "ADDRESSES"}, {"source": {"id": "Sliding Window Attention Adaptation (SWAA)", "type": "Method", "metadata": []}, "target": {"id": "Full-Attention Pretrained LLMs", "type": "Model", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "SWA-aware fine-tuning", "type": "Algorithm", "metadata": []}, "target": {"id": "SFT", "type": "Task", "metadata": []}, "type": "BUILDS_ON"}, {"source": {"id": "vLLM", "type": "Model", "metadata": []}, "target": {"id": "Kwon et al., 2023", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "LongMemEval_24k", "type": "Benchmark", "metadata": []}, "target": {"id": "Wu et al., 2024", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Chain of Thought (CoT) prompting", "type": "Concept", "metadata": []}, "target": {"id": "Performance-Efficiency Balance", "type": "Concept", "metadata": []}, "type": "SUPPORTS"}, {"source": {"id": "Chain of Thought (CoT) prompting", "type": "Concept", "metadata": []}, "target": {"id": "Sliding Window Attention Adaptation (SWAA)", "type": "Method", "metadata": []}, "type": "SUPPORTS"}, {"source": {"id": "Sliding Window Attention Adaptation (SWAA)", "type": "Method", "metadata": []}, "target": {"id": "SWA", "type": "Model", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "Sliding Window Attention Adaptation (SWAA)", "type": "Method", "metadata": []}, "target": {"id": "computational overhead", "type": "Concept", "metadata": []}, "type": "ADDRESSES"}, {"source": {"id": "Sliding Window Attention Adaptation (SWAA)", "type": "Method", "metadata": []}, "target": {"id": "sparse-attention models", "type": "Model", "metadata": []}, "type": "COMPARED_WITH"}, {"source": {"id": "Sliding Window Attention Adaptation (SWAA)", "type": "Method", "metadata": []}, "target": {"id": "catastrophic degradation", "type": "Concept", "metadata": []}, "type": "MITIGATES"}, {"source": {"id": "GRPO", "type": "Algorithm", "metadata": []}, "target": {"id": "Reinforcement Learning", "type": "Concept", "metadata": []}, "type": "EXTENDS"}, {"source": {"id": "GRPO", "type": "Algorithm", "metadata": []}, "target": {"id": "Shao et al., 2024", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "GRPO", "type": "Algorithm", "metadata": []}, "target": {"id": "reasoning trajectory", "type": "Concept", "metadata": []}, "type": "ADDRESSES"}, {"source": {"id": "KV cache eviction", "type": "Mechanism", "metadata": []}, "target": {"id": "SWA", "type": "Model", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "KV cache eviction", "type": "Mechanism", "metadata": []}, "target": {"id": "memory usage", "type": "Concept", "metadata": []}, "type": "ADDRESSES"}, {"source": {"id": "Longbench", "type": "Benchmark", "metadata": []}, "target": {"id": "Bai et al., 2023", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Longformer", "type": "Model", "metadata": []}, "target": {"id": "Beltagy et al., 2020", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Flashattention-2", "type": "Model", "metadata": []}, "target": {"id": "Dao, 2024", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Deepseek-r1", "type": "Model", "metadata": []}, "target": {"id": "DeepSeek-AI, 2025a", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Deepseek-v3.2", "type": "Model", "metadata": []}, "target": {"id": "DeepSeek-AI, 2025b", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}], "source": {"source_id": "sliding_window_attention_raw_with_image_ids_with_captions_chunks_5k.jsonl::sliding_window_attention_raw_with_image_ids_with_captions_chunks_5k_1", "source_type": "chunk", "metadata": [{"key": "chunk_file", "value": "sliding_window_attention_raw_with_image_ids_with_captions_chunks_5k.jsonl"}, {"key": "chunk_id", "value": "sliding_window_attention_raw_with_image_ids_with_captions_chunks_5k_1"}, {"key": "chunk_index", "value": "1"}]}}
{"nodes": [{"id": "Longbench v2: Towards deeper understanding and reasoning on realistic long-context multitasks.", "type": "Publication", "metadata": []}, {"id": "Yushi Bai", "type": "Person", "metadata": []}, {"id": "Shangqing Tu", "type": "Person", "metadata": []}, {"id": "Jiajie Zhang", "type": "Person", "metadata": []}, {"id": "Hao Peng", "type": "Person", "metadata": []}, {"id": "Xiaozhi Wang", "type": "Person", "metadata": []}, {"id": "Xin Lv", "type": "Person", "metadata": []}, {"id": "Shulin Cao", "type": "Person", "metadata": []}, {"id": "Jiazheng Xu", "type": "Person", "metadata": []}, {"id": "Lei Hou", "type": "Person", "metadata": []}, {"id": "Yuxiao Dong", "type": "Person", "metadata": []}, {"id": "Jie Tang", "type": "Person", "metadata": []}, {"id": "Juanzi Li", "type": "Person", "metadata": []}, {"id": "Multimodal Understanding and Reasoning", "type": "Task", "metadata": []}, {"id": "Long-context tasks", "type": "Concept", "metadata": []}, {"id": "Longformer: The long-document transformer.", "type": "Publication", "metadata": []}, {"id": "Iz Beltagy", "type": "Person", "metadata": []}, {"id": "Matthew E. Peters", "type": "Person", "metadata": []}, {"id": "Arman Cohan", "type": "Person", "metadata": []}, {"id": "Longformer", "type": "Model", "metadata": []}, {"id": "Transformer", "type": "Model", "metadata": []}, {"id": "Flashattention-2: Faster attention with better parallelism and work partitioning.", "type": "Publication", "metadata": []}, {"id": "Tri Dao", "type": "Person", "metadata": []}, {"id": "Flashattention-2", "type": "Model", "metadata": []}, {"id": "Attention mechanism", "type": "Concept", "metadata": []}, {"id": "OpenReview.net", "type": "Organization", "metadata": []}, {"id": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.", "type": "Publication", "metadata": []}, {"id": "DeepSeek-AI", "type": "Organization", "metadata": []}, {"id": "Deepseek-r1", "type": "Model", "metadata": []}, {"id": "Reinforcement Learning", "type": "Algorithm", "metadata": []}, {"id": "Large Language Models", "type": "Concept", "metadata": []}, {"id": "Deepseek-v3.2: Pushing the frontier of open large language models.", "type": "Publication", "metadata": []}, {"id": "Deepseek-v3.2", "type": "Model", "metadata": []}, {"id": "Nemotron-flash: Towards latencyoptimal hybrid small language models.", "type": "Publication", "metadata": []}, {"id": "Yonggan Fu", "type": "Person", "metadata": []}, {"id": "Xin Dong", "type": "Person", "metadata": []}, {"id": "Shizhe Diao", "type": "Person", "metadata": []}, {"id": "Hanrong Ye", "type": "Person", "metadata": []}, {"id": "Wonmin Byeon", "type": "Person", "metadata": []}, {"id": "Yashaswi Karnati", "type": "Person", "metadata": []}, {"id": "Lucas Liebenwein", "type": "Person", "metadata": []}, {"id": "Maksim Khadkevich", "type": "Person", "metadata": []}, {"id": "Alexander Keller", "type": "Person", "metadata": []}, {"id": "Jan Kautz", "type": "Person", "metadata": []}, {"id": "Nemotron-Flash", "type": "Model", "metadata": []}, {"id": "Sliding window attention training for efficient large language models.", "type": "Publication", "metadata": []}, {"id": "Zichuan Fu", "type": "Person", "metadata": []}, {"id": "Wentao Song", "type": "Person", "metadata": []}, {"id": "Yejing Wang", "type": "Person", "metadata": []}, {"id": "Xian Wu", "type": "Person", "metadata": []}, {"id": "Yefeng Zheng", "type": "Person", "metadata": []}, {"id": "Yingying Zhang", "type": "Person", "metadata": []}, {"id": "Derong Xu", "type": "Person", "metadata": []}, {"id": "Xuetao Wei", "type": "Person", "metadata": []}, {"id": "Tong Xu", "type": "Person", "metadata": []}, {"id": "Xiangyu Zhao", "type": "Person", "metadata": []}, {"id": "Sliding Window Attention Training", "type": "Algorithm", "metadata": []}, {"id": "Mamba: Linear-time sequence modeling with selective state spaces.", "type": "Publication", "metadata": []}, {"id": "Albert Gu", "type": "Person", "metadata": []}, {"id": "Mamba", "type": "Model", "metadata": []}, {"id": "Sequence Modeling", "type": "Task", "metadata": []}, {"id": "Measuring massive multitask language understanding.", "type": "Publication", "metadata": []}, {"id": "Dan Hendrycks", "type": "Person", "metadata": []}, {"id": "Collin Burns", "type": "Person", "metadata": []}, {"id": "Steven Basart", "type": "Person", "metadata": []}, {"id": "Andy Zou", "type": "Person", "metadata": []}, {"id": "Mantas Mazeika", "type": "Person", "metadata": []}, {"id": "Dawn Song", "type": "Person", "metadata": []}, {"id": "Jacob Steinhardt", "type": "Person", "metadata": []}, {"id": "MMLU", "type": "Task", "metadata": []}, {"id": "Ruler: What\u2019s the real context size of your long-context language models?", "type": "Publication", "metadata": []}, {"id": "Cheng-Ping Hsieh", "type": "Person", "metadata": []}, {"id": "Simeng Sun", "type": "Person", "metadata": []}, {"id": "Samuel Kriman", "type": "Person", "metadata": []}, {"id": "Shantanu Acharya", "type": "Person", "metadata": []}, {"id": "Dima Rekesh", "type": "Person", "metadata": []}, {"id": "Fei Jia", "type": "Person", "metadata": []}, {"id": "Yang Zhang", "type": "Person", "metadata": []}, {"id": "Boris Ginsburg", "type": "Person", "metadata": []}, {"id": "Ruler", "type": "Model", "metadata": []}, {"id": "Lora: Low-rank adaptation of large language models.", "type": "Publication", "metadata": []}, {"id": "Edward J. Hu", "type": "Person", "metadata": []}, {"id": "Yelong Shen", "type": "Person", "metadata": []}, {"id": "Phillip Wallis", "type": "Person", "metadata": []}, {"id": "Zeyuan Allen-Zhu", "type": "Person", "metadata": []}, {"id": "Yuanzhi Li", "type": "Person", "metadata": []}, {"id": "Shean Wang", "type": "Person", "metadata": []}, {"id": "Lu Wang", "type": "Person", "metadata": []}, {"id": "Weizhu Chen", "type": "Person", "metadata": []}, {"id": "LoRA", "type": "Algorithm", "metadata": []}, {"id": "Transformers are rnns: Fast autoregressive transformers with linear attention.", "type": "Publication", "metadata": []}, {"id": "Angelos Katharopoulos", "type": "Person", "metadata": []}, {"id": "Apoorv Vyas", "type": "Person", "metadata": []}, {"id": "Nikolaos Pappas", "type": "Person", "metadata": []}, {"id": "Fran\u00e7ois Fleuret", "type": "Person", "metadata": []}, {"id": "RNN", "type": "Model", "metadata": []}, {"id": "Linear attention", "type": "Concept", "metadata": []}, {"id": "PMLR", "type": "Organization", "metadata": []}, {"id": "Efficient memory management for large language model serving with pagedattention.", "type": "Publication", "metadata": []}, {"id": "Woosuk Kwon", "type": "Person", "metadata": []}, {"id": "Zhuohan Li", "type": "Person", "metadata": []}, {"id": "Siyuan Zhuang", "type": "Person", "metadata": []}, {"id": "Ying Sheng", "type": "Person", "metadata": []}, {"id": "Lianmin Zheng", "type": "Person", "metadata": []}, {"id": "Cody Hao Yu", "type": "Person", "metadata": []}, {"id": "Joseph E. Gonzalez", "type": "Person", "metadata": []}, {"id": "Hao Zhang", "type": "Person", "metadata": []}, {"id": "Ion Stoica", "type": "Person", "metadata": []}, {"id": "PagedAttention", "type": "Algorithm", "metadata": []}, {"id": "Efficient memory management", "type": "Concept", "metadata": []}, {"id": "Jamba: A hybrid transformer-mamba language model.", "type": "Publication", "metadata": []}, {"id": "Opher Lieber", "type": "Person", "metadata": []}, {"id": "Barak Lenz", "type": "Person", "metadata": []}, {"id": "Hofit Bata", "type": "Person", "metadata": []}, {"id": "Gal Cohen", "type": "Person", "metadata": []}, {"id": "Jhonathan Osin", "type": "Person", "metadata": []}, {"id": "Itay Dalmedigos", "type": "Person", "metadata": []}, {"id": "Erez Safahi", "type": "Person", "metadata": []}, {"id": "Shaked Meirom", "type": "Person", "metadata": []}, {"id": "Yonatan Belinkov", "type": "Person", "metadata": []}, {"id": "Shai ShalevShwartz", "type": "Person", "metadata": []}, {"id": "Omri Abend", "type": "Person", "metadata": []}, {"id": "Raz Alon", "type": "Person", "metadata": []}, {"id": "Tomer Asida", "type": "Person", "metadata": []}, {"id": "Amir Bergman", "type": "Person", "metadata": []}, {"id": "Roman Glozman", "type": "Person", "metadata": []}, {"id": "Michael Gokhman", "type": "Person", "metadata": []}, {"id": "Avashalom Manevich", "type": "Person", "metadata": []}, {"id": "Nir Ratner", "type": "Person", "metadata": []}, {"id": "Noam Rozen", "type": "Person", "metadata": []}, {"id": "Erez Shwartz", "type": "Person", "metadata": []}, {"id": "Mor Zusman", "type": "Person", "metadata": []}, {"id": "Yoav Shoham", "type": "Person", "metadata": []}, {"id": "Jamba", "type": "Model", "metadata": []}, {"id": "Bamba: Inference-efficient hybrid mamba2 model.", "type": "Publication", "metadata": []}, {"id": "Linsong Chu", "type": "Person", "metadata": []}, {"id": "Divya Kumari", "type": "Person", "metadata": []}, {"id": "Bamba", "type": "Model", "metadata": []}, {"id": "ChatGPT.", "type": "Publication", "metadata": []}, {"id": "OpenAI", "type": "Organization", "metadata": []}, {"id": "ChatGPT", "type": "Model", "metadata": []}, {"id": "Fusang-v1: A large curation of instruction-tuning datasets for better bilingual and long-range llms.", "type": "Publication", "metadata": []}, {"id": "Wenbo Pan", "type": "Person", "metadata": []}, {"id": "Instruction-tuning", "type": "Task", "metadata": []}, {"id": "RWKV: Reinventing RNNs for the transformer era.", "type": "Publication", "metadata": []}, {"id": "Bo Peng", "type": "Person", "metadata": []}, {"id": "Eric Alcaide", "type": "Person", "metadata": []}, {"id": "Quentin Anthony", "type": "Person", "metadata": []}, {"id": "Alon Albalak", "type": "Person", "metadata": []}, {"id": "Samuel Arcadinho", "type": "Person", "metadata": []}, {"id": "Stella Biderman", "type": "Person", "metadata": []}, {"id": "Huanqi Cao", "type": "Person", "metadata": []}, {"id": "Xin Cheng", "type": "Person", "metadata": []}, {"id": "Michael Chung", "type": "Person", "metadata": []}, {"id": "Leon Derczynski", "type": "Person", "metadata": []}, {"id": "Xingjian Du", "type": "Person", "metadata": []}, {"id": "Matteo Grella", "type": "Person", "metadata": []}, {"id": "Kranthi Gv", "type": "Person", "metadata": []}, {"id": "Xuzheng He", "type": "Person", "metadata": []}, {"id": "Haowen Hou", "type": "Person", "metadata": []}, {"id": "Przemyslaw Kazienko", "type": "Person", "metadata": []}, {"id": "Jan Kocon", "type": "Person", "metadata": []}, {"id": "Jiaming Kong", "type": "Person", "metadata": []}, {"id": "Bart\u0142omiej Koptyra", "type": "Person", "metadata": []}, {"id": "Hayden Lau", "type": "Person", "metadata": []}, {"id": "Jiaju Lin", "type": "Person", "metadata": []}, {"id": "Krishna Sri Ipsit Mantri", "type": "Person", "metadata": []}, {"id": "Ferdinand Mom", "type": "Person", "metadata": []}, {"id": "Atsushi Saito", "type": "Person", "metadata": []}, {"id": "Guangyu Song", "type": "Person", "metadata": []}, {"id": "Xiangru Tang", "type": "Person", "metadata": []}, {"id": "Johan Wind", "type": "Person", "metadata": []}, {"id": "Stanis\u0142aw Wo\u00b4zniak", "type": "Person", "metadata": []}, {"id": "Zhenyuan Zhang", "type": "Person", "metadata": []}, {"id": "Qinghua Zhou", "type": "Person", "metadata": []}, {"id": "Jian Zhu", "type": "Person", "metadata": []}, {"id": "Rui-Jie Zhu", "type": "Person", "metadata": []}, {"id": "RWKV", "type": "Model", "metadata": []}, {"id": "Association for Computational Linguistics", "type": "Organization", "metadata": []}, {"id": "Findings of the Association for Computational Linguistics: EMNLP 2023", "type": "Publication", "metadata": []}, {"id": "Gpqa: A graduate-level google-proof q&a benchmark.", "type": "Publication", "metadata": []}, {"id": "David Rein", "type": "Person", "metadata": []}, {"id": "Betty Li Hou", "type": "Person", "metadata": []}, {"id": "Asa Cooper Stickland", "type": "Person", "metadata": []}, {"id": "Jackson Petty", "type": "Person", "metadata": []}, {"id": "Richard Yuanzhe Pang", "type": "Person", "metadata": []}, {"id": "Julien Dirani", "type": "Person", "metadata": []}, {"id": "Julian Michael", "type": "Person", "metadata": []}, {"id": "Samuel R. Bowman", "type": "Person", "metadata": []}, {"id": "GPQA", "type": "Task", "metadata": []}, {"id": "Question Answering", "type": "Task", "metadata": []}, {"id": "Deepseekmath: Pushing the limits of mathematical reasoning in open language models.", "type": "Publication", "metadata": []}, {"id": "Zhihong Shao", "type": "Person", "metadata": []}, {"id": "Peiyi Wang", "type": "Person", "metadata": []}, {"id": "Qihao Zhu", "type": "Person", "metadata": []}, {"id": "Runxin Xu", "type": "Person", "metadata": []}, {"id": "Junxiao Song", "type": "Person", "metadata": []}, {"id": "Xiao Bi", "type": "Person", "metadata": []}, {"id": "Haowei Zhang", "type": "Person", "metadata": []}, {"id": "Mingchuan Zhang", "type": "Person", "metadata": []}, {"id": "Y. K. Li", "type": "Person", "metadata": []}, {"id": "Y. Wu", "type": "Person", "metadata": []}, {"id": "Daya Guo", "type": "Person", "metadata": []}, {"id": "DeepseekMath", "type": "Model", "metadata": []}, {"id": "Mathematical Reasoning", "type": "Task", "metadata": []}, {"id": "Retentive network: A successor to transformer for large language models.", "type": "Publication", "metadata": []}, {"id": "Yutao Sun", "type": "Person", "metadata": []}, {"id": "Li Dong", "type": "Person", "metadata": []}, {"id": "Shaohan Huang", "type": "Person", "metadata": []}, {"id": "Shuming Ma", "type": "Person", "metadata": []}, {"id": "Yuqing Xia", "type": "Person", "metadata": []}, {"id": "Jilong Xue", "type": "Person", "metadata": []}, {"id": "Jianyong Wang", "type": "Person", "metadata": []}, {"id": "Furu Wei", "type": "Person", "metadata": []}, {"id": "Retentive Network", "type": "Model", "metadata": []}, {"id": "Gemma 2: Improving open language models at a practical size.", "type": "Publication", "metadata": []}, {"id": "Gemma Team", "type": "Organization", "metadata": []}, {"id": "Gemma2", "type": "Model", "metadata": []}, {"id": "Gemma 3 technical report.", "type": "Publication", "metadata": []}, {"id": "Gemma3", "type": "Model", "metadata": []}, {"id": "Every attention matters: An efficient hybrid architecture for long-context reasoning.", "type": "Publication", "metadata": []}, {"id": "Ling Team", "type": "Organization", "metadata": []}, {"id": "Bin Han", "type": "Person", "metadata": []}, {"id": "Caizhi Tang", "type": "Person", "metadata": []}, {"id": "Chen Liang", "type": "Person", "metadata": []}, {"id": "Donghao Zhang", "type": "Person", "metadata": []}, {"id": "Fan Yuan", "type": "Person", "metadata": []}, {"id": "Feng Zhu", "type": "Person", "metadata": []}, {"id": "Jie Gao", "type": "Person", "metadata": []}, {"id": "Jingyu Hu", "type": "Person", "metadata": []}, {"id": "Longfei Li", "type": "Person", "metadata": []}, {"id": "Meng Li", "type": "Person", "metadata": []}, {"id": "Mingyang Zhang", "type": "Person", "metadata": []}, {"id": "Peijie Jiang", "type": "Person", "metadata": []}, {"id": "Peng Jiao", "type": "Person", "metadata": []}, {"id": "Qian Zhao", "type": "Person", "metadata": []}, {"id": "Qingyuan Yang", "type": "Person", "metadata": []}, {"id": "Wenbo Shen", "type": "Person", "metadata": []}, {"id": "Xinxing Yang", "type": "Person", "metadata": []}, {"id": "Yalin Zhang", "type": "Person", "metadata": []}, {"id": "Yankun Ren", "type": "Person", "metadata": []}, {"id": "Yao Zhao", "type": "Person", "metadata": []}, {"id": "Yibo Cao", "type": "Person", "metadata": []}, {"id": "Yixuan Sun", "type": "Person", "metadata": []}, {"id": "Yue Zhang", "type": "Person", "metadata": []}, {"id": "Yuchen Fang", "type": "Person", "metadata": []}, {"id": "Zibin Lin", "type": "Person", "metadata": []}, {"id": "Zixuan Cheng", "type": "Person", "metadata": []}, {"id": "Jun Zhou", "type": "Person", "metadata": []}, {"id": "Long-context reasoning", "type": "Concept", "metadata": []}, {"id": "Model Architectures", "type": "Concept", "metadata": []}, {"id": "The llama 3 herd of models.", "type": "Publication", "metadata": []}, {"id": "Llama Team", "type": "Organization", "metadata": []}, {"id": "Llama3", "type": "Model", "metadata": []}, {"id": "Qwen3 technical report.", "type": "Publication", "metadata": []}, {"id": "Qwen3 Team", "type": "Organization", "metadata": []}, {"id": "Qwen3", "type": "Model", "metadata": []}, {"id": "Llama: Open and efficient foundation language models.", "type": "Publication", "metadata": []}, {"id": "Hugo Touvron", "type": "Person", "metadata": []}, {"id": "Thibaut Lavril", "type": "Person", "metadata": []}, {"id": "Gautier Izacard", "type": "Person", "metadata": []}, {"id": "Xavier Martinet", "type": "Person", "metadata": []}, {"id": "Marie-Anne Lachaux", "type": "Person", "metadata": []}, {"id": "Timoth\u00e9e Lacroix", "type": "Person", "metadata": []}, {"id": "Baptiste Rozi\u00e8re", "type": "Person", "metadata": []}, {"id": "Naman Goyal", "type": "Person", "metadata": []}, {"id": "Eric Hambro", "type": "Person", "metadata": []}, {"id": "Faisal Azhar", "type": "Person", "metadata": []}, {"id": "Aurelien Rodriguez", "type": "Person", "metadata": []}, {"id": "Armand Joulin", "type": "Person", "metadata": []}, {"id": "Edouard Grave", "type": "Person", "metadata": []}, {"id": "Guillaume Lample", "type": "Person", "metadata": []}, {"id": "Llama", "type": "Model", "metadata": []}, {"id": "Attention is all you need.", "type": "Publication", "metadata": []}, {"id": "Ashish Vaswani", "type": "Person", "metadata": []}, {"id": "Noam Shazeer", "type": "Person", "metadata": []}, {"id": "Niki Parmar", "type": "Person", "metadata": []}, {"id": "Jakob Uszkoreit", "type": "Person", "metadata": []}, {"id": "Llion Jones", "type": "Person", "metadata": []}, {"id": "Aidan N. Gomez", "type": "Person", "metadata": []}, {"id": "\u0141ukasz Kaiser", "type": "Person", "metadata": []}, {"id": "Illia Polosukhin", "type": "Person", "metadata": []}, {"id": "Advances in Neural Information Processing Systems", "type": "Organization", "metadata": []}, {"id": "Rattention: Towards the minimal sliding window size in local-global attention models.", "type": "Publication", "metadata": []}, {"id": "Bailin Wang", "type": "Person", "metadata": []}, {"id": "Chang Lan", "type": "Person", "metadata": []}, {"id": "Chong Wang", "type": "Person", "metadata": []}, {"id": "Ruoming Pang", "type": "Person", "metadata": []}, {"id": "Rattention", "type": "Model", "metadata": []}, {"id": "Sliding Window Attention", "type": "Concept", "metadata": []}, {"id": "Chain-of-thought prompting elicits reasoning in large language models.", "type": "Publication", "metadata": []}, {"id": "Jason Wei", "type": "Person", "metadata": []}, {"id": "Xuezhi Wang", "type": "Person", "metadata": []}, {"id": "Dale Schuurmans", "type": "Person", "metadata": []}, {"id": "Maarten Bosma", "type": "Person", "metadata": []}, {"id": "Brian Ichter", "type": "Person", "metadata": []}, {"id": "Fei Xia", "type": "Person", "metadata": []}, {"id": "Ed H. Chi", "type": "Person", "metadata": []}, {"id": "Quoc V. Le", "type": "Person", "metadata": []}, {"id": "Denny Zhou", "type": "Person", "metadata": []}, {"id": "Chain of Thought (CoT) prompting", "type": "Concept", "metadata": []}, {"id": "Reasoning", "type": "Concept", "metadata": []}, {"id": "Longmemeval: Benchmarking chat assistants on long-term interactive memory.", "type": "Publication", "metadata": []}, {"id": "Di Wu", "type": "Person", "metadata": []}, {"id": "Hongwei Wang", "type": "Person", "metadata": []}, {"id": "Wenhao Yu", "type": "Person", "metadata": []}, {"id": "Yuwei Zhang", "type": "Person", "metadata": []}, {"id": "Kai-Wei Chang", "type": "Person", "metadata": []}, {"id": "Dong Yu", "type": "Person", "metadata": []}, {"id": "LongMemEval", "type": "Task", "metadata": []}, {"id": "Long-term interactive memory", "type": "Concept", "metadata": []}, {"id": "Why stacking sliding windows can\u2019t see very far.", "type": "Publication", "metadata": []}, {"id": "Guangxuan Xiao", "type": "Person", "metadata": []}, {"id": "Efficient streaming language models with attention sinks.", "type": "Publication", "metadata": []}, {"id": "Yuandong Tian", "type": "Person", "metadata": []}, {"id": "Beidi Chen", "type": "Person", "metadata": []}, {"id": "Song Han", "type": "Person", "metadata": []}, {"id": "Mike Lewis", "type": "Person", "metadata": []}, {"id": "Attention sinks", "type": "Concept", "metadata": []}, {"id": "Self-distillation bridges distribution gap in language model fine-tuning.", "type": "Publication", "metadata": []}, {"id": "Zhaorui Yang", "type": "Person", "metadata": []}, {"id": "Tianyu Pang", "type": "Person", "metadata": []}, {"id": "Haozhe Feng", "type": "Person", "metadata": []}, {"id": "Han Wang", "type": "Person", "metadata": []}, {"id": "Wei Chen", "type": "Person", "metadata": []}, {"id": "Minfeng Zhu", "type": "Person", "metadata": []}, {"id": "Qian Liu", "type": "Person", "metadata": []}, {"id": "Self-distillation", "type": "Algorithm", "metadata": []}, {"id": "Language model fine-tuning", "type": "Task", "metadata": []}, {"id": "Native sparse attention: Hardware-aligned and natively trainable sparse attention.", "type": "Publication", "metadata": []}, {"id": "Jingyang Yuan", "type": "Person", "metadata": []}, {"id": "Huazuo Gao", "type": "Person", "metadata": []}, {"id": "Damai Dai", "type": "Person", "metadata": []}, {"id": "Junyu Luo", "type": "Person", "metadata": []}, {"id": "Liang Zhao", "type": "Person", "metadata": []}, {"id": "Zhengyan Zhang", "type": "Person", "metadata": []}, {"id": "Zhenda Xie", "type": "Person", "metadata": []}, {"id": "Yuxing Wei", "type": "Person", "metadata": []}, {"id": "Lean Wang", "type": "Person", "metadata": []}, {"id": "Zhiping Xiao", "type": "Person", "metadata": []}, {"id": "Yuqing Wang", "type": "Person", "metadata": []}, {"id": "Chong Ruan", "type": "Person", "metadata": []}, {"id": "Ming Zhang", "type": "Person", "metadata": []}, {"id": "Wenfeng Liang", "type": "Person", "metadata": []}, {"id": "Wangding Zeng", "type": "Person", "metadata": []}, {"id": "Native Sparse Attention", "type": "Concept", "metadata": []}, {"id": "Sparse attention", "type": "Concept", "metadata": []}, {"id": "Big bird: Transformers for longer sequences.", "type": "Publication", "metadata": []}, {"id": "Manzil Zaheer", "type": "Person", "metadata": []}, {"id": "Guru Guruganesh", "type": "Person", "metadata": []}, {"id": "Kumar Avinava Dubey", "type": "Person", "metadata": []}, {"id": "Joshua Ainslie", "type": "Person", "metadata": []}, {"id": "Chris Alberti", "type": "Person", "metadata": []}, {"id": "Santiago Onta\u00f1\u00f3n", "type": "Person", "metadata": []}, {"id": "Philip Pham", "type": "Person", "metadata": []}, {"id": "Anirudh Ravula", "type": "Person", "metadata": []}, {"id": "Qifan Wang", "type": "Person", "metadata": []}, {"id": "Li Yang", "type": "Person", "metadata": []}, {"id": "Amr Ahmed", "type": "Person", "metadata": []}, {"id": "BigBird", "type": "Model", "metadata": []}, {"id": "Light-transfer: Your long-context llm is secretly a hybrid model with effortless adaptation.", "type": "Publication", "metadata": []}, {"id": "Xuan Zhang", "type": "Person", "metadata": []}, {"id": "Fengzhuo Zhang", "type": "Person", "metadata": []}, {"id": "Cunxiao Du", "type": "Person", "metadata": []}, {"id": "Chao Du", "type": "Person", "metadata": []}, {"id": "Wei Gao", "type": "Person", "metadata": []}, {"id": "Min Lin", "type": "Person", "metadata": []}, {"id": "LightTransfer", "type": "Model", "metadata": []}, {"id": "Long context processing", "type": "Concept", "metadata": []}, {"id": "Computational complexity", "type": "Concept", "metadata": []}, {"id": "GPU memory", "type": "Concept", "metadata": []}, {"id": "KV cache reusability", "type": "Concept", "metadata": []}, {"id": "FA Decode", "type": "Concept", "metadata": []}, {"id": "Prefilling", "type": "Concept", "metadata": []}, {"id": "Decoding speed", "type": "Concept", "metadata": []}, {"id": "Full Attention", "type": "Concept", "metadata": []}, {"id": "Distributed LLM services", "type": "Concept", "metadata": []}, {"id": "Keep First", "type": "Concept", "metadata": []}, {"id": "Computational overhead", "type": "Concept", "metadata": []}, {"id": "Positional Encoding", "type": "Concept", "metadata": []}, {"id": "Interleaving Layers", "type": "Concept", "metadata": []}, {"id": "Chain of Thought (CoT) format", "type": "Concept", "metadata": []}, {"id": "Long-context benchmarks", "type": "Concept", "metadata": []}, {"id": "LongBench", "type": "Task", "metadata": []}, {"id": "Bai et al., 2023", "type": "Publication", "metadata": []}, {"id": "Hsieh et al., 2024", "type": "Publication", "metadata": []}, {"id": "Needle-retrieval tasks", "type": "Task", "metadata": []}], "relationships": [{"source": {"id": "Yushi Bai", "type": "Person", "metadata": []}, "target": {"id": "Longbench v2: Towards deeper understanding and reasoning on realistic long-context multitasks.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Shangqing Tu", "type": "Person", "metadata": []}, "target": {"id": "Longbench v2: Towards deeper understanding and reasoning on realistic long-context multitasks.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Jiajie Zhang", "type": "Person", "metadata": []}, "target": {"id": "Longbench v2: Towards deeper understanding and reasoning on realistic long-context multitasks.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Hao Peng", "type": "Person", "metadata": []}, "target": {"id": "Longbench v2: Towards deeper understanding and reasoning on realistic long-context multitasks.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Xiaozhi Wang", "type": "Person", "metadata": []}, "target": {"id": "Longbench v2: Towards deeper understanding and reasoning on realistic long-context multitasks.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Xin Lv", "type": "Person", "metadata": []}, "target": {"id": "Longbench v2: Towards deeper understanding and reasoning on realistic long-context multitasks.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Shulin Cao", "type": "Person", "metadata": []}, "target": {"id": "Longbench v2: Towards deeper understanding and reasoning on realistic long-context multitasks.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Jiazheng Xu", "type": "Person", "metadata": []}, "target": {"id": "Longbench v2: Towards deeper understanding and reasoning on realistic long-context multitasks.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Lei Hou", "type": "Person", "metadata": []}, "target": {"id": "Longbench v2: Towards deeper understanding and reasoning on realistic long-context multitasks.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Yuxiao Dong", "type": "Person", "metadata": []}, "target": {"id": "Longbench v2: Towards deeper understanding and reasoning on realistic long-context multitasks.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Jie Tang", "type": "Person", "metadata": []}, "target": {"id": "Longbench v2: Towards deeper understanding and reasoning on realistic long-context multitasks.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Juanzi Li", "type": "Person", "metadata": []}, "target": {"id": "Longbench v2: Towards deeper understanding and reasoning on realistic long-context multitasks.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Longbench v2: Towards deeper understanding and reasoning on realistic long-context multitasks.", "type": "Publication", "metadata": []}, "target": {"id": "Multimodal Understanding and Reasoning", "type": "Task", "metadata": []}, "type": "ADDRESSES"}, {"source": {"id": "Longbench v2: Towards deeper understanding and reasoning on realistic long-context multitasks.", "type": "Publication", "metadata": []}, "target": {"id": "Long-context tasks", "type": "Concept", "metadata": []}, "type": "ADDRESSES"}, {"source": {"id": "Iz Beltagy", "type": "Person", "metadata": []}, "target": {"id": "Longformer: The long-document transformer.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Matthew E. Peters", "type": "Person", "metadata": []}, "target": {"id": "Longformer: The long-document transformer.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Arman Cohan", "type": "Person", "metadata": []}, "target": {"id": "Longformer: The long-document transformer.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Longformer", "type": "Model", "metadata": []}, "target": {"id": "Transformer", "type": "Model", "metadata": []}, "type": "EXTENDS"}, {"source": {"id": "Tri Dao", "type": "Person", "metadata": []}, "target": {"id": "Flashattention-2: Faster attention with better parallelism and work partitioning.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Flashattention-2", "type": "Model", "metadata": []}, "target": {"id": "Attention mechanism", "type": "Concept", "metadata": []}, "type": "EXTENDS"}, {"source": {"id": "DeepSeek-AI", "type": "Organization", "metadata": []}, "target": {"id": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Deepseek-r1", "type": "Model", "metadata": []}, "target": {"id": "Reinforcement Learning", "type": "Algorithm", "metadata": []}, "type": "BUILDS_ON"}, {"source": {"id": "Deepseek-r1", "type": "Model", "metadata": []}, "target": {"id": "Large Language Models", "type": "Concept", "metadata": []}, "type": "EXTENDS"}, {"source": {"id": "DeepSeek-AI", "type": "Organization", "metadata": []}, "target": {"id": "Deepseek-v3.2: Pushing the frontier of open large language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Deepseek-v3.2", "type": "Model", "metadata": []}, "target": {"id": "Large Language Models", "type": "Concept", "metadata": []}, "type": "EXTENDS"}, {"source": {"id": "Yonggan Fu", "type": "Person", "metadata": []}, "target": {"id": "Nemotron-flash: Towards latencyoptimal hybrid small language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Xin Dong", "type": "Person", "metadata": []}, "target": {"id": "Nemotron-flash: Towards latencyoptimal hybrid small language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Shizhe Diao", "type": "Person", "metadata": []}, "target": {"id": "Nemotron-flash: Towards latencyoptimal hybrid small language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Hanrong Ye", "type": "Person", "metadata": []}, "target": {"id": "Nemotron-flash: Towards latencyoptimal hybrid small language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Wonmin Byeon", "type": "Person", "metadata": []}, "target": {"id": "Nemotron-flash: Towards latencyoptimal hybrid small language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Yashaswi Karnati", "type": "Person", "metadata": []}, "target": {"id": "Nemotron-flash: Towards latencyoptimal hybrid small language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Lucas Liebenwein", "type": "Person", "metadata": []}, "target": {"id": "Nemotron-flash: Towards latencyoptimal hybrid small language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Maksim Khadkevich", "type": "Person", "metadata": []}, "target": {"id": "Nemotron-flash: Towards latencyoptimal hybrid small language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Alexander Keller", "type": "Person", "metadata": []}, "target": {"id": "Nemotron-flash: Towards latencyoptimal hybrid small language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Jan Kautz", "type": "Person", "metadata": []}, "target": {"id": "Nemotron-flash: Towards latencyoptimal hybrid small language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Nemotron-Flash", "type": "Model", "metadata": []}, "target": {"id": "Large Language Models", "type": "Concept", "metadata": []}, "type": "EXTENDS"}, {"source": {"id": "Zichuan Fu", "type": "Person", "metadata": []}, "target": {"id": "Sliding window attention training for efficient large language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Wentao Song", "type": "Person", "metadata": []}, "target": {"id": "Sliding window attention training for efficient large language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Yejing Wang", "type": "Person", "metadata": []}, "target": {"id": "Sliding window attention training for efficient large language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Xian Wu", "type": "Person", "metadata": []}, "target": {"id": "Sliding window attention training for efficient large language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Yefeng Zheng", "type": "Person", "metadata": []}, "target": {"id": "Sliding window attention training for efficient large language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Yingying Zhang", "type": "Person", "metadata": []}, "target": {"id": "Sliding window attention training for efficient large language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Derong Xu", "type": "Person", "metadata": []}, "target": {"id": "Sliding window attention training for efficient large language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Xuetao Wei", "type": "Person", "metadata": []}, "target": {"id": "Sliding window attention training for efficient large language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Tong Xu", "type": "Person", "metadata": []}, "target": {"id": "Sliding window attention training for efficient large language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Xiangyu Zhao", "type": "Person", "metadata": []}, "target": {"id": "Sliding window attention training for efficient large language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Sliding Window Attention Training", "type": "Algorithm", "metadata": []}, "target": {"id": "Large Language Models", "type": "Concept", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "Albert Gu", "type": "Person", "metadata": []}, "target": {"id": "Mamba: Linear-time sequence modeling with selective state spaces.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Tri Dao", "type": "Person", "metadata": []}, "target": {"id": "Mamba: Linear-time sequence modeling with selective state spaces.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Mamba", "type": "Model", "metadata": []}, "target": {"id": "Sequence Modeling", "type": "Task", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "Dan Hendrycks", "type": "Person", "metadata": []}, "target": {"id": "Measuring massive multitask language understanding.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Collin Burns", "type": "Person", "metadata": []}, "target": {"id": "Measuring massive multitask language understanding.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Steven Basart", "type": "Person", "metadata": []}, "target": {"id": "Measuring massive multitask language understanding.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Andy Zou", "type": "Person", "metadata": []}, "target": {"id": "Measuring massive multitask language understanding.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Mantas Mazeika", "type": "Person", "metadata": []}, "target": {"id": "Measuring massive multitask language understanding.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Dawn Song", "type": "Person", "metadata": []}, "target": {"id": "Measuring massive multitask language understanding.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Jacob Steinhardt", "type": "Person", "metadata": []}, "target": {"id": "Measuring massive multitask language understanding.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Measuring massive multitask language understanding.", "type": "Publication", "metadata": []}, "target": {"id": "MMLU", "type": "Task", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Cheng-Ping Hsieh", "type": "Person", "metadata": []}, "target": {"id": "Ruler: What\u2019s the real context size of your long-context language models?", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Simeng Sun", "type": "Person", "metadata": []}, "target": {"id": "Ruler: What\u2019s the real context size of your long-context language models?", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Samuel Kriman", "type": "Person", "metadata": []}, "target": {"id": "Ruler: What\u2019s the real context size of your long-context language models?", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Shantanu Acharya", "type": "Person", "metadata": []}, "target": {"id": "Ruler: What\u2019s the real context size of your long-context language models?", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Dima Rekesh", "type": "Person", "metadata": []}, "target": {"id": "Ruler: What\u2019s the real context size of your long-context language models?", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Fei Jia", "type": "Person", "metadata": []}, "target": {"id": "Ruler: What\u2019s the real context size of your long-context language models?", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Yang Zhang", "type": "Person", "metadata": []}, "target": {"id": "Ruler: What\u2019s the real context size of your long-context language models?", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Boris Ginsburg", "type": "Person", "metadata": []}, "target": {"id": "Ruler: What\u2019s the real context size of your long-context language models?", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Ruler", "type": "Model", "metadata": []}, "target": {"id": "Long-context tasks", "type": "Concept", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "Edward J. Hu", "type": "Person", "metadata": []}, "target": {"id": "Lora: Low-rank adaptation of large language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Yelong Shen", "type": "Person", "metadata": []}, "target": {"id": "Lora: Low-rank adaptation of large language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Phillip Wallis", "type": "Person", "metadata": []}, "target": {"id": "Lora: Low-rank adaptation of large language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Zeyuan Allen-Zhu", "type": "Person", "metadata": []}, "target": {"id": "Lora: Low-rank adaptation of large language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Yuanzhi Li", "type": "Person", "metadata": []}, "target": {"id": "Lora: Low-rank adaptation of large language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Shean Wang", "type": "Person", "metadata": []}, "target": {"id": "Lora: Low-rank adaptation of large language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Lu Wang", "type": "Person", "metadata": []}, "target": {"id": "Lora: Low-rank adaptation of large language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Weizhu Chen", "type": "Person", "metadata": []}, "target": {"id": "Lora: Low-rank adaptation of large language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "LoRA", "type": "Algorithm", "metadata": []}, "target": {"id": "Large Language Models", "type": "Concept", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "Angelos Katharopoulos", "type": "Person", "metadata": []}, "target": {"id": "Transformers are rnns: Fast autoregressive transformers with linear attention.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Apoorv Vyas", "type": "Person", "metadata": []}, "target": {"id": "Transformers are rnns: Fast autoregressive transformers with linear attention.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Nikolaos Pappas", "type": "Person", "metadata": []}, "target": {"id": "Transformers are rnns: Fast autoregressive transformers with linear attention.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Fran\u00e7ois Fleuret", "type": "Person", "metadata": []}, "target": {"id": "Transformers are rnns: Fast autoregressive transformers with linear attention.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Transformer", "type": "Model", "metadata": []}, "target": {"id": "RNN", "type": "Model", "metadata": []}, "type": "BUILDS_ON"}, {"source": {"id": "Transformers are rnns: Fast autoregressive transformers with linear attention.", "type": "Publication", "metadata": []}, "target": {"id": "Linear attention", "type": "Concept", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Woosuk Kwon", "type": "Person", "metadata": []}, "target": {"id": "Efficient memory management for large language model serving with pagedattention.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Zhuohan Li", "type": "Person", "metadata": []}, "target": {"id": "Efficient memory management for large language model serving with pagedattention.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Siyuan Zhuang", "type": "Person", "metadata": []}, "target": {"id": "Efficient memory management for large language model serving with pagedattention.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Ying Sheng", "type": "Person", "metadata": []}, "target": {"id": "Efficient memory management for large language model serving with pagedattention.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Lianmin Zheng", "type": "Person", "metadata": []}, "target": {"id": "Efficient memory management for large language model serving with pagedattention.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Cody Hao Yu", "type": "Person", "metadata": []}, "target": {"id": "Efficient memory management for large language model serving with pagedattention.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Joseph E. Gonzalez", "type": "Person", "metadata": []}, "target": {"id": "Efficient memory management for large language model serving with pagedattention.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Hao Zhang", "type": "Person", "metadata": []}, "target": {"id": "Efficient memory management for large language model serving with pagedattention.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Ion Stoica", "type": "Person", "metadata": []}, "target": {"id": "Efficient memory management for large language model serving with pagedattention.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "PagedAttention", "type": "Algorithm", "metadata": []}, "target": {"id": "Efficient memory management", "type": "Concept", "metadata": []}, "type": "ADDRESSES"}, {"source": {"id": "PagedAttention", "type": "Algorithm", "metadata": []}, "target": {"id": "Large Language Models", "type": "Concept", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "Opher Lieber", "type": "Person", "metadata": []}, "target": {"id": "Jamba: A hybrid transformer-mamba language model.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Barak Lenz", "type": "Person", "metadata": []}, "target": {"id": "Jamba: A hybrid transformer-mamba language model.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Hofit Bata", "type": "Person", "metadata": []}, "target": {"id": "Jamba: A hybrid transformer-mamba language model.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Gal Cohen", "type": "Person", "metadata": []}, "target": {"id": "Jamba: A hybrid transformer-mamba language model.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Jhonathan Osin", "type": "Person", "metadata": []}, "target": {"id": "Jamba: A hybrid transformer-mamba language model.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Itay Dalmedigos", "type": "Person", "metadata": []}, "target": {"id": "Jamba: A hybrid transformer-mamba language model.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Erez Safahi", "type": "Person", "metadata": []}, "target": {"id": "Jamba: A hybrid transformer-mamba language model.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Shaked Meirom", "type": "Person", "metadata": []}, "target": {"id": "Jamba: A hybrid transformer-mamba language model.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Yonatan Belinkov", "type": "Person", "metadata": []}, "target": {"id": "Jamba: A hybrid transformer-mamba language model.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Shai ShalevShwartz", "type": "Person", "metadata": []}, "target": {"id": "Jamba: A hybrid transformer-mamba language model.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Omri Abend", "type": "Person", "metadata": []}, "target": {"id": "Jamba: A hybrid transformer-mamba language model.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Raz Alon", "type": "Person", "metadata": []}, "target": {"id": "Jamba: A hybrid transformer-mamba language model.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Tomer Asida", "type": "Person", "metadata": []}, "target": {"id": "Jamba: A hybrid transformer-mamba language model.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Amir Bergman", "type": "Person", "metadata": []}, "target": {"id": "Jamba: A hybrid transformer-mamba language model.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Roman Glozman", "type": "Person", "metadata": []}, "target": {"id": "Jamba: A hybrid transformer-mamba language model.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Michael Gokhman", "type": "Person", "metadata": []}, "target": {"id": "Jamba: A hybrid transformer-mamba language model.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Avashalom Manevich", "type": "Person", "metadata": []}, "target": {"id": "Jamba: A hybrid transformer-mamba language model.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Nir Ratner", "type": "Person", "metadata": []}, "target": {"id": "Jamba: A hybrid transformer-mamba language model.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Noam Rozen", "type": "Person", "metadata": []}, "target": {"id": "Jamba: A hybrid transformer-mamba language model.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Erez Shwartz", "type": "Person", "metadata": []}, "target": {"id": "Jamba: A hybrid transformer-mamba language model.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Mor Zusman", "type": "Person", "metadata": []}, "target": {"id": "Jamba: A hybrid transformer-mamba language model.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Yoav Shoham", "type": "Person", "metadata": []}, "target": {"id": "Jamba: A hybrid transformer-mamba language model.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Jamba", "type": "Model", "metadata": []}, "target": {"id": "Transformer", "type": "Model", "metadata": []}, "type": "BUILDS_ON"}, {"source": {"id": "Jamba", "type": "Model", "metadata": []}, "target": {"id": "Mamba", "type": "Model", "metadata": []}, "type": "BUILDS_ON"}, {"source": {"id": "Tri Dao", "type": "Person", "metadata": []}, "target": {"id": "Bamba: Inference-efficient hybrid mamba2 model.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Linsong Chu", "type": "Person", "metadata": []}, "target": {"id": "Bamba: Inference-efficient hybrid mamba2 model.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Divya Kumari", "type": "Person", "metadata": []}, "target": {"id": "Bamba: Inference-efficient hybrid mamba2 model.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Bamba", "type": "Model", "metadata": []}, "target": {"id": "Mamba", "type": "Model", "metadata": []}, "type": "BUILDS_ON"}, {"source": {"id": "OpenAI", "type": "Organization", "metadata": []}, "target": {"id": "ChatGPT.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Wenbo Pan", "type": "Person", "metadata": []}, "target": {"id": "Fusang-v1: A large curation of instruction-tuning datasets for better bilingual and long-range llms.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Fusang-v1: A large curation of instruction-tuning datasets for better bilingual and long-range llms.", "type": "Publication", "metadata": []}, "target": {"id": "Instruction-tuning", "type": "Task", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "Fusang-v1: A large curation of instruction-tuning datasets for better bilingual and long-range llms.", "type": "Publication", "metadata": []}, "target": {"id": "Large Language Models", "type": "Concept", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "Bo Peng", "type": "Person", "metadata": []}, "target": {"id": "RWKV: Reinventing RNNs for the transformer era.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Eric Alcaide", "type": "Person", "metadata": []}, "target": {"id": "RWKV: Reinventing RNNs for the transformer era.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Quentin Anthony", "type": "Person", "metadata": []}, "target": {"id": "RWKV: Reinventing RNNs for the transformer era.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Alon Albalak", "type": "Person", "metadata": []}, "target": {"id": "RWKV: Reinventing RNNs for the transformer era.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Samuel Arcadinho", "type": "Person", "metadata": []}, "target": {"id": "RWKV: Reinventing RNNs for the transformer era.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Stella Biderman", "type": "Person", "metadata": []}, "target": {"id": "RWKV: Reinventing RNNs for the transformer era.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Huanqi Cao", "type": "Person", "metadata": []}, "target": {"id": "RWKV: Reinventing RNNs for the transformer era.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Xin Cheng", "type": "Person", "metadata": []}, "target": {"id": "RWKV: Reinventing RNNs for the transformer era.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Michael Chung", "type": "Person", "metadata": []}, "target": {"id": "RWKV: Reinventing RNNs for the transformer era.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Leon Derczynski", "type": "Person", "metadata": []}, "target": {"id": "RWKV: Reinventing RNNs for the transformer era.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Xingjian Du", "type": "Person", "metadata": []}, "target": {"id": "RWKV: Reinventing RNNs for the transformer era.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Matteo Grella", "type": "Person", "metadata": []}, "target": {"id": "RWKV: Reinventing RNNs for the transformer era.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Kranthi Gv", "type": "Person", "metadata": []}, "target": {"id": "RWKV: Reinventing RNNs for the transformer era.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Xuzheng He", "type": "Person", "metadata": []}, "target": {"id": "RWKV: Reinventing RNNs for the transformer era.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Haowen Hou", "type": "Person", "metadata": []}, "target": {"id": "RWKV: Reinventing RNNs for the transformer era.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Przemyslaw Kazienko", "type": "Person", "metadata": []}, "target": {"id": "RWKV: Reinventing RNNs for the transformer era.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Jan Kocon", "type": "Person", "metadata": []}, "target": {"id": "RWKV: Reinventing RNNs for the transformer era.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Jiaming Kong", "type": "Person", "metadata": []}, "target": {"id": "RWKV: Reinventing RNNs for the transformer era.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Bart\u0142omiej Koptyra", "type": "Person", "metadata": []}, "target": {"id": "RWKV: Reinventing RNNs for the transformer era.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Hayden Lau", "type": "Person", "metadata": []}, "target": {"id": "RWKV: Reinventing RNNs for the transformer era.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Jiaju Lin", "type": "Person", "metadata": []}, "target": {"id": "RWKV: Reinventing RNNs for the transformer era.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Krishna Sri Ipsit Mantri", "type": "Person", "metadata": []}, "target": {"id": "RWKV: Reinventing RNNs for the transformer era.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Ferdinand Mom", "type": "Person", "metadata": []}, "target": {"id": "RWKV: Reinventing RNNs for the transformer era.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Atsushi Saito", "type": "Person", "metadata": []}, "target": {"id": "RWKV: Reinventing RNNs for the transformer era.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Guangyu Song", "type": "Person", "metadata": []}, "target": {"id": "RWKV: Reinventing RNNs for the transformer era.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Xiangru Tang", "type": "Person", "metadata": []}, "target": {"id": "RWKV: Reinventing RNNs for the transformer era.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Johan Wind", "type": "Person", "metadata": []}, "target": {"id": "RWKV: Reinventing RNNs for the transformer era.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Stanis\u0142aw Wo\u00b4zniak", "type": "Person", "metadata": []}, "target": {"id": "RWKV: Reinventing RNNs for the transformer era.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Zhenyuan Zhang", "type": "Person", "metadata": []}, "target": {"id": "RWKV: Reinventing RNNs for the transformer era.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Qinghua Zhou", "type": "Person", "metadata": []}, "target": {"id": "RWKV: Reinventing RNNs for the transformer era.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Jian Zhu", "type": "Person", "metadata": []}, "target": {"id": "RWKV: Reinventing RNNs for the transformer era.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Rui-Jie Zhu", "type": "Person", "metadata": []}, "target": {"id": "RWKV: Reinventing RNNs for the transformer era.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "RWKV", "type": "Model", "metadata": []}, "target": {"id": "RNN", "type": "Model", "metadata": []}, "type": "BUILDS_ON"}, {"source": {"id": "RWKV", "type": "Model", "metadata": []}, "target": {"id": "Transformer", "type": "Model", "metadata": []}, "type": "EXTENDS"}, {"source": {"id": "David Rein", "type": "Person", "metadata": []}, "target": {"id": "Gpqa: A graduate-level google-proof q&a benchmark.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Betty Li Hou", "type": "Person", "metadata": []}, "target": {"id": "Gpqa: A graduate-level google-proof q&a benchmark.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Asa Cooper Stickland", "type": "Person", "metadata": []}, "target": {"id": "Gpqa: A graduate-level google-proof q&a benchmark.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Jackson Petty", "type": "Person", "metadata": []}, "target": {"id": "Gpqa: A graduate-level google-proof q&a benchmark.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Richard Yuanzhe Pang", "type": "Person", "metadata": []}, "target": {"id": "Gpqa: A graduate-level google-proof q&a benchmark.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Julien Dirani", "type": "Person", "metadata": []}, "target": {"id": "Gpqa: A graduate-level google-proof q&a benchmark.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Julian Michael", "type": "Person", "metadata": []}, "target": {"id": "Gpqa: A graduate-level google-proof q&a benchmark.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Samuel R. Bowman", "type": "Person", "metadata": []}, "target": {"id": "Gpqa: A graduate-level google-proof q&a benchmark.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "GPQA", "type": "Task", "metadata": []}, "target": {"id": "Question Answering", "type": "Task", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "Zhihong Shao", "type": "Person", "metadata": []}, "target": {"id": "Deepseekmath: Pushing the limits of mathematical reasoning in open language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Peiyi Wang", "type": "Person", "metadata": []}, "target": {"id": "Deepseekmath: Pushing the limits of mathematical reasoning in open language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Qihao Zhu", "type": "Person", "metadata": []}, "target": {"id": "Deepseekmath: Pushing the limits of mathematical reasoning in open language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Runxin Xu", "type": "Person", "metadata": []}, "target": {"id": "Deepseekmath: Pushing the limits of mathematical reasoning in open language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Junxiao Song", "type": "Person", "metadata": []}, "target": {"id": "Deepseekmath: Pushing the limits of mathematical reasoning in open language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Xiao Bi", "type": "Person", "metadata": []}, "target": {"id": "Deepseekmath: Pushing the limits of mathematical reasoning in open language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Haowei Zhang", "type": "Person", "metadata": []}, "target": {"id": "Deepseekmath: Pushing the limits of mathematical reasoning in open language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Mingchuan Zhang", "type": "Person", "metadata": []}, "target": {"id": "Deepseekmath: Pushing the limits of mathematical reasoning in open language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Y. K. Li", "type": "Person", "metadata": []}, "target": {"id": "Deepseekmath: Pushing the limits of mathematical reasoning in open language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Y. Wu", "type": "Person", "metadata": []}, "target": {"id": "Deepseekmath: Pushing the limits of mathematical reasoning in open language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Daya Guo", "type": "Person", "metadata": []}, "target": {"id": "Deepseekmath: Pushing the limits of mathematical reasoning in open language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "DeepseekMath", "type": "Model", "metadata": []}, "target": {"id": "Mathematical Reasoning", "type": "Task", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "DeepseekMath", "type": "Model", "metadata": []}, "target": {"id": "Large Language Models", "type": "Concept", "metadata": []}, "type": "EXTENDS"}, {"source": {"id": "Yutao Sun", "type": "Person", "metadata": []}, "target": {"id": "Retentive network: A successor to transformer for large language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Li Dong", "type": "Person", "metadata": []}, "target": {"id": "Retentive network: A successor to transformer for large language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Shaohan Huang", "type": "Person", "metadata": []}, "target": {"id": "Retentive network: A successor to transformer for large language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Shuming Ma", "type": "Person", "metadata": []}, "target": {"id": "Retentive network: A successor to transformer for large language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Yuqing Xia", "type": "Person", "metadata": []}, "target": {"id": "Retentive network: A successor to transformer for large language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Jilong Xue", "type": "Person", "metadata": []}, "target": {"id": "Retentive network: A successor to transformer for large language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Jianyong Wang", "type": "Person", "metadata": []}, "target": {"id": "Retentive network: A successor to transformer for large language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Furu Wei", "type": "Person", "metadata": []}, "target": {"id": "Retentive network: A successor to transformer for large language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Retentive Network", "type": "Model", "metadata": []}, "target": {"id": "Transformer", "type": "Model", "metadata": []}, "type": "EXTENDS"}, {"source": {"id": "Retentive Network", "type": "Model", "metadata": []}, "target": {"id": "Large Language Models", "type": "Concept", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "Gemma Team", "type": "Organization", "metadata": []}, "target": {"id": "Gemma 2: Improving open language models at a practical size.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Gemma2", "type": "Model", "metadata": []}, "target": {"id": "Large Language Models", "type": "Concept", "metadata": []}, "type": "EXTENDS"}, {"source": {"id": "Gemma Team", "type": "Organization", "metadata": []}, "target": {"id": "Gemma 3 technical report.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Ling Team", "type": "Organization", "metadata": []}, "target": {"id": "Every attention matters: An efficient hybrid architecture for long-context reasoning.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Bin Han", "type": "Person", "metadata": []}, "target": {"id": "Every attention matters: An efficient hybrid architecture for long-context reasoning.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Caizhi Tang", "type": "Person", "metadata": []}, "target": {"id": "Every attention matters: An efficient hybrid architecture for long-context reasoning.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Chen Liang", "type": "Person", "metadata": []}, "target": {"id": "Every attention matters: An efficient hybrid architecture for long-context reasoning.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Donghao Zhang", "type": "Person", "metadata": []}, "target": {"id": "Every attention matters: An efficient hybrid architecture for long-context reasoning.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Fan Yuan", "type": "Person", "metadata": []}, "target": {"id": "Every attention matters: An efficient hybrid architecture for long-context reasoning.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Feng Zhu", "type": "Person", "metadata": []}, "target": {"id": "Every attention matters: An efficient hybrid architecture for long-context reasoning.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Jie Gao", "type": "Person", "metadata": []}, "target": {"id": "Every attention matters: An efficient hybrid architecture for long-context reasoning.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Jingyu Hu", "type": "Person", "metadata": []}, "target": {"id": "Every attention matters: An efficient hybrid architecture for long-context reasoning.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Longfei Li", "type": "Person", "metadata": []}, "target": {"id": "Every attention matters: An efficient hybrid architecture for long-context reasoning.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Meng Li", "type": "Person", "metadata": []}, "target": {"id": "Every attention matters: An efficient hybrid architecture for long-context reasoning.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Mingyang Zhang", "type": "Person", "metadata": []}, "target": {"id": "Every attention matters: An efficient hybrid architecture for long-context reasoning.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Peijie Jiang", "type": "Person", "metadata": []}, "target": {"id": "Every attention matters: An efficient hybrid architecture for long-context reasoning.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Peng Jiao", "type": "Person", "metadata": []}, "target": {"id": "Every attention matters: An efficient hybrid architecture for long-context reasoning.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Qian Zhao", "type": "Person", "metadata": []}, "target": {"id": "Every attention matters: An efficient hybrid architecture for long-context reasoning.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Qingyuan Yang", "type": "Person", "metadata": []}, "target": {"id": "Every attention matters: An efficient hybrid architecture for long-context reasoning.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Wenbo Shen", "type": "Person", "metadata": []}, "target": {"id": "Every attention matters: An efficient hybrid architecture for long-context reasoning.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Xinxing Yang", "type": "Person", "metadata": []}, "target": {"id": "Every attention matters: An efficient hybrid architecture for long-context reasoning.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Yalin Zhang", "type": "Person", "metadata": []}, "target": {"id": "Every attention matters: An efficient hybrid architecture for long-context reasoning.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Yankun Ren", "type": "Person", "metadata": []}, "target": {"id": "Every attention matters: An efficient hybrid architecture for long-context reasoning.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Yao Zhao", "type": "Person", "metadata": []}, "target": {"id": "Every attention matters: An efficient hybrid architecture for long-context reasoning.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Yibo Cao", "type": "Person", "metadata": []}, "target": {"id": "Every attention matters: An efficient hybrid architecture for long-context reasoning.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Yixuan Sun", "type": "Person", "metadata": []}, "target": {"id": "Every attention matters: An efficient hybrid architecture for long-context reasoning.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Yue Zhang", "type": "Person", "metadata": []}, "target": {"id": "Every attention matters: An efficient hybrid architecture for long-context reasoning.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Yuchen Fang", "type": "Person", "metadata": []}, "target": {"id": "Every attention matters: An efficient hybrid architecture for long-context reasoning.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Zibin Lin", "type": "Person", "metadata": []}, "target": {"id": "Every attention matters: An efficient hybrid architecture for long-context reasoning.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Zixuan Cheng", "type": "Person", "metadata": []}, "target": {"id": "Every attention matters: An efficient hybrid architecture for long-context reasoning.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Jun Zhou", "type": "Person", "metadata": []}, "target": {"id": "Every attention matters: An efficient hybrid architecture for long-context reasoning.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Every attention matters: An efficient hybrid architecture for long-context reasoning.", "type": "Publication", "metadata": []}, "target": {"id": "Long-context reasoning", "type": "Concept", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "Every attention matters: An efficient hybrid architecture for long-context reasoning.", "type": "Publication", "metadata": []}, "target": {"id": "Model Architectures", "type": "Concept", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Llama Team", "type": "Organization", "metadata": []}, "target": {"id": "The llama 3 herd of models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Qwen3 Team", "type": "Organization", "metadata": []}, "target": {"id": "Qwen3 technical report.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Hugo Touvron", "type": "Person", "metadata": []}, "target": {"id": "Llama: Open and efficient foundation language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Thibaut Lavril", "type": "Person", "metadata": []}, "target": {"id": "Llama: Open and efficient foundation language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Gautier Izacard", "type": "Person", "metadata": []}, "target": {"id": "Llama: Open and efficient foundation language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Xavier Martinet", "type": "Person", "metadata": []}, "target": {"id": "Llama: Open and efficient foundation language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Marie-Anne Lachaux", "type": "Person", "metadata": []}, "target": {"id": "Llama: Open and efficient foundation language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Timoth\u00e9e Lacroix", "type": "Person", "metadata": []}, "target": {"id": "Llama: Open and efficient foundation language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Baptiste Rozi\u00e8re", "type": "Person", "metadata": []}, "target": {"id": "Llama: Open and efficient foundation language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Naman Goyal", "type": "Person", "metadata": []}, "target": {"id": "Llama: Open and efficient foundation language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Eric Hambro", "type": "Person", "metadata": []}, "target": {"id": "Llama: Open and efficient foundation language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Faisal Azhar", "type": "Person", "metadata": []}, "target": {"id": "Llama: Open and efficient foundation language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Aurelien Rodriguez", "type": "Person", "metadata": []}, "target": {"id": "Llama: Open and efficient foundation language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Armand Joulin", "type": "Person", "metadata": []}, "target": {"id": "Llama: Open and efficient foundation language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Edouard Grave", "type": "Person", "metadata": []}, "target": {"id": "Llama: Open and efficient foundation language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Guillaume Lample", "type": "Person", "metadata": []}, "target": {"id": "Llama: Open and efficient foundation language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Llama", "type": "Model", "metadata": []}, "target": {"id": "Large Language Models", "type": "Concept", "metadata": []}, "type": "EXTENDS"}, {"source": {"id": "Ashish Vaswani", "type": "Person", "metadata": []}, "target": {"id": "Attention is all you need.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Noam Shazeer", "type": "Person", "metadata": []}, "target": {"id": "Attention is all you need.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Niki Parmar", "type": "Person", "metadata": []}, "target": {"id": "Attention is all you need.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Jakob Uszkoreit", "type": "Person", "metadata": []}, "target": {"id": "Attention is all you need.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Llion Jones", "type": "Person", "metadata": []}, "target": {"id": "Attention is all you need.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Aidan N. Gomez", "type": "Person", "metadata": []}, "target": {"id": "Attention is all you need.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "\u0141ukasz Kaiser", "type": "Person", "metadata": []}, "target": {"id": "Attention is all you need.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Illia Polosukhin", "type": "Person", "metadata": []}, "target": {"id": "Attention is all you need.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Attention is all you need.", "type": "Publication", "metadata": []}, "target": {"id": "Transformer", "type": "Model", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Bailin Wang", "type": "Person", "metadata": []}, "target": {"id": "Rattention: Towards the minimal sliding window size in local-global attention models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Chang Lan", "type": "Person", "metadata": []}, "target": {"id": "Rattention: Towards the minimal sliding window size in local-global attention models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Chong Wang", "type": "Person", "metadata": []}, "target": {"id": "Rattention: Towards the minimal sliding window size in local-global attention models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Ruoming Pang", "type": "Person", "metadata": []}, "target": {"id": "Rattention: Towards the minimal sliding window size in local-global attention models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Rattention", "type": "Model", "metadata": []}, "target": {"id": "Sliding Window Attention", "type": "Concept", "metadata": []}, "type": "EXTENDS"}, {"source": {"id": "Rattention", "type": "Model", "metadata": []}, "target": {"id": "Attention mechanism", "type": "Concept", "metadata": []}, "type": "EXTENDS"}, {"source": {"id": "Jason Wei", "type": "Person", "metadata": []}, "target": {"id": "Chain-of-thought prompting elicits reasoning in large language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Xuezhi Wang", "type": "Person", "metadata": []}, "target": {"id": "Chain-of-thought prompting elicits reasoning in large language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Dale Schuurmans", "type": "Person", "metadata": []}, "target": {"id": "Chain-of-thought prompting elicits reasoning in large language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Maarten Bosma", "type": "Person", "metadata": []}, "target": {"id": "Chain-of-thought prompting elicits reasoning in large language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Brian Ichter", "type": "Person", "metadata": []}, "target": {"id": "Chain-of-thought prompting elicits reasoning in large language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Fei Xia", "type": "Person", "metadata": []}, "target": {"id": "Chain-of-thought prompting elicits reasoning in large language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Ed H. Chi", "type": "Person", "metadata": []}, "target": {"id": "Chain-of-thought prompting elicits reasoning in large language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Quoc V. Le", "type": "Person", "metadata": []}, "target": {"id": "Chain-of-thought prompting elicits reasoning in large language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Denny Zhou", "type": "Person", "metadata": []}, "target": {"id": "Chain-of-thought prompting elicits reasoning in large language models.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Chain of Thought (CoT) prompting", "type": "Concept", "metadata": []}, "target": {"id": "Large Language Models", "type": "Concept", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "Chain of Thought (CoT) prompting", "type": "Concept", "metadata": []}, "target": {"id": "Reasoning", "type": "Concept", "metadata": []}, "type": "ADDRESSES"}, {"source": {"id": "Di Wu", "type": "Person", "metadata": []}, "target": {"id": "Longmemeval: Benchmarking chat assistants on long-term interactive memory.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Hongwei Wang", "type": "Person", "metadata": []}, "target": {"id": "Longmemeval: Benchmarking chat assistants on long-term interactive memory.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Wenhao Yu", "type": "Person", "metadata": []}, "target": {"id": "Longmemeval: Benchmarking chat assistants on long-term interactive memory.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Yuwei Zhang", "type": "Person", "metadata": []}, "target": {"id": "Longmemeval: Benchmarking chat assistants on long-term interactive memory.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Kai-Wei Chang", "type": "Person", "metadata": []}, "target": {"id": "Longmemeval: Benchmarking chat assistants on long-term interactive memory.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Dong Yu", "type": "Person", "metadata": []}, "target": {"id": "Longmemeval: Benchmarking chat assistants on long-term interactive memory.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "LongMemEval", "type": "Task", "metadata": []}, "target": {"id": "Long-term interactive memory", "type": "Concept", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "LongMemEval", "type": "Task", "metadata": []}, "target": {"id": "Large Language Models", "type": "Concept", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "Guangxuan Xiao", "type": "Person", "metadata": []}, "target": {"id": "Why stacking sliding windows can\u2019t see very far.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Why stacking sliding windows can\u2019t see very far.", "type": "Publication", "metadata": []}, "target": {"id": "Sliding Window Attention", "type": "Concept", "metadata": []}, "type": "ADDRESSES"}, {"source": {"id": "Guangxuan Xiao", "type": "Person", "metadata": []}, "target": {"id": "Efficient streaming language models with attention sinks.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Yuandong Tian", "type": "Person", "metadata": []}, "target": {"id": "Efficient streaming language models with attention sinks.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Beidi Chen", "type": "Person", "metadata": []}, "target": {"id": "Efficient streaming language models with attention sinks.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Song Han", "type": "Person", "metadata": []}, "target": {"id": "Efficient streaming language models with attention sinks.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Mike Lewis", "type": "Person", "metadata": []}, "target": {"id": "Efficient streaming language models with attention sinks.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Attention sinks", "type": "Concept", "metadata": []}, "target": {"id": "Large Language Models", "type": "Concept", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "Zhaorui Yang", "type": "Person", "metadata": []}, "target": {"id": "Self-distillation bridges distribution gap in language model fine-tuning.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Tianyu Pang", "type": "Person", "metadata": []}, "target": {"id": "Self-distillation bridges distribution gap in language model fine-tuning.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Haozhe Feng", "type": "Person", "metadata": []}, "target": {"id": "Self-distillation bridges distribution gap in language model fine-tuning.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Han Wang", "type": "Person", "metadata": []}, "target": {"id": "Self-distillation bridges distribution gap in language model fine-tuning.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Wei Chen", "type": "Person", "metadata": []}, "target": {"id": "Self-distillation bridges distribution gap in language model fine-tuning.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Minfeng Zhu", "type": "Person", "metadata": []}, "target": {"id": "Self-distillation bridges distribution gap in language model fine-tuning.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Qian Liu", "type": "Person", "metadata": []}, "target": {"id": "Self-distillation bridges distribution gap in language model fine-tuning.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Self-distillation", "type": "Algorithm", "metadata": []}, "target": {"id": "Language model fine-tuning", "type": "Task", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "Jingyang Yuan", "type": "Person", "metadata": []}, "target": {"id": "Native sparse attention: Hardware-aligned and natively trainable sparse attention.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Huazuo Gao", "type": "Person", "metadata": []}, "target": {"id": "Native sparse attention: Hardware-aligned and natively trainable sparse attention.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Damai Dai", "type": "Person", "metadata": []}, "target": {"id": "Native sparse attention: Hardware-aligned and natively trainable sparse attention.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Junyu Luo", "type": "Person", "metadata": []}, "target": {"id": "Native sparse attention: Hardware-aligned and natively trainable sparse attention.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Liang Zhao", "type": "Person", "metadata": []}, "target": {"id": "Native sparse attention: Hardware-aligned and natively trainable sparse attention.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Zhengyan Zhang", "type": "Person", "metadata": []}, "target": {"id": "Native sparse attention: Hardware-aligned and natively trainable sparse attention.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Zhenda Xie", "type": "Person", "metadata": []}, "target": {"id": "Native sparse attention: Hardware-aligned and natively trainable sparse attention.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Yuxing Wei", "type": "Person", "metadata": []}, "target": {"id": "Native sparse attention: Hardware-aligned and natively trainable sparse attention.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Lean Wang", "type": "Person", "metadata": []}, "target": {"id": "Native sparse attention: Hardware-aligned and natively trainable sparse attention.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Zhiping Xiao", "type": "Person", "metadata": []}, "target": {"id": "Native sparse attention: Hardware-aligned and natively trainable sparse attention.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Yuqing Wang", "type": "Person", "metadata": []}, "target": {"id": "Native sparse attention: Hardware-aligned and natively trainable sparse attention.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Chong Ruan", "type": "Person", "metadata": []}, "target": {"id": "Native sparse attention: Hardware-aligned and natively trainable sparse attention.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Ming Zhang", "type": "Person", "metadata": []}, "target": {"id": "Native sparse attention: Hardware-aligned and natively trainable sparse attention.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Wenfeng Liang", "type": "Person", "metadata": []}, "target": {"id": "Native sparse attention: Hardware-aligned and natively trainable sparse attention.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Wangding Zeng", "type": "Person", "metadata": []}, "target": {"id": "Native sparse attention: Hardware-aligned and natively trainable sparse attention.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Native Sparse Attention", "type": "Concept", "metadata": []}, "target": {"id": "Sparse attention", "type": "Concept", "metadata": []}, "type": "EXTENDS"}, {"source": {"id": "Manzil Zaheer", "type": "Person", "metadata": []}, "target": {"id": "Big bird: Transformers for longer sequences.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Guru Guruganesh", "type": "Person", "metadata": []}, "target": {"id": "Big bird: Transformers for longer sequences.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Kumar Avinava Dubey", "type": "Person", "metadata": []}, "target": {"id": "Big bird: Transformers for longer sequences.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Joshua Ainslie", "type": "Person", "metadata": []}, "target": {"id": "Big bird: Transformers for longer sequences.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Chris Alberti", "type": "Person", "metadata": []}, "target": {"id": "Big bird: Transformers for longer sequences.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Santiago Onta\u00f1\u00f3n", "type": "Person", "metadata": []}, "target": {"id": "Big bird: Transformers for longer sequences.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Philip Pham", "type": "Person", "metadata": []}, "target": {"id": "Big bird: Transformers for longer sequences.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Anirudh Ravula", "type": "Person", "metadata": []}, "target": {"id": "Big bird: Transformers for longer sequences.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Qifan Wang", "type": "Person", "metadata": []}, "target": {"id": "Big bird: Transformers for longer sequences.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Li Yang", "type": "Person", "metadata": []}, "target": {"id": "Big bird: Transformers for longer sequences.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Amr Ahmed", "type": "Person", "metadata": []}, "target": {"id": "Big bird: Transformers for longer sequences.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "BigBird", "type": "Model", "metadata": []}, "target": {"id": "Transformer", "type": "Model", "metadata": []}, "type": "EXTENDS"}, {"source": {"id": "Xuan Zhang", "type": "Person", "metadata": []}, "target": {"id": "Light-transfer: Your long-context llm is secretly a hybrid model with effortless adaptation.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Fengzhuo Zhang", "type": "Person", "metadata": []}, "target": {"id": "Light-transfer: Your long-context llm is secretly a hybrid model with effortless adaptation.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Cunxiao Du", "type": "Person", "metadata": []}, "target": {"id": "Light-transfer: Your long-context llm is secretly a hybrid model with effortless adaptation.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Chao Du", "type": "Person", "metadata": []}, "target": {"id": "Light-transfer: Your long-context llm is secretly a hybrid model with effortless adaptation.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Wei Gao", "type": "Person", "metadata": []}, "target": {"id": "Light-transfer: Your long-context llm is secretly a hybrid model with effortless adaptation.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Min Lin", "type": "Person", "metadata": []}, "target": {"id": "Light-transfer: Your long-context llm is secretly a hybrid model with effortless adaptation.", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "LightTransfer", "type": "Model", "metadata": []}, "target": {"id": "Long context processing", "type": "Concept", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "LightTransfer", "type": "Model", "metadata": []}, "target": {"id": "Large Language Models", "type": "Concept", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "Sliding Window Attention", "type": "Concept", "metadata": []}, "target": {"id": "Computational complexity", "type": "Concept", "metadata": []}, "type": "MITIGATES"}, {"source": {"id": "Sliding Window Attention", "type": "Concept", "metadata": []}, "target": {"id": "GPU memory", "type": "Concept", "metadata": []}, "type": "MITIGATES"}, {"source": {"id": "Sliding Window Attention", "type": "Concept", "metadata": []}, "target": {"id": "KV cache reusability", "type": "Concept", "metadata": []}, "type": "SUPPORTS"}, {"source": {"id": "FA Decode", "type": "Concept", "metadata": []}, "target": {"id": "Prefilling", "type": "Concept", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "FA Decode", "type": "Concept", "metadata": []}, "target": {"id": "Decoding speed", "type": "Concept", "metadata": []}, "type": "ADDRESSES"}, {"source": {"id": "FA Decode", "type": "Concept", "metadata": []}, "target": {"id": "Full Attention", "type": "Concept", "metadata": []}, "type": "BUILDS_ON"}, {"source": {"id": "Distributed LLM services", "type": "Concept", "metadata": []}, "target": {"id": "Prefilling", "type": "Concept", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "Keep First", "type": "Concept", "metadata": []}, "target": {"id": "Computational overhead", "type": "Concept", "metadata": []}, "type": "ADDRESSES"}, {"source": {"id": "Positional Encoding", "type": "Concept", "metadata": []}, "target": {"id": "Keep First", "type": "Concept", "metadata": []}, "type": "INSPIRED_BY"}, {"source": {"id": "Interleaving Layers", "type": "Concept", "metadata": []}, "target": {"id": "Computational overhead", "type": "Concept", "metadata": []}, "type": "ADDRESSES"}, {"source": {"id": "Interleaving Layers", "type": "Concept", "metadata": []}, "target": {"id": "KV cache reusability", "type": "Concept", "metadata": []}, "type": "MITIGATES"}, {"source": {"id": "Chain of Thought (CoT) format", "type": "Concept", "metadata": []}, "target": {"id": "Long-context tasks", "type": "Concept", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "LongBench", "type": "Task", "metadata": []}, "target": {"id": "Long-context benchmarks", "type": "Concept", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "LongBench", "type": "Task", "metadata": []}, "target": {"id": "Bai et al., 2023", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Ruler", "type": "Model", "metadata": []}, "target": {"id": "Long-context benchmarks", "type": "Concept", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "Ruler", "type": "Model", "metadata": []}, "target": {"id": "LongBench", "type": "Task", "metadata": []}, "type": "COMPARED_WITH"}, {"source": {"id": "Ruler", "type": "Model", "metadata": []}, "target": {"id": "Hsieh et al., 2024", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Ruler", "type": "Model", "metadata": []}, "target": {"id": "Needle-retrieval tasks", "type": "Task", "metadata": []}, "type": "APPLIED_TO"}], "source": {"source_id": "sliding_window_attention_raw_with_image_ids_with_captions_chunks_5k.jsonl::sliding_window_attention_raw_with_image_ids_with_captions_chunks_5k_2", "source_type": "chunk", "metadata": [{"key": "chunk_file", "value": "sliding_window_attention_raw_with_image_ids_with_captions_chunks_5k.jsonl"}, {"key": "chunk_id", "value": "sliding_window_attention_raw_with_image_ids_with_captions_chunks_5k_2"}, {"key": "chunk_index", "value": "2"}]}}
{"nodes": [{"id": "distributed LLM services", "type": "Concept", "metadata": []}, {"id": "KV cache", "type": "Concept", "metadata": []}, {"id": "Prefilling", "type": "Concept", "metadata": []}, {"id": "Keep First", "type": "Algorithm", "metadata": []}, {"id": "computational overhead", "type": "Concept", "metadata": []}, {"id": "KV cache reuse", "type": "Concept", "metadata": []}, {"id": "Positional Encoding", "type": "Concept", "metadata": []}, {"id": "Interleaving Layers", "type": "Algorithm", "metadata": []}, {"id": "SWA", "type": "Model", "metadata": []}, {"id": "Chain of Thought (CoT) format", "type": "Concept", "metadata": []}, {"id": "generation length", "type": "Concept", "metadata": []}, {"id": "decoding speed", "type": "Concept", "metadata": []}, {"id": "LongBench", "type": "Task", "metadata": []}, {"id": "Bai et al., 2023", "type": "Publication", "metadata": []}, {"id": "modern models", "type": "Model", "metadata": []}, {"id": "Ruler", "type": "Task", "metadata": []}, {"id": "Hsieh et al., 2024", "type": "Publication", "metadata": []}, {"id": "needle-retrieval tasks", "type": "Task", "metadata": []}, {"id": "real-world scenarios", "type": "Concept", "metadata": []}, {"id": "LongBench-V2", "type": "Task", "metadata": []}, {"id": "Bai et al., 2024b", "type": "Publication", "metadata": []}, {"id": "deep understanding and reasoning", "type": "Concept", "metadata": []}, {"id": "4B-level Models", "type": "Model", "metadata": []}, {"id": "Qwen3-4B-Thinking", "type": "Model", "metadata": []}, {"id": "multiple-choice question format", "type": "Concept", "metadata": []}, {"id": "Qwen3-30B-A3B-Thinking", "type": "Model", "metadata": []}, {"id": "Qwen3-30B-A3B-Instruct", "type": "Model", "metadata": []}, {"id": "LongMemEval_24k", "type": "Task", "metadata": []}, {"id": "Llama3.1-8B-Instruct", "type": "Model", "metadata": []}, {"id": "Qwen3-30B-A3B", "type": "Model", "metadata": []}, {"id": "Llama3.1", "type": "Model", "metadata": []}, {"id": "Qwen3-4B", "type": "Model", "metadata": []}, {"id": "vLLM", "type": "Model", "metadata": []}, {"id": "Inference", "type": "Concept", "metadata": []}, {"id": "FA Decode", "type": "Algorithm", "metadata": []}, {"id": "Keep First k Tokens", "type": "Algorithm", "metadata": []}, {"id": "efficiency", "type": "Concept", "metadata": []}, {"id": "window size", "type": "Concept", "metadata": []}, {"id": "GPU utilization", "type": "Concept", "metadata": []}, {"id": "Prefilling stage", "type": "Concept", "metadata": []}, {"id": "LightTransfer", "type": "Algorithm", "metadata": []}, {"id": "Zhang et al., 2024", "type": "Publication", "metadata": []}, {"id": "SWA adaptation", "type": "Algorithm", "metadata": []}, {"id": "Full Attention models", "type": "Model", "metadata": []}, {"id": "layer selection method", "type": "Concept", "metadata": []}, {"id": "lazy ratio", "type": "Concept", "metadata": []}, {"id": "LongAlign", "type": "Task", "metadata": []}, {"id": "Bai et al., 2024a", "type": "Publication", "metadata": []}, {"id": "fixed-interval selection", "type": "Algorithm", "metadata": []}, {"id": "Qwen3-30B", "type": "Model", "metadata": []}, {"id": "Llama3.1-8B", "type": "Model", "metadata": []}], "relationships": [{"source": {"id": "distributed LLM services", "type": "Concept", "metadata": []}, "target": {"id": "KV cache", "type": "Concept", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "Prefilling", "type": "Concept", "metadata": []}, "target": {"id": "distributed LLM services", "type": "Concept", "metadata": []}, "type": "OCCURS_IN"}, {"source": {"id": "Keep First", "type": "Algorithm", "metadata": []}, "target": {"id": "computational overhead", "type": "Concept", "metadata": []}, "type": "EXTENDS"}, {"source": {"id": "Keep First", "type": "Algorithm", "metadata": []}, "target": {"id": "KV cache reuse", "type": "Concept", "metadata": []}, "type": "ADDRESSES"}, {"source": {"id": "Positional Encoding", "type": "Concept", "metadata": []}, "target": {"id": "KV cache reuse", "type": "Concept", "metadata": []}, "type": "HINDERS"}, {"source": {"id": "Interleaving Layers", "type": "Algorithm", "metadata": []}, "target": {"id": "computational overhead", "type": "Concept", "metadata": []}, "type": "EXTENDS"}, {"source": {"id": "Interleaving Layers", "type": "Algorithm", "metadata": []}, "target": {"id": "SWA", "type": "Model", "metadata": []}, "type": "COMPARED_WITH"}, {"source": {"id": "Interleaving Layers", "type": "Algorithm", "metadata": []}, "target": {"id": "KV cache reuse", "type": "Concept", "metadata": []}, "type": "ADDRESSES"}, {"source": {"id": "Chain of Thought (CoT) format", "type": "Concept", "metadata": []}, "target": {"id": "generation length", "type": "Concept", "metadata": []}, "type": "EXTENDS"}, {"source": {"id": "Chain of Thought (CoT) format", "type": "Concept", "metadata": []}, "target": {"id": "decoding speed", "type": "Concept", "metadata": []}, "type": "EXTENDS"}, {"source": {"id": "LongBench", "type": "Task", "metadata": []}, "target": {"id": "Bai et al., 2023", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "LongBench", "type": "Task", "metadata": []}, "target": {"id": "modern models", "type": "Model", "metadata": []}, "type": "COMPARED_WITH"}, {"source": {"id": "Ruler", "type": "Task", "metadata": []}, "target": {"id": "Hsieh et al., 2024", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Ruler", "type": "Task", "metadata": []}, "target": {"id": "needle-retrieval tasks", "type": "Task", "metadata": []}, "type": "ADDRESSES"}, {"source": {"id": "Ruler", "type": "Task", "metadata": []}, "target": {"id": "real-world scenarios", "type": "Concept", "metadata": []}, "type": "COMPARED_WITH"}, {"source": {"id": "LongBench-V2", "type": "Task", "metadata": []}, "target": {"id": "Bai et al., 2024b", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "LongBench-V2", "type": "Task", "metadata": []}, "target": {"id": "deep understanding and reasoning", "type": "Concept", "metadata": []}, "type": "ADDRESSES"}, {"source": {"id": "LongBench-V2", "type": "Task", "metadata": []}, "target": {"id": "4B-level Models", "type": "Model", "metadata": []}, "type": "COMPARED_WITH"}, {"source": {"id": "Qwen3-4B-Thinking", "type": "Model", "metadata": []}, "target": {"id": "LongBench-V2", "type": "Task", "metadata": []}, "type": "COMPARED_WITH"}, {"source": {"id": "LongBench-V2", "type": "Task", "metadata": []}, "target": {"id": "multiple-choice question format", "type": "Concept", "metadata": []}, "type": "HAS_FORMAT"}, {"source": {"id": "Qwen3-30B-A3B-Thinking", "type": "Model", "metadata": []}, "target": {"id": "LongMemEval_24k", "type": "Task", "metadata": []}, "type": "COMPARED_WITH"}, {"source": {"id": "Qwen3-30B-A3B-Instruct", "type": "Model", "metadata": []}, "target": {"id": "LongMemEval_24k", "type": "Task", "metadata": []}, "type": "COMPARED_WITH"}, {"source": {"id": "Llama3.1-8B-Instruct", "type": "Model", "metadata": []}, "target": {"id": "LongMemEval_24k", "type": "Task", "metadata": []}, "type": "COMPARED_WITH"}, {"source": {"id": "Qwen3-30B-A3B", "type": "Model", "metadata": []}, "target": {"id": "Llama3.1", "type": "Model", "metadata": []}, "type": "COMPARED_WITH"}, {"source": {"id": "LongBench-V2", "type": "Task", "metadata": []}, "target": {"id": "Bai et al., 2024b", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Qwen3-4B", "type": "Model", "metadata": []}, "target": {"id": "LongBench-V2", "type": "Task", "metadata": []}, "type": "COMPARED_WITH"}, {"source": {"id": "Qwen3-30B-A3B", "type": "Model", "metadata": []}, "target": {"id": "LongBench-V2", "type": "Task", "metadata": []}, "type": "COMPARED_WITH"}, {"source": {"id": "Llama3.1", "type": "Model", "metadata": []}, "target": {"id": "LongBench-V2", "type": "Task", "metadata": []}, "type": "COMPARED_WITH"}, {"source": {"id": "vLLM", "type": "Model", "metadata": []}, "target": {"id": "Inference", "type": "Concept", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "Interleaving Layers", "type": "Algorithm", "metadata": []}, "target": {"id": "Inference", "type": "Concept", "metadata": []}, "type": "EXTENDS"}, {"source": {"id": "FA Decode", "type": "Algorithm", "metadata": []}, "target": {"id": "Inference", "type": "Concept", "metadata": []}, "type": "EXTENDS"}, {"source": {"id": "Keep First k Tokens", "type": "Algorithm", "metadata": []}, "target": {"id": "efficiency", "type": "Concept", "metadata": []}, "type": "ADDRESSES"}, {"source": {"id": "window size", "type": "Concept", "metadata": []}, "target": {"id": "Inference", "type": "Concept", "metadata": []}, "type": "EXTENDS"}, {"source": {"id": "vLLM", "type": "Model", "metadata": []}, "target": {"id": "GPU utilization", "type": "Concept", "metadata": []}, "type": "IMPROVES"}, {"source": {"id": "vLLM", "type": "Model", "metadata": []}, "target": {"id": "Prefilling stage", "type": "Concept", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "LightTransfer", "type": "Algorithm", "metadata": []}, "target": {"id": "Zhang et al., 2024", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "LightTransfer", "type": "Algorithm", "metadata": []}, "target": {"id": "SWA adaptation", "type": "Algorithm", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "LightTransfer", "type": "Algorithm", "metadata": []}, "target": {"id": "Full Attention models", "type": "Model", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "LightTransfer", "type": "Algorithm", "metadata": []}, "target": {"id": "layer selection method", "type": "Concept", "metadata": []}, "type": "PROPOSES"}, {"source": {"id": "LightTransfer", "type": "Algorithm", "metadata": []}, "target": {"id": "lazy ratio", "type": "Concept", "metadata": []}, "type": "USES"}, {"source": {"id": "LongAlign", "type": "Task", "metadata": []}, "target": {"id": "Bai et al., 2024a", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "LightTransfer", "type": "Algorithm", "metadata": []}, "target": {"id": "LongAlign", "type": "Task", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "LightTransfer", "type": "Algorithm", "metadata": []}, "target": {"id": "Qwen3-4B", "type": "Model", "metadata": []}, "type": "COMPARED_WITH"}, {"source": {"id": "LightTransfer", "type": "Algorithm", "metadata": []}, "target": {"id": "Qwen3-30B", "type": "Model", "metadata": []}, "type": "COMPARED_WITH"}, {"source": {"id": "LightTransfer", "type": "Algorithm", "metadata": []}, "target": {"id": "fixed-interval selection", "type": "Algorithm", "metadata": []}, "type": "COMPARED_WITH"}, {"source": {"id": "LightTransfer", "type": "Algorithm", "metadata": []}, "target": {"id": "Llama3.1-8B", "type": "Model", "metadata": []}, "type": "COMPARED_WITH"}], "source": {"source_id": "sliding_window_attention_raw_with_image_ids_with_captions_chunks_5k.jsonl::sliding_window_attention_raw_with_image_ids_with_captions_chunks_5k_3", "source_type": "chunk", "metadata": [{"key": "chunk_file", "value": "sliding_window_attention_raw_with_image_ids_with_captions_chunks_5k.jsonl"}, {"key": "chunk_id", "value": "sliding_window_attention_raw_with_image_ids_with_captions_chunks_5k_3"}, {"key": "chunk_index", "value": "3"}]}}
