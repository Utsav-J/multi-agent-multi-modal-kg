{"nodes": [{"id": "Yijiong Yu", "type": "Person", "metadata": []}, {"id": "Jiale Liu", "type": "Person", "metadata": []}, {"id": "Qingyun Wu", "type": "Person", "metadata": []}, {"id": "Huazheng Wang", "type": "Person", "metadata": []}, {"id": "Ji Pei", "type": "Person", "metadata": []}, {"id": "Oregon State University", "type": "Organization", "metadata": []}, {"id": "Penn State University", "type": "Organization", "metadata": []}, {"id": "DeepSolution", "type": "Organization", "metadata": []}, {"id": "Self-attention mechanism", "type": "Concept", "metadata": []}, {"id": "Transformer architecture", "type": "Model", "metadata": []}, {"id": "Large Language Models", "type": "Concept", "metadata": []}, {"id": "Sliding Window Attention", "type": "Concept", "metadata": []}, {"id": "training\u2013inference mismatch", "type": "Concept", "metadata": []}, {"id": "Sliding Window Attention Adaptation", "type": "Concept", "metadata": []}, {"id": "Full Attention Decode", "type": "Concept", "metadata": []}, {"id": "Keep First k Tokens", "type": "Concept", "metadata": []}, {"id": "Interleaving FA/SWA layers", "type": "Concept", "metadata": []}, {"id": "Chain-of-Thought", "type": "Concept", "metadata": []}, {"id": "Fine-tuning", "type": "Algorithm", "metadata": []}, {"id": "performance-efficiency trade-offs", "type": "Concept", "metadata": []}, {"id": "Vaswani et al., 2017", "type": "Publication", "metadata": []}, {"id": "Long context processing", "type": "Task", "metadata": []}, {"id": "Streaming Attention", "type": "Algorithm", "metadata": []}, {"id": "Xiao et al., 2024", "type": "Publication", "metadata": []}, {"id": "sink tokens", "type": "Concept", "metadata": []}, {"id": "Qwen3", "type": "Model", "metadata": []}, {"id": "Team, 2025b", "type": "Publication", "metadata": []}, {"id": "Full Attention", "type": "Concept", "metadata": []}, {"id": "FlashAttention", "type": "Model", "metadata": []}, {"id": "vLLM", "type": "Model", "metadata": []}, {"id": "Kwon et al., 2023", "type": "Publication", "metadata": []}, {"id": "Dao, 2024", "type": "Publication", "metadata": []}, {"id": "Sparse attention", "type": "Concept", "metadata": []}, {"id": "Linear attention", "type": "Concept", "metadata": []}, {"id": "Longformer", "type": "Model", "metadata": []}, {"id": "Beltagy et al., 2020", "type": "Publication", "metadata": []}, {"id": "BigBird", "type": "Model", "metadata": []}, {"id": "Zaheer et al., 2020", "type": "Publication", "metadata": []}, {"id": "RATTENTION", "type": "Model", "metadata": []}, {"id": "Wang et al., 2025", "type": "Publication", "metadata": []}, {"id": "Gemma2", "type": "Model", "metadata": []}, {"id": "Team, 2024a", "type": "Publication", "metadata": []}, {"id": "Sliding Window Attention Training", "type": "Concept", "metadata": []}, {"id": "Fu et al., 2025b", "type": "Publication", "metadata": []}, {"id": "Deepseek-sparse-attention", "type": "Model", "metadata": []}, {"id": "Yuan et al., 2025", "type": "Publication", "metadata": []}, {"id": "DeepSeek-AI, 2025b", "type": "Publication", "metadata": []}, {"id": "LightTransfer", "type": "Model", "metadata": []}, {"id": "Zhang et al., 2024", "type": "Publication", "metadata": []}, {"id": "RNN-like linear attention transformers", "type": "Model", "metadata": []}, {"id": "Katharopoulos et al., 2020", "type": "Publication", "metadata": []}, {"id": "Peng et al., 2023", "type": "Publication", "metadata": []}, {"id": "Sun et al., 2023", "type": "Publication", "metadata": []}, {"id": "structured state-space models", "type": "Model", "metadata": []}, {"id": "Mamba", "type": "Model", "metadata": []}, {"id": "Gu and Dao, 2023", "type": "Publication", "metadata": []}, {"id": "Jamba", "type": "Model", "metadata": []}, {"id": "Lieber et al., 2024", "type": "Publication", "metadata": []}, {"id": "Nemotron-Flash", "type": "Model", "metadata": []}, {"id": "Linsong Chu et al., 2024", "type": "Publication", "metadata": []}, {"id": "Team et al., 2025", "type": "Publication", "metadata": []}, {"id": "Fu et al., 2025a", "type": "Publication", "metadata": []}, {"id": "Prefill phase", "type": "Concept", "metadata": []}, {"id": "Decode phase", "type": "Concept", "metadata": []}, {"id": "human reading comprehension", "type": "Concept", "metadata": []}, {"id": "Flash-Attention-2", "type": "Model", "metadata": []}, {"id": "Gemma3", "type": "Model", "metadata": []}, {"id": "Team, 2025a", "type": "Publication", "metadata": []}, {"id": "Wei et al., 2022", "type": "Publication", "metadata": []}, {"id": "DeepSeek-R1", "type": "Model", "metadata": []}, {"id": "DeepSeek-AI, 2025a", "type": "Publication", "metadata": []}, {"id": "Qwen3-4B-Thinking", "type": "Model", "metadata": []}, {"id": "Qwen3-4B-Instruct", "type": "Model", "metadata": []}, {"id": "SWA-aware fine-tuning", "type": "Concept", "metadata": []}, {"id": "Self-distillation", "type": "Concept", "metadata": []}, {"id": "Yang et al., 2024", "type": "Publication", "metadata": []}, {"id": "GPT-5-Mini", "type": "Model", "metadata": []}, {"id": "OpenAI, 2025", "type": "Publication", "metadata": []}, {"id": "OpenAI", "type": "Organization", "metadata": []}, {"id": "long-context performance", "type": "Concept", "metadata": []}, {"id": "Qwen3-30B-A3B-Thinking", "type": "Model", "metadata": []}, {"id": "Qwen3-30B-A3B-Instruct", "type": "Model", "metadata": []}, {"id": "Llama3.1-8B-Instruct", "type": "Model", "metadata": []}, {"id": "Touvron et al., 2023", "type": "Publication", "metadata": []}, {"id": "HuggingFace Transformers", "type": "Model", "metadata": []}, {"id": "MMLU", "type": "Task", "metadata": []}, {"id": "Hendrycks et al., 2021", "type": "Publication", "metadata": []}, {"id": "GPQA", "type": "Task", "metadata": []}, {"id": "Rein et al., 2023", "type": "Publication", "metadata": []}, {"id": "LongMemEval", "type": "Task", "metadata": []}, {"id": "Wu et al., 2024", "type": "Publication", "metadata": []}, {"id": "long-context QA tasks", "type": "Task", "metadata": []}, {"id": "LongMemEval_24k", "type": "Task", "metadata": []}, {"id": "LongBench-V2", "type": "Task", "metadata": []}, {"id": "Bai et al., 2024b", "type": "Publication", "metadata": []}, {"id": "LongAlign", "type": "Task", "metadata": []}, {"id": "Bai et al., 2024a", "type": "Publication", "metadata": []}, {"id": "Fusang-v1-long", "type": "Task", "metadata": []}, {"id": "Pan, 2024", "type": "Publication", "metadata": []}, {"id": "LoRA", "type": "Algorithm", "metadata": []}, {"id": "Hu et al., 2022", "type": "Publication", "metadata": []}, {"id": "query", "type": "Concept", "metadata": []}, {"id": "key", "type": "Concept", "metadata": []}, {"id": "value", "type": "Concept", "metadata": []}], "relationships": [{"source": {"id": "Yijiong Yu", "type": "Person", "metadata": []}, "target": {"id": "Oregon State University", "type": "Organization", "metadata": []}, "type": "AFFILIATED_WITH"}, {"source": {"id": "Jiale Liu", "type": "Person", "metadata": []}, "target": {"id": "Penn State University", "type": "Organization", "metadata": []}, "type": "AFFILIATED_WITH"}, {"source": {"id": "Qingyun Wu", "type": "Person", "metadata": []}, "target": {"id": "Penn State University", "type": "Organization", "metadata": []}, "type": "AFFILIATED_WITH"}, {"source": {"id": "Huazheng Wang", "type": "Person", "metadata": []}, "target": {"id": "Oregon State University", "type": "Organization", "metadata": []}, "type": "AFFILIATED_WITH"}, {"source": {"id": "Ji Pei", "type": "Person", "metadata": []}, "target": {"id": "DeepSolution", "type": "Organization", "metadata": []}, "type": "AFFILIATED_WITH"}, {"source": {"id": "Self-attention mechanism", "type": "Concept", "metadata": []}, "target": {"id": "Large Language Models", "type": "Concept", "metadata": []}, "type": "BUILDS_ON"}, {"source": {"id": "Sliding Window Attention", "type": "Concept", "metadata": []}, "target": {"id": "Self-attention mechanism", "type": "Concept", "metadata": []}, "type": "MITIGATES"}, {"source": {"id": "Sliding Window Attention Adaptation", "type": "Concept", "metadata": []}, "target": {"id": "Full Attention Decode", "type": "Concept", "metadata": []}, "type": "COMBINES"}, {"source": {"id": "Sliding Window Attention Adaptation", "type": "Concept", "metadata": []}, "target": {"id": "Keep First k Tokens", "type": "Concept", "metadata": []}, "type": "COMBINES"}, {"source": {"id": "Sliding Window Attention Adaptation", "type": "Concept", "metadata": []}, "target": {"id": "Interleaving FA/SWA layers", "type": "Concept", "metadata": []}, "type": "COMBINES"}, {"source": {"id": "Sliding Window Attention Adaptation", "type": "Concept", "metadata": []}, "target": {"id": "Chain-of-Thought", "type": "Concept", "metadata": []}, "type": "COMBINES"}, {"source": {"id": "Sliding Window Attention Adaptation", "type": "Concept", "metadata": []}, "target": {"id": "Fine-tuning", "type": "Algorithm", "metadata": []}, "type": "COMBINES"}, {"source": {"id": "Transformer architecture", "type": "Model", "metadata": []}, "target": {"id": "Vaswani et al., 2017", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Sliding Window Attention", "type": "Concept", "metadata": []}, "target": {"id": "Long context processing", "type": "Task", "metadata": []}, "type": "ADDRESSES"}, {"source": {"id": "Streaming Attention", "type": "Algorithm", "metadata": []}, "target": {"id": "Xiao et al., 2024", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Streaming Attention", "type": "Algorithm", "metadata": []}, "target": {"id": "sink tokens", "type": "Concept", "metadata": []}, "type": "RETAINS"}, {"source": {"id": "Qwen3", "type": "Model", "metadata": []}, "target": {"id": "Team, 2025b", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Sliding Window Attention Adaptation", "type": "Concept", "metadata": []}, "target": {"id": "Full Attention", "type": "Concept", "metadata": []}, "type": "ADDRESSES"}, {"source": {"id": "Sliding Window Attention Adaptation", "type": "Concept", "metadata": []}, "target": {"id": "Sliding Window Attention", "type": "Concept", "metadata": []}, "type": "ADDRESSES"}, {"source": {"id": "Keep First k Tokens", "type": "Concept", "metadata": []}, "target": {"id": "Xiao et al., 2024", "type": "Publication", "metadata": []}, "type": "SUPPORTS"}, {"source": {"id": "Keep First k Tokens", "type": "Concept", "metadata": []}, "target": {"id": "Team, 2024a", "type": "Publication", "metadata": []}, "type": "SUPPORTS"}, {"source": {"id": "Keep First k Tokens", "type": "Concept", "metadata": []}, "target": {"id": "Zhang et al., 2024", "type": "Publication", "metadata": []}, "type": "SUPPORTS"}, {"source": {"id": "Interleaving FA/SWA layers", "type": "Concept", "metadata": []}, "target": {"id": "Xiao et al., 2024", "type": "Publication", "metadata": []}, "type": "SUPPORTS"}, {"source": {"id": "Interleaving FA/SWA layers", "type": "Concept", "metadata": []}, "target": {"id": "Team, 2024a", "type": "Publication", "metadata": []}, "type": "SUPPORTS"}, {"source": {"id": "Interleaving FA/SWA layers", "type": "Concept", "metadata": []}, "target": {"id": "Zhang et al., 2024", "type": "Publication", "metadata": []}, "type": "SUPPORTS"}, {"source": {"id": "Sliding Window Attention Adaptation", "type": "Concept", "metadata": []}, "target": {"id": "Qwen3", "type": "Model", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "Sliding Window Attention Adaptation", "type": "Concept", "metadata": []}, "target": {"id": "Llama3.1-8B-Instruct", "type": "Model", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "Sliding Window Attention Adaptation", "type": "Concept", "metadata": []}, "target": {"id": "FlashAttention", "type": "Model", "metadata": []}, "type": "IMPLEMENTS"}, {"source": {"id": "Sliding Window Attention Adaptation", "type": "Concept", "metadata": []}, "target": {"id": "vLLM", "type": "Model", "metadata": []}, "type": "IMPLEMENTS"}, {"source": {"id": "FlashAttention", "type": "Model", "metadata": []}, "target": {"id": "Dao, 2024", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "vLLM", "type": "Model", "metadata": []}, "target": {"id": "Kwon et al., 2023", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Sliding Window Attention", "type": "Concept", "metadata": []}, "target": {"id": "Sparse attention", "type": "Concept", "metadata": []}, "type": "IS_A"}, {"source": {"id": "Longformer", "type": "Model", "metadata": []}, "target": {"id": "Beltagy et al., 2020", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Longformer", "type": "Model", "metadata": []}, "target": {"id": "Sliding Window Attention", "type": "Concept", "metadata": []}, "type": "BUILDS_ON"}, {"source": {"id": "BigBird", "type": "Model", "metadata": []}, "target": {"id": "Zaheer et al., 2020", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "BigBird", "type": "Model", "metadata": []}, "target": {"id": "Sliding Window Attention", "type": "Concept", "metadata": []}, "type": "BUILDS_ON"}, {"source": {"id": "RATTENTION", "type": "Model", "metadata": []}, "target": {"id": "Wang et al., 2025", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "RATTENTION", "type": "Model", "metadata": []}, "target": {"id": "Sliding Window Attention", "type": "Concept", "metadata": []}, "type": "BUILDS_ON"}, {"source": {"id": "Gemma2", "type": "Model", "metadata": []}, "target": {"id": "Sliding Window Attention", "type": "Concept", "metadata": []}, "type": "ADOPTS"}, {"source": {"id": "Gemma2", "type": "Model", "metadata": []}, "target": {"id": "Team, 2024a", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Sliding Window Attention Training", "type": "Concept", "metadata": []}, "target": {"id": "Fu et al., 2025b", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Deepseek-sparse-attention", "type": "Model", "metadata": []}, "target": {"id": "Yuan et al., 2025", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Deepseek-sparse-attention", "type": "Model", "metadata": []}, "target": {"id": "DeepSeek-AI, 2025b", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "LightTransfer", "type": "Model", "metadata": []}, "target": {"id": "Sliding Window Attention", "type": "Concept", "metadata": []}, "type": "ADAPTS"}, {"source": {"id": "LightTransfer", "type": "Model", "metadata": []}, "target": {"id": "Zhang et al., 2024", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "RNN-like linear attention transformers", "type": "Model", "metadata": []}, "target": {"id": "Linear attention", "type": "Concept", "metadata": []}, "type": "IS_A"}, {"source": {"id": "RNN-like linear attention transformers", "type": "Model", "metadata": []}, "target": {"id": "Katharopoulos et al., 2020", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "RNN-like linear attention transformers", "type": "Model", "metadata": []}, "target": {"id": "Peng et al., 2023", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "RNN-like linear attention transformers", "type": "Model", "metadata": []}, "target": {"id": "Sun et al., 2023", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "structured state-space models", "type": "Model", "metadata": []}, "target": {"id": "Linear attention", "type": "Concept", "metadata": []}, "type": "IS_A"}, {"source": {"id": "Mamba", "type": "Model", "metadata": []}, "target": {"id": "structured state-space models", "type": "Model", "metadata": []}, "type": "IS_A"}, {"source": {"id": "Mamba", "type": "Model", "metadata": []}, "target": {"id": "Gu and Dao, 2023", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Jamba", "type": "Model", "metadata": []}, "target": {"id": "Lieber et al., 2024", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Jamba", "type": "Model", "metadata": []}, "target": {"id": "Linear attention", "type": "Concept", "metadata": []}, "type": "INTERLEAVES"}, {"source": {"id": "Nemotron-Flash", "type": "Model", "metadata": []}, "target": {"id": "Linsong Chu et al., 2024", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Nemotron-Flash", "type": "Model", "metadata": []}, "target": {"id": "Team et al., 2025", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Nemotron-Flash", "type": "Model", "metadata": []}, "target": {"id": "Fu et al., 2025a", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Nemotron-Flash", "type": "Model", "metadata": []}, "target": {"id": "Linear attention", "type": "Concept", "metadata": []}, "type": "INTERLEAVES"}, {"source": {"id": "Full Attention Decode", "type": "Concept", "metadata": []}, "target": {"id": "Sliding Window Attention", "type": "Concept", "metadata": []}, "type": "APPLIES_TO"}, {"source": {"id": "Full Attention Decode", "type": "Concept", "metadata": []}, "target": {"id": "Prefill phase", "type": "Concept", "metadata": []}, "type": "APPLIES_TO"}, {"source": {"id": "Full Attention Decode", "type": "Concept", "metadata": []}, "target": {"id": "Full Attention", "type": "Concept", "metadata": []}, "type": "APPLIES_TO"}, {"source": {"id": "Full Attention Decode", "type": "Concept", "metadata": []}, "target": {"id": "Decode phase", "type": "Concept", "metadata": []}, "type": "APPLIES_TO"}, {"source": {"id": "Full Attention Decode", "type": "Concept", "metadata": []}, "target": {"id": "human reading comprehension", "type": "Concept", "metadata": []}, "type": "INSPIRED_BY"}, {"source": {"id": "Full Attention Decode", "type": "Concept", "metadata": []}, "target": {"id": "Chain-of-Thought", "type": "Concept", "metadata": []}, "type": "SUGGESTS_BENEFIT_FROM"}, {"source": {"id": "Keep First k Tokens", "type": "Concept", "metadata": []}, "target": {"id": "Xiao et al., 2024", "type": "Publication", "metadata": []}, "type": "DEMONSTRATED_BY"}, {"source": {"id": "Streaming Attention", "type": "Algorithm", "metadata": []}, "target": {"id": "Keep First k Tokens", "type": "Concept", "metadata": []}, "type": "INVOLVES"}, {"source": {"id": "Flash-Attention-2", "type": "Model", "metadata": []}, "target": {"id": "Dao, 2024", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Gemma2", "type": "Model", "metadata": []}, "target": {"id": "Interleaving FA/SWA layers", "type": "Concept", "metadata": []}, "type": "USES"}, {"source": {"id": "Gemma3", "type": "Model", "metadata": []}, "target": {"id": "Interleaving FA/SWA layers", "type": "Concept", "metadata": []}, "type": "USES"}, {"source": {"id": "Gemma3", "type": "Model", "metadata": []}, "target": {"id": "Team, 2025a", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Chain-of-Thought", "type": "Concept", "metadata": []}, "target": {"id": "Wei et al., 2022", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "DeepSeek-R1", "type": "Model", "metadata": []}, "target": {"id": "Chain-of-Thought", "type": "Concept", "metadata": []}, "type": "EXEMPLIFIES"}, {"source": {"id": "DeepSeek-R1", "type": "Model", "metadata": []}, "target": {"id": "DeepSeek-AI, 2025a", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Qwen3-4B-Thinking", "type": "Model", "metadata": []}, "target": {"id": "Qwen3-4B-Instruct", "type": "Model", "metadata": []}, "type": "COMPARED_WITH"}, {"source": {"id": "Fine-tuning", "type": "Algorithm", "metadata": []}, "target": {"id": "training\u2013inference mismatch", "type": "Concept", "metadata": []}, "type": "MITIGATES"}, {"source": {"id": "SWA-aware fine-tuning", "type": "Concept", "metadata": []}, "target": {"id": "long-context QA tasks", "type": "Task", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "Self-distillation", "type": "Concept", "metadata": []}, "target": {"id": "Yang et al., 2024", "type": "Publication", "metadata": []}, "type": "INSPIRED_BY"}, {"source": {"id": "GPT-5-Mini", "type": "Model", "metadata": []}, "target": {"id": "OpenAI, 2025", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "GPT-5-Mini", "type": "Model", "metadata": []}, "target": {"id": "OpenAI", "type": "Organization", "metadata": []}, "type": "AFFILIATED_WITH"}, {"source": {"id": "SWA-aware fine-tuning", "type": "Concept", "metadata": []}, "target": {"id": "long-context performance", "type": "Concept", "metadata": []}, "type": "IMPROVES"}, {"source": {"id": "Qwen3-4B-Thinking", "type": "Model", "metadata": []}, "target": {"id": "Team, 2025b", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Qwen3-4B-Instruct", "type": "Model", "metadata": []}, "target": {"id": "Team, 2025b", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Chain-of-Thought", "type": "Concept", "metadata": []}, "target": {"id": "Qwen3-4B-Thinking", "type": "Model", "metadata": []}, "type": "ENFORCED_BY"}, {"source": {"id": "Qwen3-30B-A3B-Thinking", "type": "Model", "metadata": []}, "target": {"id": "Team, 2025b", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Qwen3-30B-A3B-Instruct", "type": "Model", "metadata": []}, "target": {"id": "Team, 2025b", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Llama3.1-8B-Instruct", "type": "Model", "metadata": []}, "target": {"id": "Touvron et al., 2023", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "HuggingFace Transformers", "type": "Model", "metadata": []}, "target": {"id": "vLLM", "type": "Model", "metadata": []}, "type": "COMPARED_WITH"}, {"source": {"id": "MMLU", "type": "Task", "metadata": []}, "target": {"id": "Hendrycks et al., 2021", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "GPQA", "type": "Task", "metadata": []}, "target": {"id": "Rein et al., 2023", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "LongMemEval", "type": "Task", "metadata": []}, "target": {"id": "Wu et al., 2024", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "LongMemEval", "type": "Task", "metadata": []}, "target": {"id": "long-context QA tasks", "type": "Task", "metadata": []}, "type": "IS_A"}, {"source": {"id": "LongMemEval_24k", "type": "Task", "metadata": []}, "target": {"id": "LongMemEval", "type": "Task", "metadata": []}, "type": "CONSTRUCTED_FROM"}, {"source": {"id": "LongBench-V2", "type": "Task", "metadata": []}, "target": {"id": "Bai et al., 2024b", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "LongAlign", "type": "Task", "metadata": []}, "target": {"id": "Bai et al., 2024a", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Fusang-v1-long", "type": "Task", "metadata": []}, "target": {"id": "Pan, 2024", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Fusang-v1-long", "type": "Task", "metadata": []}, "target": {"id": "LongAlign", "type": "Task", "metadata": []}, "type": "INCORPORATED_IN"}, {"source": {"id": "LoRA", "type": "Algorithm", "metadata": []}, "target": {"id": "Hu et al., 2022", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "LoRA", "type": "Algorithm", "metadata": []}, "target": {"id": "SWA-aware fine-tuning", "type": "Concept", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "query", "type": "Concept", "metadata": []}, "target": {"id": "LoRA", "type": "Algorithm", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "key", "type": "Concept", "metadata": []}, "target": {"id": "LoRA", "type": "Algorithm", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "value", "type": "Concept", "metadata": []}, "target": {"id": "LoRA", "type": "Algorithm", "metadata": []}, "type": "APPLIED_TO"}]}
{"nodes": [{"id": "LongBench-V2", "type": "Task", "metadata": []}, {"id": "Bai et al., 2024b", "type": "Publication", "metadata": []}, {"id": "Deep reasoning tasks", "type": "Task", "metadata": []}, {"id": "Real-world tasks", "type": "Task", "metadata": []}, {"id": "LongAlign", "type": "Concept", "metadata": []}, {"id": "Bai et al., 2024a", "type": "Publication", "metadata": []}, {"id": "Long-context fine-tuning", "type": "Concept", "metadata": []}, {"id": "Long-context tasks", "type": "Task", "metadata": []}, {"id": "Fusang-v1-long", "type": "Concept", "metadata": []}, {"id": "Pan, 2024", "type": "Publication", "metadata": []}, {"id": "SWA-aware fine-tuning", "type": "Concept", "metadata": []}, {"id": "LoRA", "type": "Algorithm", "metadata": []}, {"id": "Hu et al., 2022", "type": "Publication", "metadata": []}, {"id": "Qwen3-4B", "type": "Model", "metadata": []}, {"id": "Qwen3-30B-A3B", "type": "Model", "metadata": []}, {"id": "SWA adaptation", "type": "Concept", "metadata": []}, {"id": "LongMemEval_24k", "type": "Task", "metadata": []}, {"id": "Qwen3-4B-Thinking", "type": "Model", "metadata": []}, {"id": "Qwen3-4B-Instruct", "type": "Model", "metadata": []}, {"id": "Naive SWA", "type": "Concept", "metadata": []}, {"id": "Interleaving Layers", "type": "Concept", "metadata": []}, {"id": "Keep First k Tokens", "type": "Concept", "metadata": []}, {"id": "FA Decode", "type": "Concept", "metadata": []}, {"id": "Full Attention", "type": "Concept", "metadata": []}, {"id": "Sliding Window Attention", "type": "Concept", "metadata": []}, {"id": "Chain-of-Thought", "type": "Concept", "metadata": []}, {"id": "Llama3.1-8B-Instruct", "type": "Model", "metadata": []}, {"id": "SFT", "type": "Concept", "metadata": []}, {"id": "Performance\u2013efficiency Trade-offs", "type": "Concept", "metadata": []}, {"id": "Time-to-first-token", "type": "Concept", "metadata": []}, {"id": "Time-per-output-token", "type": "Concept", "metadata": []}, {"id": "Throughput", "type": "Concept", "metadata": []}, {"id": "Average running time per request", "type": "Concept", "metadata": []}, {"id": "vLLM", "type": "Model", "metadata": []}, {"id": "Kwon et al., 2023", "type": "Publication", "metadata": []}, {"id": "Wu et al., 2024", "type": "Publication", "metadata": []}, {"id": "Qwen", "type": "Model", "metadata": []}, {"id": "Llama", "type": "Model", "metadata": []}, {"id": "Catastrophic degradation", "type": "Concept", "metadata": []}, {"id": "GRPO", "type": "Algorithm", "metadata": []}, {"id": "Shao et al., 2024", "type": "Publication", "metadata": []}, {"id": "KV cache eviction", "type": "Concept", "metadata": []}, {"id": "LongBench", "type": "Task", "metadata": []}, {"id": "Bai et al., 2023", "type": "Publication", "metadata": []}, {"id": "Longformer", "type": "Model", "metadata": []}, {"id": "Beltagy et al., 2020", "type": "Publication", "metadata": []}, {"id": "Dao, 2024", "type": "Publication", "metadata": []}, {"id": "Flash-Attention-2", "type": "Model", "metadata": []}, {"id": "DeepSeek-R1", "type": "Model", "metadata": []}, {"id": "DeepSeek-AI, 2025a", "type": "Publication", "metadata": []}, {"id": "DeepSeek-v3.2", "type": "Model", "metadata": []}, {"id": "DeepSeek-AI, 2025b", "type": "Publication", "metadata": []}, {"id": "Large Language Models", "type": "Concept", "metadata": []}], "relationships": [{"source": {"id": "LongBench-V2", "type": "Task", "metadata": []}, "target": {"id": "Bai et al., 2024b", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "LongBench-V2", "type": "Task", "metadata": []}, "target": {"id": "Deep reasoning tasks", "type": "Task", "metadata": []}, "type": "ADDRESSES"}, {"source": {"id": "LongBench-V2", "type": "Task", "metadata": []}, "target": {"id": "Real-world tasks", "type": "Task", "metadata": []}, "type": "ADDRESSES"}, {"source": {"id": "LongAlign", "type": "Concept", "metadata": []}, "target": {"id": "Bai et al., 2024a", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "LongAlign", "type": "Concept", "metadata": []}, "target": {"id": "Long-context fine-tuning", "type": "Concept", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "LongAlign", "type": "Concept", "metadata": []}, "target": {"id": "Long-context tasks", "type": "Task", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "Fusang-v1-long", "type": "Concept", "metadata": []}, "target": {"id": "Pan, 2024", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Fusang-v1-long", "type": "Concept", "metadata": []}, "target": {"id": "LongAlign", "type": "Concept", "metadata": []}, "type": "EXTENDS"}, {"source": {"id": "SWA-aware fine-tuning", "type": "Concept", "metadata": []}, "target": {"id": "LoRA", "type": "Algorithm", "metadata": []}, "type": "BUILDS_ON"}, {"source": {"id": "LoRA", "type": "Algorithm", "metadata": []}, "target": {"id": "Hu et al., 2022", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Qwen3-4B", "type": "Model", "metadata": []}, "target": {"id": "LongMemEval_24k", "type": "Task", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "Qwen3-30B-A3B", "type": "Model", "metadata": []}, "target": {"id": "LongMemEval_24k", "type": "Task", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "Qwen3-4B-Thinking", "type": "Model", "metadata": []}, "target": {"id": "LongMemEval_24k", "type": "Task", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "Qwen3-4B-Instruct", "type": "Model", "metadata": []}, "target": {"id": "LongMemEval_24k", "type": "Task", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "Qwen3-4B-Thinking", "type": "Model", "metadata": []}, "target": {"id": "Qwen3-4B-Instruct", "type": "Model", "metadata": []}, "type": "COMPARED_WITH"}, {"source": {"id": "FA Decode", "type": "Concept", "metadata": []}, "target": {"id": "Chain-of-Thought", "type": "Concept", "metadata": []}, "type": "SUPPORTS"}, {"source": {"id": "Qwen3-4B-Thinking", "type": "Model", "metadata": []}, "target": {"id": "vLLM", "type": "Model", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "vLLM", "type": "Model", "metadata": []}, "target": {"id": "Kwon et al., 2023", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "LongMemEval_24k", "type": "Task", "metadata": []}, "target": {"id": "Wu et al., 2024", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Chain-of-Thought", "type": "Concept", "metadata": []}, "target": {"id": "SWA adaptation", "type": "Concept", "metadata": []}, "type": "SUPPORTS"}, {"source": {"id": "SWA adaptation", "type": "Concept", "metadata": []}, "target": {"id": "Sliding Window Attention", "type": "Concept", "metadata": []}, "type": "EXTENDS"}, {"source": {"id": "SWA adaptation", "type": "Concept", "metadata": []}, "target": {"id": "Catastrophic degradation", "type": "Concept", "metadata": []}, "type": "MITIGATES"}, {"source": {"id": "Qwen", "type": "Model", "metadata": []}, "target": {"id": "Llama", "type": "Model", "metadata": []}, "type": "COMPARED_WITH"}, {"source": {"id": "GRPO", "type": "Algorithm", "metadata": []}, "target": {"id": "Shao et al., 2024", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "LongBench", "type": "Task", "metadata": []}, "target": {"id": "Bai et al., 2023", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Longformer", "type": "Model", "metadata": []}, "target": {"id": "Beltagy et al., 2020", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Flash-Attention-2", "type": "Model", "metadata": []}, "target": {"id": "Dao, 2024", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "DeepSeek-R1", "type": "Model", "metadata": []}, "target": {"id": "DeepSeek-AI, 2025a", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "DeepSeek-v3.2", "type": "Model", "metadata": []}, "target": {"id": "DeepSeek-AI, 2025b", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Sliding Window Attention", "type": "Concept", "metadata": []}, "target": {"id": "Large Language Models", "type": "Concept", "metadata": []}, "type": "APPLIED_TO"}]}
{"nodes": [{"id": "Yushi Bai", "type": "Person", "metadata": []}, {"id": "LongBench-V2", "type": "Model", "metadata": []}, {"id": "Bai et al., 2024b", "type": "Publication", "metadata": []}, {"id": "Long-context tasks", "type": "Concept", "metadata": []}, {"id": "Longformer", "type": "Model", "metadata": []}, {"id": "Beltagy et al., 2020", "type": "Publication", "metadata": []}, {"id": "Flash-Attention-2", "type": "Model", "metadata": []}, {"id": "Dao, 2024", "type": "Publication", "metadata": []}, {"id": "Tri Dao", "type": "Person", "metadata": []}, {"id": "DeepSeek-R1", "type": "Model", "metadata": []}, {"id": "DeepSeek-AI, 2025a", "type": "Publication", "metadata": []}, {"id": "DeepSeek-AI", "type": "Organization", "metadata": []}, {"id": "Deep reasoning tasks", "type": "Concept", "metadata": []}, {"id": "Large Language Models", "type": "Model", "metadata": []}, {"id": "Reinforcement Learning", "type": "Algorithm", "metadata": []}, {"id": "DeepSeek-v3.2", "type": "Model", "metadata": []}, {"id": "DeepSeek-AI, 2025b", "type": "Publication", "metadata": []}, {"id": "Nemotron-Flash", "type": "Model", "metadata": []}, {"id": "Fu et al., 2025a", "type": "Publication", "metadata": []}, {"id": "Yonggan Fu", "type": "Person", "metadata": []}, {"id": "Sliding Window Attention Training", "type": "Algorithm", "metadata": []}, {"id": "Fu et al., 2025b", "type": "Publication", "metadata": []}, {"id": "Zichuan Fu", "type": "Person", "metadata": []}, {"id": "Mamba", "type": "Model", "metadata": []}, {"id": "Gu and Dao, 2023", "type": "Publication", "metadata": []}, {"id": "Albert Gu", "type": "Person", "metadata": []}, {"id": "Sequence Modeling", "type": "Task", "metadata": []}, {"id": "structured state-space models", "type": "Concept", "metadata": []}, {"id": "MMLU", "type": "Task", "metadata": []}, {"id": "Hendrycks et al., 2021", "type": "Publication", "metadata": []}, {"id": "Dan Hendrycks", "type": "Person", "metadata": []}, {"id": "Ruler", "type": "Task", "metadata": []}, {"id": "Hsieh et al., 2024", "type": "Publication", "metadata": []}, {"id": "Cheng-Ping Hsieh", "type": "Person", "metadata": []}, {"id": "Long context processing", "type": "Concept", "metadata": []}, {"id": "LoRA", "type": "Model", "metadata": []}, {"id": "Hu et al., 2022", "type": "Publication", "metadata": []}, {"id": "Edward J. Hu", "type": "Person", "metadata": []}, {"id": "RNN-like linear attention transformers", "type": "Model", "metadata": []}, {"id": "Katharopoulos et al., 2020", "type": "Publication", "metadata": []}, {"id": "Angelos Katharopoulos", "type": "Person", "metadata": []}, {"id": "Transformer architecture", "type": "Model", "metadata": []}, {"id": "linear attention", "type": "Concept", "metadata": []}, {"id": "PagedAttention", "type": "Algorithm", "metadata": []}, {"id": "Kwon et al., 2023", "type": "Publication", "metadata": []}, {"id": "Woosuk Kwon", "type": "Person", "metadata": []}, {"id": "Efficient memory management", "type": "Concept", "metadata": []}, {"id": "Jamba", "type": "Model", "metadata": []}, {"id": "Lieber et al., 2024", "type": "Publication", "metadata": []}, {"id": "Opher Lieber", "type": "Person", "metadata": []}, {"id": "Bamba", "type": "Model", "metadata": []}, {"id": "Linsong Chu et al., 2024", "type": "Publication", "metadata": []}, {"id": "Linsong Chu", "type": "Person", "metadata": []}, {"id": "ChatGPT", "type": "Model", "metadata": []}, {"id": "OpenAI, 2025", "type": "Publication", "metadata": []}, {"id": "OpenAI", "type": "Organization", "metadata": []}, {"id": "Fusang-v1-long", "type": "Concept", "metadata": []}, {"id": "Pan, 2024", "type": "Publication", "metadata": []}, {"id": "Wenbo Pan", "type": "Person", "metadata": []}, {"id": "instruction-tuning", "type": "Concept", "metadata": []}, {"id": "RWKV", "type": "Model", "metadata": []}, {"id": "Peng et al., 2023", "type": "Publication", "metadata": []}, {"id": "Bo Peng", "type": "Person", "metadata": []}, {"id": "Recurrent Neural Networks", "type": "Model", "metadata": []}, {"id": "GPQA", "type": "Task", "metadata": []}, {"id": "Rein et al., 2023", "type": "Publication", "metadata": []}, {"id": "David Rein", "type": "Person", "metadata": []}, {"id": "DeepseekMath", "type": "Model", "metadata": []}, {"id": "Shao et al., 2024", "type": "Publication", "metadata": []}, {"id": "Zhihong Shao", "type": "Person", "metadata": []}, {"id": "mathematical reasoning", "type": "Task", "metadata": []}, {"id": "Retentive Network", "type": "Model", "metadata": []}, {"id": "Sun et al., 2023", "type": "Publication", "metadata": []}, {"id": "Yutao Sun", "type": "Person", "metadata": []}, {"id": "Gemma2", "type": "Model", "metadata": []}, {"id": "Gemma Team, 2024a", "type": "Publication", "metadata": []}, {"id": "Gemma Team", "type": "Organization", "metadata": []}, {"id": "Gemma3", "type": "Model", "metadata": []}, {"id": "Gemma Team, 2025a", "type": "Publication", "metadata": []}, {"id": "Hybrid Attention Architecture", "type": "Model", "metadata": []}, {"id": "Team et al., 2025", "type": "Publication", "metadata": []}, {"id": "Ling Team", "type": "Organization", "metadata": []}, {"id": "Llama3", "type": "Model", "metadata": []}, {"id": "Llama Team, 2024b", "type": "Publication", "metadata": []}, {"id": "Llama Team", "type": "Organization", "metadata": []}, {"id": "Qwen3", "type": "Model", "metadata": []}, {"id": "Qwen3 Team, 2025b", "type": "Publication", "metadata": []}, {"id": "Qwen3 Team", "type": "Organization", "metadata": []}, {"id": "Llama", "type": "Model", "metadata": []}, {"id": "Touvron et al., 2023", "type": "Publication", "metadata": []}, {"id": "Hugo Touvron", "type": "Person", "metadata": []}, {"id": "Attention mechanism", "type": "Model", "metadata": []}, {"id": "Vaswani et al., 2017", "type": "Publication", "metadata": []}, {"id": "Ashish Vaswani", "type": "Person", "metadata": []}, {"id": "Rattention", "type": "Model", "metadata": []}, {"id": "Wang et al., 2025", "type": "Publication", "metadata": []}, {"id": "Bailin Wang", "type": "Person", "metadata": []}, {"id": "Sliding Window Attention", "type": "Model", "metadata": []}, {"id": "Chain-of-Thought", "type": "Algorithm", "metadata": []}, {"id": "Wei et al., 2022", "type": "Publication", "metadata": []}, {"id": "Jason Wei", "type": "Person", "metadata": []}, {"id": "LongMemEval", "type": "Task", "metadata": []}, {"id": "Wu et al., 2024", "type": "Publication", "metadata": []}, {"id": "Di Wu", "type": "Person", "metadata": []}, {"id": "long-term interactive memory", "type": "Concept", "metadata": []}, {"id": "Xiao, 2025", "type": "Publication", "metadata": []}, {"id": "Guangxuan Xiao", "type": "Person", "metadata": []}, {"id": "stacking sliding windows", "type": "Concept", "metadata": []}, {"id": "Streaming Attention", "type": "Model", "metadata": []}, {"id": "Xiao et al., 2024", "type": "Publication", "metadata": []}, {"id": "attention sinks", "type": "Concept", "metadata": []}, {"id": "Self-distillation", "type": "Algorithm", "metadata": []}, {"id": "Yang et al., 2024", "type": "Publication", "metadata": []}, {"id": "Zhaorui Yang", "type": "Person", "metadata": []}, {"id": "language model fine-tuning", "type": "Task", "metadata": []}, {"id": "Native Sparse Attention", "type": "Model", "metadata": []}, {"id": "Yuan et al., 2025", "type": "Publication", "metadata": []}, {"id": "Jingyang Yuan", "type": "Person", "metadata": []}, {"id": "sparse attention", "type": "Concept", "metadata": []}, {"id": "BigBird", "type": "Model", "metadata": []}, {"id": "Zaheer et al., 2020", "type": "Publication", "metadata": []}, {"id": "Manzil Zaheer", "type": "Person", "metadata": []}, {"id": "LightTransfer", "type": "Model", "metadata": []}, {"id": "Zhang et al., 2024", "type": "Publication", "metadata": []}, {"id": "Xuan Zhang", "type": "Person", "metadata": []}, {"id": "computational complexity", "type": "Concept", "metadata": []}, {"id": "GPU memory", "type": "Concept", "metadata": []}, {"id": "KV cache", "type": "Concept", "metadata": []}, {"id": "KV cache reusability", "type": "Concept", "metadata": []}, {"id": "Sliding Window Attention Adaptation", "type": "Algorithm", "metadata": []}, {"id": "FA Decode", "type": "Algorithm", "metadata": []}, {"id": "prefilling", "type": "Concept", "metadata": []}, {"id": "decoding speed", "type": "Concept", "metadata": []}, {"id": "Full Attention", "type": "Concept", "metadata": []}, {"id": "distributed LLM services", "type": "Concept", "metadata": []}, {"id": "Keep First k Tokens", "type": "Algorithm", "metadata": []}, {"id": "Positional Encoding", "type": "Concept", "metadata": []}, {"id": "Interleaving Layers", "type": "Algorithm", "metadata": []}, {"id": "generation length", "type": "Concept", "metadata": []}, {"id": "Time-per-output-token", "type": "Concept", "metadata": []}, {"id": "LongBench", "type": "Concept", "metadata": []}, {"id": "Bai et al., 2023", "type": "Publication", "metadata": []}, {"id": "synthetic tasks", "type": "Concept", "metadata": []}, {"id": "needle-retrieval tasks", "type": "Task", "metadata": []}, {"id": "Real-world tasks", "type": "Concept", "metadata": []}], "relationships": [{"source": {"id": "LongBench-V2", "type": "Model", "metadata": []}, "target": {"id": "Bai et al., 2024b", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Bai et al., 2024b", "type": "Publication", "metadata": []}, "target": {"id": "Yushi Bai", "type": "Person", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "LongBench-V2", "type": "Model", "metadata": []}, "target": {"id": "Long-context tasks", "type": "Concept", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "Longformer", "type": "Model", "metadata": []}, "target": {"id": "Beltagy et al., 2020", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Flash-Attention-2", "type": "Model", "metadata": []}, "target": {"id": "Dao, 2024", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Dao, 2024", "type": "Publication", "metadata": []}, "target": {"id": "Tri Dao", "type": "Person", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "DeepSeek-R1", "type": "Model", "metadata": []}, "target": {"id": "DeepSeek-AI, 2025a", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "DeepSeek-AI, 2025a", "type": "Publication", "metadata": []}, "target": {"id": "DeepSeek-AI", "type": "Organization", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "DeepSeek-R1", "type": "Model", "metadata": []}, "target": {"id": "Deep reasoning tasks", "type": "Concept", "metadata": []}, "type": "ADDRESSES"}, {"source": {"id": "DeepSeek-R1", "type": "Model", "metadata": []}, "target": {"id": "Large Language Models", "type": "Model", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "DeepSeek-R1", "type": "Model", "metadata": []}, "target": {"id": "Reinforcement Learning", "type": "Algorithm", "metadata": []}, "type": "BUILDS_ON"}, {"source": {"id": "DeepSeek-v3.2", "type": "Model", "metadata": []}, "target": {"id": "DeepSeek-AI, 2025b", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "DeepSeek-AI, 2025b", "type": "Publication", "metadata": []}, "target": {"id": "DeepSeek-AI", "type": "Organization", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "DeepSeek-v3.2", "type": "Model", "metadata": []}, "target": {"id": "Large Language Models", "type": "Model", "metadata": []}, "type": "EXTENDS"}, {"source": {"id": "Nemotron-Flash", "type": "Model", "metadata": []}, "target": {"id": "Fu et al., 2025a", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Fu et al., 2025a", "type": "Publication", "metadata": []}, "target": {"id": "Yonggan Fu", "type": "Person", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Sliding Window Attention Training", "type": "Algorithm", "metadata": []}, "target": {"id": "Fu et al., 2025b", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Fu et al., 2025b", "type": "Publication", "metadata": []}, "target": {"id": "Zichuan Fu", "type": "Person", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Sliding Window Attention Training", "type": "Algorithm", "metadata": []}, "target": {"id": "Large Language Models", "type": "Model", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "Mamba", "type": "Model", "metadata": []}, "target": {"id": "Gu and Dao, 2023", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Gu and Dao, 2023", "type": "Publication", "metadata": []}, "target": {"id": "Albert Gu", "type": "Person", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Gu and Dao, 2023", "type": "Publication", "metadata": []}, "target": {"id": "Tri Dao", "type": "Person", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Mamba", "type": "Model", "metadata": []}, "target": {"id": "Sequence Modeling", "type": "Task", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "Mamba", "type": "Model", "metadata": []}, "target": {"id": "structured state-space models", "type": "Concept", "metadata": []}, "type": "BUILDS_ON"}, {"source": {"id": "MMLU", "type": "Task", "metadata": []}, "target": {"id": "Hendrycks et al., 2021", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Hendrycks et al., 2021", "type": "Publication", "metadata": []}, "target": {"id": "Dan Hendrycks", "type": "Person", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Ruler", "type": "Task", "metadata": []}, "target": {"id": "Hsieh et al., 2024", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Hsieh et al., 2024", "type": "Publication", "metadata": []}, "target": {"id": "Cheng-Ping Hsieh", "type": "Person", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Ruler", "type": "Task", "metadata": []}, "target": {"id": "Large Language Models", "type": "Model", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "Ruler", "type": "Task", "metadata": []}, "target": {"id": "Long context processing", "type": "Concept", "metadata": []}, "type": "ADDRESSES"}, {"source": {"id": "LoRA", "type": "Model", "metadata": []}, "target": {"id": "Hu et al., 2022", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Hu et al., 2022", "type": "Publication", "metadata": []}, "target": {"id": "Edward J. Hu", "type": "Person", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "LoRA", "type": "Model", "metadata": []}, "target": {"id": "Large Language Models", "type": "Model", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "RNN-like linear attention transformers", "type": "Model", "metadata": []}, "target": {"id": "Katharopoulos et al., 2020", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Katharopoulos et al., 2020", "type": "Publication", "metadata": []}, "target": {"id": "Angelos Katharopoulos", "type": "Person", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "RNN-like linear attention transformers", "type": "Model", "metadata": []}, "target": {"id": "Transformer architecture", "type": "Model", "metadata": []}, "type": "BUILDS_ON"}, {"source": {"id": "RNN-like linear attention transformers", "type": "Model", "metadata": []}, "target": {"id": "linear attention", "type": "Concept", "metadata": []}, "type": "BUILDS_ON"}, {"source": {"id": "PagedAttention", "type": "Algorithm", "metadata": []}, "target": {"id": "Kwon et al., 2023", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Kwon et al., 2023", "type": "Publication", "metadata": []}, "target": {"id": "Woosuk Kwon", "type": "Person", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "PagedAttention", "type": "Algorithm", "metadata": []}, "target": {"id": "Large Language Models", "type": "Model", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "PagedAttention", "type": "Algorithm", "metadata": []}, "target": {"id": "Efficient memory management", "type": "Concept", "metadata": []}, "type": "ADDRESSES"}, {"source": {"id": "Jamba", "type": "Model", "metadata": []}, "target": {"id": "Lieber et al., 2024", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Lieber et al., 2024", "type": "Publication", "metadata": []}, "target": {"id": "Opher Lieber", "type": "Person", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Jamba", "type": "Model", "metadata": []}, "target": {"id": "Transformer architecture", "type": "Model", "metadata": []}, "type": "BUILDS_ON"}, {"source": {"id": "Jamba", "type": "Model", "metadata": []}, "target": {"id": "Mamba", "type": "Model", "metadata": []}, "type": "BUILDS_ON"}, {"source": {"id": "Bamba", "type": "Model", "metadata": []}, "target": {"id": "Linsong Chu et al., 2024", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Linsong Chu et al., 2024", "type": "Publication", "metadata": []}, "target": {"id": "Linsong Chu", "type": "Person", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Linsong Chu et al., 2024", "type": "Publication", "metadata": []}, "target": {"id": "Tri Dao", "type": "Person", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Bamba", "type": "Model", "metadata": []}, "target": {"id": "Mamba", "type": "Model", "metadata": []}, "type": "BUILDS_ON"}, {"source": {"id": "ChatGPT", "type": "Model", "metadata": []}, "target": {"id": "OpenAI, 2025", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "OpenAI, 2025", "type": "Publication", "metadata": []}, "target": {"id": "OpenAI", "type": "Organization", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Fusang-v1-long", "type": "Concept", "metadata": []}, "target": {"id": "Pan, 2024", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Pan, 2024", "type": "Publication", "metadata": []}, "target": {"id": "Wenbo Pan", "type": "Person", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Fusang-v1-long", "type": "Concept", "metadata": []}, "target": {"id": "instruction-tuning", "type": "Concept", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "Fusang-v1-long", "type": "Concept", "metadata": []}, "target": {"id": "Large Language Models", "type": "Model", "metadata": []}, "type": "SUPPORTS"}, {"source": {"id": "RWKV", "type": "Model", "metadata": []}, "target": {"id": "Peng et al., 2023", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Peng et al., 2023", "type": "Publication", "metadata": []}, "target": {"id": "Bo Peng", "type": "Person", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "RWKV", "type": "Model", "metadata": []}, "target": {"id": "Recurrent Neural Networks", "type": "Model", "metadata": []}, "type": "BUILDS_ON"}, {"source": {"id": "RWKV", "type": "Model", "metadata": []}, "target": {"id": "Transformer architecture", "type": "Model", "metadata": []}, "type": "COMPARED_WITH"}, {"source": {"id": "GPQA", "type": "Task", "metadata": []}, "target": {"id": "Rein et al., 2023", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Rein et al., 2023", "type": "Publication", "metadata": []}, "target": {"id": "David Rein", "type": "Person", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "DeepseekMath", "type": "Model", "metadata": []}, "target": {"id": "Shao et al., 2024", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Shao et al., 2024", "type": "Publication", "metadata": []}, "target": {"id": "Zhihong Shao", "type": "Person", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "DeepseekMath", "type": "Model", "metadata": []}, "target": {"id": "mathematical reasoning", "type": "Task", "metadata": []}, "type": "ADDRESSES"}, {"source": {"id": "DeepseekMath", "type": "Model", "metadata": []}, "target": {"id": "Large Language Models", "type": "Model", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "Retentive Network", "type": "Model", "metadata": []}, "target": {"id": "Sun et al., 2023", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Sun et al., 2023", "type": "Publication", "metadata": []}, "target": {"id": "Yutao Sun", "type": "Person", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Retentive Network", "type": "Model", "metadata": []}, "target": {"id": "Transformer architecture", "type": "Model", "metadata": []}, "type": "EXTENDS"}, {"source": {"id": "Retentive Network", "type": "Model", "metadata": []}, "target": {"id": "Large Language Models", "type": "Model", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "Gemma2", "type": "Model", "metadata": []}, "target": {"id": "Gemma Team, 2024a", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Gemma Team, 2024a", "type": "Publication", "metadata": []}, "target": {"id": "Gemma Team", "type": "Organization", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Gemma2", "type": "Model", "metadata": []}, "target": {"id": "Large Language Models", "type": "Model", "metadata": []}, "type": "EXTENDS"}, {"source": {"id": "Gemma3", "type": "Model", "metadata": []}, "target": {"id": "Gemma Team, 2025a", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Gemma Team, 2025a", "type": "Publication", "metadata": []}, "target": {"id": "Gemma Team", "type": "Organization", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Hybrid Attention Architecture", "type": "Model", "metadata": []}, "target": {"id": "Team et al., 2025", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Team et al., 2025", "type": "Publication", "metadata": []}, "target": {"id": "Ling Team", "type": "Organization", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Hybrid Attention Architecture", "type": "Model", "metadata": []}, "target": {"id": "Long-context tasks", "type": "Concept", "metadata": []}, "type": "ADDRESSES"}, {"source": {"id": "Llama3", "type": "Model", "metadata": []}, "target": {"id": "Llama Team, 2024b", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Llama Team, 2024b", "type": "Publication", "metadata": []}, "target": {"id": "Llama Team", "type": "Organization", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Qwen3", "type": "Model", "metadata": []}, "target": {"id": "Qwen3 Team, 2025b", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Qwen3 Team, 2025b", "type": "Publication", "metadata": []}, "target": {"id": "Qwen3 Team", "type": "Organization", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Llama", "type": "Model", "metadata": []}, "target": {"id": "Touvron et al., 2023", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Touvron et al., 2023", "type": "Publication", "metadata": []}, "target": {"id": "Hugo Touvron", "type": "Person", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Llama", "type": "Model", "metadata": []}, "target": {"id": "Large Language Models", "type": "Model", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "Attention mechanism", "type": "Model", "metadata": []}, "target": {"id": "Vaswani et al., 2017", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Transformer architecture", "type": "Model", "metadata": []}, "target": {"id": "Vaswani et al., 2017", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Vaswani et al., 2017", "type": "Publication", "metadata": []}, "target": {"id": "Ashish Vaswani", "type": "Person", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Rattention", "type": "Model", "metadata": []}, "target": {"id": "Wang et al., 2025", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Wang et al., 2025", "type": "Publication", "metadata": []}, "target": {"id": "Bailin Wang", "type": "Person", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Rattention", "type": "Model", "metadata": []}, "target": {"id": "Sliding Window Attention", "type": "Model", "metadata": []}, "type": "BUILDS_ON"}, {"source": {"id": "Rattention", "type": "Model", "metadata": []}, "target": {"id": "Attention mechanism", "type": "Model", "metadata": []}, "type": "BUILDS_ON"}, {"source": {"id": "Chain-of-Thought", "type": "Algorithm", "metadata": []}, "target": {"id": "Wei et al., 2022", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Wei et al., 2022", "type": "Publication", "metadata": []}, "target": {"id": "Jason Wei", "type": "Person", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Chain-of-Thought", "type": "Algorithm", "metadata": []}, "target": {"id": "Large Language Models", "type": "Model", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "Chain-of-Thought", "type": "Algorithm", "metadata": []}, "target": {"id": "Deep reasoning tasks", "type": "Concept", "metadata": []}, "type": "ADDRESSES"}, {"source": {"id": "LongMemEval", "type": "Task", "metadata": []}, "target": {"id": "Wu et al., 2024", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Wu et al., 2024", "type": "Publication", "metadata": []}, "target": {"id": "Di Wu", "type": "Person", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "LongMemEval", "type": "Task", "metadata": []}, "target": {"id": "Large Language Models", "type": "Model", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "LongMemEval", "type": "Task", "metadata": []}, "target": {"id": "long-term interactive memory", "type": "Concept", "metadata": []}, "type": "ADDRESSES"}, {"source": {"id": "Xiao, 2025", "type": "Publication", "metadata": []}, "target": {"id": "Guangxuan Xiao", "type": "Person", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Xiao, 2025", "type": "Publication", "metadata": []}, "target": {"id": "stacking sliding windows", "type": "Concept", "metadata": []}, "type": "DEPICTS"}, {"source": {"id": "Streaming Attention", "type": "Model", "metadata": []}, "target": {"id": "Xiao et al., 2024", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Xiao et al., 2024", "type": "Publication", "metadata": []}, "target": {"id": "Guangxuan Xiao", "type": "Person", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Streaming Attention", "type": "Model", "metadata": []}, "target": {"id": "attention sinks", "type": "Concept", "metadata": []}, "type": "BUILDS_ON"}, {"source": {"id": "Streaming Attention", "type": "Model", "metadata": []}, "target": {"id": "Large Language Models", "type": "Model", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "Yang et al., 2024", "type": "Publication", "metadata": []}, "target": {"id": "Zhaorui Yang", "type": "Person", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Self-distillation", "type": "Algorithm", "metadata": []}, "target": {"id": "language model fine-tuning", "type": "Task", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "Native Sparse Attention", "type": "Model", "metadata": []}, "target": {"id": "Yuan et al., 2025", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Yuan et al., 2025", "type": "Publication", "metadata": []}, "target": {"id": "Jingyang Yuan", "type": "Person", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Native Sparse Attention", "type": "Model", "metadata": []}, "target": {"id": "sparse attention", "type": "Concept", "metadata": []}, "type": "EXTENDS"}, {"source": {"id": "BigBird", "type": "Model", "metadata": []}, "target": {"id": "Zaheer et al., 2020", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Zaheer et al., 2020", "type": "Publication", "metadata": []}, "target": {"id": "Manzil Zaheer", "type": "Person", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "BigBird", "type": "Model", "metadata": []}, "target": {"id": "Transformer architecture", "type": "Model", "metadata": []}, "type": "BUILDS_ON"}, {"source": {"id": "LightTransfer", "type": "Model", "metadata": []}, "target": {"id": "Zhang et al., 2024", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "Zhang et al., 2024", "type": "Publication", "metadata": []}, "target": {"id": "Xuan Zhang", "type": "Person", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "LightTransfer", "type": "Model", "metadata": []}, "target": {"id": "Large Language Models", "type": "Model", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "Sliding Window Attention", "type": "Model", "metadata": []}, "target": {"id": "computational complexity", "type": "Concept", "metadata": []}, "type": "MITIGATES"}, {"source": {"id": "Sliding Window Attention", "type": "Model", "metadata": []}, "target": {"id": "GPU memory", "type": "Concept", "metadata": []}, "type": "MITIGATES"}, {"source": {"id": "Sliding Window Attention", "type": "Model", "metadata": []}, "target": {"id": "KV cache reusability", "type": "Concept", "metadata": []}, "type": "SUPPORTS"}, {"source": {"id": "Sliding Window Attention Adaptation", "type": "Algorithm", "metadata": []}, "target": {"id": "Sliding Window Attention", "type": "Model", "metadata": []}, "type": "COMPARED_WITH"}, {"source": {"id": "FA Decode", "type": "Algorithm", "metadata": []}, "target": {"id": "prefilling", "type": "Concept", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "FA Decode", "type": "Algorithm", "metadata": []}, "target": {"id": "decoding speed", "type": "Concept", "metadata": []}, "type": "COMPARED_WITH"}, {"source": {"id": "FA Decode", "type": "Algorithm", "metadata": []}, "target": {"id": "Full Attention", "type": "Concept", "metadata": []}, "type": "BUILDS_ON"}, {"source": {"id": "Large Language Models", "type": "Model", "metadata": []}, "target": {"id": "distributed LLM services", "type": "Concept", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "prefilling", "type": "Concept", "metadata": []}, "target": {"id": "distributed LLM services", "type": "Concept", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "Keep First k Tokens", "type": "Algorithm", "metadata": []}, "target": {"id": "computational complexity", "type": "Concept", "metadata": []}, "type": "COMPARED_WITH"}, {"source": {"id": "Keep First k Tokens", "type": "Algorithm", "metadata": []}, "target": {"id": "KV cache reusability", "type": "Concept", "metadata": []}, "type": "COMPARED_WITH"}, {"source": {"id": "Interleaving Layers", "type": "Algorithm", "metadata": []}, "target": {"id": "computational complexity", "type": "Concept", "metadata": []}, "type": "COMPARED_WITH"}, {"source": {"id": "Interleaving Layers", "type": "Algorithm", "metadata": []}, "target": {"id": "Sliding Window Attention", "type": "Model", "metadata": []}, "type": "COMPARED_WITH"}, {"source": {"id": "Interleaving Layers", "type": "Algorithm", "metadata": []}, "target": {"id": "KV cache reusability", "type": "Concept", "metadata": []}, "type": "MITIGATES"}, {"source": {"id": "Full Attention", "type": "Concept", "metadata": []}, "target": {"id": "KV cache reusability", "type": "Concept", "metadata": []}, "type": "MITIGATES"}, {"source": {"id": "Chain-of-Thought", "type": "Algorithm", "metadata": []}, "target": {"id": "generation length", "type": "Concept", "metadata": []}, "type": "COMPARED_WITH"}, {"source": {"id": "Chain-of-Thought", "type": "Algorithm", "metadata": []}, "target": {"id": "Deep reasoning tasks", "type": "Concept", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "Chain-of-Thought", "type": "Algorithm", "metadata": []}, "target": {"id": "Time-per-output-token", "type": "Concept", "metadata": []}, "type": "COMPARED_WITH"}, {"source": {"id": "LongBench", "type": "Concept", "metadata": []}, "target": {"id": "Bai et al., 2023", "type": "Publication", "metadata": []}, "type": "INTRODUCED_BY"}, {"source": {"id": "LongBench", "type": "Concept", "metadata": []}, "target": {"id": "context length", "type": "Concept", "metadata": []}, "type": "COMPARED_WITH"}, {"source": {"id": "Ruler", "type": "Task", "metadata": []}, "target": {"id": "context length", "type": "Concept", "metadata": []}, "type": "COMPARED_WITH"}, {"source": {"id": "Ruler", "type": "Task", "metadata": []}, "target": {"id": "synthetic tasks", "type": "Concept", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "Ruler", "type": "Task", "metadata": []}, "target": {"id": "needle-retrieval tasks", "type": "Task", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "Ruler", "type": "Task", "metadata": []}, "target": {"id": "Real-world tasks", "type": "Concept", "metadata": []}, "type": "COMPARED_WITH"}]}
{"nodes": [{"id": "distributed LLM services", "type": "Concept", "metadata": []}, {"id": "KV cache", "type": "Concept", "metadata": []}, {"id": "prefilling", "type": "Concept", "metadata": []}, {"id": "Keep First k Tokens", "type": "Concept", "metadata": []}, {"id": "Positional Encoding", "type": "Concept", "metadata": []}, {"id": "Interleaving Layers", "type": "Concept", "metadata": []}, {"id": "Sliding Window Attention", "type": "Concept", "metadata": []}, {"id": "GPU memory", "type": "Concept", "metadata": []}, {"id": "KV cache reusability", "type": "Concept", "metadata": []}, {"id": "Full Attention", "type": "Concept", "metadata": []}, {"id": "Chain-of-Thought", "type": "Concept", "metadata": []}, {"id": "generation length", "type": "Concept", "metadata": []}, {"id": "decoding speed", "type": "Concept", "metadata": []}, {"id": "Long-context benchmarks", "type": "Task", "metadata": []}, {"id": "LongBench", "type": "Task", "metadata": []}, {"id": "Bai et al., 2023", "type": "Publication", "metadata": []}, {"id": "Long-context tasks", "type": "Task", "metadata": []}, {"id": "Ruler", "type": "Model", "metadata": []}, {"id": "Hsieh et al., 2024", "type": "Publication", "metadata": []}, {"id": "synthetic tasks", "type": "Task", "metadata": []}, {"id": "needle-retrieval tasks", "type": "Task", "metadata": []}, {"id": "long-context performance", "type": "Concept", "metadata": []}, {"id": "real-world tasks", "type": "Task", "metadata": []}, {"id": "LongBench-V2", "type": "Task", "metadata": []}, {"id": "Bai et al., 2024b", "type": "Publication", "metadata": []}, {"id": "deep understanding and reasoning", "type": "Concept", "metadata": []}, {"id": "Qwen3-4B-Thinking", "type": "Model", "metadata": []}, {"id": "Accuracy", "type": "Concept", "metadata": []}, {"id": "Qwen3-30B-A3B-Thinking", "type": "Model", "metadata": []}, {"id": "Qwen3-30B-A3B-Instruct", "type": "Model", "metadata": []}, {"id": "LongMemEval_24k", "type": "Task", "metadata": []}, {"id": "Llama3.1-8B-Instruct", "type": "Model", "metadata": []}, {"id": "Qwen3-30B-A3B", "type": "Model", "metadata": []}, {"id": "Llama3.1", "type": "Model", "metadata": []}, {"id": "Fine-tuning", "type": "Algorithm", "metadata": []}, {"id": "Qwen3-4B", "type": "Model", "metadata": []}, {"id": "Time-to-first-token", "type": "Concept", "metadata": []}, {"id": "Time-per-output-token", "type": "Concept", "metadata": []}, {"id": "Throughput", "type": "Concept", "metadata": []}, {"id": "vLLM", "type": "Model", "metadata": []}, {"id": "FA Decode", "type": "Concept", "metadata": []}, {"id": "GPU utilization", "type": "Concept", "metadata": []}, {"id": "LightTransfer", "type": "Model", "metadata": []}, {"id": "Zhang et al., 2024", "type": "Publication", "metadata": []}, {"id": "SWA adaptation", "type": "Concept", "metadata": []}, {"id": "Full Attention models", "type": "Model", "metadata": []}, {"id": "LongAlign", "type": "Task", "metadata": []}, {"id": "Bai et al., 2024a", "type": "Publication", "metadata": []}, {"id": "Qwen3-30B", "type": "Model", "metadata": []}, {"id": "Llama3.1-8B", "type": "Model", "metadata": []}, {"id": "fixed-interval selection", "type": "Concept", "metadata": []}], "relationships": [{"source": {"id": "LongBench", "type": "Task", "metadata": []}, "target": {"id": "Bai et al., 2023", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Ruler", "type": "Model", "metadata": []}, "target": {"id": "Hsieh et al., 2024", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "Ruler", "type": "Model", "metadata": []}, "target": {"id": "synthetic tasks", "type": "Task", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "Ruler", "type": "Model", "metadata": []}, "target": {"id": "needle-retrieval tasks", "type": "Task", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "LongBench-V2", "type": "Task", "metadata": []}, "target": {"id": "Bai et al., 2024b", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "LongBench-V2", "type": "Task", "metadata": []}, "target": {"id": "deep understanding and reasoning", "type": "Concept", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "LongBench-V2", "type": "Task", "metadata": []}, "target": {"id": "Qwen3-4B-Thinking", "type": "Model", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "LongBench-V2", "type": "Task", "metadata": []}, "target": {"id": "Qwen3-30B-A3B", "type": "Model", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "LongBench-V2", "type": "Task", "metadata": []}, "target": {"id": "Llama3.1", "type": "Model", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "LongMemEval_24k", "type": "Task", "metadata": []}, "target": {"id": "Qwen3-30B-A3B-Thinking", "type": "Model", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "LongMemEval_24k", "type": "Task", "metadata": []}, "target": {"id": "Qwen3-30B-A3B-Instruct", "type": "Model", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "LongMemEval_24k", "type": "Task", "metadata": []}, "target": {"id": "Llama3.1-8B-Instruct", "type": "Model", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "Interleaving Layers", "type": "Concept", "metadata": []}, "target": {"id": "Sliding Window Attention", "type": "Concept", "metadata": []}, "type": "COMPARED_WITH"}, {"source": {"id": "FA Decode", "type": "Concept", "metadata": []}, "target": {"id": "Sliding Window Attention", "type": "Concept", "metadata": []}, "type": "COMPARED_WITH"}, {"source": {"id": "vLLM", "type": "Model", "metadata": []}, "target": {"id": "Time-to-first-token", "type": "Concept", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "vLLM", "type": "Model", "metadata": []}, "target": {"id": "Time-per-output-token", "type": "Concept", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "vLLM", "type": "Model", "metadata": []}, "target": {"id": "Throughput", "type": "Concept", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "LightTransfer", "type": "Model", "metadata": []}, "target": {"id": "Zhang et al., 2024", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "LightTransfer", "type": "Model", "metadata": []}, "target": {"id": "SWA adaptation", "type": "Concept", "metadata": []}, "type": "EXTENDS"}, {"source": {"id": "LightTransfer", "type": "Model", "metadata": []}, "target": {"id": "Full Attention models", "type": "Model", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "LightTransfer", "type": "Model", "metadata": []}, "target": {"id": "LongAlign", "type": "Task", "metadata": []}, "type": "BUILDS_ON"}, {"source": {"id": "LongAlign", "type": "Task", "metadata": []}, "target": {"id": "Bai et al., 2024a", "type": "Publication", "metadata": []}, "type": "AUTHORED_BY"}, {"source": {"id": "LightTransfer", "type": "Model", "metadata": []}, "target": {"id": "Qwen3-4B", "type": "Model", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "LightTransfer", "type": "Model", "metadata": []}, "target": {"id": "Qwen3-30B", "type": "Model", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "LightTransfer", "type": "Model", "metadata": []}, "target": {"id": "Llama3.1-8B", "type": "Model", "metadata": []}, "type": "APPLIED_TO"}, {"source": {"id": "LightTransfer", "type": "Model", "metadata": []}, "target": {"id": "fixed-interval selection", "type": "Concept", "metadata": []}, "type": "COMPARED_WITH"}]}