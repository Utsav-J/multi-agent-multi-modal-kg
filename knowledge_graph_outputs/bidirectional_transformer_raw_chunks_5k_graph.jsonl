{"nodes": [{"id": "Figure 1", "type": "Figure"}, {"id": "_C_", "type": "Concept"}, {"id": "Next Sentence Prediction (NSP)", "type": "Task"}, {"id": "Question Answering", "type": "Task"}, {"id": "Natural Language Inference (NLI)", "type": "Task"}, {"id": "Section 5.1", "type": "Section"}, {"id": "Figure 2", "type": "Figure"}, {"id": "BERT", "type": "AI Model"}, {"id": "Input Representation", "type": "Concept"}, {"id": "Input Embeddings", "type": "Concept"}, {"id": "Token Embeddings", "type": "Concept"}, {"id": "Segmentation Embeddings", "type": "Concept"}, {"id": "Position Embeddings", "type": "Concept"}, {"id": "Jernite et al. (2017)", "type": "Publication"}, {"id": "Logeswaran and Lee (2018)", "type": "Publication"}, {"id": "Representation Learning Objectives", "type": "Concept"}, {"id": "Sentence Embeddings", "type": "Concept"}, {"id": "Parameters", "type": "Concept"}, {"id": "Downstream Tasks", "type": "Task Category"}, {"id": "End-Task Model Parameters", "type": "Concept"}, {"id": "Pre-training Data", "type": "Concept"}, {"id": "Language Model Pre-training", "type": "Process"}, {"id": "BooksCorpus", "type": "Dataset"}, {"id": "Zhu et al., 2015", "type": "Publication"}, {"id": "English Wikipedia", "type": "Dataset"}, {"id": "Wikipedia", "type": "Resource"}, {"id": "Billion Word Benchmark", "type": "Dataset"}, {"id": "Chelba et al., 2013", "type": "Publication"}, {"id": "Document-level Corpus", "type": "Concept"}, {"id": "Shuffled Sentence-level Corpus", "type": "Concept"}, {"id": "Fine-tuning BERT", "type": "Process"}, {"id": "Self-attention Mechanism", "type": "Concept"}, {"id": "Transformer", "type": "AI Model"}, {"id": "Text Pairs", "type": "Concept"}, {"id": "Bidirectional Cross Attention", "type": "Concept"}, {"id": "Parikh et al. (2016)", "type": "Publication"}, {"id": "Seo et al. (2017)", "type": "Publication"}, {"id": "Task-specific Inputs", "type": "Concept"}, {"id": "Task-specific Outputs", "type": "Concept"}, {"id": "Paraphrasing", "type": "Task"}, {"id": "Entailment", "type": "Task"}, {"id": "Text Classification", "type": "Task"}, {"id": "Sequence Tagging", "type": "Task"}, {"id": "Sentiment Analysis", "type": "Task"}, {"id": "Token Representations", "type": "Concept"}, {"id": "Output Layer", "type": "Component"}, {"id": "[CLS] Representation", "type": "Concept"}, {"id": "Cloud TPU", "type": "Hardware"}, {"id": "GPU", "type": "Hardware"}, {"id": "Section 4", "type": "Section"}, {"id": "Appendix A.5", "type": "Appendix"}, {"id": "NLP Tasks", "type": "Task Category"}, {"id": "GLUE", "type": "Benchmark"}, {"id": "Wang et al., 2018a", "type": "Publication"}, {"id": "Natural Language Understanding Tasks", "type": "Task Category"}, {"id": "Appendix B.1", "type": "Appendix"}, {"id": "Input Sequence", "type": "Concept"}, {"id": "Section 3", "type": "Section"}, {"id": "Final Hidden Vector C", "type": "Concept"}, {"id": "[CLS] Token", "type": "Concept"}, {"id": "Aggregate Representation", "type": "Concept"}, {"id": "Classification Layer Weights W", "type": "Concept"}, {"id": "Classification Loss", "type": "Concept"}, {"id": "BERT SQuAD Model", "type": "AI Model"}, {"id": "Dev F1 Score", "type": "Metric"}, {"id": "GLUE Benchmark FAQ", "type": "Resource"}, {"id": "Table 1", "type": "Table"}, {"id": "GLUE Test Results", "type": "Results"}, {"id": "GLUE Leaderboard", "type": "Resource"}, {"id": "WNLI Set", "type": "Dataset"}, {"id": "OpenAI GPT", "type": "AI Model"}, {"id": "QQP", "type": "Task"}, {"id": "MRPC", "type": "Task"}, {"id": "STS-B", "type": "Task"}, {"id": "F1 Score", "type": "Metric"}, {"id": "Spearman Correlations", "type": "Metric"}, {"id": "Accuracy Scores", "type": "Metric"}, {"id": "Single-model, Single-task", "type": "Characteristic"}, {"id": "Batch Size", "type": "Hyperparameter"}, {"id": "Epochs", "type": "Hyperparameter"}, {"id": "GLUE Tasks", "type": "Task Category"}, {"id": "Learning Rate", "type": "Hyperparameter"}, {"id": "Dev Set", "type": "Dataset"}, {"id": "BERTLARGE", "type": "AI Model"}, {"id": "Small Datasets", "type": "Dataset Category"}, {"id": "Random Restarts", "type": "Method"}, {"id": "Pre-trained Checkpoint", "type": "Concept"}, {"id": "Fine-tuning Data Shuffling", "type": "Process"}, {"id": "Classifier Layer Initialization", "type": "Process"}, {"id": "BERTBASE", "type": "AI Model"}, {"id": "Model Architecture", "type": "Concept"}, {"id": "Attention Masking", "type": "Concept"}, {"id": "MNLI", "type": "Task"}, {"id": "Effect of Model Size", "type": "Concept"}, {"id": "SQuAD v1.1", "type": "Dataset"}, {"id": "Stanford Question Answering Dataset", "type": "Dataset"}, {"id": "Question/Answer Pairs", "type": "Data Type"}, {"id": "Rajpurkar et al., 2016", "type": "Publication"}, {"id": "Answer Text Span Prediction", "type": "Task"}, {"id": "Input Question", "type": "Concept"}, {"id": "Passage", "type": "Concept"}, {"id": "Single Packed Sequence", "type": "Concept"}, {"id": "A Embedding", "type": "Concept"}, {"id": "B Embedding", "type": "Concept"}, {"id": "Start Vector S", "type": "Concept"}, {"id": "End Vector E", "type": "Concept"}, {"id": "Training Objective", "type": "Concept"}, {"id": "Log-likelihoods", "type": "Metric"}, {"id": "Table 2", "type": "Table"}, {"id": "SQuAD Leaderboard", "type": "Resource"}, {"id": "Clark and Gardner, 2018", "type": "Publication"}, {"id": "Peters et al., 2018a", "type": "Publication"}, {"id": "Hu et al., 2018", "type": "Publication"}, {"id": "Data Augmentation", "type": "Method"}, {"id": "TriviaQA", "type": "Dataset"}, {"id": "Joshi et al., 2017", "type": "Publication"}, {"id": "QANet", "type": "AI Model"}, {"id": "Yu et al. (2018)", "type": "Publication"}, {"id": "Human", "type": "Agent Type"}, {"id": "#1 Ensemble - nlnet", "type": "System"}, {"id": "#2 Ensemble - QANet", "type": "System"}, {"id": "BiDAF+ELMo (Single)", "type": "System"}, {"id": "R.M. Reader (Ensemble)", "type": "System"}, {"id": "BERTLARGE Single", "type": "AI Model"}, {"id": "BERTLARGE Ensemble", "type": "AI Model"}, {"id": "BERTLARGE Single + TriviaQA", "type": "AI Model"}, {"id": "BERTLARGE Ensemble + TriviaQA", "type": "AI Model"}, {"id": "SQuAD 2.0", "type": "Task"}, {"id": "SQuAD v1.1 BERT Model", "type": "AI Model"}, {"id": "No-answer Span", "type": "Concept"}, {"id": "Non-null Span", "type": "Concept"}, {"id": "Threshold \u03c4", "type": "Concept"}, {"id": "Sun et al., 2018", "type": "Publication"}, {"id": "Wang et al., 2018b", "type": "Publication"}, {"id": "Table 3", "type": "Table"}, {"id": "SQuAD 2.0 Results", "type": "Results"}, {"id": "#1 Single - MIR-MRC (F-Net)", "type": "System"}, {"id": "#2 Single - nlnet", "type": "System"}, {"id": "unet (Ensemble)", "type": "System"}, {"id": "SLQA+ (Single)", "type": "System"}, {"id": "SWAG", "type": "Dataset"}, {"id": "Zellers et al., 2018", "type": "Publication"}, {"id": "Sentence-pair Completion Examples", "type": "Data Type"}, {"id": "Grounded Commonsense Inference", "type": "Task"}, {"id": "Sentence A", "type": "Concept"}, {"id": "Sentence B", "type": "Concept"}, {"id": "[CLS] Token Representation C", "type": "Concept"}, {"id": "Softmax Layer", "type": "Component"}, {"id": "Table 4", "type": "Table"}, {"id": "ESIM+ELMo", "type": "System"}, {"id": "SWAG Dev and Test Accuracies", "type": "Results"}, {"id": "ESIM+GloVe", "type": "System"}, {"id": "SWAG Paper", "type": "Publication"}, {"id": "Ablation Studies", "type": "Research Method"}, {"id": "Appendix C", "type": "Appendix"}, {"id": "Pre-training Tasks", "type": "Task Category"}, {"id": "No NSP", "type": "Model Variant"}, {"id": "Masked LM (MLM)", "type": "Task"}, {"id": "Left-to-Right (LTR) LM", "type": "Model Type"}, {"id": "LTR & No NSP", "type": "Model Variant"}, {"id": "LTR Model", "type": "Model Type"}, {"id": "MLM Model", "type": "Model Type"}, {"id": "Token Predictions", "type": "Task"}, {"id": "BiLSTM", "type": "Model Component"}, {"id": "LTR Models", "type": "Model Type"}, {"id": "RTL Models", "type": "Model Type"}, {"id": "ELMo", "type": "AI Model"}, {"id": "Table 5", "type": "Table"}, {"id": "Ablation over Pre-training Tasks", "type": "Experiment"}, {"id": "BERTBASE Architecture", "type": "Architecture"}, {"id": "+ BiLSTM", "type": "Model Variant"}, {"id": "SST-2", "type": "Task"}, {"id": "Fine-tuning Task Accuracy", "type": "Metric"}, {"id": "Layers", "type": "Hyperparameter"}, {"id": "Hidden Units", "type": "Hyperparameter"}, {"id": "Attention Heads", "type": "Hyperparameter"}, {"id": "Table 6", "type": "Table"}, {"id": "Dev Set Accuracy", "type": "Metric"}, {"id": "Encoder", "type": "Component"}, {"id": "Al-Rfou et al., 2018", "type": "Publication"}, {"id": "Pre-training", "type": "Process"}, {"id": "Fine-tuning", "type": "Process"}, {"id": "Vaswani et al. (2017)", "type": "Publication"}], "relationships": [{"source": {"id": "Figure 1", "type": "Figure"}, "target": {"id": "_C_", "type": "Concept"}, "type": "SHOWS"}, {"source": {"id": "_C_", "type": "Concept"}, "target": {"id": "Next Sentence Prediction (NSP)", "type": "Task"}, "type": "USED_FOR"}, {"source": {"id": "Next Sentence Prediction (NSP)", "type": "Task"}, "target": {"id": "Question Answering", "type": "Task"}, "type": "BENEFITS"}, {"source": {"id": "Next Sentence Prediction (NSP)", "type": "Task"}, "target": {"id": "Natural Language Inference (NLI)", "type": "Task"}, "type": "BENEFITS"}, {"source": {"id": "Section 5.1", "type": "Section"}, "target": {"id": "Next Sentence Prediction (NSP)", "type": "Task"}, "type": "DISCUSSES"}, {"source": {"id": "Figure 2", "type": "Figure"}, "target": {"id": "BERT Input Representation", "type": "Concept"}, "type": "SHOWS"}, {"source": {"id": "Input Representation", "type": "Concept"}, "target": {"id": "Input Embeddings", "type": "Concept"}, "type": "HAS_PART"}, {"source": {"id": "Input Embeddings", "type": "Concept"}, "target": {"id": "Token Embeddings", "type": "Concept"}, "type": "IS_SUM_OF"}, {"source": {"id": "Input Embeddings", "type": "Concept"}, "target": {"id": "Segmentation Embeddings", "type": "Concept"}, "type": "IS_SUM_OF"}, {"source": {"id": "Input Embeddings", "type": "Concept"}, "target": {"id": "Position Embeddings", "type": "Concept"}, "type": "IS_SUM_OF"}, {"source": {"id": "Next Sentence Prediction (NSP)", "type": "Task"}, "target": {"id": "Representation Learning Objectives", "type": "Concept"}, "type": "RELATED_TO"}, {"source": {"id": "Representation Learning Objectives", "type": "Concept"}, "target": {"id": "Jernite et al. (2017)", "type": "Publication"}, "type": "USED_IN"}, {"source": {"id": "Representation Learning Objectives", "type": "Concept"}, "target": {"id": "Logeswaran and Lee (2018)", "type": "Publication"}, "type": "USED_IN"}, {"source": {"id": "Sentence Embeddings", "type": "Concept"}, "target": {"id": "Downstream Tasks", "type": "Task Category"}, "type": "TRANSFERRED_TO"}, {"source": {"id": "BERT", "type": "AI Model"}, "target": {"id": "Parameters", "type": "Concept"}, "type": "TRANSFERS"}, {"source": {"id": "Parameters", "type": "Concept"}, "target": {"id": "End-Task Model Parameters", "type": "Concept"}, "type": "INITIALIZE"}, {"source": {"id": "Pre-training Data", "type": "Concept"}, "target": {"id": "Language Model Pre-training", "type": "Process"}, "type": "USED_FOR"}, {"source": {"id": "Pre-training Data", "type": "Concept"}, "target": {"id": "BooksCorpus", "type": "Dataset"}, "type": "INCLUDES"}, {"source": {"id": "BooksCorpus", "type": "Dataset"}, "target": {"id": "Zhu et al., 2015", "type": "Publication"}, "type": "REFERENCED_BY"}, {"source": {"id": "Pre-training Data", "type": "Concept"}, "target": {"id": "English Wikipedia", "type": "Dataset"}, "type": "INCLUDES"}, {"source": {"id": "English Wikipedia", "type": "Dataset"}, "target": {"id": "Wikipedia", "type": "Resource"}, "type": "IS_A"}, {"source": {"id": "English Wikipedia", "type": "Dataset"}, "target": {"id": "Language Model Pre-training", "type": "Process"}, "type": "USED_FOR"}, {"source": {"id": "Document-level Corpus", "type": "Concept"}, "target": {"id": "Shuffled Sentence-level Corpus", "type": "Concept"}, "type": "PREFERRED_OVER"}, {"source": {"id": "Billion Word Benchmark", "type": "Dataset"}, "target": {"id": "Shuffled Sentence-level Corpus", "type": "Concept"}, "type": "IS_A"}, {"source": {"id": "Billion Word Benchmark", "type": "Dataset"}, "target": {"id": "Chelba et al., 2013", "type": "Publication"}, "type": "REFERENCED_BY"}, {"source": {"id": "Fine-tuning BERT", "type": "Process"}, "target": {"id": "Self-attention Mechanism", "type": "Concept"}, "type": "USES"}, {"source": {"id": "Self-attention Mechanism", "type": "Concept"}, "target": {"id": "Transformer", "type": "AI Model"}, "type": "PART_OF"}, {"source": {"id": "Transformer", "type": "AI Model"}, "target": {"id": "BERT", "type": "AI Model"}, "type": "ALLOWS"}, {"source": {"id": "BERT", "type": "AI Model"}, "target": {"id": "Downstream Tasks", "type": "Task Category"}, "type": "MODELS"}, {"source": {"id": "Parikh et al. (2016)", "type": "Publication"}, "target": {"id": "Bidirectional Cross Attention", "type": "Concept"}, "type": "USES"}, {"source": {"id": "Seo et al. (2017)", "type": "Publication"}, "target": {"id": "Bidirectional Cross Attention", "type": "Concept"}, "type": "USES"}, {"source": {"id": "BERT", "type": "AI Model"}, "target": {"id": "Self-attention Mechanism", "type": "Concept"}, "type": "USES"}, {"source": {"id": "Self-attention Mechanism", "type": "Concept"}, "target": {"id": "Bidirectional Cross Attention", "type": "Concept"}, "type": "INCLUDES"}, {"source": {"id": "Task-specific Inputs", "type": "Concept"}, "target": {"id": "BERT", "type": "AI Model"}, "type": "FED_INTO"}, {"source": {"id": "Task-specific Outputs", "type": "Concept"}, "target": {"id": "BERT", "type": "AI Model"}, "type": "FED_INTO"}, {"source": {"id": "BERT", "type": "AI Model"}, "target": {"id": "Parameters", "type": "Concept"}, "type": "FINE_TUNES"}, {"source": {"id": "Sentence A", "type": "Concept"}, "target": {"id": "Paraphrasing", "type": "Task"}, "type": "ANALOGOUS_TO_PAIRS_IN"}, {"source": {"id": "Sentence B", "type": "Concept"}, "target": {"id": "Entailment", "type": "Task"}, "type": "ANALOGOUS_TO_PAIRS_IN"}, {"source": {"id": "Sentence A", "type": "Concept"}, "target": {"id": "Question Answering", "type": "Task"}, "type": "ANALOGOUS_TO_PAIRS_IN"}, {"source": {"id": "Sentence B", "type": "Concept"}, "target": {"id": "Text Classification", "type": "Task"}, "type": "ANALOGOUS_TO_PAIRS_IN"}, {"source": {"id": "Token Representations", "type": "Concept"}, "target": {"id": "Output Layer", "type": "Component"}, "type": "FED_INTO"}, {"source": {"id": "Output Layer", "type": "Component"}, "target": {"id": "Sequence Tagging", "type": "Task"}, "type": "USED_FOR"}, {"source": {"id": "Output Layer", "type": "Component"}, "target": {"id": "Question Answering", "type": "Task"}, "type": "USED_FOR"}, {"source": {"id": "[CLS] Representation", "type": "Concept"}, "target": {"id": "Output Layer", "type": "Component"}, "type": "FED_INTO"}, {"source": {"id": "Output Layer", "type": "Component"}, "target": {"id": "Entailment", "type": "Task"}, "type": "USED_FOR"}, {"source": {"id": "Output Layer", "type": "Component"}, "target": {"id": "Sentiment Analysis", "type": "Task"}, "type": "USED_FOR"}, {"source": {"id": "Fine-tuning", "type": "Process"}, "target": {"id": "Cloud TPU", "type": "Hardware"}, "type": "REPLICABLE_ON"}, {"source": {"id": "Fine-tuning", "type": "Process"}, "target": {"id": "GPU", "type": "Hardware"}, "type": "REPLICABLE_ON"}, {"source": {"id": "Section 4", "type": "Section"}, "target": {"id": "Task-specific Details", "type": "Concept"}, "type": "CONTAINS"}, {"source": {"id": "Appendix A.5", "type": "Appendix"}, "target": {"id": "Details", "type": "Concept"}, "type": "CONTAINS"}, {"source": {"id": "BERT", "type": "AI Model"}, "target": {"id": "NLP Tasks", "type": "Task Category"}, "type": "FINE_TUNED_ON"}, {"source": {"id": "GLUE", "type": "Benchmark"}, "target": {"id": "Wang et al., 2018a", "type": "Publication"}, "type": "REFERENCED_BY"}, {"source": {"id": "GLUE", "type": "Benchmark"}, "target": {"id": "Natural Language Understanding Tasks", "type": "Task Category"}, "type": "INCLUDES"}, {"source": {"id": "Appendix B.1", "type": "Appendix"}, "target": {"id": "GLUE", "type": "Benchmark"}, "type": "CONTAINS_DATASET_DESCRIPTIONS_OF"}, {"source": {"id": "Input Sequence", "type": "Concept"}, "target": {"id": "Section 3", "type": "Section"}, "type": "REPRESENTED_AS_DESCRIBED_IN"}, {"source": {"id": "Final Hidden Vector C", "type": "Concept"}, "target": {"id": "Aggregate Representation", "type": "Concept"}, "type": "USED_AS"}, {"source": {"id": "Final Hidden Vector C", "type": "Concept"}, "target": {"id": "[CLS] Token", "type": "Concept"}, "type": "CORRESPONDS_TO"}, {"source": {"id": "Classification Layer Weights W", "type": "Concept"}, "target": {"id": "Fine-tuning", "type": "Process"}, "type": "INTRODUCED_DURING"}, {"source": {"id": "Classification Loss", "type": "Concept"}, "target": {"id": "Final Hidden Vector C", "type": "Concept"}, "type": "COMPUTED_WITH"}, {"source": {"id": "Classification Loss", "type": "Concept"}, "target": {"id": "Classification Layer Weights W", "type": "Concept"}, "type": "COMPUTED_WITH"}, {"source": {"id": "BERT SQuAD Model", "type": "AI Model"}, "target": {"id": "Cloud TPU", "type": "Hardware"}, "type": "TRAINED_ON"}, {"source": {"id": "BERT SQuAD Model", "type": "AI Model"}, "target": {"id": "Dev F1 Score", "type": "Metric"}, "type": "ACHIEVES"}, {"source": {"id": "GLUE Benchmark FAQ", "type": "Resource"}, "target": {"id": "(10)", "type": "Concept"}, "type": "CONTAINS"}, {"source": {"id": "Table 1", "type": "Table"}, "target": {"id": "GLUE Test Results", "type": "Results"}, "type": "SHOWS"}, {"source": {"id": "GLUE Test Results", "type": "Results"}, "target": {"id": "GLUE Leaderboard", "type": "Resource"}, "type": "SCORED_BY"}, {"source": {"id": "GLUE Test Results", "type": "Results"}, "target": {"id": "WNLI Set", "type": "Dataset"}, "type": "EXCLUDES_FROM_AVERAGE"}, {"source": {"id": "BERT", "type": "AI Model"}, "target": {"id": "Single-model, Single-task", "type": "Characteristic"}, "type": "IS_A"}, {"source": {"id": "OpenAI GPT", "type": "AI Model"}, "target": {"id": "Single-model, Single-task", "type": "Characteristic"}, "type": "IS_A"}, {"source": {"id": "F1 Score", "type": "Metric"}, "target": {"id": "QQP", "type": "Task"}, "type": "REPORTED_FOR"}, {"source": {"id": "F1 Score", "type": "Metric"}, "target": {"id": "MRPC", "type": "Task"}, "type": "REPORTED_FOR"}, {"source": {"id": "Spearman Correlations", "type": "Metric"}, "target": {"id": "STS-B", "type": "Task"}, "type": "REPORTED_FOR"}, {"source": {"id": "Accuracy Scores", "type": "Metric"}, "target": {"id": "GLUE Tasks", "type": "Task Category"}, "type": "REPORTED_FOR"}, {"source": {"id": "GLUE Tasks", "type": "Task Category"}, "target": {"id": "Batch Size", "type": "Hyperparameter"}, "type": "USED_BATCH_SIZE"}, {"source": {"id": "GLUE Tasks", "type": "Task Category"}, "target": {"id": "Epochs", "type": "Hyperparameter"}, "type": "FINE_TUNED_FOR"}, {"source": {"id": "Learning Rate", "type": "Hyperparameter"}, "target": {"id": "Dev Set", "type": "Dataset"}, "type": "SELECTED_ON"}, {"source": {"id": "BERTLARGE", "type": "AI Model"}, "target": {"id": "Random Restarts", "type": "Method"}, "type": "USED"}, {"source": {"id": "Random Restarts", "type": "Method"}, "target": {"id": "Pre-trained Checkpoint", "type": "Concept"}, "type": "USES"}, {"source": {"id": "Random Restarts", "type": "Method"}, "target": {"id": "Fine-tuning Data Shuffling", "type": "Process"}, "type": "INVOLVES"}, {"source": {"id": "Random Restarts", "type": "Method"}, "target": {"id": "Classifier Layer Initialization", "type": "Process"}, "type": "INVOLVES"}, {"source": {"id": "Table 1", "type": "Table"}, "target": {"id": "GLUE Test Results", "type": "Results"}, "type": "PRESENTS"}, {"source": {"id": "BERTBASE", "type": "AI Model"}, "target": {"id": "All Systems", "type": "System"}, "type": "OUTPERFORMS"}, {"source": {"id": "BERTLARGE", "type": "AI Model"}, "target": {"id": "All Systems", "type": "System"}, "type": "OUTPERFORMS"}, {"source": {"id": "BERTBASE", "type": "AI Model"}, "target": {"id": "OpenAI GPT", "type": "AI Model"}, "type": "HAS_NEARLY_IDENTICAL_MODEL_ARCHITECTURE_TO"}, {"source": {"id": "Model Architecture", "type": "Concept"}, "target": {"id": "Attention Masking", "type": "Concept"}, "type": "DIFFERS_BY"}, {"source": {"id": "BERT", "type": "AI Model"}, "target": {"id": "MNLI", "type": "Task"}, "type": "OBTAINS_ACCURACY_IMPROVEMENT_ON"}, {"source": {"id": "BERTLARGE", "type": "AI Model"}, "target": {"id": "GLUE Leaderboard", "type": "Resource"}, "type": "OBTAINS_SCORE_ON"}, {"source": {"id": "OpenAI GPT", "type": "AI Model"}, "target": {"id": "GLUE Leaderboard", "type": "Resource"}, "type": "OBTAINS_SCORE_ON"}, {"source": {"id": "BERTLARGE", "type": "AI Model"}, "target": {"id": "BERTBASE", "type": "AI Model"}, "type": "OUTPERFORMS"}, {"source": {"id": "Section 5.2", "type": "Section"}, "target": {"id": "Effect of Model Size", "type": "Concept"}, "type": "EXPLORES"}, {"source": {"id": "SQuAD v1.1", "type": "Dataset"}, "target": {"id": "Stanford Question Answering Dataset", "type": "Dataset"}, "type": "IS_A"}, {"source": {"id": "SQuAD v1.1", "type": "Dataset"}, "target": {"id": "Question/Answer Pairs", "type": "Data Type"}, "type": "CONTAINS"}, {"source": {"id": "SQuAD v1.1", "type": "Dataset"}, "target": {"id": "Rajpurkar et al., 2016", "type": "Publication"}, "type": "REFERENCED_BY"}, {"source": {"id": "SQuAD v1.1", "type": "Dataset"}, "target": {"id": "Answer Text Span Prediction", "type": "Task"}, "type": "INVOLVES"}, {"source": {"id": "Input Question", "type": "Concept"}, "target": {"id": "Wikipedia", "type": "Resource"}, "type": "FROM"}, {"source": {"id": "Figure 1", "type": "Figure"}, "target": {"id": "Question Answering", "type": "Task"}, "type": "SHOWS"}, {"source": {"id": "Input Question", "type": "Concept"}, "target": {"id": "Single Packed Sequence", "type": "Concept"}, "type": "REPRESENTED_AS"}, {"source": {"id": "Passage", "type": "Concept"}, "target": {"id": "Single Packed Sequence", "type": "Concept"}, "type": "REPRESENTED_AS"}, {"source": {"id": "Input Question", "type": "Concept"}, "target": {"id": "A Embedding", "type": "Concept"}, "type": "USES"}, {"source": {"id": "Passage", "type": "Concept"}, "target": {"id": "B Embedding", "type": "Concept"}, "type": "USES"}, {"source": {"id": "Start Vector S", "type": "Concept"}, "target": {"id": "Fine-tuning", "type": "Process"}, "type": "INTRODUCED_DURING"}, {"source": {"id": "End Vector E", "type": "Concept"}, "target": {"id": "Fine-tuning", "type": "Process"}, "type": "INTRODUCED_DURING"}, {"source": {"id": "Training Objective", "type": "Concept"}, "target": {"id": "Log-likelihoods", "type": "Metric"}, "type": "IS_SUM_OF"}, {"source": {"id": "SQuAD v1.1", "type": "Dataset"}, "target": {"id": "Epochs", "type": "Hyperparameter"}, "type": "FINE_TUNED_FOR"}, {"source": {"id": "SQuAD v1.1", "type": "Dataset"}, "target": {"id": "Learning Rate", "type": "Hyperparameter"}, "type": "USED_LEARNING_RATE"}, {"source": {"id": "SQuAD v1.1", "type": "Dataset"}, "target": {"id": "Batch Size", "type": "Hyperparameter"}, "type": "USED_BATCH_SIZE"}, {"source": {"id": "Table 2", "type": "Table"}, "target": {"id": "SQuAD Leaderboard", "type": "Resource"}, "type": "SHOWS_ENTRIES_FROM"}, {"source": {"id": "Table 2", "type": "Table"}, "target": {"id": "Seo et al. (2017)", "type": "Publication"}, "type": "SHOWS_RESULTS_FROM"}, {"source": {"id": "Table 2", "type": "Table"}, "target": {"id": "Clark and Gardner, 2018", "type": "Publication"}, "type": "SHOWS_RESULTS_FROM"}, {"source": {"id": "Table 2", "type": "Table"}, "target": {"id": "Peters et al., 2018a", "type": "Publication"}, "type": "SHOWS_RESULTS_FROM"}, {"source": {"id": "Table 2", "type": "Table"}, "target": {"id": "Hu et al., 2018", "type": "Publication"}, "type": "SHOWS_RESULTS_FROM"}, {"source": {"id": "System", "type": "System"}, "target": {"id": "Data Augmentation", "type": "Method"}, "type": "USES"}, {"source": {"id": "System", "type": "System"}, "target": {"id": "TriviaQA", "type": "Dataset"}, "type": "FINE_TUNED_ON"}, {"source": {"id": "TriviaQA", "type": "Dataset"}, "target": {"id": "Joshi et al., 2017", "type": "Publication"}, "type": "REFERENCED_BY"}, {"source": {"id": "BERT", "type": "AI Model"}, "target": {"id": "Top Leaderboard System", "type": "System"}, "type": "OUTPERFORMS"}, {"source": {"id": "BERT", "type": "AI Model"}, "target": {"id": "Top Ensemble System", "type": "System"}, "type": "OUTPERFORMS"}, {"source": {"id": "QANet", "type": "AI Model"}, "target": {"id": "Yu et al. (2018)", "type": "Publication"}, "type": "DESCRIBED_IN"}, {"source": {"id": "QANet", "type": "AI Model"}, "target": {"id": "Publication", "type": "Concept"}, "type": "IMPROVED_AFTER"}, {"source": {"id": "Table 2", "type": "Table"}, "target": {"id": "SQuAD 1.1 Results", "type": "Results"}, "type": "SHOWS"}, {"source": {"id": "BERTLARGE Single + TriviaQA", "type": "AI Model"}, "target": {"id": "TriviaQA", "type": "Dataset"}, "type": "USES"}, {"source": {"id": "BERTLARGE Ensemble + TriviaQA", "type": "AI Model"}, "target": {"id": "TriviaQA", "type": "Dataset"}, "type": "USES"}, {"source": {"id": "SQuAD 2.0", "type": "Task"}, "target": {"id": "SQuAD v1.1", "type": "Dataset"}, "type": "EXTENDS"}, {"source": {"id": "SQuAD v1.1 BERT Model", "type": "AI Model"}, "target": {"id": "SQuAD 2.0", "type": "Task"}, "type": "USED_FOR"}, {"source": {"id": "No-answer Span", "type": "Concept"}, "target": {"id": "[CLS] Token", "type": "Concept"}, "type": "USES_START_END_AT"}, {"source": {"id": "Threshold \u03c4", "type": "Concept"}, "target": {"id": "Dev Set", "type": "Dataset"}, "type": "SELECTED_ON"}, {"source": {"id": "SQuAD 2.0", "type": "Task"}, "target": {"id": "TriviaQA", "type": "Dataset"}, "type": "DID_NOT_USE"}, {"source": {"id": "SQuAD 2.0", "type": "Task"}, "target": {"id": "Epochs", "type": "Hyperparameter"}, "type": "FINE_TUNED_FOR"}, {"source": {"id": "SQuAD 2.0", "type": "Task"}, "target": {"id": "Learning Rate", "type": "Hyperparameter"}, "type": "USED_LEARNING_RATE"}, {"source": {"id": "SQuAD 2.0", "type": "Task"}, "target": {"id": "Batch Size", "type": "Hyperparameter"}, "type": "USED_BATCH_SIZE"}, {"source": {"id": "Table 3", "type": "Table"}, "target": {"id": "SQuAD 2.0 Results", "type": "Results"}, "type": "SHOWS"}, {"source": {"id": "Table 3", "type": "Table"}, "target": {"id": "Sun et al., 2018", "type": "Publication"}, "type": "COMPARES_TO"}, {"source": {"id": "Table 3", "type": "Table"}, "target": {"id": "Wang et al., 2018b", "type": "Publication"}, "type": "COMPARES_TO"}, {"source": {"id": "SWAG", "type": "Dataset"}, "target": {"id": "Situations With Adversarial Generations", "type": "Dataset"}, "type": "IS_A"}, {"source": {"id": "SWAG", "type": "Dataset"}, "target": {"id": "Zellers et al., 2018", "type": "Publication"}, "type": "REFERENCED_BY"}, {"source": {"id": "SWAG", "type": "Dataset"}, "target": {"id": "Sentence-pair Completion Examples", "type": "Data Type"}, "type": "CONTAINS"}, {"source": {"id": "SWAG", "type": "Dataset"}, "target": {"id": "Grounded Commonsense Inference", "type": "Task"}, "type": "EVALUATES"}, {"source": {"id": "SWAG", "type": "Dataset"}, "target": {"id": "Fine-tuning", "type": "Process"}, "type": "USED_FOR"}, {"source": {"id": "Sentence A", "type": "Concept"}, "target": {"id": "Sentence B", "type": "Concept"}, "type": "CONCATENATED_WITH"}, {"source": {"id": "[CLS] Token Representation C", "type": "Concept"}, "target": {"id": "Score", "type": "Concept"}, "type": "USED_TO_COMPUTE"}, {"source": {"id": "Score", "type": "Concept"}, "target": {"id": "Softmax Layer", "type": "Component"}, "type": "NORMALIZED_WITH"}, {"source": {"id": "SWAG", "type": "Dataset"}, "target": {"id": "Epochs", "type": "Hyperparameter"}, "type": "FINE_TUNED_FOR"}, {"source": {"id": "SWAG", "type": "Dataset"}, "target": {"id": "Learning Rate", "type": "Hyperparameter"}, "type": "USED_LEARNING_RATE"}, {"source": {"id": "SWAG", "type": "Dataset"}, "target": {"id": "Batch Size", "type": "Hyperparameter"}, "type": "USED_BATCH_SIZE"}, {"source": {"id": "Table 4", "type": "Table"}, "target": {"id": "SWAG Dev and Test Accuracies", "type": "Results"}, "type": "PRESENTS"}, {"source": {"id": "BERTLARGE", "type": "AI Model"}, "target": {"id": "ESIM+ELMo", "type": "System"}, "type": "OUTPERFORMS"}, {"source": {"id": "BERTLARGE", "type": "AI Model"}, "target": {"id": "OpenAI GPT", "type": "AI Model"}, "type": "OUTPERFORMS"}, {"source": {"id": "Human Performance", "type": "Metric"}, "target": {"id": "SWAG Paper", "type": "Publication"}, "type": "REPORTED_IN"}, {"source": {"id": "Ablation Studies", "type": "Research Method"}, "target": {"id": "BERT", "type": "AI Model"}, "type": "PERFORMED_ON"}, {"source": {"id": "Appendix C", "type": "Appendix"}, "target": {"id": "Ablation Studies", "type": "Research Method"}, "type": "CONTAINS"}, {"source": {"id": "No NSP", "type": "Model Variant"}, "target": {"id": "Bidirectional Model", "type": "Model Type"}, "type": "IS_A"}, {"source": {"id": "No NSP", "type": "Model Variant"}, "target": {"id": "Masked LM (MLM)", "type": "Task"}, "type": "TRAINED_USING"}, {"source": {"id": "No NSP", "type": "Model Variant"}, "target": {"id": "Next Sentence Prediction (NSP)", "type": "Task"}, "type": "WITHOUT"}, {"source": {"id": "LTR & No NSP", "type": "Model Variant"}, "target": {"id": "Left-context-only Model", "type": "Model Type"}, "type": "IS_A"}, {"source": {"id": "LTR & No NSP", "type": "Model Variant"}, "target": {"id": "Left-to-Right (LTR) LM", "type": "Model Type"}, "type": "TRAINED_USING"}, {"source": {"id": "LTR & No NSP", "type": "Model Variant"}, "target": {"id": "Next Sentence Prediction (NSP)", "type": "Task"}, "type": "WITHOUT"}, {"source": {"id": "LTR & No NSP", "type": "Model Variant"}, "target": {"id": "OpenAI GPT", "type": "AI Model"}, "type": "COMPARABLE_TO"}, {"source": {"id": "Table 5", "type": "Table"}, "target": {"id": "Next Sentence Prediction (NSP)", "type": "Task"}, "type": "SHOWS_IMPACT_OF"}, {"source": {"id": "No NSP", "type": "Model Variant"}, "target": {"id": "QNLI", "type": "Task"}, "type": "HURTS_PERFORMANCE_ON"}, {"source": {"id": "No NSP", "type": "Model Variant"}, "target": {"id": "MNLI", "type": "Task"}, "type": "HURTS_PERFORMANCE_ON"}, {"source": {"id": "No NSP", "type": "Model Variant"}, "target": {"id": "SQuAD v1.1", "type": "Dataset"}, "type": "HURTS_PERFORMANCE_ON"}, {"source": {"id": "LTR Model", "type": "Model Type"}, "target": {"id": "MLM Model", "type": "Model Type"}, "type": "PERFORMS_WORSE_THAN"}, {"source": {"id": "LTR Model", "type": "Model Type"}, "target": {"id": "Token Predictions", "type": "Task"}, "type": "PERFORMS_POORLY_AT"}, {"source": {"id": "LTR Model", "type": "Model Type"}, "target": {"id": "BiLSTM", "type": "Model Component"}, "type": "STRENGTHENED_WITH"}, {"source": {"id": "BiLSTM", "type": "Model Component"}, "target": {"id": "SQuAD", "type": "Dataset"}, "type": "IMPROVES_RESULTS_ON"}, {"source": {"id": "BiLSTM", "type": "Model Component"}, "target": {"id": "GLUE Tasks", "type": "Task Category"}, "type": "HURTS_PERFORMANCE_ON"}, {"source": {"id": "ELMo", "type": "AI Model"}, "target": {"id": "LTR Models", "type": "Model Type"}, "type": "USES_CONCATENATION_OF"}, {"source": {"id": "ELMo", "type": "AI Model"}, "target": {"id": "RTL Models", "type": "Model Type"}, "type": "USES_CONCATENATION_OF"}, {"source": {"id": "RTL Models", "type": "Model Type"}, "target": {"id": "QA", "type": "Task"}, "type": "NON_INTUITIVE_FOR"}, {"source": {"id": "Table 5", "type": "Table"}, "target": {"id": "Ablation over Pre-training Tasks", "type": "Experiment"}, "type": "SHOWS"}, {"source": {"id": "Ablation over Pre-training Tasks", "type": "Experiment"}, "target": {"id": "BERTBASE Architecture", "type": "Architecture"}, "type": "USES"}, {"source": {"id": "No NSP", "type": "Model Variant"}, "target": {"id": "Next Sentence Prediction (NSP)", "type": "Task"}, "type": "TRAINED_WITHOUT"}, {"source": {"id": "LTR & No NSP", "type": "Model Variant"}, "target": {"id": "Left-to-Right (LTR) LM", "type": "Model Type"}, "type": "TRAINED_AS"}, {"source": {"id": "+ BiLSTM", "type": "Model Variant"}, "target": {"id": "BiLSTM", "type": "Model Component"}, "type": "ADDS"}, {"source": {"id": "Section 5.2", "type": "Section"}, "target": {"id": "Effect of Model Size", "type": "Concept"}, "type": "EXPLORES"}, {"source": {"id": "Effect of Model Size", "type": "Concept"}, "target": {"id": "Fine-tuning Task Accuracy", "type": "Metric"}, "type": "IMPACTS"}, {"source": {"id": "BERT", "type": "AI Model"}, "target": {"id": "Layers", "type": "Hyperparameter"}, "type": "TRAINED_WITH_DIFFERING"}, {"source": {"id": "BERT", "type": "AI Model"}, "target": {"id": "Hidden Units", "type": "Hyperparameter"}, "type": "TRAINED_WITH_DIFFERING"}, {"source": {"id": "BERT", "type": "AI Model"}, "target": {"id": "Attention Heads", "type": "Hyperparameter"}, "type": "TRAINED_WITH_DIFFERING"}, {"source": {"id": "Table 6", "type": "Table"}, "target": {"id": "GLUE Tasks", "type": "Task Category"}, "type": "SHOWS_RESULTS_ON"}, {"source": {"id": "Dev Set Accuracy", "type": "Metric"}, "target": {"id": "Random Restarts", "type": "Method"}, "type": "REPORTED_FROM"}, {"source": {"id": "Larger Models", "type": "Model Type"}, "target": {"id": "Accuracy Improvement", "type": "Concept"}, "type": "LEAD_TO"}, {"source": {"id": "MRPC", "type": "Task"}, "target": {"id": "Dataset", "type": "Dataset"}, "type": "IS_A"}, {"source": {"id": "Transformer", "type": "AI Model"}, "target": {"id": "Vaswani et al. (2017)", "type": "Publication"}, "type": "EXPLORED_IN"}, {"source": {"id": "Largest Transformer", "type": "AI Model"}, "target": {"id": "Al-Rfou et al., 2018", "type": "Publication"}, "type": "REFERENCED_IN"}, {"source": {"id": "BERTBASE", "type": "AI Model"}, "target": {"id": "Parameters", "type": "Concept"}, "type": "CONTAINS"}, {"source": {"id": "BERTLARGE", "type": "AI Model"}, "target": {"id": "Parameters", "type": "Concept"}, "type": "CONTAINS"}], "original_chunk_ids": ["bidirectional_transformer_raw_chunks_5k_1"], "original_chunk_indices": [1], "original_metadata": [{"source": "bidirectional_transformer_raw.md"}]}
{"nodes": [{"id": "BiLSTM", "type": "Model"}, {"id": "GLUE", "type": "Task"}, {"id": "LTR models", "type": "Architecture"}, {"id": "RTL models", "type": "Architecture"}, {"id": "ELMo", "type": "Model"}, {"id": "bidirectional model", "type": "Architecture"}, {"id": "QA", "type": "Task"}, {"id": "deep bidirectional model", "type": "Architecture"}, {"id": "left context", "type": "Concept"}, {"id": "right context", "type": "Concept"}, {"id": "model size", "type": "Concept"}, {"id": "Accuracy", "type": "Metric"}, {"id": "BERT", "type": "Model"}, {"id": "number of layers", "type": "Concept"}, {"id": "hidden units", "type": "Concept"}, {"id": "Attention Heads", "type": "Component"}, {"id": "Hyperparameter", "type": "Concept"}, {"id": "training procedure", "type": "Method"}, {"id": "Table 6", "type": "Table"}, {"id": "Dev Set accuracy", "type": "Metric"}, {"id": "Fine-tuning", "type": "Approach"}, {"id": "MRPC", "type": "Task"}, {"id": "Pre-training Tasks", "type": "Task"}, {"id": "Transformer", "type": "Architecture"}, {"id": "Vaswani et al., 2017", "type": "Publication"}, {"id": "Transformer (Vaswani et al., 2017)", "type": "Model"}, {"id": "Al-Rfou et al., 2018", "type": "Publication"}, {"id": "Transformer (Al-Rfou et al., 2018)", "type": "Model"}, {"id": "BERTBASE", "type": "Model"}, {"id": "BERTLARGE", "type": "Model"}, {"id": "large-scale tasks", "type": "Task"}, {"id": "Machine Translation", "type": "Task"}, {"id": "Language Modeling", "type": "Task"}, {"id": "LM Perplexity", "type": "Metric"}, {"id": "held-out training data", "type": "Dataset"}, {"id": "small scale tasks", "type": "Task"}, {"id": "Peters et al., 2018b", "type": "Publication"}, {"id": "downstream task impact", "type": "Concept"}, {"id": "pre-trained bi-LM", "type": "Model"}, {"id": "Melamud et al., 2016", "type": "Publication"}, {"id": "hidden dimension size", "type": "Concept"}, {"id": "Feature-based Approach", "type": "Approach"}, {"id": "prior works", "type": "Concept"}, {"id": "task-specific models", "type": "Model"}, {"id": "pre-trained representations", "type": "Concept"}, {"id": "downstream task data", "type": "Dataset"}, {"id": "classification layer", "type": "Component"}, {"id": "pre-trained model", "type": "Model"}, {"id": "Transformer encoder architecture", "type": "Architecture"}, {"id": "task-specific model architecture", "type": "Architecture"}, {"id": "CoNLL-2003 Named Entity Recognition", "type": "Task"}, {"id": "Tjong Kim Sang and De Meulder, 2003", "type": "Publication"}, {"id": "WordPiece model", "type": "Model"}, {"id": "document context", "type": "Concept"}, {"id": "tagging task", "type": "Task"}, {"id": "CRF Layer", "type": "Component"}, {"id": "token-level classifier", "type": "Component"}, {"id": "NER label set", "type": "Concept"}, {"id": "activations", "type": "Concept"}, {"id": "contextual embeddings", "type": "Concept"}, {"id": "Two-Layer 768-Dimensional BiLSTM", "type": "Model"}, {"id": "Table 7", "type": "Table"}, {"id": "state-of-the-art methods", "type": "Concept"}, {"id": "token representations", "type": "Concept"}, {"id": "top four hidden layers", "type": "Component"}, {"id": "pre-trained Transformer", "type": "Model"}, {"id": "transfer learning", "type": "Concept"}, {"id": "language models", "type": "Model"}, {"id": "unsupervised pre-training", "type": "Method"}, {"id": "language understanding systems", "type": "System"}, {"id": "low-resource tasks", "type": "Task"}, {"id": "deep unidirectional architectures", "type": "Architecture"}, {"id": "deep bidirectional architectures", "type": "Architecture"}, {"id": "NLP Tasks", "type": "Task"}, {"id": "Clark et al., 2018", "type": "Publication"}, {"id": "Peters et al., 2018a", "type": "Publication"}, {"id": "CSE", "type": "Model"}, {"id": "Results", "type": "Concept"}], "relationships": [{"source": {"id": "BiLSTM", "type": "Model"}, "target": {"id": "GLUE", "type": "Task"}, "type": "DECREASES_PERFORMANCE_ON"}, {"source": {"id": "ELMo", "type": "Model"}, "target": {"id": "LTR models", "type": "Architecture"}, "type": "USES"}, {"source": {"id": "ELMo", "type": "Model"}, "target": {"id": "RTL models", "type": "Architecture"}, "type": "USES"}, {"source": {"id": "bidirectional model", "type": "Architecture"}, "target": {"id": "LTR models", "type": "Architecture"}, "type": "MORE_POWERFUL_THAN"}, {"source": {"id": "bidirectional model", "type": "Architecture"}, "target": {"id": "RTL models", "type": "Architecture"}, "type": "MORE_POWERFUL_THAN"}, {"source": {"id": "deep bidirectional model", "type": "Architecture"}, "target": {"id": "left context", "type": "Concept"}, "type": "CAN_USE"}, {"source": {"id": "deep bidirectional model", "type": "Architecture"}, "target": {"id": "right context", "type": "Concept"}, "type": "CAN_USE"}, {"source": {"id": "Table 6", "type": "Table"}, "target": {"id": "Dev Set accuracy", "type": "Metric"}, "type": "REPORTS"}, {"source": {"id": "model size", "type": "Concept"}, "target": {"id": "Accuracy", "type": "Metric"}, "type": "AFFECTS"}, {"source": {"id": "BERT", "type": "Model"}, "target": {"id": "number of layers", "type": "Concept"}, "type": "HAS_PROPERTY"}, {"source": {"id": "BERT", "type": "Model"}, "target": {"id": "hidden units", "type": "Concept"}, "type": "HAS_PROPERTY"}, {"source": {"id": "BERT", "type": "Model"}, "target": {"id": "Attention Heads", "type": "Component"}, "type": "HAS_PROPERTY"}, {"source": {"id": "BERT", "type": "Model"}, "target": {"id": "Hyperparameter", "type": "Concept"}, "type": "USES"}, {"source": {"id": "BERT", "type": "Model"}, "target": {"id": "training procedure", "type": "Method"}, "type": "USES"}, {"source": {"id": "Transformer", "type": "Architecture"}, "target": {"id": "Vaswani et al., 2017", "type": "Publication"}, "type": "EXPLORED_BY"}, {"source": {"id": "Transformer (Al-Rfou et al., 2018)", "type": "Model"}, "target": {"id": "Al-Rfou et al., 2018", "type": "Publication"}, "type": "IDENTIFIED_BY"}, {"source": {"id": "model size", "type": "Concept"}, "target": {"id": "continual improvements", "type": "Concept"}, "type": "LEADS_TO"}, {"source": {"id": "continual improvements", "type": "Concept"}, "target": {"id": "large-scale tasks", "type": "Task"}, "type": "ON_TASKS"}, {"source": {"id": "large-scale tasks", "type": "Task"}, "target": {"id": "Machine Translation", "type": "Task"}, "type": "INCLUDES"}, {"source": {"id": "large-scale tasks", "type": "Task"}, "target": {"id": "Language Modeling", "type": "Task"}, "type": "INCLUDES"}, {"source": {"id": "held-out training data", "type": "Dataset"}, "target": {"id": "LM Perplexity", "type": "Metric"}, "type": "HAS_METRIC"}, {"source": {"id": "model size", "type": "Concept"}, "target": {"id": "small scale tasks", "type": "Task"}, "type": "IMPROVES_PERFORMANCE_ON"}, {"source": {"id": "Peters et al., 2018b", "type": "Publication"}, "target": {"id": "downstream task impact", "type": "Concept"}, "type": "STUDIED"}, {"source": {"id": "Melamud et al., 2016", "type": "Publication"}, "target": {"id": "hidden dimension size", "type": "Concept"}, "type": "REPORTED_IMPACT_OF"}, {"source": {"id": "prior works", "type": "Concept"}, "target": {"id": "Feature-based Approach", "type": "Approach"}, "type": "USED_APPROACH"}, {"source": {"id": "task-specific models", "type": "Model"}, "target": {"id": "pre-trained representations", "type": "Concept"}, "type": "BENEFITS_FROM"}, {"source": {"id": "BERT", "type": "Model"}, "target": {"id": "Fine-tuning", "type": "Approach"}, "type": "USED_APPROACH"}, {"source": {"id": "Fine-tuning", "type": "Approach"}, "target": {"id": "classification layer", "type": "Component"}, "type": "ADDS_COMPONENT"}, {"source": {"id": "classification layer", "type": "Component"}, "target": {"id": "pre-trained model", "type": "Model"}, "type": "TO_MODEL"}, {"source": {"id": "Transformer encoder architecture", "type": "Architecture"}, "target": {"id": "tagging task", "type": "Task"}, "type": "NOT_SUITABLE_FOR"}, {"source": {"id": "CoNLL-2003 Named Entity Recognition", "type": "Task"}, "target": {"id": "Tjong Kim Sang and De Meulder, 2003", "type": "Publication"}, "type": "INTRODUCED_BY"}, {"source": {"id": "BERT", "type": "Model"}, "target": {"id": "WordPiece model", "type": "Model"}, "type": "USES"}, {"source": {"id": "CoNLL-2003 Named Entity Recognition", "type": "Task"}, "target": {"id": "tagging task", "type": "Task"}, "type": "FORMULATED_AS"}, {"source": {"id": "tagging task", "type": "Task"}, "target": {"id": "CRF Layer", "type": "Component"}, "type": "DOES_NOT_USE"}, {"source": {"id": "token-level classifier", "type": "Component"}, "target": {"id": "NER label set", "type": "Concept"}, "type": "OPERATES_ON"}, {"source": {"id": "Feature-based Approach", "type": "Approach"}, "target": {"id": "activations", "type": "Concept"}, "type": "EXTRACTS"}, {"source": {"id": "activations", "type": "Concept"}, "target": {"id": "BERT", "type": "Model"}, "type": "FROM"}, {"source": {"id": "contextual embeddings", "type": "Concept"}, "target": {"id": "Two-Layer 768-Dimensional BiLSTM", "type": "Model"}, "type": "INPUT_TO"}, {"source": {"id": "Two-Layer 768-Dimensional BiLSTM", "type": "Model"}, "target": {"id": "classification layer", "type": "Component"}, "type": "PRECEDES"}, {"source": {"id": "Table 7", "type": "Table"}, "target": {"id": "Results", "type": "Concept"}, "type": "PRESENTS"}, {"source": {"id": "BERTLARGE", "type": "Model"}, "target": {"id": "state-of-the-art methods", "type": "Concept"}, "type": "PERFORMS_COMPETITIVELY_WITH"}, {"source": {"id": "Feature-based Approach", "type": "Approach"}, "target": {"id": "token representations", "type": "Concept"}, "type": "CONCATENATES"}, {"source": {"id": "token representations", "type": "Concept"}, "target": {"id": "top four hidden layers", "type": "Component"}, "type": "FROM"}, {"source": {"id": "top four hidden layers", "type": "Component"}, "target": {"id": "pre-trained Transformer", "type": "Model"}, "type": "PART_OF"}, {"source": {"id": "BERT", "type": "Model"}, "target": {"id": "Fine-tuning", "type": "Approach"}, "type": "EFFECTIVE_FOR"}, {"source": {"id": "BERT", "type": "Model"}, "target": {"id": "Feature-based Approach", "type": "Approach"}, "type": "EFFECTIVE_FOR"}, {"source": {"id": "transfer learning", "type": "Concept"}, "target": {"id": "language models", "type": "Model"}, "type": "USES_MODEL_TYPE"}, {"source": {"id": "unsupervised pre-training", "type": "Method"}, "target": {"id": "language understanding systems", "type": "System"}, "type": "INTEGRAL_PART_OF"}, {"source": {"id": "low-resource tasks", "type": "Task"}, "target": {"id": "deep unidirectional architectures", "type": "Architecture"}, "type": "BENEFIT_FROM"}, {"source": {"id": "deep bidirectional architectures", "type": "Architecture"}, "target": {"id": "BERT", "type": "Model"}, "type": "ALLOWS"}, {"source": {"id": "BERT", "type": "Model"}, "target": {"id": "NLP Tasks", "type": "Task"}, "type": "TACKLES"}], "original_chunk_ids": ["bidirectional_transformer_raw_chunks_5k_2"], "original_chunk_indices": [2], "original_metadata": [{"source": "bidirectional_transformer_raw.md"}]}
{"nodes": [{"id": "Richard Socher", "type": "Person"}, {"id": "Alex Perelygin", "type": "Person"}, {"id": "Jean Wu", "type": "Person"}, {"id": "Jason Chuang", "type": "Person"}, {"id": "Christopher D Manning", "type": "Person"}, {"id": "Andrew Ng", "type": "Person"}, {"id": "Christopher Potts", "type": "Person"}, {"id": "Recursive deep models for semantic compositionality over a sentiment treebank", "type": "Publication"}, {"id": "Proceedings of the 2013 conference on empirical methods in natural language processing", "type": "Conference"}, {"id": "Fu Sun", "type": "Person"}, {"id": "Linyang Li", "type": "Person"}, {"id": "Xipeng Qiu", "type": "Person"}, {"id": "Yang Liu", "type": "Person"}, {"id": "U-net: Machine reading comprehension with unanswerable questions", "type": "Publication"}, {"id": "arXiv preprint arXiv:1810.06638", "type": "Publication"}, {"id": "Wilson L Taylor", "type": "Person"}, {"id": "Cloze procedure: A new tool for measuring readability", "type": "Publication"}, {"id": "Journalism Bulletin", "type": "Journal"}, {"id": "Erik F Tjong Kim Sang", "type": "Person"}, {"id": "Fien De Meulder", "type": "Person"}, {"id": "Introduction to the conll-2003 shared task: Language-independent named entity recognition", "type": "Publication"}, {"id": "CoNLL", "type": "Conference"}, {"id": "Joseph Turian", "type": "Person"}, {"id": "Lev Ratinov", "type": "Person"}, {"id": "Yoshua Bengio", "type": "Person"}, {"id": "Word representations: A simple and general method for semi-supervised learning", "type": "Publication"}, {"id": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics", "type": "Conference"}, {"id": "Ashish Vaswani", "type": "Person"}, {"id": "Noam Shazeer", "type": "Person"}, {"id": "Niki Parmar", "type": "Person"}, {"id": "Jakob Uszkoreit", "type": "Person"}, {"id": "Llion Jones", "type": "Person"}, {"id": "Aidan N Gomez", "type": "Person"}, {"id": "Lukasz Kaiser", "type": "Person"}, {"id": "Illia Polosukhin", "type": "Person"}, {"id": "Attention is all you need", "type": "Publication"}, {"id": "Advances in Neural Information Processing Systems", "type": "Conference"}, {"id": "Pascal Vincent", "type": "Person"}, {"id": "Hugo Larochelle", "type": "Person"}, {"id": "Pierre-Antoine Manzagol", "type": "Person"}, {"id": "Extracting and composing robust features with denoising autoencoders", "type": "Publication"}, {"id": "Proceedings of the 25th international conference on Machine learning", "type": "Conference"}, {"id": "ACM", "type": "Organization"}, {"id": "Alex Wang", "type": "Person"}, {"id": "Amanpreet Singh", "type": "Person"}, {"id": "Julian Michael", "type": "Person"}, {"id": "Felix Hill", "type": "Person"}, {"id": "Omer Levy", "type": "Person"}, {"id": "Samuel R Bowman", "type": "Person"}, {"id": "Glue: A multi-task benchmark and analysis platform for natural language understanding", "type": "Publication"}, {"id": "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP", "type": "Conference"}, {"id": "Wei Wang", "type": "Person"}, {"id": "Ming Yan", "type": "Person"}, {"id": "Chen Wu", "type": "Person"}, {"id": "Multi-granularity hierarchical attention fusion networks for reading comprehension and question answering", "type": "Publication"}, {"id": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)", "type": "Conference"}, {"id": "Association for Computational Linguistics", "type": "Organization"}, {"id": "Alex Warstadt", "type": "Person"}, {"id": "Neural network acceptability judgments", "type": "Publication"}, {"id": "arXiv preprint arXiv:1805.12471", "type": "Publication"}, {"id": "Adina Williams", "type": "Person"}, {"id": "Nikita Nangia", "type": "Person"}, {"id": "A broad-coverage challenge corpus for sentence understanding through inference", "type": "Publication"}, {"id": "NAACL", "type": "Conference"}, {"id": "Yonghui Wu", "type": "Person"}, {"id": "Mike Schuster", "type": "Person"}, {"id": "Zhifeng Chen", "type": "Person"}, {"id": "Quoc V Le", "type": "Person"}, {"id": "Mohammad Norouzi", "type": "Person"}, {"id": "Wolfgang Macherey", "type": "Person"}, {"id": "Maxim Krikun", "type": "Person"}, {"id": "Yuan Cao", "type": "Person"}, {"id": "Qin Gao", "type": "Person"}, {"id": "Klaus Macherey", "type": "Person"}, {"id": "Google", "type": "Organization"}, {"id": "Google\u2019s neural machine translation system: Bridging the gap between human and machine translation", "type": "Publication"}, {"id": "arXiv preprint arXiv:1609.08144", "type": "Publication"}, {"id": "Jason Yosinski", "type": "Person"}, {"id": "Jeff Clune", "type": "Person"}, {"id": "Hod Lipson", "type": "Person"}, {"id": "How transferable are features in deep neural networks?", "type": "Publication"}, {"id": "Adams Wei Yu", "type": "Person"}, {"id": "David Dohan", "type": "Person"}, {"id": "Minh-Thang Luong", "type": "Person"}, {"id": "Rui Zhao", "type": "Person"}, {"id": "Kai Chen", "type": "Person"}, {"id": "ICLR", "type": "Conference"}, {"id": "QANet: Combining local convolution with global self-attention for reading comprehension", "type": "Publication"}, {"id": "Rowan Zellers", "type": "Person"}, {"id": "Yonatan Bisk", "type": "Person"}, {"id": "Roy Schwartz", "type": "Person"}, {"id": "Yejin Choi", "type": "Person"}, {"id": "Swag: A large-scale adversarial dataset for grounded commonsense inference", "type": "Publication"}, {"id": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)", "type": "Conference"}, {"id": "Yukun Zhu", "type": "Person"}, {"id": "Ryan Kiros", "type": "Person"}, {"id": "Rich Zemel", "type": "Person"}, {"id": "Ruslan Salakhutdinov", "type": "Person"}, {"id": "Raquel Urtasun", "type": "Person"}, {"id": "Antonio Torralba", "type": "Person"}, {"id": "Sanja Fidler", "type": "Person"}, {"id": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books", "type": "Publication"}, {"id": "Proceedings of the IEEE international conference on computer vision", "type": "Conference"}, {"id": "IEEE", "type": "Organization"}, {"id": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "type": "Publication"}, {"id": "BERT", "type": "Model"}, {"id": "Appendix A", "type": "Concept"}, {"id": "Appendix B", "type": "Concept"}, {"id": "Appendix C", "type": "Concept"}, {"id": "Effect of Number of Training Steps", "type": "Concept"}, {"id": "Ablation for Different Masking Procedures", "type": "Concept"}, {"id": "A.1 Illustration of the Pre-training Tasks", "type": "Concept"}, {"id": "Masked LM and the Masking Procedure", "type": "Concept"}, {"id": "Next Sentence Prediction", "type": "Concept"}, {"id": "[MASK] token", "type": "Token"}, {"id": "Transformer encoder", "type": "Model Component"}, {"id": "Language understanding capability", "type": "Concept"}, {"id": "Pre-training Task", "type": "Concept"}, {"id": "MLM pre-training", "type": "Concept"}, {"id": "LTR pre-training", "type": "Concept"}, {"id": "MLM model", "type": "Model"}, {"id": "LTR model", "type": "Model"}, {"id": "A.2 Pre-training Procedure", "type": "Concept"}, {"id": "Training Input Sequence", "type": "Concept"}, {"id": "Corpus", "type": "Concept"}, {"id": "A embedding", "type": "Concept"}, {"id": "B embedding", "type": "Concept"}, {"id": "WordPiece model", "type": "Technique"}, {"id": "Masking Rate", "type": "Hyperparameter"}, {"id": "Batch Size", "type": "Hyperparameter"}, {"id": "Adam", "type": "Optimizer"}, {"id": "Learning Rate", "type": "Hyperparameter"}, {"id": "beta 1", "type": "Hyperparameter"}, {"id": "beta 2", "type": "Hyperparameter"}, {"id": "L2 weight decay", "type": "Hyperparameter"}, {"id": "Learning rate warmup", "type": "Technique"}, {"id": "gelu activation", "type": "Activation Function"}, {"id": "Hendrycks and Gimpel (2016)", "type": "Publication"}, {"id": "OpenAI GPT", "type": "Model"}, {"id": "Training Loss", "type": "Metric"}, {"id": "Masked LM likelihood", "type": "Metric"}, {"id": "Next Sentence Prediction likelihood", "type": "Metric"}, {"id": "BERTBASE", "type": "Model"}, {"id": "Cloud TPUs", "type": "Hardware"}, {"id": "TPU chips", "type": "Hardware"}, {"id": "BERTLARGE", "type": "Model"}, {"id": "Sequence Length", "type": "Hyperparameter"}, {"id": "Positional embeddings", "type": "Concept"}, {"id": "A.3 Fine-tuning Procedure", "type": "Concept"}, {"id": "Fine-tuning", "type": "Technique"}, {"id": "Number of epochs", "type": "Hyperparameter"}, {"id": "Dropout probability", "type": "Hyperparameter"}, {"id": "A.4 Comparison of BERT, ELMo, and OpenAI GPT", "type": "Concept"}, {"id": "ELMo", "type": "Model"}, {"id": "Bi-directionality", "type": "Concept"}, {"id": "BooksCorpus", "type": "Corpus"}, {"id": "Wikipedia", "type": "Corpus"}, {"id": "[SEP] token", "type": "Token"}, {"id": "[CLS] token", "type": "Token"}, {"id": "Sentence A/B embeddings", "type": "Concept"}, {"id": "Task-specific fine-tuning learning rate", "type": "Concept"}, {"id": "A.5 Illustrations of Fine-tuning on Different Tasks", "type": "Concept"}, {"id": "Task-specific models", "type": "Concept"}, {"id": "Output layer", "type": "Model Component"}, {"id": "Sequence-level tasks", "type": "Task"}, {"id": "Token-level tasks", "type": "Task"}, {"id": "Input embedding E", "type": "Concept"}, {"id": "Contextual representation of token i", "type": "Concept"}, {"id": "[CLS] as classification output symbol", "type": "Concept"}, {"id": "[SEP] as separator symbol", "type": "Concept"}, {"id": "B Detailed Experimental Setup", "type": "Concept"}, {"id": "B.1 Detailed Descriptions for the GLUE Benchmark Experiments", "type": "Concept"}, {"id": "GLUE Benchmark", "type": "Benchmark"}, {"id": "MNLI", "type": "Dataset"}, {"id": "Williams et al. (2018)", "type": "Publication"}, {"id": "Entailment classification", "type": "Task"}, {"id": "QQP", "type": "Dataset"}, {"id": "Chen et al. (2018)", "type": "Publication"}, {"id": "Binary classification", "type": "Task"}, {"id": "QNLI", "type": "Dataset"}, {"id": "SQuAD", "type": "Dataset"}, {"id": "Rajpurkar et al. (2016)", "type": "Publication"}, {"id": "Wang et al. (2018a)", "type": "Publication"}, {"id": "SST-2", "type": "Dataset"}, {"id": "Socher et al. (2013)", "type": "Publication"}, {"id": "Binary single-sentence classification", "type": "Task"}, {"id": "CoLA", "type": "Dataset"}, {"id": "Warstadt et al. (2018)", "type": "Publication"}, {"id": "STS-B", "type": "Dataset"}, {"id": "Cer et al. (2017)", "type": "Publication"}, {"id": "Semantic Textual Similarity Benchmark", "type": "Benchmark"}, {"id": "MRPC", "type": "Dataset"}, {"id": "Dolan and Brockett (2005)", "type": "Publication"}, {"id": "Semantic equivalence classification", "type": "Task"}, {"id": "RTE", "type": "Dataset"}, {"id": "Bentivogli et al. (2009)", "type": "Publication"}, {"id": "WNLI", "type": "Dataset"}, {"id": "Levesque et al. (2011)", "type": "Publication"}, {"id": "Natural Language Inference dataset", "type": "Dataset"}, {"id": "C Additional Ablation Studies", "type": "Concept"}, {"id": "C.1 Effect of Number of Training Steps", "type": "Concept"}, {"id": "MNLI Dev accuracy", "type": "Metric"}, {"id": "C.2 Ablation for Different Masking Procedures", "type": "Concept"}, {"id": "Masking Procedures", "type": "Technique"}, {"id": "Mixed strategy for masking", "type": "Strategy"}, {"id": "Masked Language Model (MLM) objective", "type": "Objective"}, {"id": "Mismatch between pre-training and fine-tuning", "type": "Concept"}, {"id": "Dev results", "type": "Metric"}, {"id": "Named Entity Recognition", "type": "Task"}, {"id": "Feature-based approach", "type": "Approach"}, {"id": "MASK strategy", "type": "Strategy"}, {"id": "SAME strategy", "type": "Strategy"}, {"id": "RND strategy", "type": "Strategy"}], "relationships": [{"source": {"id": "Richard Socher", "type": "Person"}, "target": {"id": "Recursive deep models for semantic compositionality over a sentiment treebank", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Alex Perelygin", "type": "Person"}, "target": {"id": "Recursive deep models for semantic compositionality over a sentiment treebank", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Jean Wu", "type": "Person"}, "target": {"id": "Recursive deep models for semantic compositionality over a sentiment treebank", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Jason Chuang", "type": "Person"}, "target": {"id": "Recursive deep models for semantic compositionality over a sentiment treebank", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Christopher D Manning", "type": "Person"}, "target": {"id": "Recursive deep models for semantic compositionality over a sentiment treebank", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Andrew Ng", "type": "Person"}, "target": {"id": "Recursive deep models for semantic compositionality over a sentiment treebank", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Christopher Potts", "type": "Person"}, "target": {"id": "Recursive deep models for semantic compositionality over a sentiment treebank", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Recursive deep models for semantic compositionality over a sentiment treebank", "type": "Publication"}, "target": {"id": "Proceedings of the 2013 conference on empirical methods in natural language processing", "type": "Conference"}, "type": "PUBLISHED_IN"}, {"source": {"id": "Fu Sun", "type": "Person"}, "target": {"id": "U-net: Machine reading comprehension with unanswerable questions", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Linyang Li", "type": "Person"}, "target": {"id": "U-net: Machine reading comprehension with unanswerable questions", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Xipeng Qiu", "type": "Person"}, "target": {"id": "U-net: Machine reading comprehension with unanswerable questions", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Yang Liu", "type": "Person"}, "target": {"id": "U-net: Machine reading comprehension with unanswerable questions", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "U-net: Machine reading comprehension with unanswerable questions", "type": "Publication"}, "target": {"id": "arXiv preprint arXiv:1810.06638", "type": "Publication"}, "type": "PUBLISHED_IN"}, {"source": {"id": "Wilson L Taylor", "type": "Person"}, "target": {"id": "Cloze procedure: A new tool for measuring readability", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Cloze procedure: A new tool for measuring readability", "type": "Publication"}, "target": {"id": "Journalism Bulletin", "type": "Journal"}, "type": "PUBLISHED_IN"}, {"source": {"id": "Erik F Tjong Kim Sang", "type": "Person"}, "target": {"id": "Introduction to the conll-2003 shared task: Language-independent named entity recognition", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Fien De Meulder", "type": "Person"}, "target": {"id": "Introduction to the conll-2003 shared task: Language-independent named entity recognition", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Introduction to the conll-2003 shared task: Language-independent named entity recognition", "type": "Publication"}, "target": {"id": "CoNLL", "type": "Conference"}, "type": "PUBLISHED_IN"}, {"source": {"id": "Joseph Turian", "type": "Person"}, "target": {"id": "Word representations: A simple and general method for semi-supervised learning", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Lev Ratinov", "type": "Person"}, "target": {"id": "Word representations: A simple and general method for semi-supervised learning", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Yoshua Bengio", "type": "Person"}, "target": {"id": "Word representations: A simple and general method for semi-supervised learning", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Word representations: A simple and general method for semi-supervised learning", "type": "Publication"}, "target": {"id": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics", "type": "Conference"}, "type": "PUBLISHED_IN"}, {"source": {"id": "Ashish Vaswani", "type": "Person"}, "target": {"id": "Attention is all you need", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Noam Shazeer", "type": "Person"}, "target": {"id": "Attention is all you need", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Niki Parmar", "type": "Person"}, "target": {"id": "Attention is all you need", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Jakob Uszkoreit", "type": "Person"}, "target": {"id": "Attention is all you need", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Llion Jones", "type": "Person"}, "target": {"id": "Attention is all you need", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Aidan N Gomez", "type": "Person"}, "target": {"id": "Attention is all you need", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Lukasz Kaiser", "type": "Person"}, "target": {"id": "Attention is all you need", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Illia Polosukhin", "type": "Person"}, "target": {"id": "Attention is all you need", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Attention is all you need", "type": "Publication"}, "target": {"id": "Advances in Neural Information Processing Systems", "type": "Conference"}, "type": "PUBLISHED_IN"}, {"source": {"id": "Pascal Vincent", "type": "Person"}, "target": {"id": "Extracting and composing robust features with denoising autoencoders", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Hugo Larochelle", "type": "Person"}, "target": {"id": "Extracting and composing robust features with denoising autoencoders", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Yoshua Bengio", "type": "Person"}, "target": {"id": "Extracting and composing robust features with denoising autoencoders", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Pierre-Antoine Manzagol", "type": "Person"}, "target": {"id": "Extracting and composing robust features with denoising autoencoders", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Extracting and composing robust features with denoising autoencoders", "type": "Publication"}, "target": {"id": "Proceedings of the 25th international conference on Machine learning", "type": "Conference"}, "type": "PUBLISHED_IN"}, {"source": {"id": "Proceedings of the 25th international conference on Machine learning", "type": "Conference"}, "target": {"id": "ACM", "type": "Organization"}, "type": "ORGANIZED_BY"}, {"source": {"id": "Alex Wang", "type": "Person"}, "target": {"id": "Glue: A multi-task benchmark and analysis platform for natural language understanding", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Amanpreet Singh", "type": "Person"}, "target": {"id": "Glue: A multi-task benchmark and analysis platform for natural language understanding", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Julian Michael", "type": "Person"}, "target": {"id": "Glue: A multi-task benchmark and analysis platform for natural language understanding", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Felix Hill", "type": "Person"}, "target": {"id": "Glue: A multi-task benchmark and analysis platform for natural language understanding", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Omer Levy", "type": "Person"}, "target": {"id": "Glue: A multi-task benchmark and analysis platform for natural language understanding", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Samuel R Bowman", "type": "Person"}, "target": {"id": "Glue: A multi-task benchmark and analysis platform for natural language understanding", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Glue: A multi-task benchmark and analysis platform for natural language understanding", "type": "Publication"}, "target": {"id": "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP", "type": "Conference"}, "type": "PUBLISHED_IN"}, {"source": {"id": "Wei Wang", "type": "Person"}, "target": {"id": "Multi-granularity hierarchical attention fusion networks for reading comprehension and question answering", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Ming Yan", "type": "Person"}, "target": {"id": "Multi-granularity hierarchical attention fusion networks for reading comprehension and question answering", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Chen Wu", "type": "Person"}, "target": {"id": "Multi-granularity hierarchical attention fusion networks for reading comprehension and question answering", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Multi-granularity hierarchical attention fusion networks for reading comprehension and question answering", "type": "Publication"}, "target": {"id": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)", "type": "Conference"}, "type": "PUBLISHED_IN"}, {"source": {"id": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)", "type": "Conference"}, "target": {"id": "Association for Computational Linguistics", "type": "Organization"}, "type": "ORGANIZED_BY"}, {"source": {"id": "Alex Warstadt", "type": "Person"}, "target": {"id": "Neural network acceptability judgments", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Amanpreet Singh", "type": "Person"}, "target": {"id": "Neural network acceptability judgments", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Samuel R Bowman", "type": "Person"}, "target": {"id": "Neural network acceptability judgments", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Neural network acceptability judgments", "type": "Publication"}, "target": {"id": "arXiv preprint arXiv:1805.12471", "type": "Publication"}, "type": "PUBLISHED_IN"}, {"source": {"id": "Adina Williams", "type": "Person"}, "target": {"id": "A broad-coverage challenge corpus for sentence understanding through inference", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Nikita Nangia", "type": "Person"}, "target": {"id": "A broad-coverage challenge corpus for sentence understanding through inference", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Samuel R Bowman", "type": "Person"}, "target": {"id": "A broad-coverage challenge corpus for sentence understanding through inference", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "A broad-coverage challenge corpus for sentence understanding through inference", "type": "Publication"}, "target": {"id": "NAACL", "type": "Conference"}, "type": "PUBLISHED_IN"}, {"source": {"id": "Yonghui Wu", "type": "Person"}, "target": {"id": "Google\u2019s neural machine translation system: Bridging the gap between human and machine translation", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Mike Schuster", "type": "Person"}, "target": {"id": "Google\u2019s neural machine translation system: Bridging the gap between human and machine translation", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Zhifeng Chen", "type": "Person"}, "target": {"id": "Google\u2019s neural machine translation system: Bridging the gap between human and machine translation", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Quoc V Le", "type": "Person"}, "target": {"id": "Google\u2019s neural machine translation system: Bridging the gap between human and machine translation", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Mohammad Norouzi", "type": "Person"}, "target": {"id": "Google\u2019s neural machine translation system: Bridging the gap between human and machine translation", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Wolfgang Macherey", "type": "Person"}, "target": {"id": "Google\u2019s neural machine translation system: Bridging the gap between human and machine translation", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Maxim Krikun", "type": "Person"}, "target": {"id": "Google\u2019s neural machine translation system: Bridging the gap between human and machine translation", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Yuan Cao", "type": "Person"}, "target": {"id": "Google\u2019s neural machine translation system: Bridging the gap between human and machine translation", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Qin Gao", "type": "Person"}, "target": {"id": "Google\u2019s neural machine translation system: Bridging the gap between human and machine translation", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Klaus Macherey", "type": "Person"}, "target": {"id": "Google\u2019s neural machine translation system: Bridging the gap between human and machine translation", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Google", "type": "Organization"}, "target": {"id": "Google\u2019s neural machine translation system: Bridging the gap between human and machine translation", "type": "Publication"}, "type": "DEVELOPED"}, {"source": {"id": "Google\u2019s neural machine translation system: Bridging the gap between human and machine translation", "type": "Publication"}, "target": {"id": "arXiv preprint arXiv:1609.08144", "type": "Publication"}, "type": "PUBLISHED_IN"}, {"source": {"id": "Jason Yosinski", "type": "Person"}, "target": {"id": "How transferable are features in deep neural networks?", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Jeff Clune", "type": "Person"}, "target": {"id": "How transferable are features in deep neural networks?", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Yoshua Bengio", "type": "Person"}, "target": {"id": "How transferable are features in deep neural networks?", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Hod Lipson", "type": "Person"}, "target": {"id": "How transferable are features in deep neural networks?", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "How transferable are features in deep neural networks?", "type": "Publication"}, "target": {"id": "Advances in Neural Information Processing Systems", "type": "Conference"}, "type": "PUBLISHED_IN"}, {"source": {"id": "Adams Wei Yu", "type": "Person"}, "target": {"id": "QANet: Combining local convolution with global self-attention for reading comprehension", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "David Dohan", "type": "Person"}, "target": {"id": "QANet: Combining local convolution with global self-attention for reading comprehension", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Minh-Thang Luong", "type": "Person"}, "target": {"id": "QANet: Combining local convolution with global self-attention for reading comprehension", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Rui Zhao", "type": "Person"}, "target": {"id": "QANet: Combining local convolution with global self-attention for reading comprehension", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Kai Chen", "type": "Person"}, "target": {"id": "QANet: Combining local convolution with global self-attention for reading comprehension", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Mohammad Norouzi", "type": "Person"}, "target": {"id": "QANet: Combining local convolution with global self-attention for reading comprehension", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Quoc V Le", "type": "Person"}, "target": {"id": "QANet: Combining local convolution with global self-attention for reading comprehension", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "QANet: Combining local convolution with global self-attention for reading comprehension", "type": "Publication"}, "target": {"id": "ICLR", "type": "Conference"}, "type": "PUBLISHED_IN"}, {"source": {"id": "Rowan Zellers", "type": "Person"}, "target": {"id": "Swag: A large-scale adversarial dataset for grounded commonsense inference", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Yonatan Bisk", "type": "Person"}, "target": {"id": "Swag: A large-scale adversarial dataset for grounded commonsense inference", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Roy Schwartz", "type": "Person"}, "target": {"id": "Swag: A large-scale adversarial dataset for grounded commonsense inference", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Yejin Choi", "type": "Person"}, "target": {"id": "Swag: A large-scale adversarial dataset for grounded commonsense inference", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Swag: A large-scale adversarial dataset for grounded commonsense inference", "type": "Publication"}, "target": {"id": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)", "type": "Conference"}, "type": "PUBLISHED_IN"}, {"source": {"id": "Yukun Zhu", "type": "Person"}, "target": {"id": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Ryan Kiros", "type": "Person"}, "target": {"id": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Rich Zemel", "type": "Person"}, "target": {"id": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Ruslan Salakhutdinov", "type": "Person"}, "target": {"id": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Raquel Urtasun", "type": "Person"}, "target": {"id": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Antonio Torralba", "type": "Person"}, "target": {"id": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Sanja Fidler", "type": "Person"}, "target": {"id": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books", "type": "Publication"}, "type": "AUTHORED"}, {"source": {"id": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books", "type": "Publication"}, "target": {"id": "Proceedings of the IEEE international conference on computer vision", "type": "Conference"}, "type": "PUBLISHED_IN"}, {"source": {"id": "Proceedings of the IEEE international conference on computer vision", "type": "Conference"}, "target": {"id": "IEEE", "type": "Organization"}, "type": "ORGANIZED_BY"}, {"source": {"id": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "type": "Publication"}, "target": {"id": "Appendix A", "type": "Concept"}, "type": "HAS_APPENDIX"}, {"source": {"id": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "type": "Publication"}, "target": {"id": "Appendix B", "type": "Concept"}, "type": "HAS_APPENDIX"}, {"source": {"id": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "type": "Publication"}, "target": {"id": "Appendix C", "type": "Concept"}, "type": "HAS_APPENDIX"}, {"source": {"id": "Appendix C", "type": "Concept"}, "target": {"id": "Effect of Number of Training Steps", "type": "Concept"}, "type": "CONTAINS_STUDY"}, {"source": {"id": "Appendix C", "type": "Concept"}, "target": {"id": "Ablation for Different Masking Procedures", "type": "Concept"}, "type": "CONTAINS_STUDY"}, {"source": {"id": "Appendix A", "type": "Concept"}, "target": {"id": "A.1 Illustration of the Pre-training Tasks", "type": "Concept"}, "type": "CONTAINS_DETAILS"}, {"source": {"id": "A.1 Illustration of the Pre-training Tasks", "type": "Concept"}, "target": {"id": "Masked LM and the Masking Procedure", "type": "Concept"}, "type": "DESCRIBES"}, {"source": {"id": "A.1 Illustration of the Pre-training Tasks", "type": "Concept"}, "target": {"id": "Next Sentence Prediction", "type": "Concept"}, "type": "DESCRIBES"}, {"source": {"id": "Masked LM and the Masking Procedure", "type": "Concept"}, "target": {"id": "[MASK] token", "type": "Token"}, "type": "USES_TOKEN"}, {"source": {"id": "Masked LM and the Masking Procedure", "type": "Concept"}, "target": {"id": "Pre-training Task", "type": "Concept"}, "type": "IS_A_TYPE_OF"}, {"source": {"id": "Next Sentence Prediction", "type": "Concept"}, "target": {"id": "Pre-training Task", "type": "Concept"}, "type": "IS_A_TYPE_OF"}, {"source": {"id": "BERT", "type": "Model"}, "target": {"id": "Masked LM and the Masking Procedure", "type": "Concept"}, "type": "USES"}, {"source": {"id": "BERT", "type": "Model"}, "target": {"id": "Next Sentence Prediction", "type": "Concept"}, "type": "USES"}, {"source": {"id": "Transformer encoder", "type": "Model Component"}, "target": {"id": "Masked LM and the Masking Procedure", "type": "Concept"}, "type": "USES"}, {"source": {"id": "MLM model", "type": "Model"}, "target": {"id": "LTR model", "type": "Model"}, "type": "COMPARED_TO"}, {"source": {"id": "MLM model", "type": "Model"}, "target": {"id": "LTR model", "type": "Model"}, "type": "OUTPERFORMS"}, {"source": {"id": "Appendix A", "type": "Concept"}, "target": {"id": "A.2 Pre-training Procedure", "type": "Concept"}, "type": "CONTAINS_DETAILS"}, {"source": {"id": "A.2 Pre-training Procedure", "type": "Concept"}, "target": {"id": "Training Input Sequence", "type": "Concept"}, "type": "DESCRIBES"}, {"source": {"id": "Training Input Sequence", "type": "Concept"}, "target": {"id": "Corpus", "type": "Concept"}, "type": "SAMPLED_FROM"}, {"source": {"id": "Training Input Sequence", "type": "Concept"}, "target": {"id": "A embedding", "type": "Concept"}, "type": "USES"}, {"source": {"id": "Training Input Sequence", "type": "Concept"}, "target": {"id": "B embedding", "type": "Concept"}, "type": "USES"}, {"source": {"id": "Training Input Sequence", "type": "Concept"}, "target": {"id": "WordPiece model", "type": "Technique"}, "type": "USES"}, {"source": {"id": "WordPiece model", "type": "Technique"}, "target": {"id": "Masking Rate", "type": "Hyperparameter"}, "type": "HAS_PROPERTY"}, {"source": {"id": "BERT", "type": "Model"}, "target": {"id": "Batch Size", "type": "Hyperparameter"}, "type": "TRAINED_WITH"}, {"source": {"id": "BERT", "type": "Model"}, "target": {"id": "Adam", "type": "Optimizer"}, "type": "USES_OPTIMIZER"}, {"source": {"id": "Adam", "type": "Optimizer"}, "target": {"id": "Learning Rate", "type": "Hyperparameter"}, "type": "HAS_PROPERTY"}, {"source": {"id": "Adam", "type": "Optimizer"}, "target": {"id": "beta 1", "type": "Hyperparameter"}, "type": "HAS_PROPERTY"}, {"source": {"id": "Adam", "type": "Optimizer"}, "target": {"id": "beta 2", "type": "Hyperparameter"}, "type": "HAS_PROPERTY"}, {"source": {"id": "Adam", "type": "Optimizer"}, "target": {"id": "L2 weight decay", "type": "Hyperparameter"}, "type": "HAS_PROPERTY"}, {"source": {"id": "Adam", "type": "Optimizer"}, "target": {"id": "Learning rate warmup", "type": "Technique"}, "type": "USES"}, {"source": {"id": "BERT", "type": "Model"}, "target": {"id": "gelu activation", "type": "Activation Function"}, "type": "USES_ACTIVATION"}, {"source": {"id": "gelu activation", "type": "Activation Function"}, "target": {"id": "Hendrycks and Gimpel (2016)", "type": "Publication"}, "type": "PROPOSED_IN"}, {"source": {"id": "gelu activation", "type": "Activation Function"}, "target": {"id": "OpenAI GPT", "type": "Model"}, "type": "FOLLOWS_MODEL"}, {"source": {"id": "Training Loss", "type": "Metric"}, "target": {"id": "Masked LM likelihood", "type": "Metric"}, "type": "COMPRISES"}, {"source": {"id": "Training Loss", "type": "Metric"}, "target": {"id": "Next Sentence Prediction likelihood", "type": "Metric"}, "type": "COMPRISES"}, {"source": {"id": "BERTBASE", "type": "Model"}, "target": {"id": "Cloud TPUs", "type": "Hardware"}, "type": "TRAINED_ON"}, {"source": {"id": "Cloud TPUs", "type": "Hardware"}, "target": {"id": "TPU chips", "type": "Hardware"}, "type": "HAS_COMPONENT"}, {"source": {"id": "BERTLARGE", "type": "Model"}, "target": {"id": "Cloud TPUs", "type": "Hardware"}, "type": "TRAINED_ON"}, {"source": {"id": "BERT", "type": "Model"}, "target": {"id": "Sequence Length", "type": "Hyperparameter"}, "type": "USES_HYPERPARAMETER"}, {"source": {"id": "Pre-training", "type": "Concept"}, "target": {"id": "Positional embeddings", "type": "Concept"}, "type": "LEARNS"}, {"source": {"id": "Appendix A", "type": "Concept"}, "target": {"id": "A.3 Fine-tuning Procedure", "type": "Concept"}, "type": "CONTAINS_DETAILS"}, {"source": {"id": "A.3 Fine-tuning Procedure", "type": "Concept"}, "target": {"id": "Fine-tuning", "type": "Technique"}, "type": "DESCRIBES"}, {"source": {"id": "Fine-tuning", "type": "Technique"}, "target": {"id": "Batch Size", "type": "Hyperparameter"}, "type": "USES_HYPERPARAMETER"}, {"source": {"id": "Fine-tuning", "type": "Technique"}, "target": {"id": "Learning Rate", "type": "Hyperparameter"}, "type": "USES_HYPERPARAMETER"}, {"source": {"id": "Fine-tuning", "type": "Technique"}, "target": {"id": "Number of epochs", "type": "Hyperparameter"}, "type": "USES_HYPERPARAMETER"}, {"source": {"id": "Fine-tuning", "type": "Technique"}, "target": {"id": "Dropout probability", "type": "Hyperparameter"}, "type": "USES_HYPERPARAMETER"}, {"source": {"id": "Appendix A", "type": "Concept"}, "target": {"id": "A.4 Comparison of BERT, ELMo, and OpenAI GPT", "type": "Concept"}, "type": "CONTAINS_DETAILS"}, {"source": {"id": "A.4 Comparison of BERT, ELMo, and OpenAI GPT", "type": "Concept"}, "target": {"id": "BERT", "type": "Model"}, "type": "COMPARES"}, {"source": {"id": "A.4 Comparison of BERT, ELMo, and OpenAI GPT", "type": "Concept"}, "target": {"id": "ELMo", "type": "Model"}, "type": "COMPARES"}, {"source": {"id": "A.4 Comparison of BERT, ELMo, and OpenAI GPT", "type": "Concept"}, "target": {"id": "OpenAI GPT", "type": "Model"}, "type": "COMPARES"}, {"source": {"id": "BERT", "type": "Model"}, "target": {"id": "Fine-tuning", "type": "Technique"}, "type": "USES"}, {"source": {"id": "OpenAI GPT", "type": "Model"}, "target": {"id": "Fine-tuning", "type": "Technique"}, "type": "USES"}, {"source": {"id": "ELMo", "type": "Model"}, "target": {"id": "Feature-based approach", "type": "Approach"}, "type": "USES"}, {"source": {"id": "BERT", "type": "Model"}, "target": {"id": "Bi-directionality", "type": "Concept"}, "type": "USES"}, {"source": {"id": "BERT", "type": "Model"}, "target": {"id": "Pre-training Task", "type": "Concept"}, "type": "USES_TWO_TYPES_OF"}, {"source": {"id": "OpenAI GPT", "type": "Model"}, "target": {"id": "BooksCorpus", "type": "Corpus"}, "type": "TRAINED_ON"}, {"source": {"id": "BERT", "type": "Model"}, "target": {"id": "BooksCorpus", "type": "Corpus"}, "type": "TRAINED_ON"}, {"source": {"id": "BERT", "type": "Model"}, "target": {"id": "Wikipedia", "type": "Corpus"}, "type": "TRAINED_ON"}, {"source": {"id": "OpenAI GPT", "type": "Model"}, "target": {"id": "[SEP] token", "type": "Token"}, "type": "USES_TOKEN"}, {"source": {"id": "OpenAI GPT", "type": "Model"}, "target": {"id": "[CLS] token", "type": "Token"}, "type": "USES_TOKEN"}, {"source": {"id": "BERT", "type": "Model"}, "target": {"id": "[SEP] token", "type": "Token"}, "type": "LEARNS_EMBEDDING"}, {"source": {"id": "BERT", "type": "Model"}, "target": {"id": "[CLS] token", "type": "Token"}, "type": "LEARNS_EMBEDDING"}, {"source": {"id": "BERT", "type": "Model"}, "target": {"id": "Sentence A/B embeddings", "type": "Concept"}, "type": "LEARNS_EMBEDDING"}, {"source": {"id": "OpenAI GPT", "type": "Model"}, "target": {"id": "Batch Size", "type": "Hyperparameter"}, "type": "TRAINED_WITH"}, {"source": {"id": "BERT", "type": "Model"}, "target": {"id": "Batch Size", "type": "Hyperparameter"}, "type": "TRAINED_WITH"}, {"source": {"id": "OpenAI GPT", "type": "Model"}, "target": {"id": "Learning Rate", "type": "Hyperparameter"}, "type": "USES_LEARNING_RATE"}, {"source": {"id": "BERT", "type": "Model"}, "target": {"id": "Task-specific fine-tuning learning rate", "type": "Concept"}, "type": "USES"}, {"source": {"id": "Appendix A", "type": "Concept"}, "target": {"id": "A.5 Illustrations of Fine-tuning on Different Tasks", "type": "Concept"}, "type": "CONTAINS_DETAILS"}, {"source": {"id": "A.5 Illustrations of Fine-tuning on Different Tasks", "type": "Concept"}, "target": {"id": "Fine-tuning", "type": "Technique"}, "type": "DESCRIBES"}, {"source": {"id": "BERT", "type": "Model"}, "target": {"id": "Task-specific models", "type": "Concept"}, "type": "USES_FOR"}, {"source": {"id": "Task-specific models", "type": "Concept"}, "target": {"id": "Output layer", "type": "Model Component"}, "type": "INCLUDES"}, {"source": {"id": "Task-specific models", "type": "Concept"}, "target": {"id": "Sequence-level tasks", "type": "Task"}, "type": "APPLIES_TO"}, {"source": {"id": "Task-specific models", "type": "Concept"}, "target": {"id": "Token-level tasks", "type": "Task"}, "type": "APPLIES_TO"}, {"source": {"id": "Input embedding E", "type": "Concept"}, "target": {"id": "BERT", "type": "Model"}, "type": "IS_INPUT_TO"}, {"source": {"id": "Contextual representation of token i", "type": "Concept"}, "target": {"id": "BERT", "type": "Model"}, "type": "IS_OUTPUT_OF"}, {"source": {"id": "[CLS] as classification output symbol", "type": "Concept"}, "target": {"id": "BERT", "type": "Model"}, "type": "USED_BY"}, {"source": {"id": "[SEP] as separator symbol", "type": "Concept"}, "target": {"id": "BERT", "type": "Model"}, "type": "USED_BY"}, {"source": {"id": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "type": "Publication"}, "target": {"id": "B Detailed Experimental Setup", "type": "Concept"}, "type": "HAS_APPENDIX"}, {"source": {"id": "B Detailed Experimental Setup", "type": "Concept"}, "target": {"id": "B.1 Detailed Descriptions for the GLUE Benchmark Experiments", "type": "Concept"}, "type": "CONTAINS_DETAILS"}, {"source": {"id": "B.1 Detailed Descriptions for the GLUE Benchmark Experiments", "type": "Concept"}, "target": {"id": "GLUE Benchmark", "type": "Benchmark"}, "type": "DESCRIBES"}, {"source": {"id": "GLUE Benchmark", "type": "Benchmark"}, "target": {"id": "MNLI", "type": "Dataset"}, "type": "INCLUDES"}, {"source": {"id": "MNLI", "type": "Dataset"}, "target": {"id": "Williams et al. (2018)", "type": "Publication"}, "type": "DESCRIBED_IN"}, {"source": {"id": "MNLI", "type": "Dataset"}, "target": {"id": "Entailment classification", "type": "Task"}, "type": "IS_A_TASK_OF"}, {"source": {"id": "GLUE Benchmark", "type": "Benchmark"}, "target": {"id": "QQP", "type": "Dataset"}, "type": "INCLUDES"}, {"source": {"id": "QQP", "type": "Dataset"}, "target": {"id": "Chen et al. (2018)", "type": "Publication"}, "type": "DESCRIBED_IN"}, {"source": {"id": "QQP", "type": "Dataset"}, "target": {"id": "Binary classification", "type": "Task"}, "type": "IS_A_TASK_OF"}, {"source": {"id": "GLUE Benchmark", "type": "Benchmark"}, "target": {"id": "QNLI", "type": "Dataset"}, "type": "INCLUDES"}, {"source": {"id": "QNLI", "type": "Dataset"}, "target": {"id": "SQuAD", "type": "Dataset"}, "type": "IS_VERSION_OF"}, {"source": {"id": "SQuAD", "type": "Dataset"}, "target": {"id": "Rajpurkar et al. (2016)", "type": "Publication"}, "type": "DESCRIBED_IN"}, {"source": {"id": "QNLI", "type": "Dataset"}, "target": {"id": "Wang et al. (2018a)", "type": "Publication"}, "type": "DESCRIBED_IN"}, {"source": {"id": "QNLI", "type": "Dataset"}, "target": {"id": "Binary classification", "type": "Task"}, "type": "IS_A_TASK_OF"}, {"source": {"id": "GLUE Benchmark", "type": "Benchmark"}, "target": {"id": "SST-2", "type": "Dataset"}, "type": "INCLUDES"}, {"source": {"id": "SST-2", "type": "Dataset"}, "target": {"id": "Socher et al. (2013)", "type": "Publication"}, "type": "DESCRIBED_IN"}, {"source": {"id": "SST-2", "type": "Dataset"}, "target": {"id": "Binary single-sentence classification", "type": "Task"}, "type": "IS_A_TASK_OF"}, {"source": {"id": "GLUE Benchmark", "type": "Benchmark"}, "target": {"id": "CoLA", "type": "Dataset"}, "type": "INCLUDES"}, {"source": {"id": "CoLA", "type": "Dataset"}, "target": {"id": "Warstadt et al. (2018)", "type": "Publication"}, "type": "DESCRIBED_IN"}, {"source": {"id": "CoLA", "type": "Dataset"}, "target": {"id": "Binary single-sentence classification", "type": "Task"}, "type": "IS_A_TASK_OF"}, {"source": {"id": "GLUE Benchmark", "type": "Benchmark"}, "target": {"id": "STS-B", "type": "Dataset"}, "type": "INCLUDES"}, {"source": {"id": "STS-B", "type": "Dataset"}, "target": {"id": "Cer et al. (2017)", "type": "Publication"}, "type": "DESCRIBED_IN"}, {"source": {"id": "STS-B", "type": "Dataset"}, "target": {"id": "Semantic Textual Similarity Benchmark", "type": "Benchmark"}, "type": "IS_A_TYPE_OF"}, {"source": {"id": "GLUE Benchmark", "type": "Benchmark"}, "target": {"id": "MRPC", "type": "Dataset"}, "type": "INCLUDES"}, {"source": {"id": "MRPC", "type": "Dataset"}, "target": {"id": "Dolan and Brockett (2005)", "type": "Publication"}, "type": "DESCRIBED_IN"}, {"source": {"id": "MRPC", "type": "Dataset"}, "target": {"id": "Semantic equivalence classification", "type": "Task"}, "type": "IS_A_TASK_OF"}, {"source": {"id": "GLUE Benchmark", "type": "Benchmark"}, "target": {"id": "RTE", "type": "Dataset"}, "type": "INCLUDES"}, {"source": {"id": "RTE", "type": "Dataset"}, "target": {"id": "Bentivogli et al. (2009)", "type": "Publication"}, "type": "DESCRIBED_IN"}, {"source": {"id": "RTE", "type": "Dataset"}, "target": {"id": "Binary entailment task", "type": "Task"}, "type": "IS_A_TASK_OF"}, {"source": {"id": "GLUE Benchmark", "type": "Benchmark"}, "target": {"id": "WNLI", "type": "Dataset"}, "type": "INCLUDES"}, {"source": {"id": "WNLI", "type": "Dataset"}, "target": {"id": "Levesque et al. (2011)", "type": "Publication"}, "type": "DESCRIBED_IN"}, {"source": {"id": "WNLI", "type": "Dataset"}, "target": {"id": "Natural Language Inference dataset", "type": "Dataset"}, "type": "IS_A_TYPE_OF"}, {"source": {"id": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "type": "Publication"}, "target": {"id": "C Additional Ablation Studies", "type": "Concept"}, "type": "HAS_APPENDIX"}, {"source": {"id": "C Additional Ablation Studies", "type": "Concept"}, "target": {"id": "C.1 Effect of Number of Training Steps", "type": "Concept"}, "type": "CONTAINS_DETAILS"}, {"source": {"id": "C.1 Effect of Number of Training Steps", "type": "Concept"}, "target": {"id": "Effect of Number of Training Steps", "type": "Concept"}, "type": "EVALUATES"}, {"source": {"id": "Effect of Number of Training Steps", "type": "Concept"}, "target": {"id": "MNLI Dev accuracy", "type": "Metric"}, "type": "IMPACTS"}, {"source": {"id": "MLM pre-training", "type": "Concept"}, "target": {"id": "LTR pre-training", "type": "Concept"}, "type": "COMPARED_TO"}, {"source": {"id": "C Additional Ablation Studies", "type": "Concept"}, "target": {"id": "C.2 Ablation for Different Masking Procedures", "type": "Concept"}, "type": "CONTAINS_DETAILS"}, {"source": {"id": "C.2 Ablation for Different Masking Procedures", "type": "Concept"}, "target": {"id": "Masking Procedures", "type": "Technique"}, "type": "EVALUATES"}, {"source": {"id": "BERT", "type": "Model"}, "target": {"id": "Mixed strategy for masking", "type": "Strategy"}, "type": "USES"}, {"source": {"id": "Masking Procedures", "type": "Technique"}, "target": {"id": "Masked Language Model (MLM) objective", "type": "Objective"}, "type": "APPLIED_IN"}, {"source": {"id": "Masking Procedures", "type": "Technique"}, "target": {"id": "Mismatch between pre-training and fine-tuning", "type": "Concept"}, "type": "REDUCES"}, {"source": {"id": "[MASK] token", "type": "Token"}, "target": {"id": "Fine-tuning", "type": "Technique"}, "type": "NOT_USED_DURING"}, {"source": {"id": "Dev results", "type": "Metric"}, "target": {"id": "MNLI", "type": "Dataset"}, "type": "REPORTED_FOR"}, {"source": {"id": "Dev results", "type": "Metric"}, "target": {"id": "Named Entity Recognition", "type": "Task"}, "type": "REPORTED_FOR"}, {"source": {"id": "Feature-based approach", "type": "Approach"}, "target": {"id": "Mismatch between pre-training and fine-tuning", "type": "Concept"}, "type": "AMPLIFIES"}, {"source": {"id": "MASK strategy", "type": "Strategy"}, "target": {"id": "[MASK] token", "type": "Token"}, "type": "REPLACES_WITH"}, {"source": {"id": "SAME strategy", "type": "Strategy"}, "target": {"id": "MASK strategy", "type": "Strategy"}, "type": "KEEPS_TOKEN_AS_IS"}, {"source": {"id": "RND strategy", "type": "Strategy"}, "target": {"id": "Mixed strategy for masking", "type": "Strategy"}, "type": "PERFORMS_WORSE_THAN"}], "original_chunk_ids": ["bidirectional_transformer_raw_chunks_5k_3", "bidirectional_transformer_raw_chunks_5k_4"], "original_chunk_indices": [3, 4], "original_metadata": [{"source": "bidirectional_transformer_raw.md"}, {"source": "bidirectional_transformer_raw.md"}]}
