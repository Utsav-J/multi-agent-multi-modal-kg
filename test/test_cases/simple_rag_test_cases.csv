query,category,retrieval_context,final_answer,retrieval_latency_ms,generation_latency_ms,total_latency_ms,token_usage,rag_chunks_count,rag_error,timestamp
"How is the entity ""DeepSeek-R1"" related to the concept of ""Chain-of-Thought"" in the context of SWA adaptation?",Entity Relationship Reasoning,"[Document 1]
Source: sliding_window_attention_annotated_chunks_2k.jsonl
Chunk ID: sliding_window_attention_annotated_chunks_2k_1
Content: LightTransfer (Zhang et al., 2024) is a promising
attempt at adapting existing models to SWA without pretraining, which has the same motivation as
ours. But it may generalize poorly across model
families (see Appendix G).


**2.2** **Linear Attention**


An alternative approach involves reformulating
the attention mechanism entirely to achieve linear, _O_ ( _N_ ), complexity. This includes methods
such as RNN-like linear attention transformers

(Katharopoulos et al., 2020; Peng et al., 2023;
Sun et al., 2023) and structured state-space models
(SSMs) like Mamba (Gu and Dao, 2023). Many
works such as Jamba and Nemotron-Flash(Lieber
et al., 2024; Linsong Chu et al., 2024; Team et al.,
2025; Fu et al., 2025a) interleave linear attention
layers with traditional attention layers to create hybrid model structures. While promising, they represent a fundamental architectural departure from the
standard Transformer, and their performance is generally weaker than traditional Transformer-based

LLMs.



2


**3** **Candidate Methods for SWA**

**Adaptation**


As established, a naive application of SWA leads
to severe long-context performance degradation.
Therefore, we investigate five methods that can potentially facilitate SWA adaptation from distinct
perspectives. However, every method except finetuning has some drawbacks to LLM inference efficiency, as discussed in Appendix A. Therefore,
although these methods are not mutually exclusive,
we should not indiscriminately adopt all of them.
Instead, we must evaluate various combinations to
identify the optimal trade-off between performance
and efficiency.


**3.1** **Full Attention Decode**


This method applies SWA **only** to the prefilling
stage. During the decoding (auto-regressive generation) stage, each token still employs full attention,
allowing access to all previous tokens in the context. The resulting attention mask is depicted in
Figure 1a.
This novel approach, first proposed in our study,
is inspired by human reading comprehension: humans typically scan a passage casually (prefilling)
before thinking deeply to formulate an answer (decoding) for a specific problem. We term this strategy ""reading casually, thinking carefully."" In our
design, the SWA-constrained prefilling stage corresponds to casual reading, while the full-attention
decoding stage enables careful thinking. This analogy also suggests that Chain-of-Thought (CoT)
during decoding may be particularly beneficial, as
extended generation (i.e. ""thinking"") could compensate for the insufficient contextual information
gathered during the prefilling stage.


(a) FA Decode (b) Keep First


Figure 1: (a) Attention mask for FA Decode. SWA is
used for prompt tokens (prefill), and full attention is
used for generated tokens (decode). (b) Attention mask
for SWA combined with Keep First _k_ Tokens.



**3.1.1** **Keep First k Tokens**

Xiao et al. demonstrate that models pretrained with
full attention allocate a disproportionate amount
of attention to the initial tokens (""attention sink""),
and removing the visibility of these tokens causes
performance collapse. Their solution, streaming
attention, involves permanently retaining the attention to these ""sink"" tokens while using SWA, which
successfully maintains the stability of the attention
distribution and the model’s output. Our method
is basically the same: as shown in Figure 1b, any
subsequent token can attend to its local window
**and** the initial _k_ tokens.

Notably, however, our method extends beyond
its original version. Streaming attention operates
only at the KV cache level; specifically, the KV
cache of the sink tokens is retained during decoding, while the prefilling stage is not modified or accelerated at all. In contrast, we directly customize
the Flash-Attention-2 (Dao, 2024) kernel to implement such attention mask, thereby accelerating
prefilling via SWA as well, and eliminating the
need to modify KV cache.


**3.2** **Interleaving Layers**


This method retains full attention on a subset of

layers while applying SWA to the remainder, providing a simple hybrid mechanism to balance the
performance of full attention with the efficiency
of pure SWA. A common strategy involves designating one in every _n_ layers to use full attention
(e.g., layers 0, 2, 4, . . . retain full attention, while all
others use SWA). For example, Gemma-2 (Team,
2024a) uses SWA only for layers [1, 3, 5, . . . ], and
Gemma-3 (Team, 2025a) uses SWA only for layers

[5, 11, 17, ...].
However, for an FA-pretrained model, layers
may exhibit distinct behaviors, suggesting it may
not be optimal to simply assign SWA to layers [1,
3, 5, ...]. Instead, it might be preferable to use
statistical features to select ""lazy"" (mostly focusing
on just recent tokens) layers, as adopted by LightTransfer (Zhang et al., 2024). However, we find
that LightTransfer is not consistently superior in
practice (see Appendix G). Therefore, for simplicity, we still choose the simplest fixed-interval layer
selections in our experiments, such as [0, 2, 4, . . . ]
and [1, 3, 5, ...].


**3.3** **Chain-of-Thought**


Chain-of-Thought (CoT) (Wei et al., 2022) is a
widely used technique for improving model ac


3



```
### Figure - Sliding Window Attention
![Sliding Window Attention](sliding_window_attention.pdf-2-0.png)

**Caption:**  
This figure illustrates the sliding window attention mechanism in a transformer model. The ""Prefill"" region represents the initial input, while the ""Decode"" region shows the subsequent processing.

**Description:**
- The image depicts a grid representing a sequence of data, with a sliding window moving across it.
- The green area indicates the window's current position, highlighting the portion of the sequence being attended to.
- The blue area represents the decoded output based on the attended input.
- The window progressively moves across the sequence, enabling contextual understanding.
```

```
### Figure - Sliding Window Attention
![Sliding Window Attention](sliding_window_attention.pdf-2-1.png)

**Caption:**  
The figure illustrates the sliding window attention mechanism, where attention is retained over a limited window. Attention decreases as the distance from the current position increases.

**Description:**
- The image depicts a grid representing a sequence of tokens.
- The blue squares indicate attention retained across the sliding window.
- The green squares represent attention calculated within the sliding window.
- Attention decreases progressively as the distance 'k' from the current position increases, reflecting a decaying attention mechanism.
- The sliding window size is decreasing as it moves along the sequence.
curacy via reasoning. With the advent of ""thinking"" models, such as DeepSeek-R1 (DeepSeek-AI,
2025a), CoT has evolved from a prompting strategy
to an intrinsic LLM capability. However, whether
CoT has a specific impact in SWA scenarios remains uninvestigated. We explore this by comparing a thinking model with its non-thinking variant,
e.g., Qwen3-4B-Thinking and Qwen3-4B-Instruct,
to verify the effect of CoT on SWA adaptation,
which can produce more notable differences compared to simply adding CoT prompting to the same
model.


**3.4** **Fine-tuning**

---

[Document 2]
Source: sliding_window_attention_annotated_chunks_2k.jsonl
Chunk ID: sliding_window_attention_annotated_chunks_2k_2
Content: **3.4** **Fine-tuning**


This is the most natural way to mitigate traininginference mismatch. Apparently, the model should
be fine-tuned while SWA is applied, so that the
model’s parameters can be trained to better adapt
to SWA. Meanwhile, the training data must be longcontext tasks, where SWA actually works.
However, most available long-context datasets
only contain brief ground-truth answers, lacking the reasoning steps required for fine-tuning
a ""thinking"" model. Since our goal is to _restore_
the model’s original capabilities under SWA rather
than teach it new ones, instead of directly using the
original dataset, we adopt an approach similar to
self-distillation (Yang et al., 2024). Specifically, we
utilize the original full-attention model to generate
new answers for the dataset’s questions, and these
generated answers are then filtered for correctness
using GPT-5-Mini (OpenAI, 2025), to make up our
training dataset. For each question, we sample 4
answers with temperature 1, because we find this
strategy is slightly better than generating only one
answer with temperature 0.


**4** **Experiment Setup**


We organize our experiments around three research
questions:


**RQ1: Is SWA adaptation feasible without any**
**additional training?** We evaluate whether
an FA LLM can be adapted to SWA using
only inference-time modifications, and which
combinations of techniques are necessary.


**RQ2: How much does fine-tuning with SWA im-**
**prove performance?** We study the effect of
SWA-aware fine-tuning on long-context performance and identify which components of
SWAA are still required.



**RQ3: Which SWAA configurations achieves the**
**optimal performance-efficiency trade-offs?**
We evaluate how different SWAA configurations trade off accuracy against inference la
tency.


**4.1** **Models**


Our primary experiments use Qwen3-4B-Thinking
and Qwen3-4B-Instruct (Team, 2025b). The Thinking variant enforces chain-of-thought (CoT) style
reasoning, whereas the Instruct variant usually just
answers briefly. To ensure generality, we additionally evaluate Qwen3-30B-A3B-Thinking, Qwen330B-A3B-Instruct (Team, 2025b), and Llama3.18B-Instruct (Touvron et al., 2023).

All models are served with vLLM in float16

precision using a batch size of 64. We use greedy
decoding (temperature = 0) for all evaluations. In
preliminary experiments, we observed that vLLM
yields slightly lower (about 1% to 5%) scores
than HuggingFace Transformers due to precisionrelated discrepancies.


**4.2** **Evaluation Dataset**


SWA is identical to full attention when the context

length is within the window size. Even if the model
is fine-tuned, we can pre-calculate the prompt
length and simply disable the LoRA adapters for
short prompts to get completely the same response
as the original model. Therefore, our experiments
focus exclusively on long-context benchmarks with
inputs exceeding 16k tokens, as re-evaluating models on standard short-context benchmarks (e.g.,
MMLU (Hendrycks et al., 2021), GPQA (Rein
et al., 2023)) is completely unnecessary.
Since we find other long-context benchmarks are
either too easy or too difficult for 4B-level models (see Appendix B), we ultimately select LongMemEval (Wu et al., 2024), a benchmark consisting of various types of long-context QA tasks with
moderate difficulty, although it is originally designed for agent memory system evaluation. Its
context length is controllable by selecting a specific number of chat sessions to concatenate as the
context from a pool of hundreds of sessions (a session contains the chat history between user and
assistant within a day). To create a moderately difficult and discriminative evaluation, we construct
**LongMemEval_24k** by sampling 10 sessions, resulting in 500 samples ranging from 16k to 32k
with an average context length of 24k.



4


For additional validation of generalizability, as
shown in Appendix D, we also experiment on
LongBench-V2 (Bai et al., 2024b), a more modern and challenging benchmark that requires deep
reasoning across various real-world tasks.


**4.3** **Training Details**


For the fine-tuning dataset, we initially considered LongAlign (Bai et al., 2024a), a widely used
long-context fine-tuning dataset for adapt a regularlength model to long-context tasks. However, since

_∼_
its sample count ( 10,000) is insufficient, we incorporate an additional 6,000 samples from Fusangv1-long (Pan, 2024), a more comprehensive corpus
of over 40,000 long-context samples that includes
LongAlign as a subset.
We perform SWA-aware fine-tuning using LoRA
(Hu et al., 2022). Unless otherwise noted, we use
rank _r_ = 16 and _α_ = 128, and apply LoRA only
to the query, key, and value projection modules.
We adopt this parameter-efficient setting because
full-parameter fine-tuning often leads to overfitting
and degradation of the model’s original capabilities
in our preliminary experiments. We use a learning
rate of 1e-4 with a cosine decay schedule. Models
are fine-tuned for a single epoch on the sampled
long-context dataset since we observe no meaningful gains from additional epochs (see Appendix F).
Once training takes approximately 12 hours on an
8*H20 GPU server for Qwen3-4B and 30 hours for
Qwen3-30B-A3B.


**5** **Experiment Results**


**5.1** **SWA Adaptation Without Fine-tuning**


We first study SWA adaptation without any additional training. Table 1 reports LongMemEval_24k
accuracy for Qwen3-4B-Thinking (""think"") and
Qwen3-4B-Instruct (""non-think"") under different
combinations of SWAA components. In most settings, we use an aggressive 2k window to amplify
the impact of SWA. The configurations are ranked
by the number of methods applied (0, 1, 2, or 3 of
Interleaving Layers, Keep First and FA Decode).
Rows 1 (original model) and 2 (naive SWA) serve
as upper and lower baselines, respectively. In the
column ""FA layers"", the value records which layers use full attention, and [] means all the layers
use SWA, i.e., this method is not enabled. In the
column ""keep first"", the value is _k_ in Keep First
_k_ Tokens. When comparing results, an accuracy
difference of less than 5% is usually considered



statistically insignificant. From the results, we find
that:


**Naive SWA is not viable.** Naively replacing FA
with a 2k sliding window attention (row 1) drops
accuracy significantly to 3 _._ 2 and 11 _._ 0, respectively.
Even with an 8k window (row 2), accuracy only recovers to 13 _._ 2 and 19 _._ 8, far below the FA baseline.


**Single method helps, but cannot close the gap.**
Each method—Keep First, FA Decode, or Interleaving Layers—improves over naive SWA (rows 3–6),
yet each alone recovers only a small fraction of the
FA gap and remains well below the baseline. In
short, no single method is sufficient.


**Combinations exhibit strong synergy.** Recipes
that combine multiple methods deliver large gains:


  - **FA Decode + Keep First** _k_ substantially improves over naive SWA (rows 7–9), recovering roughly half to two-thirds of the gap on
the thinking model as _k_ increases. However,
increasing _k_ from 100 to 1000 yields almost
no improvement, indicating that _k_ does not
need to be exceedingly large.


  - **Interleaving Layers + FA Decode** is
markedly stronger (row 13), recovering most
of the gap for the thinking model.


  - **FA Decode + Interleaving Layers + Keep**
**First** _k_ perform best (rows 18). The thinking
model recovers close to 90% of the FA gap
even at 2k window.

---

[Document 3]
Source: sliding_window_attention_annotated_chunks_2k.jsonl
Chunk ID: sliding_window_attention_annotated_chunks_2k_0
Content: ## **Sliding Window Attention Adaptation**

Yijiong Yu [a], Jiale Liu [b], Qingyun Wu [b], Huazheng Wang [a], and Ji Pei [c]


aOregon State University, {yuyiji, huazheng.wang}@oregonstate.edu
bPenn State University, {jiale.liu, qingyun.wu}@psu.edu
cDeepSolution, research@deepsolution.chat



**Abstract**


The self-attention mechanism in Transformer
based Large Language Models (LLMs) scales
quadratically with input length, making longcontext inference expensive. Sliding window attention (SWA) reduces this cost to linear complexity, but naively enabling complete
SWA at inference-time for models pretrained
with full attention (FA) causes severe longcontext performance degradation due to training–inference mismatch. This makes us wonder: _Can FA-pretrained LLMs be well adapted_
_to SWA without pretraining?_ We investigate
this by proposing Sliding Window Attention
Adaptation (SWAA), a set of practical recipes
that combine five methods for better adaptation: (i) applying SWA only during prefilling;
(ii) preserving “sink” tokens; (iii) interleaving
FA/SWA layers; (iv) chain-of-thought (CoT);
and (v) fine-tuning. Our experiments show that
SWA adaptation is feasible while non-trivial:
no single method suffices, yet specific synergistic combinations effectively recover the original
long-context performance. We further analyze
the performance-efficiency trade-offs of different SWAA configurations and provide recommended recipes for diverse scenarios. Our code
[is available at github.](https://github.com/yuyijiong/sliding-window-attention-adaptation)


**1** **Introduction**


Transformer-based Large Language Models
(LLMs) (Vaswani et al., 2017) demonstrate
remarkable capabilities, but their self-attention
scales quadratically with the input sequence length,
making long context processing inefficient. Sliding
Window Attention (SWA), the most straightforward and widely adopted sparse attention
pattern, which restricts each token’s attention to a
fixed-size local window, reduces the computational
complexity to linearity, along with some other
benefits (see Appendix A).
To apply SWA to LLMs, typical solutions involve training a model with SWA from scratch, but



are prohibitively costly and cannot match the performance of state-of-the-art full-causal-attention

models like Qwen3 (Team, 2025b), mainly due
to the inability to reproduce pretraining data.
Training-free methods like streaming attention
(Xiao et al., 2024) can stabilize LLM outputs by retaining “sink tokens” while applying SWA, which
greatly improve efficiency but inevitably suffer
from severe long-context performance degradation
possibly due to the inaccessibility of distant tokens’
information (Xiao, 2025). This motivates a critical,
unexplored question: _Can a full-attention model_
_be adapted to sliding window attention at low cost_
_while maintaining long-context performance?_
We answer Yes to this question by proposing
Sliding Window Attention Adaptation(SWAA), a
set of recipes for adapting FA-pretrained models to
SWA, which requires neither costly pretraining nor
modifications to the standard Transformer architecture. Specifically, it systematically combines five
practical and composable methods:


1. **Full Attention (FA) Decode** : applying SWA
only during the prefilling stage while switching back to full attention for decoding.


2. **Keep First** _k_ **Tokens** : explicitly preserving
attention to the first _k_ “sink” tokens.


3. **Interleaving FA/SWA layers** : mix fullattention and SWA layers (e.g., assigning
SWA to half layers).


4. **Chain-of-Thought (CoT)** : enforcing an explicit ""thinking"" process during decoding.


5. **Fine-tuning with SWA** : lightweight SWAaware supervised fine-tuning on long-context
data.


Among these, FA Decode is a novel method we
introduce. Keep First _k_ Tokens and FA/SWA Interleaving have been proven effective in prior work



1


(Xiao et al., 2024; Team, 2024a; Zhang et al., 2024),
while CoT and fine-tuning are common LLM techniques. However, how these methods should be
combined to be actually effective for SWA adaptation remains unexplored.
Therefore, in our experiments, we evaluate
SWAA on Qwen3 (Team, 2025b) and Llama3.1
(Team, 2024b) across several long-context benchmarks, measuring both performance and efficiency
under a wide range of SWAA recipes. First, we
find that each method makes a distinct contribution,
but no single ingredient suffices to make SWA competitive with full attention. Second, we show that
specific synergistic combinations of methods can
recover a large fraction of the original long-context
performance. Third, we analyze the performanceefficiency trade-offs of different SWAA recipes and
identify some recommended configurations suitable for different deployment scenarios.
Rather than proposing a single globally optimal
configuration, we view SWAA as a flexible toolkit
of practical recipes: practitioners can select SWAA
recipes that match their accuracy and efficiency
constraints, or compose their own SWA adaptation
strategies by combining the available ingredients.
Our key contributions are:


1. We perform the first systematic study on adapting FA-pretrained LLMs to SWA without pretraining, revealing novel insights about how
SWA impacts LLMs and providing a foundation for future research in efficient sparse
attention.


2. We propose SWAA, a set of practical
SWA adaptation recipes that offer a robust
performance-efficiency balance for various
use cases, accelerating LLM inference from
the bottom level.


3. We implement our methods with FlashAttention (Dao, 2024) and vLLM (Kwon
et al., 2023), making it plug-and-play and userfriendly for practical deployment.


**2** **Related Works**


The _O_ ( _N_ [2] ) complexity of self-attention in Transformers (Vaswani et al., 2017) has spurred a wide
field of research about more efficient language
model architectures. Among the two most popular technological routes are sparse attention and
linear attention.



**2.1** **Sparse Attention**


Our work falls in this category. Sliding Window
Attention (SWA) represents the most basic form
of local sparse attention, yet its performance is inherently limited. Therefore, model architectures
such as Longformer (Beltagy et al., 2020), BigBird
(Zaheer et al., 2020), and RATTENTION (Wang
et al., 2025) combine local SWA on most tokens
with special global attention on specific tokens to
create a more powerful, albeit still sparse, pattern.
Popular LLMs like Gemma2 (Team, 2024a) adopt
SWA in half of their layers to balance the efficiency
of SWA and peformance of FA. Sliding Window
Attention Training (SWAT) (Fu et al., 2025b) introduces architectural changes, such as sigmoid
activation and balanced position embeddings, to
stabilize SWA performance. More advanced methods like Deepseek-sparse-attention (Yuan et al.,
2025; DeepSeek-AI, 2025b), although achieving
excellent quality, involve more complicated implementation and optimization due to semantic-aware
attention operations (e.g., selecting the most important tokens based on attention weights). Regardless,
almost all of the above methods require pretraining with a specific sparse pattern, which is costly
and fails to leverage the advantages of existing pretrained models.


LightTransfer (Zhang et al., 2024) is a promising
attempt at adapting existing models to SWA without pretraining, which has the same motivation as
ours. But it may generalize poorly across model
families (see Appendix G).


**2.2** **Linear Attention**


An alternative approach involves reformulating
the attention mechanism entirely to achieve linear, _O_ ( _N_ ), complexity. This includes methods
such as RNN-like linear attention transformers
","DeepSeek-R1 is presented as an example of a ""thinking"" model, which signifies the evolution of Chain-of-Thought (CoT) from a prompting strategy to an intrinsic capability of Large Language Models (LLMs).

In the context of SWA adaptation, Chain-of-Thought (CoT) is one of the five methods investigated to facilitate adaptation. The study explores the specific impact of CoT in SWA scenarios by comparing ""thinking"" models (like DeepSeek-R1 is an example of) with their non-thinking variants to verify CoT's effect on SWA adaptation. The ""Full Attention Decode"" strategy also suggests that CoT during decoding could be beneficial to compensate for insufficient contextual information during prefilling.",3673,5778,9453,,3,,2026-01-02T20:14:19.265952
"What is the relationship between ""Flash-Attention-2"" and the ""Keep First k Tokens"" implementation in SWAA?",Entity Relationship Reasoning,"[Document 1]
Source: sliding_window_attention_annotated_chunks_2k.jsonl
Chunk ID: sliding_window_attention_annotated_chunks_2k_1
Content: LightTransfer (Zhang et al., 2024) is a promising
attempt at adapting existing models to SWA without pretraining, which has the same motivation as
ours. But it may generalize poorly across model
families (see Appendix G).


**2.2** **Linear Attention**


An alternative approach involves reformulating
the attention mechanism entirely to achieve linear, _O_ ( _N_ ), complexity. This includes methods
such as RNN-like linear attention transformers

(Katharopoulos et al., 2020; Peng et al., 2023;
Sun et al., 2023) and structured state-space models
(SSMs) like Mamba (Gu and Dao, 2023). Many
works such as Jamba and Nemotron-Flash(Lieber
et al., 2024; Linsong Chu et al., 2024; Team et al.,
2025; Fu et al., 2025a) interleave linear attention
layers with traditional attention layers to create hybrid model structures. While promising, they represent a fundamental architectural departure from the
standard Transformer, and their performance is generally weaker than traditional Transformer-based

LLMs.



2


**3** **Candidate Methods for SWA**

**Adaptation**


As established, a naive application of SWA leads
to severe long-context performance degradation.
Therefore, we investigate five methods that can potentially facilitate SWA adaptation from distinct
perspectives. However, every method except finetuning has some drawbacks to LLM inference efficiency, as discussed in Appendix A. Therefore,
although these methods are not mutually exclusive,
we should not indiscriminately adopt all of them.
Instead, we must evaluate various combinations to
identify the optimal trade-off between performance
and efficiency.


**3.1** **Full Attention Decode**


This method applies SWA **only** to the prefilling
stage. During the decoding (auto-regressive generation) stage, each token still employs full attention,
allowing access to all previous tokens in the context. The resulting attention mask is depicted in
Figure 1a.
This novel approach, first proposed in our study,
is inspired by human reading comprehension: humans typically scan a passage casually (prefilling)
before thinking deeply to formulate an answer (decoding) for a specific problem. We term this strategy ""reading casually, thinking carefully."" In our
design, the SWA-constrained prefilling stage corresponds to casual reading, while the full-attention
decoding stage enables careful thinking. This analogy also suggests that Chain-of-Thought (CoT)
during decoding may be particularly beneficial, as
extended generation (i.e. ""thinking"") could compensate for the insufficient contextual information
gathered during the prefilling stage.


(a) FA Decode (b) Keep First


Figure 1: (a) Attention mask for FA Decode. SWA is
used for prompt tokens (prefill), and full attention is
used for generated tokens (decode). (b) Attention mask
for SWA combined with Keep First _k_ Tokens.



**3.1.1** **Keep First k Tokens**

Xiao et al. demonstrate that models pretrained with
full attention allocate a disproportionate amount
of attention to the initial tokens (""attention sink""),
and removing the visibility of these tokens causes
performance collapse. Their solution, streaming
attention, involves permanently retaining the attention to these ""sink"" tokens while using SWA, which
successfully maintains the stability of the attention
distribution and the model’s output. Our method
is basically the same: as shown in Figure 1b, any
subsequent token can attend to its local window
**and** the initial _k_ tokens.

Notably, however, our method extends beyond
its original version. Streaming attention operates
only at the KV cache level; specifically, the KV
cache of the sink tokens is retained during decoding, while the prefilling stage is not modified or accelerated at all. In contrast, we directly customize
the Flash-Attention-2 (Dao, 2024) kernel to implement such attention mask, thereby accelerating
prefilling via SWA as well, and eliminating the
need to modify KV cache.


**3.2** **Interleaving Layers**


This method retains full attention on a subset of

layers while applying SWA to the remainder, providing a simple hybrid mechanism to balance the
performance of full attention with the efficiency
of pure SWA. A common strategy involves designating one in every _n_ layers to use full attention
(e.g., layers 0, 2, 4, . . . retain full attention, while all
others use SWA). For example, Gemma-2 (Team,
2024a) uses SWA only for layers [1, 3, 5, . . . ], and
Gemma-3 (Team, 2025a) uses SWA only for layers

[5, 11, 17, ...].
However, for an FA-pretrained model, layers
may exhibit distinct behaviors, suggesting it may
not be optimal to simply assign SWA to layers [1,
3, 5, ...]. Instead, it might be preferable to use
statistical features to select ""lazy"" (mostly focusing
on just recent tokens) layers, as adopted by LightTransfer (Zhang et al., 2024). However, we find
that LightTransfer is not consistently superior in
practice (see Appendix G). Therefore, for simplicity, we still choose the simplest fixed-interval layer
selections in our experiments, such as [0, 2, 4, . . . ]
and [1, 3, 5, ...].


**3.3** **Chain-of-Thought**


Chain-of-Thought (CoT) (Wei et al., 2022) is a
widely used technique for improving model ac


3



```
### Figure - Sliding Window Attention
![Sliding Window Attention](sliding_window_attention.pdf-2-0.png)

**Caption:**  
This figure illustrates the sliding window attention mechanism in a transformer model. The ""Prefill"" region represents the initial input, while the ""Decode"" region shows the subsequent processing.

**Description:**
- The image depicts a grid representing a sequence of data, with a sliding window moving across it.
- The green area indicates the window's current position, highlighting the portion of the sequence being attended to.
- The blue area represents the decoded output based on the attended input.
- The window progressively moves across the sequence, enabling contextual understanding.
```

```
### Figure - Sliding Window Attention
![Sliding Window Attention](sliding_window_attention.pdf-2-1.png)

**Caption:**  
The figure illustrates the sliding window attention mechanism, where attention is retained over a limited window. Attention decreases as the distance from the current position increases.

**Description:**
- The image depicts a grid representing a sequence of tokens.
- The blue squares indicate attention retained across the sliding window.
- The green squares represent attention calculated within the sliding window.
- Attention decreases progressively as the distance 'k' from the current position increases, reflecting a decaying attention mechanism.
- The sliding window size is decreasing as it moves along the sequence.
curacy via reasoning. With the advent of ""thinking"" models, such as DeepSeek-R1 (DeepSeek-AI,
2025a), CoT has evolved from a prompting strategy
to an intrinsic LLM capability. However, whether
CoT has a specific impact in SWA scenarios remains uninvestigated. We explore this by comparing a thinking model with its non-thinking variant,
e.g., Qwen3-4B-Thinking and Qwen3-4B-Instruct,
to verify the effect of CoT on SWA adaptation,
which can produce more notable differences compared to simply adding CoT prompting to the same
model.


**3.4** **Fine-tuning**

---

[Document 2]
Source: sliding_window_attention_annotated_chunks_2k.jsonl
Chunk ID: sliding_window_attention_annotated_chunks_2k_3
Content: **Combinations exhibit strong synergy.** Recipes
that combine multiple methods deliver large gains:


  - **FA Decode + Keep First** _k_ substantially improves over naive SWA (rows 7–9), recovering roughly half to two-thirds of the gap on
the thinking model as _k_ increases. However,
increasing _k_ from 100 to 1000 yields almost
no improvement, indicating that _k_ does not
need to be exceedingly large.


  - **Interleaving Layers + FA Decode** is
markedly stronger (row 13), recovering most
of the gap for the thinking model.


  - **FA Decode + Interleaving Layers + Keep**
**First** _k_ perform best (rows 18). The thinking
model recovers close to 90% of the FA gap
even at 2k window.


**CoT synergizes with FA Decode.** Under recipes
that include FA Decode, the thinking model consistently benefits more than the non-thinking model
(rows 13 and 18), suggesting that CoT synergizes
with FA Decode: preserving global attention at
decoding time enables longer reasoning traces to
capitalize on context processed by SWA, confirming our hypothesis in Section 3.1.


**Sliding window size affects, but is not the deci-**
**sive role.** With FA Decode + Keep First _k_, accuracy improves as the window increases (rows 7,
14, 15), though benefits are modest until 8k. When
added with interleaving FA layers, moving from a
2k to a 4k window is enough to close the remaining
gap on the thinking model (row 22 matches the FA
baseline), indicating that FA Decode and Interleaving are the dominant drivers, and larger windows
mainly smooth residual error.



5


Table 1: Qwen3-4B-Thinking and Qwen3-4B-Instruct results on LongMemEval without SFT


**No.** **window size** **FA layers** **keep first** **FA decode** **Acc think** **Acc non-think**


0 Full [] 0 False **73.0** **62.0**
1 2k [] 0 False 3.2 11.0
2 8k [] 0 False 13.2 19.8


3 2k [] 10 False 16.0 15.6
4 2k [] 0 True 11.8 14.2
5 2k [1, 3, 5, ...] 0 False 13.4 18.4
6 8k [] 0 True 26.2 25.0


7 2k [] 10 True 38.2 20.6
8 2k [] 100 True 50.0 17.8
9 2k [] 1000 True 50.0 20.2
10 2k [0, 2, 4, ...] 10 False 17.0 14.8
11 2k [0, 2, 4, ...] 0 True 32.2 26.0
12 2k [1, 3, 5, ...] 10 False 25.8 36.4
13 2k [1, 3, 5, ...] 0 True 59.2 34.8
14 4k [] 10 True 38.0 24.4
15 8k [] 10 True 49.2 35.2


16 2k [0, 2, 4, ...] 10 True 36.0 17.2
17 2k [1, 3, 5, ...] 10 True 65.0 **53.6**
18 2k [1, 3, 5, ...] 100 True **68.8** 50.6
19 2k [1, 5, 9, ...] 10 True 53.2 31.4
20 2k [1, 9, 17, ...] 10 True 36.4 18.8
21 2k [3, 7, 11, ...] 10 True 54.2 34.6
22 4k [1, 3, 5, ...] 100 True **73.0** **54.2**
23 8k [1, 3, 5, ...] 100 True **71.6** **56.6**



**SWA/FA layer selection has large impacts.** Selecting less layers for FA, e.g. only one fourth or
eighth (row 19, 20), though more efficient, will
significantly decrease the improvement brought by
Interleaving Layers. More importantly, for Qwen34B, configuring _odd-numbered_ layers with full attention is significantly better than _even-numbered_
layers (row 11, 13, 16, 17). However, surprisingly,
this result is reversed for Qwen3-30B-A3B and
Llama3.1-8B-Instruct (row 10 and 11 in Table 4
and 5). This suggests that layer functionalities differ across model families and sizes, necessitating
model-specific layer selection strategies, as discussed in Section 3.2.


Therefore, we answer RQ1: adapting an FA
LLM to SWA is feasible even without any training. But it requires specific combinations of at
least 2 methods, which could be less efficient for
inference.



**5.2** **SWA Adaptation With Fine-tuning**


We next evaluate SWA-aware supervised finetuning, which is expected to provide higher improvement. Table 2 reports LongMemEval_24k
accuracy after SFT under various SWAA configurations. The original full-attention model is also
fine-tuned as a baseline (row 0). Since training is
relatively time-consuming, we only test a representative subset of configurations. Our findings are as
follows:


**Fine-tuning substantially lifts all SWA config-**
**urations.** Comparing all the fine-tuning results
with non-training ones, it is clear that fine-tuning
consistently provides great improvement. However,
simply fine-tuning with naive SWA remains insufficient, only achieving 18.8% and 23.8% accuracy
(row 1).


**FA Decode and Interleaving Layers emerge as**
**dominant components.** After SFT, FA Decode
and Interleaving Layers provide the largest gains.



6


Table 2: Qwen3-4B-Thinking and Qwen3-4B-Instruct results on LongMemEval with SFT


**No.** **window size** **FA layers** **keep first** **FA decode** **Acc think** **Acc non-think**


0 Full [] 0 True **74.6** **63.4**
1 2k [] 0 False 18.8 23.8


2 2k [] 10 False 15.6 /
3 2k [] 0 True 57.9 42.0
4 2k [1, 3, 5, ...] 0 False 63.6 54.6
5 4k [] 0 True 62.6 /


6 2k [] 10 True 56.7 /
7 2k [] 100 True 62.2 42.6
8 2k [1, 3, 5, ...] 0 True **73.2** **58.8**
9 2k [0, 2, 4, ...] 0 True **66.0** /
10 2k [1, 5, 9, ...] 0 True **68.8** 47.0


11 2k [1, 3, 5, ...] 100 True **73.2** **61.4**


Table 3: Recommended SWA adaptation recipes for different needs and scenarios. � means optional.


**Training** ? **Thinking Model?** **Prefer?** **FA Decode** **Interleaving Layers** **Keep First**


No No Any � � �
No Yes Efficiency � � �
No Yes Accuracy � � �


Yes Any Efficiency � � �
Yes Any Accuracy � � �



Using FA Decode alone (row 3) or using Interleaving Layers alone (row 4) both get high accuracy.
And combining both (row 8) further boosts performance to 73 _._ 2 (think) and 58 _._ 8 (non-think), nearly
matching the full-attention SFT baseline in row 0.


**Keep First becomes optional rather than essen-**
**tial.** Before fine-tuning, Keep First is crucial for
stability under SWA. But after SFT, it only provides
minor additional improvement. With FA Decode,
adding _k_ = 100 (row 7) improves over _k_ = 0
(row 3) only 4.5%, and if further combining FA
layers, it almost offers no improvements (row 11
and row 8).


**Effect of sliding window size.** Row 3 and 5
shows that increasing the window from 2k to 4k
with FA Decode improves thinking-model accuracy from 57 _._ 9 to 62 _._ 6. This mirrors the non-SFT
trend that larger windows help, but the dominant
improvements still come from FA Decode and Interleaving Layers.

---

[Document 3]
Source: sliding_window_attention_annotated_chunks_2k.jsonl
Chunk ID: sliding_window_attention_annotated_chunks_2k_4
Content: **Keep First becomes optional rather than essen-**
**tial.** Before fine-tuning, Keep First is crucial for
stability under SWA. But after SFT, it only provides
minor additional improvement. With FA Decode,
adding _k_ = 100 (row 7) improves over _k_ = 0
(row 3) only 4.5%, and if further combining FA
layers, it almost offers no improvements (row 11
and row 8).


**Effect of sliding window size.** Row 3 and 5
shows that increasing the window from 2k to 4k
with FA Decode improves thinking-model accuracy from 57 _._ 9 to 62 _._ 6. This mirrors the non-SFT
trend that larger windows help, but the dominant
improvements still come from FA Decode and Interleaving Layers.

So, we answer RQ2: fine-tuning brings remarkably high performance restoration, provided we ap


ply **FA Decode**, **Interleaving Layers**, or a combination thereof, while **Keep First** becomes optional.
And the improvement brought by SFT under each
configuration varies significantly, meaning a nearoptimal training-free configuration need not remain
optimal after SFT, and vice versa.


**5.3** **Performance–efficiency Trade-offs and**
**Recommended Recipes**


Although integrating more methods can typically
achieve higher accuracy, it introduces more overhead, indicating that the efficiency of each recipe
must also be evaluated. To assess the performanceefficiency trade-off of different SWAA configurations, we evaluate time-to-first-token (TTFT), timeper-output-token (TPOT), total throughput, and average running time per request. Concretely, we
benchmark Qwen3-4B-Thinking on a single H100
GPU using vLLM’s bench_serve utility (Kwon
et al., 2023) with random input data and 100 total
requests. The prompt length and output length are
set to 128k and 512 tokens, respectively, representing a typical long-context QA setting.



7


```
### Figure - Accuracy vs. Time with SFT
![Accuracy graph](sliding_window_attention.pdf-7-0.png)

**Caption:**  
The figure shows the accuracy of a model with and without Sliding Factor Token (SFT) over time. Different configurations of window size, FA layers, and decode settings are compared.

**Description:**
- The x-axis represents time in seconds, ranging from 0.5 to 3.5 seconds.
- The y-axis represents accuracy in percentage.
- Two lines are plotted: a solid line represents the accuracy with SFT, and a dashed line represents the accuracy without SFT.
- Different colored markers indicate different configurations of the model:
  - Blue markers represent ""Full"" window, 0 FA layers, and False decode.
  - Green markers represent ""2k"" window, 0 FA layers, and False decode.
  - Orange markers represent ""2k"" window, 0 FA layers, and True decode.
  - Pink markers represent ""2k"" window, 1/2 FA layers, and False decode.
  - Light blue markers represent ""2k"" window, 1/2 FA layers, and True decode.
- The accuracy with SFT generally increases with time for all configurations.
- The accuracy without SFT is consistently lower than the accuracy with SFT.

```
### Figure - Accuracy vs. Time with and without SFT
![Accuracy graph](sliding_window_attention.pdf-7-1.png)

**Caption:**  
The figure shows the accuracy of a model with and without Sliding Factor Token (SFT) as a function of time window. Different configurations of FA layers and decode settings are also shown.

**Description:**
- The graph plots accuracy (%) on the y-axis against time (s) on the x-axis.
- Data points are color-coded to represent different configurations: w/ SFT (blue) and w/o SFT (black).
- Each color represents a specific combination of window size, FA layers, FA decode, and decode setting.
- The blue line represents the performance with SFT, while the black line represents the performance without SFT.
- The x-axis ranges from 0.5s to 3.5s, indicating the time window used for analysis.
- The y-axis ranges from 20% to 60%, representing the accuracy percentage.

(a) Qwen3-4B-Thinking (b) Qwen3-4B-Instruct


Figure 2: Accuracy and inference time of each configuration of Qwen3-4B on LongMemEval



To visualize the performance-efficiency tradeoff, Figure 2 plots each configuration’s accuracy
on LongMemEval_24k (Wu et al., 2024) against its
average running time, while detailed TTFT, TPOT,
and throughput statistics for each configuration are
provided in Appendix E. We draw a line between
the full-attention point and the naive-SWA point
as a baseline curve: configurations above this line
offer a better accuracy-latency balance intuitively.
For configurations with nearly identical time costs,
we display only the one with the highest accuracy.
Since **Keep First** _k_ has negligible impact on runtime (Appendix E), all plotted configurations fix
_k_ = 10.


From Figure 2, we observe that many configurations in Figure 2 achieve a clearly better
performance-efficiency ratio than baselines. And
for the thinking model, more points lie above the
baseline curve compared to non-thinking, indicating that **CoT** generally has a positive effect on improving the performance-efficiency ratio of SWAA.


Thus, we finally answer RQ3: many SWAA
configurations all reach excellent performanceefficiency trade-off, but there is no single metric to
quantify such trade-off to decide the globally optimal one. We therefore summarize **recommended**

**SWA adaptation recipes** tailored to various deployment scenarios in Table 3. And we must note
that specific parameters should be flexibly set to
meet application-specific requirements, without the
need to follow our experimental parameters (e.g.,



a 2k window, _k_ = 10). For example, you can increase the window size to 4k or _k_ to 128 for higher
accuracy and acceptable additional overhead.


**6** **Conclusion**


In this work, we validate the feasibility of adapting
full-attention pretrained LLMs to Sliding Window
Attention (SWA) for better efficiency, offering a
cost-effective alternative that avoids training sparseattention models from scratch. By systematically
deconstructing the adaptation process, we identify that the catastrophic degradation observed in
naive implementations can be effectively mitigated
through synergistic combinations of auxiliary methods. Our extensive experiments across the Qwen
and Llama families demonstrate that while trade
offs between computational overhead and model
performance are inevitable, optimized configurations can get an excellent performance-efficiency
balance.


**7** **Limitations**


We speculate that the ideal reasoning trajectory of
the model adapted to SWA should be longer than
the original model, to compensate for the information loss caused by SWA. That means, using
the answers generated by the original model as
fine-tuning data may not be the optimal training
method. Rather, RL methods like GRPO (Shao
et al., 2024) might further help the model adapted
to SWA learn a better reasoning trajectory. How


8


ever, we did not experiment with them since they
are too time-consuming and unstable.
We have not yet implemented the KV cache eviction (or overwriting) mechanism when using SWA;
that is, although the speed is improved, memory
usage is not effectively reduced.
Further experiments may be needed to confirm
whether our conclusions generalize to larger model
sizes, such as 70B.


**References**


Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei
Hou, Jie Tang, Yuxiao Dong, and Juanzi Li. 2024a.
[Longalign: A recipe for long context alignment of](https://arxiv.org/abs/2401.18058)
[large language models.](https://arxiv.org/abs/2401.18058)


Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu,
Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao
Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang,
[and Juanzi Li. 2023. Longbench: A bilingual, multi-](https://arxiv.org/abs/2308.14508)
[task benchmark for long context understanding.](https://arxiv.org/abs/2308.14508)
","The ""Keep First k Tokens"" method in SWAA directly customizes the Flash-Attention-2 kernel to implement its attention mask. This customization accelerates prefilling via Sliding Window Attention (SWA) and eliminates the need to modify the KV cache, which was a requirement in the original streaming attention approach.",122,2360,2483,,3,,2026-01-02T20:14:21.750934
"How does the ""sensory neuron"" entity in NAC interact with the ""backbone"" entity to compute attention logits?",Entity Relationship Reasoning,"[Document 1]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_3
Content: Scores: _S ←_ _Q · K_ _[⊤]_

Effective Top- _K_ : _K_ eff _←_ min( _K, Tk_ )
Indices: _I_ topk _←_ top ~~k~~ ( _S, K_ eff)
Gather: _K_ selected _←_ gather( _K, I_ topk) _∈_ R _[B][×][H][×][T][q]_ _[×][K]_ [eff] _[×][D]_

Tiled: _Q_ tiled _←_ tile( _Q, K_ eff) _∈_ R _[B][×][H][×][T][q]_ _[×][K]_ [eff] _[×][D]_

Concatenate: _U_ topk _←_ [ _Q_ tiled; _K_ selected ] _∈_ R _[B][×][H][×][T][q]_ _[×][K]_ [eff] _[×]_ [2] _[D]_

**return** _U_ topk


**2.2. Designing the Neural Network**


We now outline the design of a neural network layer guided
by the preceding analysis. The process involves five steps:
(i) repurposing NCPs; (ii) input curation; (iii) construction
of the time vector ( _t_ ); (iv) computing attention logits and
weights; and (v) generating the attention output. Figure 2
provides a graphical overview of NAC.
**Repurposing NCPs:** We repurpose the NCPs framework
by converting its fixed, biologically derived wiring (see Figure 1(a)) into a flexible recurrent architecture that allows
configurable input–output mappings. Instead of enforcing
a static connectome, our approach exposes adjacency matrices as modifiable structures defining sparse input and
recurrent connections. This enables selective information

routing across neuron groups while retaining the original circuit topology. Decoupling wiring specifications from model
instantiation allows dynamic connectivity adjustments to
accommodate different input modalities without full retraining. Algorithm 1 summarizes the steps for repurposing the
NCPs wiring mechanism. Key features include group-wise
masking for neuron isolation, adaptive remapping of inputs
and outputs for task-specific adaptation, and tunable sparsity
_s_ to balance expressiveness and efficiency.
In our implementation, the sensory neuron gate ( _NN_ sensory)
projects the _q_, _k_, and _v_ representations (see Figure 1(b)).
This enables sensory neurons to maintain structured, contextaware representations rather than collapsing inputs into fully



connected layers. As a result, the network preserves locality
and modularity, which improves information routing.


_NN_ sensory = NCPCell( _G_ input = [ _Ns_ ] _, G_ output = [ _Ns_ ] _,_

_D_ = [ _Ni, Nc, Nm_ ] _, s_ )
(13)
The inter-to-motor pathways form a backbone network
( _NN_ backbone) with branches that compute _ϕ_ and _ωτ_ (see
Figure 1(c)). Instead of learning _ϕ_ and _ωτ_ independently,
this backbone allows the model to learn shared representations, enabling multiple benefits: (i) separate head layers
enable the system to capture temporal and structural dependencies independently; (ii) accelerates convergence during
training.


_NN_ backbone = NCPCell( _G_ input = [ _Ni_ ] _, G_ output = [ _Nm_ ] _,_

_D_ = [ _Ns_ ] _, s_ )
(14)
The output heads are defined as:


_ϕ_ = _σ_ ( _NN_ backbone( **u** )) (15)

_ωτ_ = softplus( _NN_ backbone( **u** )) + _ε,_ _ε >_ 0 (16)


Here, _ϕ_ serves as a _content–target gate_ head, where the
sigmoid function _σ_ ( _·_ ) determines the target signal strength.
In contrast, _ωτ_ is a strictly positive _time–constant gate_ head
that controls the rate of convergence and the steady-state
amplitude. Conceptually, this parallels recurrent gating: _ϕ_
regulates _what_ content to emphasize, while _ωτ_ governs _how_
_quickly_ and _to what extent_ it is expressed.
**Input Curation:** We experimented with different
strategies for constructing query–key inputs. Initially, we implemented full pairwise concatenation,
where queries _Q ∈_ R _[B][×][H][×][T][q][×][D]_ are combined with
all keys _K_ _∈_ R _[B][×][H][×][T][k][×][D]_ to form a joint tensor
_U ∈_ R _[B][×][H][×][T][q][×][T][k][×]_ [2] _[D]_ . While this preserved complete
feature information and enabled expressive, learnable
similarity functions, it was memory-intensive, making it impractical for longer sequences. To mitigate this, we applied
a sparse Top- _K_ optimization: for each query, we compute
pairwise scores _S_ = _Q · K_ _[⊤]_ _∈_ R _[B][×][H][×][T][q][×][T][k]_, select the
Top- _K_ eff = min( _K, Tk_ ) keys, and construct concatenated
pairs _U_ topk _∈_ R _[B][×][H][×][T][q][×][K]_ [eff] _[×]_ [2] _[D]_ . This approach preserves
the most relevant interactions while substantially reducing
memory requirements in the concatenation and subsequent
backbone processing stages, allowing the method to scale
linearly with the sequence length in those components.
However, the initial computation of _S_ remains quadratic
(see Appendix C.3). Algorithm 2 outlines the steps required
for input curation.
**Time Vector:** NAC builds on continuous-depth models
as (Hasani et al., 2022) that adapt their temporal dynamics to the task. It constructs an internal, normalized
pseudo-time vector _t_ pseudo using a sigmoidal transformation,
_t_ pseudo = _σ_ ( _ta · t_ + _tb_ ), where _ta_ and _tb_ are learnable affine



4


**Neuronal Attention Circuit (NAC) for Representation Learning**



parameters and _σ_ is the sigmoid function. For time-varying
datasets (e.g., irregularly sampled series), each time point
_t_ is derived from the sample’s timestamp, while for tasks
without meaningful timing, _t_ is set to 1. The resulting _t_ pseudo
lies in [0 _,_ 1] and provides a smooth, bounded representation
of time for modulating the network’s dynamics.
**Attention logits and weights:** Starting from Eqn. 3,
consider the trajectory of a query–key pair with initial
condition _a_ 0 = 0:



_at_ = _[ϕ]_

_ωτ_



�1 _−_ _e_ _[−][ω][τ][ t]_ [�] _,_ (17)



followed by the _softmax_ normalization to calculate attention weights. The resulting attention weights _αt_ [(] _[h]_ [)] are then
used to integrate with the value vector _v_ [(] _[h]_ [)], producing headspecific attention outputs. Finally, these outputs are concatenated and linearly projected back into the model dimension.
This formulation ensures that each head learns distinct dynamic compatibilities governed by its own parameterization
of _ϕ_ and _ωτ_, while the aggregation across heads preserves
the expressive capacity of the standard multi-head attention
mechanism.


**2.3. NAC as Universal Approximator**


We now establish the universal approximation capability of
NAC by extending the classical Universal Approximation
Theorem (UAT) (Nishijima, 2021) to the proposed mechanism. For brevity, we consider a network with a single
NAC layer processing fixed-dimensional inputs, though the
argument generalizes to sequences.

---

[Document 2]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_14
Content: **D.2. Ablations Details**


The brief descriptions of variants and ablation are also divided into four subcategories:
**Top-** _**K**_ **Ablations:** _NAC-2k_ uses Top- _K_ =2 to compute the
logits and _NAC-32k_ uses Top- _K_ =32. All variants use the
exact computation mode with 50% sparsity.
**Sparsity Ablations:** _NAC-02s_ uses 20% sparsity to compute the logits and _NAC-09s_ uses 90%. _NAC-PW_ employs
full pairwise (non-sparse) concatenation for input curation.
_NAC-FC_ replaces the sparse NCP gating mechanism with
a simple fully connected layer. All variants use the exact
computation mode with Top- _K_ =8.
**Modes variants:** _NAC-Euler_ computes attention logits using the explicit Euler integration method. _NAC-Steady_ derives attention logits from the steady-state solution of the
exact formulation. _NAC-Exact/05s/8k_ computes attention
logits using the closed-form exact solution. It also overlaps
with other ablations, so we combined it into a single one.
All modes use Top- _K_ =8, 50% sparsity and _δt_ =1.0. The
sensitivity of NAC to _δt_ is visualized in Figure 4


_Figure 4._ Effect of _δt_ on output of NAC.



14



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/neuronal_attention_circuits.pdf-13-0.png)
**Neuronal Attention Circuit (NAC) for Representation Learning**



**D.3. Experimental Details**


D.3.1. EVENT-BASED MNIST


**Dataset Explanation and Curation:** The MNIST dataset,
introduced by (Deng, 2012), is a widely used benchmark for
computer vision and image classification tasks. It consists
of 70,000 grayscale images of handwritten digits (0–9), each
of size 28 _×_ 28 pixels, split into 60,000 training and 10,000
testing samples.
**Preprocessing:** We follow the preprocessing pipeline described in (Lechner & Hasani, 2022), which proceeds as
follows. First, a threshold is applied to convert the 8-bit
pixel values into binary values, with 128 as the threshold
on a scale from 0 (minimum intensity) to 255 (maximum
intensity). Second, each 28 _×_ 28 image is reshaped into a
one-dimensional time series of length 784. Third, the binary
time series is encoded in an event-based format, eliminating
consecutive occurrences of the same value; for example, the
sequence [1 _,_ 1 _,_ 1 _,_ 1] is transformed into (1 _, t_ = 4). This encoding introduces a temporal dimension and compresses the
sequences from 784 to an average of 53 time steps. Finally,
to facilitate efficient batching and training, each sequence
is padded to a fixed length of 256, and the time dimension
is normalized such that each symbol corresponds to one
unit of time. The resulting dataset defines a per-sequence
classification problem on irregularly sampled time series.
**Neural Network Architecture:** We develop an end-to-end
hybrid neural network by combining compact convolutional
layers with NAC or counterparts baselines for fair comparison. Detailed hyperparameters and architectural specifications are provided in Table 4.


D.3.2. PERSON ACTIVITY RECOGNITION (PAR)


**Dataset Explanation and Curation:** We used the
Localized Person Activity Recognition dataset provided by
UC Irvine (Vidulin et al., 2010). The dataset comprises
25 recordings of human participants performing different
physical activities. The eleven possible activities are
“walking,” “falling,” “lying down,” “lying,” “sitting down,”
“sitting,” “standing up from lying,” “on all fours,” “sitting
on the ground,” “standing up from sitting,” and “standing
up from sitting on the ground.” The objective of this
experiment is to recognize the participant’s activity from
inertial sensors, formulating the task as a per-time-step
classification problem. The input data consist of sensor
readings from four inertial measurement units placed on
participants’ arms and feet. While the sensors are sampled
at a fixed interval of 211 ms, recordings exhibit different
phase shifts and are thus treated as irregularly sampled time
series.

**Preprocessing:** We first separated each participant’s
recordings based on sequence identity and calculated
elapsed time in seconds using the sampling period. To



mitigate class imbalance, we removed excess samples from
overrepresented classes to match the size of the smallest
class. Subsequently, the data were normalized using a
standard scaler. Finally, the dataset was split into a 90:10
ratio for training and testing.
**Neural Network Architecture:** Following the approach in
Section D.3.1, we developed an end-to-end hybrid neural
network combining convolutional heads with NAC or other
baselines. Hyperparameter details are summarized in Table
4.


D.3.3. AUTONOMOUS VEHICLE


**Dataset Explanation and Curation:** We followed the data
collection methodology described in (Razzaq & Hongwei,
2023). For OpenAI-CarRacing, a PPO-trained agent (5M
timesteps) was used to record 20 episodes, yielding approximately 48,174 RGB images of size 92 _×_ 92 _×_ 3 with
corresponding action labels across five discrete actions (noact, move left, forward, move right, stop). The dataset was
split with 10% reserved for testing and the remaining 90%
for training. For the Udacity simulator, we manually controlled the vehicle for 50 minutes, producing 15647 RGB
images of size 320 _×_ 160 _×_ 3, captured from three camera
streams (left, center, right) along with their corresponding
continuous steering values. This dataset was split into 20%
testing and 80% training.
**Preprocessing:** No preprocessing was applied to the
OpenAI-CarRacing dataset. For the Udacity simulator, we
followed the preprocessing steps in (Shibuya, 2017). Each
image was first cropped to remove irrelevant regions and
resized to 66 _×_ 120 _×_ 3. Images were then converted from
RGB to YUV color space to match the network input. To improve robustness, data augmentation techniques, including
random flips, translations, shadow overlays, and brightness
variations, were applied to simulate lateral shifts and diverse
lighting conditions.
**Neural Network Architecture:** For OpenAI-CarRacing,
we modified the neural network architecture proposed in
(Razzaq & Hongwei, 2023), which combines compact CNN
layers for spatial feature extraction with LNNs to capture
temporal dynamics. In our implementation, the LNN layers
were replaced with NAC and its comparable alternatives
for fair evaluation. Full hyperparameter configurations are
provided in Table 4. For the Udacity simulator, we modified
the network proposed in (Bojarski et al., 2016) by replacing
three latent MLP layers with NAC and its counterparts. Full
hyperparameters for this configuration are summarized in
Table 4.

**Saliency Maps:** A saliency map visualizes the regions of
the input that a model attends to when making decisions.
Figure 5 shows the saliency maps for the OpenAI CarRacing environment. We observe that only NAC (Steady, Euler,



15


**Neuronal Attention Circuit (NAC) for Representation Learning**


_Figure 5._ Saliency maps for OpenAI CarRacing


_Figure 6._ Saliency maps for Udacity Simulator



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/neuronal_attention_circuits.pdf-15-0.png)

![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/neuronal_attention_circuits.pdf-15-1.png)

---

[Document 3]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_0
Content: ## **Neuronal Attention Circuit (NAC) for Representation Learning**

**Waleed Razzaq** [1] **Izis Kankaraway** [1] **Yun-Bo Zhao** [1 2]



**Abstract**

Attention improves representation learning over
RNNs, but its discrete nature limits continuoustime (CT) modeling. We introduce Neuronal Attention Circuit (NAC), a novel, biologically plausible CT-Attention mechanism that reformulates

attention logits computation as the solution to a
linear first-order ODE with nonlinear interlinked
gates derived from repurposing _C. elegans_ Neuronal Circuit Policies (NCPs) wiring mechanism.
NAC replaces dense projections with sparse sensory gates for key-query projections and a sparse
backbone network with two heads for computing
_content-target_ and _learnable time-constant_ gates,
enabling efficient adaptive dynamics. NAC supports three attention logit computation modes: (i)
explicit Euler integration, (ii) exact closed-form
solution, and (iii) steady-state approximation. To
improve memory intensity, we implemented a
sparse Top- _K_ pairwise concatenation scheme that
selectively curates key-query interactions. We
provide rigorous theoretical guarantees, including
state stability, bounded approximation errors, and
universal approximation. Empirically, we implemented NAC in diverse domains, including irregular time-series classification, lane-keeping for
autonomous vehicles, and industrial prognostics.
We observed that NAC matches or outperforms
competing baselines in accuracy and occupies an
intermediate position in runtime and memory efficiency compared with several CT baselines.


**1. Introduction**


Learning representations of sequential data in temporal or
spatio-temporal domains is essential for capturing patterns
and enabling accurate forecasting. Discrete-time Recurrent neural networks (DT-RNNs) such as RNN (Rumelhart et al., 1985; Jordan, 1997), Long-short term memory


1Department of Automation, University of Science & Technology of China, Hefei, China [2] Institute of Artificial Intelligence,
Hefei Comprehensive National Science Center. Correspondence to:
Yun-Bo Zhao _<_ ybzhao@ustc.edu.cn _>_, Waleed Razzaq _<_ waleedrazzaq@mail.ustc.edu.cn _>_ .


_Preprint. December 12, 2025._



(LSTM) (Hochreiter & Schmidhuber, 1997), and Gated Recurrent Unit (GRU) (Cho et al., 2014) model sequential dependencies by iteratively updating hidden states to represent
or predict future elements in a sequence. While effective
for regularly sampled sequences, DT-RNNs face challenges
with irregularly sampled data because they assume uniform
time intervals. In addition, vanishing gradients can make
it difficult to capture long-term dependencies (Hochreiter,
1998).
Continuous-time RNNs (CT-RNNs) (Rubanova et al., 2019)
model hidden states as ordinary differential equations
(ODEs), allowing them to process inputs that arrive at arbitrary or irregular time intervals. Mixed-memory RNNs
(mmRNNs) (Lechner & Hasani, 2022) build on this idea
by separating memory compartments from time-continuous
states, helping maintain stable error propagation while capturing continuous-time dynamics. Liquid neural networks
(LNNs) (Hasani et al., 2021; 2022) take a biologically inspired approach by assigning variable time-constants to hidden states, improving adaptability and robustness, though
vanishing gradients can still pose challenges during training.
The attention mechanisms (Vaswani et al., 2017) mitigate
this limitation by treating all time steps equally and allowing models to focus on the most relevant observations. It
computes the similarity between queries ( _q_ ) and keys ( _k_ ),
scaling by the key dimension to keep gradients stable. MultiHead Attention (MHA) (Vaswani et al., 2017) extends this
by allowing the model to attend to different representation
subspaces in parallel. Variants like Sparse Attention (Tay
et al., 2020; Roy et al., 2021), BigBird (Zaheer et al., 2020),
and Longformer (Beltagy et al., 2020) modify the attention
pattern to reduce computational cost, particularly for long sequences, by attending only to selected positions rather than
all pairs. Even with these improvements, attention-based
methods still rely on discrete scaled dot-product operations,
limiting their ability to model continuous trajectories often
captured by CT counterparts.
Recent work has explored bridging this gap through NeuralODE (Chen et al., 2018) formulation. mTAN (Shukla &
Marlin, 2021) learns CT embeddings and uses time-based
attention to interpolate irregular observations into a fixedlength representation for downstream encoder-decoder modeling. ODEFormer (d’Ascoli et al., 2023) trains a sequenceto-sequence transformer on synthetic trajectories to directly
infer symbolic ODE systems from noisy, irregular data,
though it struggles with chaotic systems and generalization



1


**Neuronal Attention Circuit (NAC) for Representation Learning**



beyond observed conditions. Continuous-time Attention
(CTA) (Chien & Chen, 2021) embeds a continuous-time
attention mechanism within a Neural ODE, allowing attention weights and hidden states to evolve jointly over time.
Still, it remains computationally intensive and sensitive to
the accuracy of the ODE solver. ContiFormer (Chen et al.,
2023) builds a CT-transformer by pairing ODE-defined latent trajectories with a time-aware attention mechanism to
model dynamic relationships in data.
Despite these innovations, a persistent and underexplored
gap remains in developing a biologically plausible attention
mechanism that seamlessly integrates CT dynamics with the
abstraction of the brain’s connectome to handle irregular sequences without prohibitive computational costs. Building
on this, we propose a novel attention mechanism called the
_Neuronal Attention Circuit_ (NAC), in which attention logits
are computed as the solution to a first-order ODE modulated
by nonlinear, interlinked gates derived from repurposing
Neuronal Circuit Policies (NCPs) from the nervous system
of _C. elegans_ nematode (refer to Appendix A.2 for more
information). Unlike standard attention, which projects keyquery pairs through a dense layer, NAC employs a sensory
gate to transform input features and a backbone to model
nonlinear interactions, with multiple heads producing outputs structured for attention logits computation. Based on
the solutions to ODE, we define three computation modes:
(i) Exact, using the closed-form ODE solution; (ii) Euler,
approximating the solution via _explicit Euler_ integration;
and (iii) Steady, using only the steady-state solution, analogous to standard attention scores. To reduce computational
complexity, we implemented a sparse Top- _K_ pairwise concatenation algorithm that selectively curates key-query inputs. We evaluate NAC across multiple domains, including
irregularly sampled time series, autonomous vehicle lanekeeping, and Industry 4.0, comparing it to state-of-the-art
baselines. NAC consistently matches or outperforms these
models, while runtime and peak memory benchmarks place
it between CT-RNNs in terms of speed and CT-Attentions
in terms of memory requirements.


**2. Neuronal Attention Circuit (NAC)**


We propose a simple alternative formulation of the attention
logits _a_ (refer to Appendix A.1 for more information), interpreting them as the solution to a first-order linear ODE
modulated by nonlinear, interlinked gates:



from repurposing NCPs. We refer to this formulation as
the Neuronal Attention Circuit (NAC). It enables the logits
_at_ to evolve dynamically with input-dependent, variable
time constants, mirroring the adaptive temporal dynamics
found in _C. elegans_ nervous systems while improving
computational efficiency and expressiveness. Moreover, it
introduces continuous depth into the attention mechanism,
bridging discrete-layer computation with dynamic temporal
evolution.

**Motivation behind this formulation:** The proposed
formulation is loosely motivated by the input-dependent
time-constant mechanism of Liquid Neural Networks
(LNNs), a class of CT-RNNs inspired by biological nervous
systems and synaptic transmission. In this framework, the
dynamics of non-spiking neurons are described by a linear
ODE with nonlinear, interlinked gates: _ddt_ **xt** = **[x]** _τ_ **[t]** [+] **[ S][t]** _[,]_
","In NAC, the ""sensory neuron"" entity (`NN_sensory`) projects the `q` (query) and `k` (key) representations. These `q` and `k` representations are then used to compute pairwise scores, and a sparse Top-K optimization selects the most relevant keys. The selected keys are concatenated with the queries to form `U_topk` (or `u`).

This `U_topk` is then fed into the ""backbone"" entity (`NN_backbone`). The `NN_backbone` models nonlinear interactions and has two branches that compute `ϕ` (content-target gate) and `ωτ` (time-constant gate) based on `u`.

Finally, these `ϕ` and `ωτ` values, which are derived from the backbone's output, are used to compute the attention logits (`at`) as the solution to a first-order ODE: `at = [ϕ/ωτ] * [1 - e^(-ωτ * t)]`. Thus, the sensory neuron provides the initial representations, and the backbone processes the curated query-key pairs to generate the gates that modulate the ODE for attention logit computation.",116,7167,7284,,3,,2026-01-02T20:14:29.035533
"What connects the ""occipital lobe"" entity to the ""High-Level Visual Reception"" function in the CogVision framework?",Entity Relationship Reasoning,"[Document 1]
Source: attention_functional_roles_raw_chunks_2k.jsonl
Chunk ID: attention_functional_roles_raw_chunks_2k_1
Content: Furthermore, we validate the functional importance of these heads by showing that their removal
degrades performance on complex tasks and leads to specific error patterns, while their enhancement improves reasoning capabilities. Our findings provide compelling evidence that these attention
heads play a critical role in multimodal reasoning. This insight not only deepens our understanding of the internal organization of VLMs but also suggests potential avenues for designing more
interpretable and cognitive-inspired multimodal AI systems.


2 COGVISION


In this section, we present our dataset Cognitive Vision (CogVision) that contains cognitive process
in multimodal reasoning. CogVision contains 1,409 main questions and 5,744 subquestions. Each


2


example comprises a main question and answer, its subquestions and subanswers and an annotation
specifying the receptive or cognitive function required for each subquestion.


2.1 COGNITIVE FUNCTIONS


To systematically capture the cognitive processes involved in complex reasoning tasks, we consider eight functions related to complex multimodal reasoning. These functions are inspired by
established frameworks in cognitive science (Anderson, 2014; Diamond, 2013), which highlight the
importance of perception, working memory, and reasoning in human cognition.


The functions related to vision include:


    - **Low-level Visual Reception** : Recognizing basic visual features such as color, shape, size,
position, motion.

    - **High-level Visual Reception** : Integrating visual information to recognize objects, patterns,
and scene structure.

    - **Visual Knowledge Recall** : Applying long-term visual knowledge related to visual concepts and their properties.


The functions related to language include:


    - **Language Information Extraction and Understanding** : locating and understanding relevant information from an external source or prior context.

    - **Language Knowledge Recall** : Accessing domain-specific knowledge without visual input.

    - **Math Reasoning** : Performing counting, arithmetic, comparison, and logic-based operations.

    - **Inference** : deriving implicit information that is not directly stated.

    - **Decision-Making** : Selecting the best outcome among alternatives based on reasoning.


This categorization reflects a natural progression from basic information processing to complex cognitive integration. Both the human brain and VLMs encompass a wide range of functional modules.
Our focus in this work is specifically on reasoning-related cognitive functions. By identifying and
organizing these eight core reasoning functions, we can more clearly examine how VLMs handle
different types of thinking steps, in a way that is both systematic and easy to interpret. Detail descriptions of each function and examples in CogVision can be found in Appendix A.1.


2.2 DATA COLLECTIONS


Based on our categorization of reasoning functions, we sampled 2000 diverse questions from existing commonly used visual reasoning benchmarks, selecting 200 examples each from ClevrMath (Lindstr¨om & Abraham, 2022), MathVision (Wang et al., 2024a), MathVista (Lu et al., 2024b),
MMMU (Yue et al., 2024), ScienceQA (Saikh et al., 2022), OKVQA (Marino et al., 2019) (400
examples), VCR (Zellers et al., 2019), VisuLogic (Xu et al., 2025) and Super-Clevr (Li et al.,
2023b) datasets. These datasets span a range of reasoning problems, including logical, mathematical, and commonsense reasoning. Using the chain-of-thought (CoT) paradigm, we prompted
GPT-4.1 (Achiam et al., 2023) to decompose each main question (with its final answer) into subquestions, each targeting a subanswer and a specific function. The prompt encourages structured,
step-by-step reasoning, ensuring that each subquestion is clear, answerable, and sequentially dependent. This process yields a set of subquestion–answer–function (subQAF) triples for each QA pair:
subQAFs = _{_ ( _qi, ai, fi_ ) _}_ _[k]_ _i_ =1 [, where each contains a subquestion] _[ q][i]_ [, its concise answer] _[ a][i]_ [, and the]
corresponding function label _fi_ . The prompt for generating subquestions are list in Appendix A.6.


2.3 DATA FILTERING AND ANNOTATION


Recent advances enable leveraging large pre-trained models for dataset construction, thanks to their
reasoning capabilities and ability to generate high-quality annotations at scale (Wang et al., 2024b).
While our dataset is automatically generated using a VLM to minimize manual effort, we employ a
rigorous two-stage human verification pipeline to ensure data quality and mitigate hallucination. In
the first stage, three expert annotator independently evaluate whether each subquestion is logically
structured and consistent with natural human reasoning. QA pairs with incoherent or inconsistent


3


decompositions are moved out. In the second stage, annotators verify and, if necessary, relabel the
cognitive function associated with each subquestion to ensure accurate alignment with the intended
mental process. Subanswers are further cross-checked with GPT-o3 (OpenAI, 2024) and adjudicated
by humans in cases of disagreement (details in Appendix A.5). This multi-step verification ensures
that each retained subQAF triplet represents a coherent, interpretable reasoning step grounded in
core cognitive functions. After this refinement, our final dataset comprises 1,409 main QA pairs
and 5,744 validated subQAF triplets. The detailed statistics of the dataset including the number of
triplet in training and testing set and distributions of each function can be found in Appendix A.3.
We analyze in-group subquestion diversity, and the results in the Appendix A.2 show that the eight
functions exhibit broad and overlapping distributions in phrasing patterns and token lengths, indicating no systematic surface-form differences.


3 DETECTION OF COGNITIVE FUNCTIONS


Using the CogVision dataset, we adopt a probing-based framework (Alain & Bengio, 2016; Belinkov, 2022; Tenney et al., 2019) to identify which attention heads in VLMs are associated with
specific functions in reasoning process. Specifically, for each functional annotated subquestion, we
extract head activations (see Subsection 3.1), train classifiers and compute accuracies to identify
contributing heads (see Subsection 3.2). Unlike prior work focusing on a single label, our formulation captures many-to-many relationships between heads and cognitive functions, enabling a more
nuanced analysis of functional specialization and overlap within the model.


3.1 HEAD FEATURE EXTRACTION


Given a large VLM _M_, we generate an answer _a_ _[M]_ _i_ for each subquestion _qi_ derived from a main
question _Qi_ . To support coherent multi-step reasoning, we include preceding subquestions and
their answers as contextual input, emulating the incremental reasoning process observed in human
cognition.


During inference, each input token is first mapped into an embedding and then propagated through
the transformer’s layers. At each layer, attention and feedforward operations update the residual
stream, which is ultimately decoded into token predictions. For each generated token _i_, we extract
attention head outputs _Xi_ = _{x_ _[m]_ _l_ _| l ∈_ 1 _, . . ., Nl, m ∈_ 1 _, . . ., Nh}_ across all layers, where _x_ _[m]_ _l_
denotes the value vector from the _m_ -th head in layer _l_ projected into the residual stream, with _Nh_
the number of heads per layer and _Nl_ the total number of layers.


Let _Nt_ denote the number of tokens in the generated answer _a_ _[M]_ _i_ [. To isolate semantically informative]
content relevant to reasoning, we select the top- _k_ most important tokens, determined by prompting
Qwen3-30B LLM (Yang et al., 2025), yielding an index set _Ik_ with _|Ik|_ = _k_ . For each index
_j ∈Ik_, we extract the corresponding attention head activations _Xj_, and compute the averaged
activation feature for the _m_ -th head in layer _l_ as ¯ _x_ _[m]_ _l_ = _k_ [1] � _j ∈Ikxml_ [. This results in a full set of]

---

[Document 2]
Source: attention_functional_roles_raw_chunks_2k.jsonl
Chunk ID: attention_functional_roles_raw_chunks_2k_5
Content: pendix A.11 shows that masking the **math reasoning** heads leads to errors in arithmetic tasks, while
visual receptive functions remain largely unaffected. This confirms that these cognitive heads are
crucial for specific functions and highlights the robustness and generalizability of our method.


**Positive Intervention:** We calculate the activation directions of different functions using the CogVision dataset. For each function, the activation direction of a head at layer _l_ and index _h_ is computed

as:
dir _[h]_ _l_ [=][ E] _[i][∈D]_ correct � _x_ _[h]_ _l_ [(] _[i]_ [)] � _−_ E _i∈D_ incorrect � _x_ _[h]_ _l_ [(] _[i]_ [)] � (3)

where _x_ _[h]_ _l_ [(] _[i]_ [)][ denotes the activation of head at layer] _[ l]_ [ and index] _[ h]_ [, and] _[ D]_ [correct][ and] _[ D]_ [incorrect][ represent]
the sets of samples answered correctly and incorrectly, respectively. Then we estimate the standard
deviation of activations (Li et al., 2023a) along the cognitive function direction to be _σl_ _[h]_ [, and shift]
original head activation as _x_ _[h]_ _l_ [(] _[i]_ [)] _[ ←]_ _[x][h]_ _l_ [(] _[i]_ [) +] _[ ασ]_ _l_ _[h]_ [dir] _l_ _[h]_ [, where] _[ α]_ [ is a parameter.]


The experimental results in Table 5 show that enhancing the activation of functional heads along
their corresponding functional directions improves performance on the related tasks. For example,
positive intervention on vision knowledge recall heads in InternVL3-8B increased accuracy on the
corresponding CogVision question-answering task from 88.54% to 91.67%. Similarly, enhancing
function-specific heads can also boost performance on downstream tasks. Here, we set _α_ = 0 _._ 1
for all datasets, though tuning this parameter may further improve performance. Case analyses are
provided in Appendix A.11.


5 RELATED WORKS


**Neural Networks and the Brain.** Understanding the relationship between artificial neural networks (ANNs) and the biological brain has been a long-standing goal in both neuroscience and
machine learning. Early studies demonstrated that convolutional neural networks (CNNs) trained
on visual tasks develop hierarchical representations reminiscent of the ventral visual stream in primates (Yamins et al., 2014; Cadieu et al., 2014). Subsequent work extended this line of inquiry
to recurrent and transformer-based architectures, showing that attention mechanisms can emulate
aspects of selective processing observed in cortical circuits (Tsividis et al., 2017). More recently,
large language models (LLMs) have exhibited striking parallels with human brain activity during
language processing. In particular, transformer-based models such as GPT-2 produce internal representations that align with neural responses in language-selective brain regions (Caucheteux et al.,
2022; Schrimpf et al., 2021). Some works (Schulze Buschoff et al., 2025; Li et al., 2024) have
studied how VLMs perform differently from humans from a cognitive perspective. Furthermore, the


9



Table 4: Negative Intervention on Visual Question Answering task (OK-VQA and Clevr-Math). The scores
are based on LLM-Judge.








Table 5: Positive Intervention on CogVision test set and other Visual Question Answering benchmarks (OK-VQA, MathVista and Visulogic). The scores are based on LLM-Judge. For OK-VQA,
we perform positive intervention by masking 10% high-level visual reception head, MathVista for
math reasoning heads, and Visulogic for decision-making heads.

|In Domain Out of Domain<br>Model InterHead<br>Math Vision Recall Lang Recall Info Low-Level Inference High-Level Decision OK-VQA MathVista Visulogic|Col2|Col3|
|---|---|---|
|Qwen3B<br>before<br>after|46.40<br>82.29<br>81.37<br>48.03<br>78.26<br>65.87<br>77.38<br>29.73<br>46.40<br>**85.42**<br>**84.31**<br>44.74<br>78.26<br>**69.84**<br>**77.98**<br>**32.43**|62.50<br>60.00<br>26.50<br>62.00<br>60.00<br>**28.00**|
|Qwen7B<br>before<br>after|52.00<br>84.38<br>86.27<br>43.42<br>77.18<br>69.84<br>82.44<br>36.94<br>**52.80**<br>**85.42**<br>86.27<br>**46.05**<br>**82.61**<br>**74.60**<br>82.44<br>36.94|66.00<br>63.00<br>24.00<br>**67.50**<br>63.00<br>**24.50**|
|Intern2B<br>before<br>after|38.40<br>79.17<br>80.39<br>44.08<br>78.26<br>61.90<br>81.55<br>34.23<br>38.40<br>78.13<br>**84.31**<br>42.76<br>**80.43**<br>**64.29**<br>**82.44**<br>**35.14**|58.50<br>51.50<br>24.00<br>**61.50**<br>**52.00**<br>**26.00**|
|Intern8B<br>before<br>after|52.00<br>88.54<br>82.35<br>45.39<br>88.04<br>73.81<br>86.61<br>42.34<br>**52.80**<br>**91.67**<br>**84.31**<br>**46.71**<br>86.96<br>73.81<br>**87.80**<br>**43.24**|67.00<br>66.00<br>24.00<br>**67.50**<br>66.00<br>**26.00**|
|gemma2B<br>before<br>after|26.40<br>62.50<br>80.39<br>32.24<br>34.78<br>48.41<br>33.93<br>23.42<br>**28.00**<br>**64.58**<br>**84.31**<br>**38.16**<br>34.78<br>**50.00**<br>30.95<br>20.72|29.00<br>24.50<br>26.00<br>**29.50**<br>24.50<br>**29.00**|
|gemma4B<br>before<br>after|32.80<br>62.50<br>83.33<br>32.89<br>27.17<br>52.38<br>33.63<br>18.92<br>**35.20**<br>**65.63**<br>**85.29**<br>**35.53**<br>27.17<br>**53.97**<br>**33.63**<br>**19.82**|31.50<br>24.00<br>27.00<br>**33.00**<br>**26.50**<br>**27.50**|



chain-of-thought (CoT) paradigm has been argued to mirror step-by-step human reasoning, leading to improved problem-solving performance. These findings motivate the design of interpretable,
functionally specialized modules in artificial networks, bridging insights from neuroscience with
advances in multimodal reasoning.

---

[Document 3]
Source: attention_functional_roles_raw_chunks_2k.jsonl
Chunk ID: attention_functional_roles_raw_chunks_2k_0
Content: ## - INVESTIGATING THE FUNCTIONAL ROLES OF ATTEN ### TION HEADS IN VISION LANGUAGE MODELS: EVI DENCE FOR REASONING MODULES

**Yanbei Jiang** **[1]** _[∗]_ **Xueqi Ma** **[1]** _[∗]_ **Shu Liu** **[1]** **Sarah Monazam Erfani** **[1]**

**Tongliang Liu** **[2]** **James Bailey** **[1]** **Jey Han Lau** **[1]** **Krista A. Ehinger** **[1]**

1The University of Melbourne 2The University of Sydney
_{_ yanbeij, xueqim, shu6 _}_ @student.unimelb.edu.au
_{_ sarah.erfani, baileyj, jeyhan.lau, kris.ehinger _}_ @unimelb.edu.au
tongliang.liu@sydney.edu.au


ABSTRACT


Despite excelling on multimodal benchmarks, vision–language models (VLMs)
largely remain a black box. In this paper, we propose a novel interpretability framework to systematically analyze the internal mechanisms of VLMs, focusing on the functional roles of attention heads in multimodal reasoning. To
this end, we introduce CogVision, a dataset that decomposes complex multimodal questions into step-by-step subquestions designed to simulate human reasoning through a chain-of-thought paradigm, with each subquestion associated
with specific receptive or cognitive functions such as high-level visual reception and inference. Using a probing-based methodology, we identify attention
heads that specialize in these functions and characterize them as functional heads.
Our analysis across diverse VLM families reveals that these functional heads
are universally sparse, vary in number and distribution across functions, and
mediate interactions and hierarchical organization. Furthermore, intervention
experiments demonstrate their critical role in multimodal reasoning: removing
functional heads leads to performance degradation, while emphasizing them enhances accuracy. These findings provide new insights into the cognitive organization of VLMs and suggest promising directions for designing models with more
human-aligned perceptual and reasoning abilities. Code and data are available at
https://github.com/YanbeiJiang/CogVision.


1 INTRODUCTION


Large Vision-Language Models (VLMs) (Zhu et al., 2023; Liu et al., 2023; Lu et al., 2024a) have
demonstrated remarkable success across diverse multimodal tasks, ranging from image captioning
to visual question answering. Although VLMs can solve mathematical reasoning problems with
visual context (as shown in Fig. 1), their internal mechanisms remain poorly understood.


For humans, solving such complex problems (illustrated in Fig. 1) typically requires the collaboration of vision and language, engaging multiple brain regions (Barsalou, 2014): the occipital lobe
for visual reception, capturing and processing the content of the images; the temporal lobe supports
long-term memory and the recall of relevant factual knowledge, such as chemical concentration formulas (Wheeler et al., 1997); and the parietal and prefrontal cortices are involved in higher-order
reasoning (Hubbard et al., 2005), to produce the correct answer.


Recent research in interpretability has begun probing the internal organization of large language
models (LLMs), revealing specialized attention heads for specific functions (Wu et al.; Li et al.,
2023a; Zheng et al.). In the case of VLMs, several studies (Kang et al., 2025; Bi et al., 2025) have
identified sparse attention heads with special functional roles in tasks such as grounding. However,
studying VLMs in complex, multi-step reasoning scenarios remains underexplored. A deeper under

_∗_ Both authors contributed equally to this research.


1


![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-1-0.png)



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-1-1.png)

![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-1-2.png)



Decision making



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-1-3.png)

High-level Vision Reception



Language Knowledge Recall



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-1-5.png)



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-1-6.png)



Math Reasoning



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-1-7.png)



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-1-4.png)

Occipital lobe Temporal lobe Parietal lobe Frontal lobe









By dividing the number of Solution A has a higher

particles by the solvent concentration of A, Solution A.









![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-1-8.png)

![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-1-11.png)

Figure 1: To answer a complex question, the human brain engages multiple regions, each performing
distinct cognitive functions. We investigate whether specific attention heads in large vision language
models play analogous functional roles in generating responses.


standing of whether such specialized components exist, how they are organized, and what functional
roles they play in multimodal reasoning is therefore critical.


In this paper, we propose a novel interpretability framework for systematically analyzing the functional roles of attention heads-parallel units in transformer models that compute token-to-token
attention-an important component in VLMs, with a focus on their contributions to reception (perceptual processing) and cognition. To facilitate this, we introduce CogVision, a dataset that bridges the
gap between model analysis and human cognitive processes. CogVision decomposes multimodal
queries into step-by-step subquestions, each aligned with specific cognitive functions (such as math
reasoning, decision-masking), thus enabling a fine-grained evaluation of reasoning aligned with the
chain-of-thought (CoT) paradigm. Leveraging CogVision, we develop a probing method to identify and characterize attention heads responsible for distinct cognitive operations across vision and
language within the transformer architecture.


We conduct extensive experiments on three major VLM families, including Intern (Zhu et al., 2025),
Qwen (Yang et al., 2025), and Gemma (Team et al., 2025) with different model scales. Our results
reveal the existence of cognitive heads that consistently exhibit **universal**, **sparse**, and **intrinsic**
properties across architectures. Further analysis of the correlations among these functional heads
reveals **cross-function interactions**, where a single head may support multiple functions or modalities, and uncovers a **hierarchical structure** in which lower-level functional heads modulate higherlevel ones, showing the complexity of neural networks (Barsalou, 2014; Ono et al., 2022).


Furthermore, we validate the functional importance of these heads by showing that their removal
degrades performance on complex tasks and leads to specific error patterns, while their enhancement improves reasoning capabilities. Our findings provide compelling evidence that these attention
heads play a critical role in multimodal reasoning. This insight not only deepens our understanding of the internal organization of VLMs but also suggests potential avenues for designing more
interpretable and cognitive-inspired multimodal AI systems.


2 COGVISION


In this section, we present our dataset Cognitive Vision (CogVision) that contains cognitive process
in multimodal reasoning. CogVision contains 1,409 main questions and 5,744 subquestions. Each


2
","In the CogVision framework, the ""occipital lobe"" is connected to ""High-Level Visual Reception"" by drawing an analogy to human brain function.

Specifically, for humans, the ""occipital lobe"" is identified as the brain region responsible for ""visual reception, capturing and processing the content of the images."" The CogVision framework's cognitive functions, including ""High-level Visual Reception"" (which involves integrating visual information to recognize objects, patterns, and scene structure), are inspired by established frameworks in cognitive science and are designed to simulate human reasoning. Therefore, the occipital lobe represents the biological counterpart in humans for the visual reception functions that CogVision aims to model.",122,4855,4978,,3,,2026-01-02T20:14:34.015963
"How does the ""Adam optimizer"" entity function differently in the training regimes of the Transformer and NAC models?",Entity Relationship Reasoning,"[Document 1]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_13
Content: which depends on ( _ϕ, ωτ_ ) both through the equilibrium _a_ _[∗]_

and the exponential term.
**Derivative with respect to** _ϕ_ **:** We obtain



_Interpretation_ : The gradient with respect to _ωτ_ contains
a transient term proportional to _te_ _[−][ω][τ][ t]_, which dominates
at intermediate times, and a steady-state contribution proportional to _−ϕ/ωτ_ [2][, which persists asymptotically. Thus,]
sensitivity to _ωτ_ is time-dependent, peaking before vanishing exponentially in the transient component.


**C.2. Gradient-Based Training**


Like Neural ODEs (Chen et al., 2018) and CT-RNNs
(Rubanova et al., 2019), NAC produces differentiable computational graphs and can be trained using gradient-based
optimization, such as the adjoint sensitivity method (Cao
et al., 2003) or backpropagation through time (BPTT) (LeCun et al., 1988). In this work, we use BPTT exclusively,
as the adjoint sensitivity method can introduce numerical
errors (Zhuang et al., 2020).


**C.3. Efficiency and Complexity**


Table 3 summarizes the computational complexity of different sequence models. For sequence prediction over length
_n_ with hidden dimension _k_, RNNs scale linearly, _O_ ( _nk_ ),
while Attention and NAC scale quadratically, _O_ ( _n_ [2] _k_ ).
ODE-based models, such as LNNs, incur an additional
multiplicative factor _S_ for the number of solver steps.
For single-time-step prediction, RNNs and LSTMs require
_O_ ( _k_ ), whereas Attention and NAC require _O_ ( _nk_ ) when
recomputing attention over the full sequence.



_∂at_

_∂ϕ_ [= 1] _[ −]_ _ω_ _[e]_ _τ_ _[−][ω][τ][ t]_



_∂at_



(46)
_ωτ_



_Interpretation_ : For large _ωτ_, the gradient with respect to _ϕ_
saturates quickly but shrinks to scale _O_ (1 _/ωτ_ ), potentially
slowing learning of _ϕ_ . Conversely, very small _ωτ_ leads
to large steady-state gradients, which may destabilize optimization.

**Derivative with respect to** _ωτ_ **:** Here, both the equilibrium
and the decay rate depend on _ωτ_, yielding



_∂at_
= _−_ _[ϕ]_
_∂ωτ_ _ωτ_ [2]



�1 _−_ _e_ _[−][ω][τ][ t]_ [�] _−_ ( _a_ 0 _−_ _a_ _[∗]_ ) _t e_ _[−][ω][τ][ t]_ _._ (47)



13


**Neuronal Attention Circuit (NAC) for Representation Learning**



_Table 3._ Sequence and time-step prediction complexity. _n_ is the
sequence length and _k_ is the hidden/model dimension.


**Model** **Sequence** **Time-step**
RNN _O_ ( _nk_ ) _O_ ( _k_ )
Attention _O_ ( _n_ [2] _k_ ) _O_ ( _nk_ )
LNN (ODEsolve) _O_ ( _nk · S_ ) _O_ ( _k · S_ )
NAC-Exact _O_ ( _n_ [2] _k_ ) _O_ ( _nk_ )
NAC-Euler _O_ ( _n_ [2] _k_ ) _O_ ( _nk_ )


**D. Evaluation**


**D.1. Related Works**


The brief description for related works is divided into four
subcategories.
**DT-RNNs:** RNN (Rumelhart et al., 1985) captures sequential dependencies in time-series data by updating a hidden state from the current observation and the previous
state. LSTM (Hochreiter & Schmidhuber, 1997) extends
RNNs with input, output, and forget gates, allowing the
network to maintain and update long-term memory, which
improves modeling of long-term dependencies in time-series
sequences. GRU (Cho et al., 2014) simplifies the LSTM
architecture by combining the forget and input gates into a
single update gate, allowing efficient modeling of long-term
dependencies in time-series sequences.
**CT-RNNs:** CT-RNN (Rubanova et al., 2019) model temporal dynamics using differential equations, enabling hidden
states to evolve continuously over time in response to inputs, which is particularly useful for irregularly sampled
time-series data. PhasedLSTM (Neil et al., 2016) introduces a time gate that updates hidden states according to
a rhythmic schedule, enabling efficient modeling of asynchronous or irregularly sampled time-series. GRU-ODE
(De Brouwer et al., 2019) extends the GRU to continuous
time, evolving hidden states via ODEs to handle sequences
with non-uniform time intervals. mmRNN (Lechner &
Hasani, 2022) combines short-term and long-term memory units to capture both fast-changing and slowly evolving
patterns in sequential data. LTC (Hasani et al., 2021) use
neurons with learnable, input-dependent time constants to
adapt the speed of dynamics and capture complex temporal
patterns in continuous-time data. CfC (Hasani et al., 2022)
approximate LTC dynamics analytically, providing efficient
continuous-time modeling without relying on numerical
ODE solvers.

**DT-Attentions:** Attention (Vaswani et al., 2017) computes
attention weights by measuring similarity between queries
and keys, scaling the results, and applying softmax to weigh
time-step contributions. Multi-Head Attention (Vaswani
et al., 2017) applies multiple parallel scaled dot-product
attention mechanisms, capturing different types of temporal
dependencies simultaneously for complex time-series modeling.



**CT-Attentions:** mTAN (Shukla & Marlin, 2021) learns
continuous-time embeddings and uses time-based attention
to interpolate irregular observations into a fixed-length representation for downstream encoder-decoder modeling. CTA
(Chien & Chen, 2021) generalizes discrete-time attention
to continuous-time by representing hidden states, context
vectors, and attention scores as functions whose dynamics are modeled via neural networks and integrated using
ODE solvers for irregular sequences. ODEFormer(d’Ascoli
et al., 2023) trains a sequence-to-sequence transformer on
synthetic trajectories to directly output a symbolic ODE
system from noisy, irregular time-series data. ContiFormer
(Chen et al., 2023) builds a continuous-time Transformer by
pairing ODE-defined latent trajectories with a time-aware
attention mechanism to model dynamic relationships in irregular time-series data.


**D.2. Ablations Details**

---

[Document 2]
Source: attention_is_all_you_need_raw_chunks_2k.jsonl
Chunk ID: attention_is_all_you_need_raw_chunks_2k_3
Content: As side benefit, self-attention could yield more interpretable models. We inspect attention distributions
from our models and present and discuss examples in the appendix. Not only do individual attention
heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic
and semantic structure of the sentences.


**5** **Training**


This section describes the training regime for our models.


**5.1** **Training Data and Batching**


We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million
sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared sourcetarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT
2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece
vocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training
batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000
target tokens.


**5.2** **Hardware and Schedule**


We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using
the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We
trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the
bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps
(3.5 days).


**5.3** **Optimizer**


We used the Adam optimizer [20] with _β_ 1 = 0 _._ 9, _β_ 2 = 0 _._ 98 and _ϵ_ = 10 _[−]_ [9] . We varied the learning
rate over the course of training, according to the formula:


_lrate_ = _d_ _[−]_ model [0] _[.]_ [5] _[·]_ [ min(] _[step]_ [_] _[num][−]_ [0] _[.]_ [5] _[, step]_ [_] _[num][ ·][ warmup]_ [_] _[steps][−]_ [1] _[.]_ [5][)] (3)


This corresponds to increasing the learning rate linearly for the first _warmup_ _ _steps_ training steps,
and decreasing it thereafter proportionally to the inverse square root of the step number. We used
_warmup_ _ _steps_ = 4000.


**5.4** **Regularization**


We employ three types of regularization during training:


7


Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the
English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.


BLEU Training Cost (FLOPs)
Model

EN-DE EN-FR EN-DE EN-FR
ByteNet [18] 23.75
Deep-Att + PosUnk [39] 39.2 1 _._ 0 _·_ 10 [20]

GNMT + RL [38] 24.6 39.92 2 _._ 3 _·_ 10 [19] 1 _._ 4 _·_ 10 [20]

ConvS2S [9] 25.16 40.46 9 _._ 6 _·_ 10 [18] 1 _._ 5 _·_ 10 [20]

MoE [32] 26.03 40.56 2 _._ 0 _·_ 10 [19] 1 _._ 2 _·_ 10 [20]

Deep-Att + PosUnk Ensemble [39] 40.4 8 _._ 0 _·_ 10 [20]

GNMT + RL Ensemble [38] 26.30 41.16 1 _._ 8 _·_ 10 [20] 1 _._ 1 _·_ 10 [21]

ConvS2S Ensemble [9] 26.36 **41.29** 7 _._ 7 _·_ 10 [19] 1 _._ 2 _·_ 10 [21]

Transformer (base model) 27.3 38.1 **3** _**.**_ **3** _**·**_ **10** **[18]**

Transformer (big) **28.4** **41.8** 2 _._ 3 _·_ 10 [19]


**Residual Dropout** We apply dropout [33] to the output of each sub-layer, before it is added to the
sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the
positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of
_Pdrop_ = 0 _._ 1.


**Label Smoothing** During training, we employed label smoothing of value _ϵls_ = 0 _._ 1 [36]. This
hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.


**6** **Results**


**6.1** **Machine Translation**


On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)
in Table 2) outperforms the best previously reported models (including ensembles) by more than 2 _._ 0
BLEU, establishing a new state-of-the-art BLEU score of 28 _._ 4. The configuration of this model is
listed in the bottom line of Table 3. Training took 3 _._ 5 days on 8 P100 GPUs. Even our base model
surpasses all previously published models and ensembles, at a fraction of the training cost of any of
the competitive models.


On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41 _._ 0,
outperforming all of the previously published single models, at less than 1 _/_ 4 the training cost of the
previous state-of-the-art model. The Transformer (big) model trained for English-to-French used
dropout rate _Pdrop_ = 0 _._ 1, instead of 0 _._ 3.


For the base models, we used a single model obtained by averaging the last 5 checkpoints, which
were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We
used beam search with a beam size of 4 and length penalty _α_ = 0 _._ 6 [38]. These hyperparameters
were chosen after experimentation on the development set. We set the maximum output length during
inference to input length + 50, but terminate early when possible [38].


Table 2 summarizes our results and compares our translation quality and training costs to other model
architectures from the literature. We estimate the number of floating point operations used to train a
model by multiplying the training time, the number of GPUs used, and an estimate of the sustained
single-precision floating-point capacity of each GPU [5] .


**6.2** **Model Variations**


To evaluate the importance of different components of the Transformer, we varied our base model
in different ways, measuring the change in performance on English-to-German translation on the


5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.


8


Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base
model. All metrics are on the English-to-German translation development set, newstest2013. Listed
perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to
per-word perplexities.

---

[Document 3]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_12
Content: _a_ ( _t_ + ∆ _t_ ) = _at_ + ∆ _t ·_ _[da]_ (36)

_dt_ _[.]_



At _a_ = _M_, _[da]_ _dt_ _[≤]_ [0 =] _[⇒]_ _[a]_ [(] _[t]_ [ + ∆] _[t]_ [)] _[ ≤]_ _[M]_ [. At] _[ a]_ [ =] _[ m]_ [,]

_dadt_ _[≥]_ [0 =] _[⇒]_ _[a]_ [(] _[t]_ [ + ∆] _[t]_ [)] _[ ≥]_ _[m]_ [. By induction over steps,]

_at ∈_ [ _m, M_ ] for all _t ∈_ [0 _, T_ ].



12


**Neuronal Attention Circuit (NAC) for Representation Learning**



multi-head, scale _H_ proportionally to target complexity,
with output projection _Wo_ aggregating as in classical UAT
proofs (Stinchcomb, 1989).
**Input Projections:** The input _x_ is projected via NCPbased sensory projections to obtain query _q_ = _q_ proj( _x_ ), key
_k_ = _k_ proj( _x_ ), and value _v_ = _v_ proj( _x_ ), each in R _[d]_ [model] . For
emulation, set _q_ proj = _k_ proj = _In_ (identity on R _[n]_ ) and adjust


**Head Splitting and Sparse Top-** _**k**_ **Pairwise Computation:**
Split into _H_ heads, yielding _q_ [(] _[h]_ [)] _, k_ [(] _[h]_ [)] _∈_ R _[d]_ per head _h_,
where _d_ = _d_ model _/H_ . For _T_ = 1, compute sparse top- _k_
pairs, but since _T_ = 1, _K_ eff = 1, yielding concatenated
pair _u_ [(] _[h]_ [)] = [ _q_ [(] _[h]_ [)] ; _k_ [(] _[h]_ [)] ] _∈_ R [2] _[d]_ . Since _q_ [(] _[h]_ [)] = _k_ [(] _[h]_ [)], this is

[ _x_ [(] _[h]_ [)] ; _x_ [(] _[h]_ [)] ], but the NCP processes it generally.
**Computation of** _ϕ_ [(] _[h]_ [)] **and** _ωτ_ [(] _[h]_ [)] **:** The scalar _ϕ_ [(] _[h]_ [)] is computed via the NCP-based inter-to-motor projection on the
pair:
_ϕ_ [(] _[h]_ [)] = _σ_ ( _NN_ backbone( _u_ [(] _[h]_ [)] )) (42)


where _σ_ ( _z_ ) = (1 + _e_ _[−][z]_ ) _[−]_ [1] is the sigmoid. This NCP, with
sufficiently large units and low sparsity, approximates any
continuous scalar function _ϕ_ [˜] : R [2] _[d]_ _→_ [0 _,_ 1] to arbitrary precision on compact sets (by the UAT for multi-layer networks
(Stinchcomb, 1989)). Similarly, _ωτ_ [(] _[h]_ [)] is computed via:


_ωτ_ [(] _[h]_ [)] = softplus( _NN_ backbone( _u_ [(] _[h]_ [)] )) + _ε,_ _ε >_ 0 (43)


By setting weights to make _ωτ_ [(] _[h]_ [)] _≡_ 1 (constant), the steadymode logit simplifies to _a_ [(] _[h]_ [)] = _ϕ_ [(] _[h]_ [)] _/ωτ_ [(] _[h]_ [)] = _ϕ_ [(] _[h]_ [)] . Thus,
_a_ [(] _[h]_ [)] _≈_ _σ_ � _w_ [(] _[h]_ [)] _x_ + _b_ [(] _[h]_ [)][�] for chosen weights _w_ [(] _[h]_ [)] _, b_ [(] _[h]_ [)], emulating a sigmoid hidden unit.
**Attention Weights and output:** For _T_ = 1, the softmax
over one “key” yields _α_ [(] _[h]_ [)] = exp( _a_ [(] _[h]_ [)] ) _/_ exp( _a_ [(] _[h]_ [)] ) = 1.
The head output is _y_ [(] _[h]_ [)] = � _T_ _[α]_ [(] _[h]_ [)] _[v]_ [(] _[h]_ [)] _[dt]_ [. Set] _[ v]_ [proj][ such that]

_v_ [(] _[h]_ [)] = 1 (scalar), yielding _y_ [(] _[h]_ [)] _≈_ _σ_ � _w_ [(] _[h]_ [)] _x_ + _b_ [(] _[h]_ [)][�] . For
vector-valued _v_ [(] _[h]_ [)], more complex combinations are possible, but scalars suffice here.
**Output Projection:** Concatenate head outputs: _Y_ =

[ _y_ [(1)] ; _y_ [(2)] ; _. . ._ ; _y_ [(] _[H]_ [)] ] _∈_ R _[H]_ . Apply the final dense layer:


_g_ ( _x_ ) = ( _Y · Wo_ ) + _bo ∈_ R _[m]_ _._ (44)


With _y_ [(] _[h]_ [)] _≈_ _σ_ � _w_ [(] _[h]_ [)] _x_ + _b_ [(] _[h]_ [)][�], this matches a single-hiddenlayer network with _H_ units. By the UAT, for large _H_, such
networks approximate any continuous _f_ on compact _K_ to
accuracy _ϵ_, by choosing appropriate _w_ [(] _[h]_ [)] _, b_ [(] _[h]_ [)] _, Wo, bo_ .


**C. Training, Gradients and Complexity**


**C.1. Gradient Characterization**


We analyze the sensitivity of the dynamics with respect to
the underlying learnable parameters. Specifically, we compute closed-form derivatives of both the steady state and the
full trajectory _at_ with respect to the parameters _ϕ_ and _ωτ_ .
These expressions illuminate how gradients flow through



the system, and provide guidance for selecting parameterizations that avoid vanishing or exploding gradients.


C.1.1. TRAJECTORY SENSITIVITIES FOR CLOSED-FORM


FORMULATION


The trajectory is given by


_at_ = _a_ _[∗]_ + ( _a_ 0 _−_ _a_ _[∗]_ ) _e_ _[−][ω][τ][ t]_ _,_ (45)


which depends on ( _ϕ, ωτ_ ) both through the equilibrium _a_ _[∗]_

and the exponential term.
**Derivative with respect to** _ϕ_ **:** We obtain



_Interpretation_ : The gradient with respect to _ωτ_ contains
a transient term proportional to _te_ _[−][ω][τ][ t]_, which dominates
at intermediate times, and a steady-state contribution proportional to _−ϕ/ωτ_ [2][, which persists asymptotically. Thus,]
sensitivity to _ωτ_ is time-dependent, peaking before vanishing exponentially in the transient component.


**C.2. Gradient-Based Training**
","The provided context states that the Transformer model uses the Adam optimizer with specific hyperparameters (_β_1 = 0.9, _β_2 = 0.98, _ϵ_ = 10_[-9]_) and a custom learning rate schedule. This schedule involves a linear increase for the first 4000 _warmup_ _ _steps_ and then a decrease proportional to the inverse square root of the step number.

For the NAC model, the context mentions that it can be trained using gradient-based optimization, such as the adjoint sensitivity method or backpropagation through time (BPTT), and that BPTT is used exclusively in that work. However, the context does not specify whether the Adam optimizer is used for NAC, nor does it detail any specific functioning of Adam in the NAC training regime.

Therefore, the context only provides details on how the Adam optimizer functions for the Transformer, but not for the NAC model.",134,4974,5109,,3,,2026-01-02T20:14:39.127204
"What is the relationship between ""LongBench-V2"" and ""LongMemEval"" in the evaluation of SWA adaptation strategies?",Entity Relationship Reasoning,"[Document 1]
Source: sliding_window_attention_annotated_chunks_2k.jsonl
Chunk ID: sliding_window_attention_annotated_chunks_2k_8
Content: **C** **Results of Other Models**


We show the results of Qwen3-30B-A3BThinking and Qwen3-30B-A3B-Instruct on LongMemEval_24k in Table 4, and the results of

Llama3.1-8B-Instruct in Table 5. The scores of

Qwen3-30B-A3B are generally higher and those
of Llama3.1 are generally lower, but all results are
consistent with our previous conclusions, demonstrating their generalizability.
Due to the time-intensive nature of training, we
only test a small set of configurations with finetuning.


**D** **Results of LongBench V2**


We present the results of LongBench V2 (Bai et al.,

2024b) in Tables 6, 7 and 8. We retain only the
samples whose context length is under 128k due to
GPU memory limitations; thus, 384 of 500 samples
are kept. However, due to the high difficulty, the
performance is generally poor. Some scores are
even below the random guessing baseline (25%).
For Qwen3-4B and Qwen3-30B-A3B models,

the results show less noticeable differences be
tween various methods. But fortunately, the trend
of accuracy changes is generally consistent with
that of other datasets, so they do not conflict with
all of our previous conclusions. For Llama3.1, due
to its weaker long-context capability, accuracy consistently hovers around 30%.


**E** **Inference Efficiency**


The TTFT, TPOT and total throughput when using vLLM are shown in Table 9. Since inference
speed is highly dependent on hardware, implementation details, and workload characteristics, these
numbers should be interpreted as reference values.
However, from the results, we can still conclude

that:


1. Interleaving Layers and FA Decode significantly slow down the speed compared to pure



11


SWA.


2. Keep First _k_ Tokens has a negligible impact
on efficiency.


3. Increasing the window size slightly increases
inference time. For example, increasing from
2k to 4k decreases throughput by only 10%,
but a 4k window generally achieves higher accuracy based on previous experiments. Therefore, in practice, a 4k window is a more common choice.


In theory, FA Decode should yield a decoding
speed identical to that of full attention. Yet, in this
table, we observe acceleration on TPOT. This is because vLLM-v1 typically mixes different requests’
prefilling and decoding tokens in one sequence
to improve GPU utilization. Thus, the speeds of
prefilling and decoding may affect each other. If
processing only a single request, the situation differs. For example, when the generation length is
set to 2000, we find decoding takes over 95% of the
total time, rendering the acceleration of the prefilling stage negligible—i.e., SWA with FA Decode is
almost unable to improve efficiency in such cases.


**F** **Influence of Training Epochs**


As shown in Table 10, training for more than
1 epoch yields no improvement. Therefore, we
choose to train for only 1 epoch.


**G** **Results of LightTransfer**


LightTransfer (Zhang et al., 2024) represents a
promising attempt at SWA adaptation on fullattention models without pretraining. It proposes
a layer selection method for SWA adaptation that
calculates a ""lazy ratio,"" represented by the ratio
of attention from tokens at the end of the sequence
(from a calibration dataset) to recent tokens versus
global tokens. Layers with a higher ""lazy ratio"" are
selected to apply SWA, while the rest retain full
attention. This method is intuitive and theoretically
sound, but our experiments reveal some negative
results.

Since the complete code of LightTransfer is not
open-source, we reproduce this method using LongAlign (Bai et al., 2024a) as the calibration dataset
for lazy layer detection, where the number of last
tokens is set to 64, and the recent token window is
set to 1024. From our experimental results shown
in Table 11, we find that:



1. For Qwen3-4B, LightTransfer even has a
counterproductive effect; allowing lazy layers to use FA yields higher scores, while following the original method (letting non-lazy
layers use FA) results in significantly lower

scores.


2. For Qwen3-30B, it provides nearly no improvement over fixed-interval selection.


3. Only for Llama3.1-8B does LightTransfer
show advantages.


Therefore, we conclude that LightTransfer does
not yield stable performance across various models.
Although fine-grained layer selection methods are
theoretically superior, we believe they require further investigation before integration into our SWAA
recipes.



12


Table 4: Results of Qwen3-30B-A3B-Thinking and Qwen3-30B-A3B-Instruct on LongMemEval


**No.** **SFT** **window size** **FA layers** **keep first** **FA decode** **Acc think** **Acc non-think**


0 False Full [] 0 False **79.2** **71.6**
1 False 2k [] 0 False 0.0 0.4
2 False 8k [] 0 False 0.0 0.2


3 False 2k [] 10 False 0.0 2.8
4 False 2k [] 0 True 0.2 0.2
5 False 2k [0, 2, 4, ...] 0 False 21.0 28.4


6 False 2k [] 10 True 43.8 23.6
7 False 2k [] 100 True 58.6 22.2
8 False 2k [] 1000 True 59.0 25.4
9 False 4k [] 10 True 49.8 26.6


10 False 2k [0, 2, 4, ...] 10 True **74.8** **63.0**
11 False 2k [1, 3, 5, ...] 10 True 51.6 24.0
12 False 2k [0, 4, 8, ...] 10 True 48.8 23.8
13 False 2k [2, 6, 10, ...] 10 True 64.8 44.2
14 False 4k [0, 2, 4, ...] 100 True **74.6** **64.4**


15 True _\_ [] 0 False **79.6** _\_


16 True 2k [] 0 True 62.2 51.0


17 True 2k [] 100 True 65.6 50.8
18 True 2k [0, 2, 4, ...] 0 True **72.6** _\_


19 True 2k [0, 2, 4, ...] 100 True **77.8** **68.0**


Table 5: Results of Llama3.1-8B-Instruct on LongMemEval


**No.** **SFT** **window size** **FA layers** **keep first** **FA decode** **Acc non-think**


0 False Full [] 0 False **61.0**
1 False 2k [] 0 False 0.6
2 False 8k [] 0 False 1.2


3 False 2k [] 10 False 1.8
4 False 2k [] 0 True 0.0
5 False 2k [0, 2, 4, ...] 0 False 3.0


6 False 2k [] 10 True 16.8
7 False 2k [] 100 True 20.0
8 False 2k [] 1k True 24.2
9 False 4k [] 10 True 23.8


10 False 2k [0, 2, 4, ...] 10 True **42.6**
11 False 2k [1, 3, 5, ...] 10 True 21.0
12 False 2k [0, 4, 8, ...] 10 True 17.8
13 False 2k [2, 6, 10, ...] 10 True 24.4
14 False 4k [0, 2, 4, ...] 100 True **44.0**


13


Table 6: Qwen3-4B-Thinking and Qwen3-4B-Instruct results on LongBench-V2


**No.** **SFT** **window size** **FA layers** **keep first** **FA decode** **Acc think** **Acc non-think**


0 False Full [] 0 False **34.6** **35.2**
1 False 2k [] 0 False 9.4 25.8
2 False 8k [] 0 False 15.1 22.1

---

[Document 2]
Source: sliding_window_attention_annotated_chunks_2k.jsonl
Chunk ID: sliding_window_attention_annotated_chunks_2k_2
Content: **3.4** **Fine-tuning**


This is the most natural way to mitigate traininginference mismatch. Apparently, the model should
be fine-tuned while SWA is applied, so that the
model’s parameters can be trained to better adapt
to SWA. Meanwhile, the training data must be longcontext tasks, where SWA actually works.
However, most available long-context datasets
only contain brief ground-truth answers, lacking the reasoning steps required for fine-tuning
a ""thinking"" model. Since our goal is to _restore_
the model’s original capabilities under SWA rather
than teach it new ones, instead of directly using the
original dataset, we adopt an approach similar to
self-distillation (Yang et al., 2024). Specifically, we
utilize the original full-attention model to generate
new answers for the dataset’s questions, and these
generated answers are then filtered for correctness
using GPT-5-Mini (OpenAI, 2025), to make up our
training dataset. For each question, we sample 4
answers with temperature 1, because we find this
strategy is slightly better than generating only one
answer with temperature 0.


**4** **Experiment Setup**


We organize our experiments around three research
questions:


**RQ1: Is SWA adaptation feasible without any**
**additional training?** We evaluate whether
an FA LLM can be adapted to SWA using
only inference-time modifications, and which
combinations of techniques are necessary.


**RQ2: How much does fine-tuning with SWA im-**
**prove performance?** We study the effect of
SWA-aware fine-tuning on long-context performance and identify which components of
SWAA are still required.



**RQ3: Which SWAA configurations achieves the**
**optimal performance-efficiency trade-offs?**
We evaluate how different SWAA configurations trade off accuracy against inference la
tency.


**4.1** **Models**


Our primary experiments use Qwen3-4B-Thinking
and Qwen3-4B-Instruct (Team, 2025b). The Thinking variant enforces chain-of-thought (CoT) style
reasoning, whereas the Instruct variant usually just
answers briefly. To ensure generality, we additionally evaluate Qwen3-30B-A3B-Thinking, Qwen330B-A3B-Instruct (Team, 2025b), and Llama3.18B-Instruct (Touvron et al., 2023).

All models are served with vLLM in float16

precision using a batch size of 64. We use greedy
decoding (temperature = 0) for all evaluations. In
preliminary experiments, we observed that vLLM
yields slightly lower (about 1% to 5%) scores
than HuggingFace Transformers due to precisionrelated discrepancies.


**4.2** **Evaluation Dataset**


SWA is identical to full attention when the context

length is within the window size. Even if the model
is fine-tuned, we can pre-calculate the prompt
length and simply disable the LoRA adapters for
short prompts to get completely the same response
as the original model. Therefore, our experiments
focus exclusively on long-context benchmarks with
inputs exceeding 16k tokens, as re-evaluating models on standard short-context benchmarks (e.g.,
MMLU (Hendrycks et al., 2021), GPQA (Rein
et al., 2023)) is completely unnecessary.
Since we find other long-context benchmarks are
either too easy or too difficult for 4B-level models (see Appendix B), we ultimately select LongMemEval (Wu et al., 2024), a benchmark consisting of various types of long-context QA tasks with
moderate difficulty, although it is originally designed for agent memory system evaluation. Its
context length is controllable by selecting a specific number of chat sessions to concatenate as the
context from a pool of hundreds of sessions (a session contains the chat history between user and
assistant within a day). To create a moderately difficult and discriminative evaluation, we construct
**LongMemEval_24k** by sampling 10 sessions, resulting in 500 samples ranging from 16k to 32k
with an average context length of 24k.



4


For additional validation of generalizability, as
shown in Appendix D, we also experiment on
LongBench-V2 (Bai et al., 2024b), a more modern and challenging benchmark that requires deep
reasoning across various real-world tasks.


**4.3** **Training Details**


For the fine-tuning dataset, we initially considered LongAlign (Bai et al., 2024a), a widely used
long-context fine-tuning dataset for adapt a regularlength model to long-context tasks. However, since

_∼_
its sample count ( 10,000) is insufficient, we incorporate an additional 6,000 samples from Fusangv1-long (Pan, 2024), a more comprehensive corpus
of over 40,000 long-context samples that includes
LongAlign as a subset.
We perform SWA-aware fine-tuning using LoRA
(Hu et al., 2022). Unless otherwise noted, we use
rank _r_ = 16 and _α_ = 128, and apply LoRA only
to the query, key, and value projection modules.
We adopt this parameter-efficient setting because
full-parameter fine-tuning often leads to overfitting
and degradation of the model’s original capabilities
in our preliminary experiments. We use a learning
rate of 1e-4 with a cosine decay schedule. Models
are fine-tuned for a single epoch on the sampled
long-context dataset since we observe no meaningful gains from additional epochs (see Appendix F).
Once training takes approximately 12 hours on an
8*H20 GPU server for Qwen3-4B and 30 hours for
Qwen3-30B-A3B.


**5** **Experiment Results**


**5.1** **SWA Adaptation Without Fine-tuning**


We first study SWA adaptation without any additional training. Table 1 reports LongMemEval_24k
accuracy for Qwen3-4B-Thinking (""think"") and
Qwen3-4B-Instruct (""non-think"") under different
combinations of SWAA components. In most settings, we use an aggressive 2k window to amplify
the impact of SWA. The configurations are ranked
by the number of methods applied (0, 1, 2, or 3 of
Interleaving Layers, Keep First and FA Decode).
Rows 1 (original model) and 2 (naive SWA) serve
as upper and lower baselines, respectively. In the
column ""FA layers"", the value records which layers use full attention, and [] means all the layers
use SWA, i.e., this method is not enabled. In the
column ""keep first"", the value is _k_ in Keep First
_k_ Tokens. When comparing results, an accuracy
difference of less than 5% is usually considered



statistically insignificant. From the results, we find
that:


**Naive SWA is not viable.** Naively replacing FA
with a 2k sliding window attention (row 1) drops
accuracy significantly to 3 _._ 2 and 11 _._ 0, respectively.
Even with an 8k window (row 2), accuracy only recovers to 13 _._ 2 and 19 _._ 8, far below the FA baseline.


**Single method helps, but cannot close the gap.**
Each method—Keep First, FA Decode, or Interleaving Layers—improves over naive SWA (rows 3–6),
yet each alone recovers only a small fraction of the
FA gap and remains well below the baseline. In
short, no single method is sufficient.


**Combinations exhibit strong synergy.** Recipes
that combine multiple methods deliver large gains:


  - **FA Decode + Keep First** _k_ substantially improves over naive SWA (rows 7–9), recovering roughly half to two-thirds of the gap on
the thinking model as _k_ increases. However,
increasing _k_ from 100 to 1000 yields almost
no improvement, indicating that _k_ does not
need to be exceedingly large.


  - **Interleaving Layers + FA Decode** is
markedly stronger (row 13), recovering most
of the gap for the thinking model.


  - **FA Decode + Interleaving Layers + Keep**
**First** _k_ perform best (rows 18). The thinking
model recovers close to 90% of the FA gap
even at 2k window.

---

[Document 3]
Source: sliding_window_attention_annotated_chunks_2k.jsonl
Chunk ID: sliding_window_attention_annotated_chunks_2k_7
Content: Di Wu, Hongwei Wang, Wenhao Yu, Yuwei Zhang,
[Kai-Wei Chang, and Dong Yu. 2024. Longmemeval:](https://arxiv.org/abs/2410.10813)
[Benchmarking chat assistants on long-term interac-](https://arxiv.org/abs/2410.10813)
[tive memory.](https://arxiv.org/abs/2410.10813)



Guangxuan Xiao. 2025. Why stacking sliding win[dows can’t see very far. https://guangxuanx.com/](https://guangxuanx.com/blog/stacking-swa.html)
[blog/stacking-swa.html.](https://guangxuanx.com/blog/stacking-swa.html)


Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song
[Han, and Mike Lewis. 2024. Efficient streaming lan-](https://openreview.net/forum?id=NG7sS51zVF)
[guage models with attention sinks. In](https://openreview.net/forum?id=NG7sS51zVF) _The Twelfth_
_International Conference on Learning Representa-_
_tions, ICLR 2024, Vienna, Austria, May 7-11, 2024_ .
OpenReview.net.


Zhaorui Yang, Tianyu Pang, Haozhe Feng, Han Wang,
[Wei Chen, Minfeng Zhu, and Qian Liu. 2024. Self-](https://arxiv.org/abs/2402.13669)
[distillation bridges distribution gap in language](https://arxiv.org/abs/2402.13669)
[model fine-tuning.](https://arxiv.org/abs/2402.13669)


Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo,
Liang Zhao, Zhengyan Zhang, Zhenda Xie, Yuxing
Wei, Lean Wang, Zhiping Xiao, Yuqing Wang, Chong
Ruan, Ming Zhang, Wenfeng Liang, and Wangding
Zeng. 2025. [Native sparse attention: Hardware-](https://doi.org/10.18653/v1/2025.acl-long.1126)
[aligned and natively trainable sparse attention. In](https://doi.org/10.18653/v1/2025.acl-long.1126)
_Proceedings of the 63rd Annual Meeting of the As-_
_sociation for Computational Linguistics (Volume 1:_
_Long Papers)_, pages 23078–23097, Vienna, Austria.
Association for Computational Linguistics.


Manzil Zaheer, Guru Guruganesh, Kumar Avinava
Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontañón, Philip Pham, Anirudh Ravula, Qifan Wang,
[Li Yang, and Amr Ahmed. 2020. Big bird: Trans-](https://proceedings.neurips.cc/paper/2020/hash/c8512d142a2d849725f31a9a7a361ab9-Abstract.html)
[formers for longer sequences. In](https://proceedings.neurips.cc/paper/2020/hash/c8512d142a2d849725f31a9a7a361ab9-Abstract.html) _Advances in Neural_
_Information Processing Systems 33: Annual Confer-_
_ence on Neural Information Processing Systems 2020,_
_NeurIPS 2020, December 6-12, 2020, virtual_ .


Xuan Zhang, Fengzhuo Zhang, Cunxiao Du, Chao Du,
[Tianyu Pang, Wei Gao, and Min Lin. 2024. Light-](https://arxiv.org/abs/2410.13846)
[transfer: Your long-context llm is secretly a hybrid](https://arxiv.org/abs/2410.13846)
[model with effortless adaptation.](https://arxiv.org/abs/2410.13846)


**A** **SWA’s Benefits and Each Method’s**
**Drawbacks**


SWA reduces the computational complexity to
_O_ ( _N · W_ ), where _W_ is the window size. The
benefits are threefold: (1) SWA reduces the computational load, (2) conserves GPU memory by limiting the required Key-Value (KV) cache, and (3)
enhances KV cache reusability beyond traditional
prefix caching, since a token’s state is independent
of tokens outside its local window. However, there

is no free lunch—each method of SWAA has some

drawbacks, impairing the benefits brought by SWA
to varying degrees.
**FA Decode** presents two primary drawbacks:
(1) the benefits apply only to prefilling, while decoding speed is not accelerated as it utilizes full
attention, and (2) the GPU memory required for
the KV cache is not reduced, as the KV cache for
the full context must be retained for decoding. In



10


practice, however, many distributed LLM services
have to recompute the KV cache of the entire chat
history because storing and loading the KV cache
complicates engineering systems, making prefilling occurs more frequently than expected, thereby
amplifying the advantage of this method.
**Keep First** introduces very minor computational
overhead, but it complicates efficient KV cache
reuse. Due to positional encoding, a token’s KV
state depends on its position relative to the initial
_k_ tokens, hindering simple cache reuse across different requests. A position encoding separation or
offsetting mechanism may be needed.
**Interleaving Layers** introduces the most significant overhead, as only a subset of layers benefits from the computational savings of SWA. Furthermore, the GPU memory required for the KV
cache is not reduced for the full-attention layers.
Additionally, this method negates the KV cache
reusability advantage of SWA, as the existence of
full-attention layers violates the independence of
the KV cache beyond the local window.
**CoT** will greatly increase the generation length,
especially for difficult tasks. So the decoding time
will be much longer.


**B** **Other Long-context Benchmarks**


We find existing long-context benchmarks problematic for our specific needs. For example:


1. LongBench (Bai et al., 2023) is classic and
widely used, but its average context length
(most are under 16k) is relatively short for
modern models, i.e., it is already too easy.
And its data source is too old, leading to a risk
of test data leakage.


2. Ruler(Hsieh et al., 2024) has controllable context length, but its tasks are almost all synthetic and most of them are needle-retrieval

tasks, failing to reflect the model’s overall
long-context capability in real-world scenarios.


3. LongBench-V2 (Bai et al., 2024b) is welldesigned to necessitate deep understanding
and reasoning over very long context. But it
is too challenging for 4B-level models (e.g.,
Qwen3-4B-Thinking only gets 35% accuracy,
which is too close to the random guessing
baseline of 25%), making the improvement of
different methods less distinguishable. Moreover, since it is in a multiple-choice question



format, the results may not be sufficiently reliable because the model has a 25% chance of

guessing the correct option.


However, despite the extreme difficulty of
LongBench-V2 (Bai et al., 2024b), it remains
a high-quality long-context benchmark after
all. Thus we still elect to conduct our experiments on it to verify the generalizability of
our conclusions, as shown in Appendix D.


**C** **Results of Other Models**


We show the results of Qwen3-30B-A3BThinking and Qwen3-30B-A3B-Instruct on LongMemEval_24k in Table 4, and the results of

Llama3.1-8B-Instruct in Table 5. The scores of

Qwen3-30B-A3B are generally higher and those
of Llama3.1 are generally lower, but all results are
consistent with our previous conclusions, demonstrating their generalizability.
Due to the time-intensive nature of training, we
only test a small set of configurations with finetuning.


**D** **Results of LongBench V2**
","LongMemEval and LongBench-V2 are both long-context benchmarks used to evaluate SWA adaptation strategies, but they serve different roles in the evaluation:

*   **LongMemEval** (specifically LongMemEval_24k) was selected as the **primary evaluation dataset** for the experiments. It consists of various types of long-context QA tasks with ""moderate difficulty"" and was constructed to be ""moderately difficult and discriminative"" for the models being tested (e.g., 4B-level models). The main results and conclusions regarding SWA adaptation without fine-tuning (Table 1) and other model results (Tables 4 and 5) are reported using LongMemEval_24k.

*   **LongBench-V2** is a ""more modern and challenging benchmark"" that requires ""deep reasoning across various real-world tasks."" It was used for **additional validation of generalizability** of the conclusions drawn from LongMemEval. While it is a high-quality benchmark, it was found to be ""too challenging for 4B-level models,"" resulting in generally poor performance and less noticeable differences between various SWA methods (Tables 6, 7, and 8). However, the trend of accuracy changes on LongBench-V2 was generally consistent with that of other datasets, supporting the generalizability of the findings.",124,6709,6834,,3,,2026-01-02T20:14:45.963321
"How does the ""Multi-Head Attention"" entity relate to the ""sub-layers"" entity in the Transformer architecture?",Entity Relationship Reasoning,"[Document 1]
Source: attention_is_all_you_need_raw_chunks_2k.jsonl
Chunk ID: attention_is_all_you_need_raw_chunks_2k_1
Content: [10], consuming the previously generated symbols as additional input when generating the next.


2


![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_is_all_you_need.pdf-2-0.png)

Figure 1: The Transformer - model architecture.


The Transformer follows this overall architecture using stacked self-attention and point-wise, fully
connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,
respectively.


**3.1** **Encoder and Decoder Stacks**


**Encoder:** The encoder is composed of a stack of _N_ = 6 identical layers. Each layer has two
sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network. We employ a residual connection [11] around each of
the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is
LayerNorm( _x_ + Sublayer( _x_ )), where Sublayer( _x_ ) is the function implemented by the sub-layer
itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding
layers, produce outputs of dimension _d_ model = 512.


**Decoder:** The decoder is also composed of a stack of _N_ = 6 identical layers. In addition to the two
sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head
attention over the output of the encoder stack. Similar to the encoder, we employ residual connections
around each of the sub-layers, followed by layer normalization. We also modify the self-attention
sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This
masking, combined with fact that the output embeddings are offset by one position, ensures that the
predictions for position _i_ can depend only on the known outputs at positions less than _i_ .


**3.2** **Attention**


An attention function can be described as mapping a query and a set of key-value pairs to an output,
where the query, keys, values, and output are all vectors. The output is computed as a weighted sum


3


Scaled Dot-Product Attention Multi-Head Attention


Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several
attention layers running in parallel.


of the values, where the weight assigned to each value is computed by a compatibility function of the
query with the corresponding key.


**3.2.1** **Scaled Dot-Product Attention**


We call our particular attention ""Scaled Dot-Product Attention"" (Figure 2). The input consists of
queries and keys of dimension _dk_, and values of dimension _dv_ . We compute the dot products of the
query with all keys, divide each by _[√]_ _dk_, and apply a softmax function to obtain the weights on the
values.


In practice, we compute the attention function on a set of queries simultaneously, packed together
into a matrix _Q_ . The keys and values are also packed together into matrices _K_ and _V_ . We compute
the matrix of outputs as:


Attention( _Q, K, V_ ) = softmax( _[Q][K]_ _[T]_ ) _V_ (1)

~~_√_~~ _dk_


The two most commonly used attention functions are additive attention [2], and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor
of ~~_√_~~ 1 _dk_ . Additive attention computes the compatibility function using a feed-forward network with
a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is
much faster and more space-efficient in practice, since it can be implemented using highly optimized
matrix multiplication code.


While for small values of _dk_ the two mechanisms perform similarly, additive attention outperforms
dot product attention without scaling for larger values of _dk_ [3]. We suspect that for large values of
_dk_, the dot products grow large in magnitude, pushing the softmax function into regions where it has
extremely small gradients [4] . To counteract this effect, we scale the dot products by ~~_√_~~ 1 _dk_ .


**3.2.2** **Multi-Head Attention**


Instead of performing a single attention function with _d_ model-dimensional keys, values and queries,
we found it beneficial to linearly project the queries, keys and values _h_ times with different, learned
linear projections to _dk_, _dk_ and _dv_ dimensions, respectively. On each of these projected versions of
queries, keys and values we then perform the attention function in parallel, yielding _dv_ -dimensional


4To illustrate why the dot products get large, assume that the components of _q_ and _k_ are independent random
variables with mean 0 and variance 1. Then their dot product, _q · k_ = [�] _[d]_ _i_ =1 _[k]_ _[q][i][k][i]_ [, has mean][ 0][ and variance] _[ d][k]_ [.]


4



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_is_all_you_need.pdf-3-0.png)

![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_is_all_you_need.pdf-3-1.png)
output values. These are concatenated and once again projected, resulting in the final values, as
depicted in Figure 2.


Multi-head attention allows the model to jointly attend to information from different representation
subspaces at different positions. With a single attention head, averaging inhibits this.


MultiHead( _Q, K, V_ ) = Concat(head1 _, ...,_ headh) _W_ _[O]_

where headi = Attention( _QWi_ _[Q][, KW][ K]_ _i_ _[, V W][ V]_ _i_ [)]


Where the projections are parameter matrices _Wi_ _[Q]_ _∈_ R _[d]_ [model] _[×][d][k]_, _Wi_ _[K]_ _∈_ R _[d]_ [model] _[×][d][k]_, _Wi_ _[V]_ _∈_ R _[d]_ [model] _[×][d][v]_
and _W_ _[O]_ _∈_ R _[hd][v][×][d]_ [model] .


In this work we employ _h_ = 8 parallel attention layers, or heads. For each of these we use
_dk_ = _dv_ = _d_ model _/h_ = 64. Due to the reduced dimension of each head, the total computational cost
is similar to that of single-head attention with full dimensionality.


**3.2.3** **Applications of Attention in our Model**


The Transformer uses multi-head attention in three different ways:


    - In ""encoder-decoder attention"" layers, the queries come from the previous decoder layer,
and the memory keys and values come from the output of the encoder. This allows every
position in the decoder to attend over all positions in the input sequence. This mimics the
typical encoder-decoder attention mechanisms in sequence-to-sequence models such as

[38, 2, 9].


    - The encoder contains self-attention layers. In a self-attention layer all of the keys, values
and queries come from the same place, in this case, the output of the previous layer in the
encoder. Each position in the encoder can attend to all positions in the previous layer of the
encoder.


    - Similarly, self-attention layers in the decoder allow each position in the decoder to attend to
all positions in the decoder up to and including that position. We need to prevent leftward
information flow in the decoder to preserve the auto-regressive property. We implement this
inside of scaled dot-product attention by masking out (setting to _−∞_ ) all values in the input
of the softmax which correspond to illegal connections. See Figure 2.

---

[Document 2]
Source: attention_is_all_you_need_raw_chunks_2k.jsonl
Chunk ID: attention_is_all_you_need_raw_chunks_2k_0
Content: Provided proper attribution is provided, Google hereby grants permission to
reproduce the tables and figures in this paper solely for use in journalistic or
scholarly works.

## **Attention Is All You Need**



**Niki Parmar** _[∗]_
Google Research
```
nikip@google.com

```


**Ashish Vaswani** _[∗]_
Google Brain
```
avaswani@google.com

```

**Llion Jones** _[∗]_
Google Research
```
 llion@google.com

```


**Noam Shazeer** _[∗]_
Google Brain
```
noam@google.com

```


**Jakob Uszkoreit** _[∗]_
Google Research
```
usz@google.com

```


**Aidan N. Gomez** _[∗†]_
University of Toronto
```
aidan@cs.toronto.edu

```


**Łukasz Kaiser** _[∗]_
Google Brain
```
lukaszkaiser@google.com

```


**Illia Polosukhin** _[∗‡]_

```
             illia.polosukhin@gmail.com

```

**Abstract**


The dominant sequence transduction models are based on complex recurrent or
convolutional neural networks that include an encoder and a decoder. The best
performing models also connect the encoder and decoder through an attention
mechanism. We propose a new simple network architecture, the Transformer,
based solely on attention mechanisms, dispensing with recurrence and convolutions
entirely. Experiments on two machine translation tasks show these models to
be superior in quality while being more parallelizable and requiring significantly
less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including
ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,
our model establishes a new single-model state-of-the-art BLEU score of 41.8 after
training for 3.5 days on eight GPUs, a small fraction of the training costs of the
best models from the literature. We show that the Transformer generalizes well to
other tasks by applying it successfully to English constituency parsing both with
large and limited training data.


_∗_ Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started
the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and
has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head
attention and the parameter-free position representation and became the other person involved in nearly every
detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and
tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and
efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and
implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating
our research.

_†_ Work performed while at Google Brain.

_‡_ Work performed while at Google Research.


31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.


**1** **Introduction**


Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks
in particular, have been firmly established as state of the art approaches in sequence modeling and
transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous
efforts have since continued to push the boundaries of recurrent language models and encoder-decoder
architectures [38, 24, 15].


Recurrent models typically factor computation along the symbol positions of the input and output
sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden
states _ht_, as a function of the previous hidden state _ht−_ 1 and the input for position _t_ . This inherently
sequential nature precludes parallelization within training examples, which becomes critical at longer
sequence lengths, as memory constraints limit batching across examples. Recent work has achieved
significant improvements in computational efficiency through factorization tricks [21] and conditional
computation [32], while also improving model performance in case of the latter. The fundamental
constraint of sequential computation, however, remains.


Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in
the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms
are used in conjunction with a recurrent network.


In this work we propose the Transformer, a model architecture eschewing recurrence and instead
relying entirely on an attention mechanism to draw global dependencies between input and output.
The Transformer allows for significantly more parallelization and can reach a new state of the art in
translation quality after being trained for as little as twelve hours on eight P100 GPUs.


**2** **Background**


The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU

[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building
block, computing hidden representations in parallel for all input and output positions. In these models,
the number of operations required to relate signals from two arbitrary input or output positions grows
in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes
it more difficult to learn dependencies between distant positions [12]. In the Transformer this is
reduced to a constant number of operations, albeit at the cost of reduced effective resolution due
to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as
described in section 3.2.


Self-attention, sometimes called intra-attention is an attention mechanism relating different positions
of a single sequence in order to compute a representation of the sequence. Self-attention has been
used successfully in a variety of tasks including reading comprehension, abstractive summarization,
textual entailment and learning task-independent sentence representations [4, 27, 28, 22].


End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering and
language modeling tasks [34].


To the best of our knowledge, however, the Transformer is the first transduction model relying
entirely on self-attention to compute representations of its input and output without using sequencealigned RNNs or convolution. In the following sections, we will describe the Transformer, motivate
self-attention and discuss its advantages over models such as [17, 18] and [9].


**3** **Model Architecture**


Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].
Here, the encoder maps an input sequence of symbol representations ( _x_ 1 _, ..., xn_ ) to a sequence
of continuous representations **z** = ( _z_ 1 _, ..., zn_ ). Given **z**, the decoder then generates an output
sequence ( _y_ 1 _, ..., ym_ ) of symbols one element at a time. At each step the model is auto-regressive

[10], consuming the previously generated symbols as additional input when generating the next.


2


![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_is_all_you_need.pdf-2-0.png)

Figure 1: The Transformer - model architecture.


The Transformer follows this overall architecture using stacked self-attention and point-wise, fully
connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,
respectively.


**3.1** **Encoder and Decoder Stacks**

---

[Document 3]
Source: attention_functional_roles_raw_chunks_2k.jsonl
Chunk ID: attention_functional_roles_raw_chunks_2k_2
Content: Let _Nt_ denote the number of tokens in the generated answer _a_ _[M]_ _i_ [. To isolate semantically informative]
content relevant to reasoning, we select the top- _k_ most important tokens, determined by prompting
Qwen3-30B LLM (Yang et al., 2025), yielding an index set _Ik_ with _|Ik|_ = _k_ . For each index
_j ∈Ik_, we extract the corresponding attention head activations _Xj_, and compute the averaged
activation feature for the _m_ -th head in layer _l_ as ¯ _x_ _[m]_ _l_ = _k_ [1] � _j ∈Ikxml_ [. This results in a full set of]

head-level features _X_ [¯] _i_ = _{x_ ¯ _[m]_ _l_ _[|][ l][ ∈]_ [1] _[, . . ., L, m][ ∈]_ [1] _[, . . ., M]_ _[}]_ [.]


3.2 FUNCTION PROBING


For the dataset with _N_ subQAF triplets, we collect all activations to construct the probing dataset:


_D_ probe = _{_ (¯ _x_ _[m]_ _l_ _[, c]_ [)] _[i][}][N]_ _i_ =1 _[, l][ ∈{]_ [1] _[, . . ., L][}][, m][ ∈{]_ [1] _[, . . ., M]_ _[}]_ (1)


For classification based on CogVision, the training set includes 1,124 main questions with 4,604
subQAF triplet, while the testing set has 285 main questions with 1,141 triplets. Our probe takes
the form _pθ_ ( _x_ _[m]_ _l_ [) = sigmoid (] _[⟨][θ, x][m]_ _l_ _[⟩]_ [)][. There is one probe per attention head per layer per function.]
For each target function, the probe is trained by treating the attention-head outputs that lead to
correct answers for that function as the positive class, and those associated with correct answers
from other functions as the negative class. To ensure data balance, we select an equal number of
negative samples to match the positive ones. Given prior findings suggesting that cognitive functions
may vary by layer depth (Zheng et al.), we incorporate layer-wise information by computing the
average activation ¯ _xl_ = _M_ 1 � _Mm_ =1 _[x]_ [¯] _l_ _[m]_ [for each layer. We then augment each head-level vector with]


4


its corresponding layer summary, resulting in enriched features _x_ _[m]_ _l_ _[′]_ = [¯ _x_ _[m]_ _l_ [; ¯] _[x][l]_ []][ for probing. The]
importance for each head are then calculated based on the accuracy of predicting target function. The
effectiveness of top-k tokens and layer information, as well as the sensitivity analysis with respect
to the parameter _k_ and the choice of LLM fused for top-k token extraction, and prompt format, can
be found in Appendix A.8.


4 EXPERIMENTS


We conduct a series of experiments on three VLM families across various model scales, including
Intern (Zhu et al., 2025) (InternVL3-8B and InternVL3-2B), Qwen (Yang et al., 2025) (Qwen2.5VL-7B and Qwen2.5-VL-3B), and Gemma (Team et al., 2025) (Gemma3-4B and Gemma3-2B).
We analyze the commonalities and differences of functional heads (Subsection 4.1), validate their
contributions (Subsection 4.2), and examine correlations, including cross-function interactions and
hierarchical organization (Subsection 4.3). We also assess their causal impact on downstream reasoning tasks (Subsection 4.4). Results confirm the existence of sparse, function-specific heads and
highlight their critical contribution to structured cognitive processing within VLMs.


4.1 PROPERTIES OF COGNITIVE HEADS


**Sparsity, Universality, and Intrinsic Organization:** Fig 2 shows the heatmap of attention head
accuracy across eight functions in Qwen2.5-VL-7B on the CogVision test set, revealing a sparse
distribution. In total, fewer than 7% of all heads achieve accuracies above 0.9 across the eight functions (about 2% for high-level visual reception and math reasoning, and less than 1% for the others),
suggesting that only a small subset of heads meaningfully contributes to different reasoning tasks.
These results demonstrate that VLMs rely on highly specialized, localized components for distinct
cognitive abilities. Pearson correlations between head-activation heatmaps across the eight functions
(Fig. 3) are generally low, confirming that different functions tend to depend on partially separable
subsets of heads. Moreover, this sparse functional organization is consistent across architectures
and scales: heatmaps for five additional models (Appendix A.4) confirm its universality, and the
relatively high Pearson correlation coefficients between models further verify this consistency (in
Appendix A.9). Within the same model family (e.g., Qwen2.5-VL-7B in Fig 2 vs. Qwen2.5-VL-3B
in Fig 8), we observe similar distributions, suggesting that such specialization is intrinsic to VLMs.


**Functional Personalization:** Beyond sparsity, attention heads exhibit a structured distribution
across model layers. Math-related heads are dispersed throughout the network, whereas inferencerelated heads appear more frequently in higher layers. This task-dependent localization suggests an
emergent modular organization in which different layers support distinct cognitive operations. We
also observe notable variation in head counts across functions. For example, in the Qwen family,
math reasoning and high-level visual reception heads are more prevalent than others, reflecting differences in representational and computational complexity. Smaller models contain fewer functional
heads compared to their larger counterparts.


4.2 FUNCTIONAL CONTRIBUTIONS OF COGNITIVE HEADS


After identifying the cognitive heads associated with each function, we examine their functional
roles by evaluating the model’s behavior on the CogVision test set under targeted interventions. We
perform head ablation by scaling the output of a specific attention head with a small factor _ϵ_ (e.g.,
0.001), effectively suppressing its contribution:



�



_x_ [mask] _i_ = Softmax



_Wq_ _[i][W][ iT]_ _k_
� ~~�~~ _dk/n_




_· ϵWv_ _[i]_ (2)



Specifically, we compare model performance when masking identified cognitive heads versus masking an equal number of randomly-selected heads. To quantify the impact, we employ both an LLMbased judge and an integrated accuracy metric. For LLM-based judge, we use LLM (Qwen3-30B
LLM (Yang et al., 2025)) to judge the correctness of the output. For the integrated accuracy metric,
an output is considered unaffected if its BLEU score (Papineni et al., 2002) exceeds 0.8, or if either


5


![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-5-0.png)

Figure 2: The existence of cognitive heads in Qwen2.5-VL-7B responsible for eight distinct functions in complex reasoning tasks. The x-axis represents the head index, while the y-axis indicates
the layer index. The values denote head importance scores, capped at a cutoff of 0.60.


Figure 3: Pearson Correlation between different functions across two models.


the ROUGE score (Chin-Yew, 2004) or the semantic similarity score surpasses 0.6. This provides a
comprehensive evaluation of performance degradation.
","In the Transformer architecture, ""Multi-Head Attention"" is a specific type of ""sub-layer"" found within both the encoder and decoder stacks.

Specifically:
*   **Encoder layers** consist of two sub-layers: the first is a multi-head self-attention mechanism, and the second is a position-wise fully connected feed-forward network.
*   **Decoder layers** have three sub-layers: a multi-head self-attention mechanism (modified to prevent attending to future positions), a multi-head attention mechanism that attends over the output of the encoder stack (referred to as ""encoder-decoder attention""), and a position-wise fully connected feed-forward network.

Residual connections and layer normalization are applied around each of these sub-layers.",120,2711,2833,,3,,2026-01-02T20:14:48.798139
"What is the connection between ""C. elegans"" and the ""Neuronal Circuit Policies (NCPs)"" entity in the NAC paper?",Entity Relationship Reasoning,"[Document 1]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_10
Content: Zhuang, J., Jia, M., Ding, Y., and Ding, P. Temporal
convolution-based transferable cross-domain adaptation
approach for remaining useful life estimation under variable failure behaviors. _Reliability Engineering & System_
_Safety_, 216:107946, 2021.



10


**Neuronal Attention Circuit (NAC) for Representation Learning**



**Appendix**


**A. Preliminaries**


**A.1. Attention Mechanism**


Attention mechanisms have become a cornerstone in

modern neural architectures, enabling models to dynamically focus on relevant parts of the input. The concept
was first introduced in the context of neural machine
translation, where it allowed the decoder to weight encoder
outputs according to their importance for generating each
target token. Formally, given a query vector _q ∈_ R _[d]_, key
vectors _K_ = [ _k_ 1 _, k_ 2 _, . . ., kn_ ] _∈_ R _[n][×][d]_, and value vectors
_V_ = [ _v_ 1 _, v_ 2 _, . . ., vn_ ] _∈_ R _[n][×][d]_, the attention mechanism can
be expressed in two steps:


1. Compute the scaled dot attention logits:


_ai_ = _[q][T][ k][i]_ (21)
~~_√_~~ _d_


2. Normalize the logits to get attention weights and compute the output:


_e_ _[a][i]_
_αi_ = softmax( _ai_ ) = ~~_n_~~ (22)
~~�~~ _j_ =1 _[e][a][j]_



exhibit a resting potential at _−_ 70 mV and an activation potential near _−_ 20 mV. Similarly, each _Nm_ is composed of
two subneurons, _Mp_ and _Mn_, and is driven by a controllable variable _y_, which also maps to a biologically plausible
range [ _−_ 70 mV _, −_ 20 mV]. The connections in the NCP
architecture are designed to reflect the biological sparsity
and abstraction of neural circuits. Specifically, connections
from _Ns_ to _Ni_ are feedforward, while those between _Nc_
and _Nm_ are highly recurrent (Lechner et al., 2018). Figure
1(a) illustrates the connectome of NCPs.


**B. Proofs**


In this section, we provide all the proofs.


**B.1. Deriving Closed-form (Exact) Solution**


Although _ϕ_ and _ωτ_ are nonlinear functions of the input
**u** = [ **q** ; **k** ], we derive closed-form solution by treating them
as locally constant over the pseudo-time integration interval
for each query–key pair based on frozen-coefficient approximation (John, 1952). This is accurate whenever the interval
is short or when input variations are slow compared with
the relaxation rate _ωτ_ . Under approximation assumption,
rewrite Eqn. 1 as


_dadtt_ [+] _[ ω][τ]_ _[a][t]_ [ =] _[ ϕ.]_ (24)


This is now a linear first-order ODE. The integrating factor
is



Attention( _q, k, v_ ) =



_n_
� _αivi_ (23)


_i_ =1



Here, _ai_ is the raw attention logit between the query and
each key, and the scaling factor _√d_ prevents large dot prod
ucts from destabilizing the softmax (Vaswani et al., 2017).


**A.2. Neuronal Circuit Policies (NCPs)**


NCPs represent a biologically inspired framework for developing interpretable neural control agents by adapting the
tap-withdrawal circuit found in the nematode _C. elegans_
(Lechner et al., 2018). Unlike traditional spiking neural
networks, the majority of neurons in this circuit exhibit
electronic dynamics, characterized by the passive flow of
electrical charges, resulting in graded potentials. NCPs
are structured as a four-layer hierarchical architecture comprising sensory neurons ( _Ns_ ), interneurons ( _Ni_ ), command
neurons ( _Nc_ ), and motor neurons ( _Nm_ ). The _Ns_ perceive
and respond to external stimulus inputs and are responsible for the initial signal transduction. Each _Ns_ consists
of subneurons _Sp_ and _Sn_ and a system variable _x_ . The
activation of _Sp_ and _Sn_ depends upon the sign of _x_ : _Sp_
becomes activated for _x >_ 0, whereas _Sn_ becomes activated for _x <_ 0. The variable _x_ is mapped to the membrane
potential range of [ _−_ 70 mV _, −_ 20 mV], which is consistent
with the biophysical behavior of nerve cells, which typically



� _ωτ dt_

_µ_ = _e_ � �


Multiply both sides by _µ_ ( _t_ ):



Substitute back:


_e_ _[ω][τ][ t]_ _at −_ _a_ 0 = _ϕ ·_ _[e][ω][τ][ t][ −]_ [1] _._ (30)

_ωτ_



= _e_ _[ω][τ][ t]_ _._ (25)




_[da][t]_
_e_ _[ω][τ][ t]_ _dt_ [+] _[ ω][τ]_ _[e][ω][τ][ t][a][t]_ [ =] _[ ϕe][ω][τ][ t][.]_ (26)


Recognize the left-hand side as the derivative of _e_ _[ω][τ][ t]_ _at_ :


_d_
� _e_ _[ω][τ][ t]_ _at_ � = _ϕe_ _[ω][τ][ t]_ _._ (27)
_dt_


Integrate from 0 to _t_ :


_t_
_e_ _[ω][τ][ t]_ _at −_ _e_ [0] _a_ 0 = _ϕ_ _e_ _[ω][τ][ s]_ _ds._ (28)
�0


Compute the integral (since _ωτ ̸_ = 0):



�0 _t_



_e_ _[ω][τ][ s]_ _ds_ = [1]
0 _ωτ_



_ωτ_



� _e_ _[ω][τ][ t]_ _−_ 1� _._ (29)



11


Rearrange:



**Neuronal Attention Circuit (NAC) for Representation Learning**


CASE 2: MULTIPLE CONNECTIONS ( _M >_ 1).



_e_ _[ω][τ][ t]_ _at_ = _a_ 0 + _[ϕ]_

_ωτ_


Divide both sides by _e_ _[ω][τ][ t]_ :


_at_ = _a_ 0 _e_ _[−][ω][τ][ t]_ + _[ϕ]_

_ωτ_



� _e_ _[ω][τ][ t]_ _−_ 1� _._ (31)


�1 _−_ _e_ _[−][ω][τ][ t]_ [�] _._ (32)



The ODE is



with per-connection equilibria _Aj_ = _ϕj/fj_ . The effective
equilibrium is



_da_

_dt_ [=] _[ −]_ � � _[M]_



_da_



� _fj_ � _a_ +

_j_ =1



_M_
� _fjAj,_ (37)

_j_ =1



Set _a_ _[∗]_ := _[ϕ]_ . Then _at_ = _a_ _[∗]_ + ( _a_ 0 _−_ _a_ _[∗]_ ) _e_ _[−][ω][τ][ t]_, proved.

_ωτ_


**B.2. Proof of Theorem 1**

---

[Document 2]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_8
Content: the ceiling and floor functions, respectively. This results
in a larger overall architectural size and increased runtime.



Future work will support user-defined NCPs configurations
or randomized wiring to enable more efficient architectures.
**Learnable sparse Top-** _**K**_ **selection:** Sparse Top- _K_ attention can miss important context, is sensitive to _k_, and may
be harder to optimize. A further limitation is that it still
computes the full _QK_ _[⊤]_ matrix, which can dominate the
cost for very long sequences. Future work includes adaptive
or learnable Top- _K_ selection, improved key scoring, and
hardware-aware optimization to strengthen accuracy and
robustness.


**5. Conclusion**


In this paper, we introduce the Neuronal Attention Circuit
(NAC), a biologically inspired attention mechanism that
reformulates attention logits as the solution to a first-order
ODE modulated by nonlinear, interlinked gates derived from
repurposing _C.elegans_ nematode NCPs. NAC bridges discrete attention with continuous-time dynamics, enabling
adaptive temporal processing without the limitations inherent to traditional scaled dot-product attention. Based on the
solution to ODE, we introduce three computational modes:
(i) Euler, based on _explicit Euler_ integration; (ii) Exact, providing closed-form solutions; and (iii) Steady, approximating equilibrium states. In addition, a sparse Top- _K_ pairwise
concatenation scheme is introduced to mitigate the memory intensity. Theoretically, we establish NAC’s log-state
stability, exponential error bounds, and universal approximation, thereby providing rigorous guarantees of convergence
and expressiveness. Empirical evaluations demonstrate that
NAC achieves state-of-the-art performance across diverse
tasks, including irregularly sampled time-series benchmarks,
autonomous vehicle lane-keeping, and industrial prognostics. Moreover, NAC occupies an intermediate position
between CT-RNNs and CT-Attention, offering robust temporal modeling while requiring less runtime than CT-RNNs
and less memory than CT-Attention models.


**Reproducibility Statement**


The code for reproducibility is available at
[https://github.com/itxwaleedrazzaq/](https://github.com/itxwaleedrazzaq/neuronal_attention_circuit)

[neuronal_attention_circuit](https://github.com/itxwaleedrazzaq/neuronal_attention_circuit)


**Impact Statement**


The work addresses the growing field of continuous-time
attention and pioneers a biologically plausible mechanism.
It encourages research into sparse, adaptive networks that
resemble natural wiring. From a societal perspective, it
supports more robust AI in resource-limited settings, but it
also raises ethical concerns when applied to areas such as
surveillance or autonomous systems.



8


**Neuronal Attention Circuit (NAC) for Representation Learning**



**References**


Introduction to self-driving cars. URL
[https://www.udacity.com/course/](https://www.udacity.com/course/intro-to-self-driving-cars--nd113)
[intro-to-self-driving-cars--nd113.](https://www.udacity.com/course/intro-to-self-driving-cars--nd113)


Aguiar-Conraria, L. and Soares, M. J. The continuous
wavelet transform: Moving beyond uni-and bivariate analysis. _Journal of economic surveys_, 28(2):344–375, 2014.


Beltagy, I., Peters, M. E., and Cohan, A. Longformer: The long-document transformer. _arXiv preprint_
_arXiv:2004.05150_, 2020.


Bojarski, M., Del Testa, D., Dworakowski, D., Firner, B.,
Flepp, B., Goyal, P., Jackel, L. D., Monfort, M., Muller,
U., Zhang, J., et al. End to end learning for self-driving
cars. _arXiv preprint arXiv:1604.07316_, 2016.


Brockman, G., Cheung, V., Pettersson, L., Schneider, J.,
Schulman, J., Tang, J., and Zaremba, W. Openai gym.
_arXiv preprint arXiv:1606.01540_, 2016.


Cao, Y., Li, S., Petzold, L., and Serban, R. Adjoint sensitivity analysis for differential-algebraic equations: The
adjoint dae system and its numerical solution. _SIAM_
_journal on scientific computing_, 24(3):1076–1089, 2003.


Chen, R. T., Rubanova, Y., Bettencourt, J., and Duvenaud,
D. K. Neural ordinary differential equations. _Advances_
_in neural information processing systems_, 31, 2018.


Chen, Y., Ren, K., Wang, Y., Fang, Y., Sun, W., and Li, D.
Contiformer: Continuous-time transformer for irregular
time series modeling. _Advances in Neural Information_
_Processing Systems_, 36:47143–47175, 2023.


Chien, J.-T. and Chen, Y.-H. Continuous-time attention for
sequential learning. In _Proceedings of the AAAI confer-_
_ence on artificial intelligence_, volume 35, pp. 7116–7124,
2021.


Cho, K., Van Merrienboer, B., Gulcehre, C., Bahdanau,¨
D., Bougares, F., Schwenk, H., and Bengio, Y. Learning phrase representations using rnn encoder-decoder
for statistical machine translation. _arXiv preprint_
_arXiv:1406.1078_, 2014.


d’Ascoli, S., Becker, S., Mathis, A., Schwaller, P., and
Kilbertus, N. Odeformer: Symbolic regression of dynamical systems with transformers. _arXiv preprint_
_arXiv:2310.05573_, 2023.


De Brouwer, E., Simm, J., Arany, A., and Moreau, Y.
Gru-ode-bayes: Continuous modeling of sporadicallyobserved time series. _Advances in neural information_
_processing systems_, 32, 2019.



Deng, L. The mnist database of handwritten digit images
for machine learning research [best of the web]. _IEEE_
_signal processing magazine_, 29(6):141–142, 2012.


Ding, Y., Jia, M., Miao, Q., and Huang, P. Remaining
useful life estimation using deep metric transfer learning
for kernel regression. _Reliability Engineering & System_
_Safety_, 212:107583, 2021.


Hasani, R., Lechner, M., Amini, A., Rus, D., and Grosu,
R. Liquid time-constant networks. In _Proceedings of the_
_AAAI Conference on Artificial Intelligence_, volume 35,
pp. 7657–7666, 2021.


Hasani, R., Lechner, M., Amini, A., Liebenwein, L., Ray,
A., Tschaikowski, M., Teschl, G., and Rus, D. Closed
form continuous-time neural networks. _Nature Machine_

_Intelligence_, 4(11):992–1003, 2022.


Hochreiter, S. The vanishing gradient problem during learning recurrent neural nets and problem solutions. _Interna-_
_tional Journal of Uncertainty, Fuzziness and Knowledge-_
_Based Systems_, 6(02):107–116, 1998.


Hochreiter, S. and Schmidhuber, J. Long short-term memory.
_Neural computation_, 9(8):1735–1780, 1997.


Hong, H. S. and Thuan, N. Hust bearing: a practical dataset
for ball bearing fault diagnosis. _Mendeley Data_, 3, 2023.


John, F. On integration of parabolic equations by difference methods: I. linear and quasi-linear equations for the
infinite interval. _Communications on Pure and Applied_
_Mathematics_, 5(2):155–211, 1952.

---

[Document 3]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_0
Content: ## **Neuronal Attention Circuit (NAC) for Representation Learning**

**Waleed Razzaq** [1] **Izis Kankaraway** [1] **Yun-Bo Zhao** [1 2]



**Abstract**

Attention improves representation learning over
RNNs, but its discrete nature limits continuoustime (CT) modeling. We introduce Neuronal Attention Circuit (NAC), a novel, biologically plausible CT-Attention mechanism that reformulates

attention logits computation as the solution to a
linear first-order ODE with nonlinear interlinked
gates derived from repurposing _C. elegans_ Neuronal Circuit Policies (NCPs) wiring mechanism.
NAC replaces dense projections with sparse sensory gates for key-query projections and a sparse
backbone network with two heads for computing
_content-target_ and _learnable time-constant_ gates,
enabling efficient adaptive dynamics. NAC supports three attention logit computation modes: (i)
explicit Euler integration, (ii) exact closed-form
solution, and (iii) steady-state approximation. To
improve memory intensity, we implemented a
sparse Top- _K_ pairwise concatenation scheme that
selectively curates key-query interactions. We
provide rigorous theoretical guarantees, including
state stability, bounded approximation errors, and
universal approximation. Empirically, we implemented NAC in diverse domains, including irregular time-series classification, lane-keeping for
autonomous vehicles, and industrial prognostics.
We observed that NAC matches or outperforms
competing baselines in accuracy and occupies an
intermediate position in runtime and memory efficiency compared with several CT baselines.


**1. Introduction**


Learning representations of sequential data in temporal or
spatio-temporal domains is essential for capturing patterns
and enabling accurate forecasting. Discrete-time Recurrent neural networks (DT-RNNs) such as RNN (Rumelhart et al., 1985; Jordan, 1997), Long-short term memory


1Department of Automation, University of Science & Technology of China, Hefei, China [2] Institute of Artificial Intelligence,
Hefei Comprehensive National Science Center. Correspondence to:
Yun-Bo Zhao _<_ ybzhao@ustc.edu.cn _>_, Waleed Razzaq _<_ waleedrazzaq@mail.ustc.edu.cn _>_ .


_Preprint. December 12, 2025._



(LSTM) (Hochreiter & Schmidhuber, 1997), and Gated Recurrent Unit (GRU) (Cho et al., 2014) model sequential dependencies by iteratively updating hidden states to represent
or predict future elements in a sequence. While effective
for regularly sampled sequences, DT-RNNs face challenges
with irregularly sampled data because they assume uniform
time intervals. In addition, vanishing gradients can make
it difficult to capture long-term dependencies (Hochreiter,
1998).
Continuous-time RNNs (CT-RNNs) (Rubanova et al., 2019)
model hidden states as ordinary differential equations
(ODEs), allowing them to process inputs that arrive at arbitrary or irregular time intervals. Mixed-memory RNNs
(mmRNNs) (Lechner & Hasani, 2022) build on this idea
by separating memory compartments from time-continuous
states, helping maintain stable error propagation while capturing continuous-time dynamics. Liquid neural networks
(LNNs) (Hasani et al., 2021; 2022) take a biologically inspired approach by assigning variable time-constants to hidden states, improving adaptability and robustness, though
vanishing gradients can still pose challenges during training.
The attention mechanisms (Vaswani et al., 2017) mitigate
this limitation by treating all time steps equally and allowing models to focus on the most relevant observations. It
computes the similarity between queries ( _q_ ) and keys ( _k_ ),
scaling by the key dimension to keep gradients stable. MultiHead Attention (MHA) (Vaswani et al., 2017) extends this
by allowing the model to attend to different representation
subspaces in parallel. Variants like Sparse Attention (Tay
et al., 2020; Roy et al., 2021), BigBird (Zaheer et al., 2020),
and Longformer (Beltagy et al., 2020) modify the attention
pattern to reduce computational cost, particularly for long sequences, by attending only to selected positions rather than
all pairs. Even with these improvements, attention-based
methods still rely on discrete scaled dot-product operations,
limiting their ability to model continuous trajectories often
captured by CT counterparts.
Recent work has explored bridging this gap through NeuralODE (Chen et al., 2018) formulation. mTAN (Shukla &
Marlin, 2021) learns CT embeddings and uses time-based
attention to interpolate irregular observations into a fixedlength representation for downstream encoder-decoder modeling. ODEFormer (d’Ascoli et al., 2023) trains a sequenceto-sequence transformer on synthetic trajectories to directly
infer symbolic ODE systems from noisy, irregular data,
though it struggles with chaotic systems and generalization



1


**Neuronal Attention Circuit (NAC) for Representation Learning**



beyond observed conditions. Continuous-time Attention
(CTA) (Chien & Chen, 2021) embeds a continuous-time
attention mechanism within a Neural ODE, allowing attention weights and hidden states to evolve jointly over time.
Still, it remains computationally intensive and sensitive to
the accuracy of the ODE solver. ContiFormer (Chen et al.,
2023) builds a CT-transformer by pairing ODE-defined latent trajectories with a time-aware attention mechanism to
model dynamic relationships in data.
Despite these innovations, a persistent and underexplored
gap remains in developing a biologically plausible attention
mechanism that seamlessly integrates CT dynamics with the
abstraction of the brain’s connectome to handle irregular sequences without prohibitive computational costs. Building
on this, we propose a novel attention mechanism called the
_Neuronal Attention Circuit_ (NAC), in which attention logits
are computed as the solution to a first-order ODE modulated
by nonlinear, interlinked gates derived from repurposing
Neuronal Circuit Policies (NCPs) from the nervous system
of _C. elegans_ nematode (refer to Appendix A.2 for more
information). Unlike standard attention, which projects keyquery pairs through a dense layer, NAC employs a sensory
gate to transform input features and a backbone to model
nonlinear interactions, with multiple heads producing outputs structured for attention logits computation. Based on
the solutions to ODE, we define three computation modes:
(i) Exact, using the closed-form ODE solution; (ii) Euler,
approximating the solution via _explicit Euler_ integration;
and (iii) Steady, using only the steady-state solution, analogous to standard attention scores. To reduce computational
complexity, we implemented a sparse Top- _K_ pairwise concatenation algorithm that selectively curates key-query inputs. We evaluate NAC across multiple domains, including
irregularly sampled time series, autonomous vehicle lanekeeping, and Industry 4.0, comparing it to state-of-the-art
baselines. NAC consistently matches or outperforms these
models, while runtime and peak memory benchmarks place
it between CT-RNNs in terms of speed and CT-Attentions
in terms of memory requirements.


**2. Neuronal Attention Circuit (NAC)**


We propose a simple alternative formulation of the attention
logits _a_ (refer to Appendix A.1 for more information), interpreting them as the solution to a first-order linear ODE
modulated by nonlinear, interlinked gates:



from repurposing NCPs. We refer to this formulation as
the Neuronal Attention Circuit (NAC). It enables the logits
_at_ to evolve dynamically with input-dependent, variable
time constants, mirroring the adaptive temporal dynamics
found in _C. elegans_ nervous systems while improving
computational efficiency and expressiveness. Moreover, it
introduces continuous depth into the attention mechanism,
bridging discrete-layer computation with dynamic temporal
evolution.

**Motivation behind this formulation:** The proposed
formulation is loosely motivated by the input-dependent
time-constant mechanism of Liquid Neural Networks
(LNNs), a class of CT-RNNs inspired by biological nervous
systems and synaptic transmission. In this framework, the
dynamics of non-spiking neurons are described by a linear
ODE with nonlinear, interlinked gates: _ddt_ **xt** = **[x]** _τ_ **[t]** [+] **[ S][t]** _[,]_
","Neuronal Circuit Policies (NCPs) are a biologically inspired framework for developing neural control agents, specifically by adapting the tap-withdrawal circuit found in the nematode *C. elegans*.

In the NAC paper, the Neuronal Attention Circuit (NAC) is a biologically inspired attention mechanism that reformulates attention logits. This reformulation involves using nonlinear, interlinked gates that are *derived from repurposing* the *C. elegans* nematode NCPs wiring mechanism. Essentially, the biological structure and dynamics of *C. elegans*'s tap-withdrawal circuit, as modeled by NCPs, serve as the inspiration and source for the gates within the NAC model.",125,3489,3616,,3,,2026-01-02T20:14:52.416097
"How does the ""KV cache"" entity relate to the ""FA Decode"" method's efficiency in SWAA?",Entity Relationship Reasoning,"[Document 1]
Source: sliding_window_attention_annotated_chunks_2k.jsonl
Chunk ID: sliding_window_attention_annotated_chunks_2k_3
Content: **Combinations exhibit strong synergy.** Recipes
that combine multiple methods deliver large gains:


  - **FA Decode + Keep First** _k_ substantially improves over naive SWA (rows 7–9), recovering roughly half to two-thirds of the gap on
the thinking model as _k_ increases. However,
increasing _k_ from 100 to 1000 yields almost
no improvement, indicating that _k_ does not
need to be exceedingly large.


  - **Interleaving Layers + FA Decode** is
markedly stronger (row 13), recovering most
of the gap for the thinking model.


  - **FA Decode + Interleaving Layers + Keep**
**First** _k_ perform best (rows 18). The thinking
model recovers close to 90% of the FA gap
even at 2k window.


**CoT synergizes with FA Decode.** Under recipes
that include FA Decode, the thinking model consistently benefits more than the non-thinking model
(rows 13 and 18), suggesting that CoT synergizes
with FA Decode: preserving global attention at
decoding time enables longer reasoning traces to
capitalize on context processed by SWA, confirming our hypothesis in Section 3.1.


**Sliding window size affects, but is not the deci-**
**sive role.** With FA Decode + Keep First _k_, accuracy improves as the window increases (rows 7,
14, 15), though benefits are modest until 8k. When
added with interleaving FA layers, moving from a
2k to a 4k window is enough to close the remaining
gap on the thinking model (row 22 matches the FA
baseline), indicating that FA Decode and Interleaving are the dominant drivers, and larger windows
mainly smooth residual error.



5


Table 1: Qwen3-4B-Thinking and Qwen3-4B-Instruct results on LongMemEval without SFT


**No.** **window size** **FA layers** **keep first** **FA decode** **Acc think** **Acc non-think**


0 Full [] 0 False **73.0** **62.0**
1 2k [] 0 False 3.2 11.0
2 8k [] 0 False 13.2 19.8


3 2k [] 10 False 16.0 15.6
4 2k [] 0 True 11.8 14.2
5 2k [1, 3, 5, ...] 0 False 13.4 18.4
6 8k [] 0 True 26.2 25.0


7 2k [] 10 True 38.2 20.6
8 2k [] 100 True 50.0 17.8
9 2k [] 1000 True 50.0 20.2
10 2k [0, 2, 4, ...] 10 False 17.0 14.8
11 2k [0, 2, 4, ...] 0 True 32.2 26.0
12 2k [1, 3, 5, ...] 10 False 25.8 36.4
13 2k [1, 3, 5, ...] 0 True 59.2 34.8
14 4k [] 10 True 38.0 24.4
15 8k [] 10 True 49.2 35.2


16 2k [0, 2, 4, ...] 10 True 36.0 17.2
17 2k [1, 3, 5, ...] 10 True 65.0 **53.6**
18 2k [1, 3, 5, ...] 100 True **68.8** 50.6
19 2k [1, 5, 9, ...] 10 True 53.2 31.4
20 2k [1, 9, 17, ...] 10 True 36.4 18.8
21 2k [3, 7, 11, ...] 10 True 54.2 34.6
22 4k [1, 3, 5, ...] 100 True **73.0** **54.2**
23 8k [1, 3, 5, ...] 100 True **71.6** **56.6**



**SWA/FA layer selection has large impacts.** Selecting less layers for FA, e.g. only one fourth or
eighth (row 19, 20), though more efficient, will
significantly decrease the improvement brought by
Interleaving Layers. More importantly, for Qwen34B, configuring _odd-numbered_ layers with full attention is significantly better than _even-numbered_
layers (row 11, 13, 16, 17). However, surprisingly,
this result is reversed for Qwen3-30B-A3B and
Llama3.1-8B-Instruct (row 10 and 11 in Table 4
and 5). This suggests that layer functionalities differ across model families and sizes, necessitating
model-specific layer selection strategies, as discussed in Section 3.2.


Therefore, we answer RQ1: adapting an FA
LLM to SWA is feasible even without any training. But it requires specific combinations of at
least 2 methods, which could be less efficient for
inference.



**5.2** **SWA Adaptation With Fine-tuning**


We next evaluate SWA-aware supervised finetuning, which is expected to provide higher improvement. Table 2 reports LongMemEval_24k
accuracy after SFT under various SWAA configurations. The original full-attention model is also
fine-tuned as a baseline (row 0). Since training is
relatively time-consuming, we only test a representative subset of configurations. Our findings are as
follows:


**Fine-tuning substantially lifts all SWA config-**
**urations.** Comparing all the fine-tuning results
with non-training ones, it is clear that fine-tuning
consistently provides great improvement. However,
simply fine-tuning with naive SWA remains insufficient, only achieving 18.8% and 23.8% accuracy
(row 1).


**FA Decode and Interleaving Layers emerge as**
**dominant components.** After SFT, FA Decode
and Interleaving Layers provide the largest gains.



6


Table 2: Qwen3-4B-Thinking and Qwen3-4B-Instruct results on LongMemEval with SFT


**No.** **window size** **FA layers** **keep first** **FA decode** **Acc think** **Acc non-think**


0 Full [] 0 True **74.6** **63.4**
1 2k [] 0 False 18.8 23.8


2 2k [] 10 False 15.6 /
3 2k [] 0 True 57.9 42.0
4 2k [1, 3, 5, ...] 0 False 63.6 54.6
5 4k [] 0 True 62.6 /


6 2k [] 10 True 56.7 /
7 2k [] 100 True 62.2 42.6
8 2k [1, 3, 5, ...] 0 True **73.2** **58.8**
9 2k [0, 2, 4, ...] 0 True **66.0** /
10 2k [1, 5, 9, ...] 0 True **68.8** 47.0


11 2k [1, 3, 5, ...] 100 True **73.2** **61.4**


Table 3: Recommended SWA adaptation recipes for different needs and scenarios. � means optional.


**Training** ? **Thinking Model?** **Prefer?** **FA Decode** **Interleaving Layers** **Keep First**


No No Any � � �
No Yes Efficiency � � �
No Yes Accuracy � � �


Yes Any Efficiency � � �
Yes Any Accuracy � � �



Using FA Decode alone (row 3) or using Interleaving Layers alone (row 4) both get high accuracy.
And combining both (row 8) further boosts performance to 73 _._ 2 (think) and 58 _._ 8 (non-think), nearly
matching the full-attention SFT baseline in row 0.


**Keep First becomes optional rather than essen-**
**tial.** Before fine-tuning, Keep First is crucial for
stability under SWA. But after SFT, it only provides
minor additional improvement. With FA Decode,
adding _k_ = 100 (row 7) improves over _k_ = 0
(row 3) only 4.5%, and if further combining FA
layers, it almost offers no improvements (row 11
and row 8).


**Effect of sliding window size.** Row 3 and 5
shows that increasing the window from 2k to 4k
with FA Decode improves thinking-model accuracy from 57 _._ 9 to 62 _._ 6. This mirrors the non-SFT
trend that larger windows help, but the dominant
improvements still come from FA Decode and Interleaving Layers.

---

[Document 2]
Source: sliding_window_attention_annotated_chunks_2k.jsonl
Chunk ID: sliding_window_attention_annotated_chunks_2k_4
Content: **Keep First becomes optional rather than essen-**
**tial.** Before fine-tuning, Keep First is crucial for
stability under SWA. But after SFT, it only provides
minor additional improvement. With FA Decode,
adding _k_ = 100 (row 7) improves over _k_ = 0
(row 3) only 4.5%, and if further combining FA
layers, it almost offers no improvements (row 11
and row 8).


**Effect of sliding window size.** Row 3 and 5
shows that increasing the window from 2k to 4k
with FA Decode improves thinking-model accuracy from 57 _._ 9 to 62 _._ 6. This mirrors the non-SFT
trend that larger windows help, but the dominant
improvements still come from FA Decode and Interleaving Layers.

So, we answer RQ2: fine-tuning brings remarkably high performance restoration, provided we ap


ply **FA Decode**, **Interleaving Layers**, or a combination thereof, while **Keep First** becomes optional.
And the improvement brought by SFT under each
configuration varies significantly, meaning a nearoptimal training-free configuration need not remain
optimal after SFT, and vice versa.


**5.3** **Performance–efficiency Trade-offs and**
**Recommended Recipes**


Although integrating more methods can typically
achieve higher accuracy, it introduces more overhead, indicating that the efficiency of each recipe
must also be evaluated. To assess the performanceefficiency trade-off of different SWAA configurations, we evaluate time-to-first-token (TTFT), timeper-output-token (TPOT), total throughput, and average running time per request. Concretely, we
benchmark Qwen3-4B-Thinking on a single H100
GPU using vLLM’s bench_serve utility (Kwon
et al., 2023) with random input data and 100 total
requests. The prompt length and output length are
set to 128k and 512 tokens, respectively, representing a typical long-context QA setting.



7


```
### Figure - Accuracy vs. Time with SFT
![Accuracy graph](sliding_window_attention.pdf-7-0.png)

**Caption:**  
The figure shows the accuracy of a model with and without Sliding Factor Token (SFT) over time. Different configurations of window size, FA layers, and decode settings are compared.

**Description:**
- The x-axis represents time in seconds, ranging from 0.5 to 3.5 seconds.
- The y-axis represents accuracy in percentage.
- Two lines are plotted: a solid line represents the accuracy with SFT, and a dashed line represents the accuracy without SFT.
- Different colored markers indicate different configurations of the model:
  - Blue markers represent ""Full"" window, 0 FA layers, and False decode.
  - Green markers represent ""2k"" window, 0 FA layers, and False decode.
  - Orange markers represent ""2k"" window, 0 FA layers, and True decode.
  - Pink markers represent ""2k"" window, 1/2 FA layers, and False decode.
  - Light blue markers represent ""2k"" window, 1/2 FA layers, and True decode.
- The accuracy with SFT generally increases with time for all configurations.
- The accuracy without SFT is consistently lower than the accuracy with SFT.

```
### Figure - Accuracy vs. Time with and without SFT
![Accuracy graph](sliding_window_attention.pdf-7-1.png)

**Caption:**  
The figure shows the accuracy of a model with and without Sliding Factor Token (SFT) as a function of time window. Different configurations of FA layers and decode settings are also shown.

**Description:**
- The graph plots accuracy (%) on the y-axis against time (s) on the x-axis.
- Data points are color-coded to represent different configurations: w/ SFT (blue) and w/o SFT (black).
- Each color represents a specific combination of window size, FA layers, FA decode, and decode setting.
- The blue line represents the performance with SFT, while the black line represents the performance without SFT.
- The x-axis ranges from 0.5s to 3.5s, indicating the time window used for analysis.
- The y-axis ranges from 20% to 60%, representing the accuracy percentage.

(a) Qwen3-4B-Thinking (b) Qwen3-4B-Instruct


Figure 2: Accuracy and inference time of each configuration of Qwen3-4B on LongMemEval



To visualize the performance-efficiency tradeoff, Figure 2 plots each configuration’s accuracy
on LongMemEval_24k (Wu et al., 2024) against its
average running time, while detailed TTFT, TPOT,
and throughput statistics for each configuration are
provided in Appendix E. We draw a line between
the full-attention point and the naive-SWA point
as a baseline curve: configurations above this line
offer a better accuracy-latency balance intuitively.
For configurations with nearly identical time costs,
we display only the one with the highest accuracy.
Since **Keep First** _k_ has negligible impact on runtime (Appendix E), all plotted configurations fix
_k_ = 10.


From Figure 2, we observe that many configurations in Figure 2 achieve a clearly better
performance-efficiency ratio than baselines. And
for the thinking model, more points lie above the
baseline curve compared to non-thinking, indicating that **CoT** generally has a positive effect on improving the performance-efficiency ratio of SWAA.


Thus, we finally answer RQ3: many SWAA
configurations all reach excellent performanceefficiency trade-off, but there is no single metric to
quantify such trade-off to decide the globally optimal one. We therefore summarize **recommended**

**SWA adaptation recipes** tailored to various deployment scenarios in Table 3. And we must note
that specific parameters should be flexibly set to
meet application-specific requirements, without the
need to follow our experimental parameters (e.g.,



a 2k window, _k_ = 10). For example, you can increase the window size to 4k or _k_ to 128 for higher
accuracy and acceptable additional overhead.


**6** **Conclusion**


In this work, we validate the feasibility of adapting
full-attention pretrained LLMs to Sliding Window
Attention (SWA) for better efficiency, offering a
cost-effective alternative that avoids training sparseattention models from scratch. By systematically
deconstructing the adaptation process, we identify that the catastrophic degradation observed in
naive implementations can be effectively mitigated
through synergistic combinations of auxiliary methods. Our extensive experiments across the Qwen
and Llama families demonstrate that while trade
offs between computational overhead and model
performance are inevitable, optimized configurations can get an excellent performance-efficiency
balance.


**7** **Limitations**


We speculate that the ideal reasoning trajectory of
the model adapted to SWA should be longer than
the original model, to compensate for the information loss caused by SWA. That means, using
the answers generated by the original model as
fine-tuning data may not be the optimal training
method. Rather, RL methods like GRPO (Shao
et al., 2024) might further help the model adapted
to SWA learn a better reasoning trajectory. How


8


ever, we did not experiment with them since they
are too time-consuming and unstable.
We have not yet implemented the KV cache eviction (or overwriting) mechanism when using SWA;
that is, although the speed is improved, memory
usage is not effectively reduced.
Further experiments may be needed to confirm
whether our conclusions generalize to larger model
sizes, such as 70B.


**References**


Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei
Hou, Jie Tang, Yuxiao Dong, and Juanzi Li. 2024a.
[Longalign: A recipe for long context alignment of](https://arxiv.org/abs/2401.18058)
[large language models.](https://arxiv.org/abs/2401.18058)


Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu,
Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao
Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang,
[and Juanzi Li. 2023. Longbench: A bilingual, multi-](https://arxiv.org/abs/2308.14508)
[task benchmark for long context understanding.](https://arxiv.org/abs/2308.14508)

---

[Document 3]
Source: sliding_window_attention_annotated_chunks_2k.jsonl
Chunk ID: sliding_window_attention_annotated_chunks_2k_0
Content: ## **Sliding Window Attention Adaptation**

Yijiong Yu [a], Jiale Liu [b], Qingyun Wu [b], Huazheng Wang [a], and Ji Pei [c]


aOregon State University, {yuyiji, huazheng.wang}@oregonstate.edu
bPenn State University, {jiale.liu, qingyun.wu}@psu.edu
cDeepSolution, research@deepsolution.chat



**Abstract**


The self-attention mechanism in Transformer
based Large Language Models (LLMs) scales
quadratically with input length, making longcontext inference expensive. Sliding window attention (SWA) reduces this cost to linear complexity, but naively enabling complete
SWA at inference-time for models pretrained
with full attention (FA) causes severe longcontext performance degradation due to training–inference mismatch. This makes us wonder: _Can FA-pretrained LLMs be well adapted_
_to SWA without pretraining?_ We investigate
this by proposing Sliding Window Attention
Adaptation (SWAA), a set of practical recipes
that combine five methods for better adaptation: (i) applying SWA only during prefilling;
(ii) preserving “sink” tokens; (iii) interleaving
FA/SWA layers; (iv) chain-of-thought (CoT);
and (v) fine-tuning. Our experiments show that
SWA adaptation is feasible while non-trivial:
no single method suffices, yet specific synergistic combinations effectively recover the original
long-context performance. We further analyze
the performance-efficiency trade-offs of different SWAA configurations and provide recommended recipes for diverse scenarios. Our code
[is available at github.](https://github.com/yuyijiong/sliding-window-attention-adaptation)


**1** **Introduction**


Transformer-based Large Language Models
(LLMs) (Vaswani et al., 2017) demonstrate
remarkable capabilities, but their self-attention
scales quadratically with the input sequence length,
making long context processing inefficient. Sliding
Window Attention (SWA), the most straightforward and widely adopted sparse attention
pattern, which restricts each token’s attention to a
fixed-size local window, reduces the computational
complexity to linearity, along with some other
benefits (see Appendix A).
To apply SWA to LLMs, typical solutions involve training a model with SWA from scratch, but



are prohibitively costly and cannot match the performance of state-of-the-art full-causal-attention

models like Qwen3 (Team, 2025b), mainly due
to the inability to reproduce pretraining data.
Training-free methods like streaming attention
(Xiao et al., 2024) can stabilize LLM outputs by retaining “sink tokens” while applying SWA, which
greatly improve efficiency but inevitably suffer
from severe long-context performance degradation
possibly due to the inaccessibility of distant tokens’
information (Xiao, 2025). This motivates a critical,
unexplored question: _Can a full-attention model_
_be adapted to sliding window attention at low cost_
_while maintaining long-context performance?_
We answer Yes to this question by proposing
Sliding Window Attention Adaptation(SWAA), a
set of recipes for adapting FA-pretrained models to
SWA, which requires neither costly pretraining nor
modifications to the standard Transformer architecture. Specifically, it systematically combines five
practical and composable methods:


1. **Full Attention (FA) Decode** : applying SWA
only during the prefilling stage while switching back to full attention for decoding.


2. **Keep First** _k_ **Tokens** : explicitly preserving
attention to the first _k_ “sink” tokens.


3. **Interleaving FA/SWA layers** : mix fullattention and SWA layers (e.g., assigning
SWA to half layers).


4. **Chain-of-Thought (CoT)** : enforcing an explicit ""thinking"" process during decoding.


5. **Fine-tuning with SWA** : lightweight SWAaware supervised fine-tuning on long-context
data.


Among these, FA Decode is a novel method we
introduce. Keep First _k_ Tokens and FA/SWA Interleaving have been proven effective in prior work



1


(Xiao et al., 2024; Team, 2024a; Zhang et al., 2024),
while CoT and fine-tuning are common LLM techniques. However, how these methods should be
combined to be actually effective for SWA adaptation remains unexplored.
Therefore, in our experiments, we evaluate
SWAA on Qwen3 (Team, 2025b) and Llama3.1
(Team, 2024b) across several long-context benchmarks, measuring both performance and efficiency
under a wide range of SWAA recipes. First, we
find that each method makes a distinct contribution,
but no single ingredient suffices to make SWA competitive with full attention. Second, we show that
specific synergistic combinations of methods can
recover a large fraction of the original long-context
performance. Third, we analyze the performanceefficiency trade-offs of different SWAA recipes and
identify some recommended configurations suitable for different deployment scenarios.
Rather than proposing a single globally optimal
configuration, we view SWAA as a flexible toolkit
of practical recipes: practitioners can select SWAA
recipes that match their accuracy and efficiency
constraints, or compose their own SWA adaptation
strategies by combining the available ingredients.
Our key contributions are:


1. We perform the first systematic study on adapting FA-pretrained LLMs to SWA without pretraining, revealing novel insights about how
SWA impacts LLMs and providing a foundation for future research in efficient sparse
attention.


2. We propose SWAA, a set of practical
SWA adaptation recipes that offer a robust
performance-efficiency balance for various
use cases, accelerating LLM inference from
the bottom level.


3. We implement our methods with FlashAttention (Dao, 2024) and vLLM (Kwon
et al., 2023), making it plug-and-play and userfriendly for practical deployment.


**2** **Related Works**


The _O_ ( _N_ [2] ) complexity of self-attention in Transformers (Vaswani et al., 2017) has spurred a wide
field of research about more efficient language
model architectures. Among the two most popular technological routes are sparse attention and
linear attention.



**2.1** **Sparse Attention**


Our work falls in this category. Sliding Window
Attention (SWA) represents the most basic form
of local sparse attention, yet its performance is inherently limited. Therefore, model architectures
such as Longformer (Beltagy et al., 2020), BigBird
(Zaheer et al., 2020), and RATTENTION (Wang
et al., 2025) combine local SWA on most tokens
with special global attention on specific tokens to
create a more powerful, albeit still sparse, pattern.
Popular LLMs like Gemma2 (Team, 2024a) adopt
SWA in half of their layers to balance the efficiency
of SWA and peformance of FA. Sliding Window
Attention Training (SWAT) (Fu et al., 2025b) introduces architectural changes, such as sigmoid
activation and balanced position embeddings, to
stabilize SWA performance. More advanced methods like Deepseek-sparse-attention (Yuan et al.,
2025; DeepSeek-AI, 2025b), although achieving
excellent quality, involve more complicated implementation and optimization due to semantic-aware
attention operations (e.g., selecting the most important tokens based on attention weights). Regardless,
almost all of the above methods require pretraining with a specific sparse pattern, which is costly
and fails to leverage the advantages of existing pretrained models.


LightTransfer (Zhang et al., 2024) is a promising
attempt at adapting existing models to SWA without pretraining, which has the same motivation as
ours. But it may generalize poorly across model
families (see Appendix G).


**2.2** **Linear Attention**


An alternative approach involves reformulating
the attention mechanism entirely to achieve linear, _O_ ( _N_ ), complexity. This includes methods
such as RNN-like linear attention transformers
","The provided context states that ""FA Decode"" is a method within Sliding Window Attention Adaptation (SWAA) that contributes to improving efficiency, particularly by applying SWA during the prefilling stage and switching back to full attention for decoding.

Regarding the ""KV cache,"" the context mentions a limitation: ""We have not yet implemented the KV cache eviction (or overwriting) mechanism when using SWA; that is, although the speed is improved, memory usage is not effectively reduced.""

Therefore, while FA Decode is a method aimed at improving SWAA's efficiency (including speed), the current implementation of SWAA (which includes FA Decode) does not effectively reduce memory usage because the KV cache eviction mechanism has not been implemented. The context does not detail a direct relationship between the KV cache and FA Decode's efficiency beyond this general limitation for SWA's memory management.",113,7808,7922,,3,,2026-01-02T20:15:00.340296
"What relationship exists between ""GPT-4.1"" and the ""CogVision"" dataset construction process?",Entity Relationship Reasoning,"[Document 1]
Source: attention_functional_roles_raw_chunks_2k.jsonl
Chunk ID: attention_functional_roles_raw_chunks_2k_1
Content: Furthermore, we validate the functional importance of these heads by showing that their removal
degrades performance on complex tasks and leads to specific error patterns, while their enhancement improves reasoning capabilities. Our findings provide compelling evidence that these attention
heads play a critical role in multimodal reasoning. This insight not only deepens our understanding of the internal organization of VLMs but also suggests potential avenues for designing more
interpretable and cognitive-inspired multimodal AI systems.


2 COGVISION


In this section, we present our dataset Cognitive Vision (CogVision) that contains cognitive process
in multimodal reasoning. CogVision contains 1,409 main questions and 5,744 subquestions. Each


2


example comprises a main question and answer, its subquestions and subanswers and an annotation
specifying the receptive or cognitive function required for each subquestion.


2.1 COGNITIVE FUNCTIONS


To systematically capture the cognitive processes involved in complex reasoning tasks, we consider eight functions related to complex multimodal reasoning. These functions are inspired by
established frameworks in cognitive science (Anderson, 2014; Diamond, 2013), which highlight the
importance of perception, working memory, and reasoning in human cognition.


The functions related to vision include:


    - **Low-level Visual Reception** : Recognizing basic visual features such as color, shape, size,
position, motion.

    - **High-level Visual Reception** : Integrating visual information to recognize objects, patterns,
and scene structure.

    - **Visual Knowledge Recall** : Applying long-term visual knowledge related to visual concepts and their properties.


The functions related to language include:


    - **Language Information Extraction and Understanding** : locating and understanding relevant information from an external source or prior context.

    - **Language Knowledge Recall** : Accessing domain-specific knowledge without visual input.

    - **Math Reasoning** : Performing counting, arithmetic, comparison, and logic-based operations.

    - **Inference** : deriving implicit information that is not directly stated.

    - **Decision-Making** : Selecting the best outcome among alternatives based on reasoning.


This categorization reflects a natural progression from basic information processing to complex cognitive integration. Both the human brain and VLMs encompass a wide range of functional modules.
Our focus in this work is specifically on reasoning-related cognitive functions. By identifying and
organizing these eight core reasoning functions, we can more clearly examine how VLMs handle
different types of thinking steps, in a way that is both systematic and easy to interpret. Detail descriptions of each function and examples in CogVision can be found in Appendix A.1.


2.2 DATA COLLECTIONS


Based on our categorization of reasoning functions, we sampled 2000 diverse questions from existing commonly used visual reasoning benchmarks, selecting 200 examples each from ClevrMath (Lindstr¨om & Abraham, 2022), MathVision (Wang et al., 2024a), MathVista (Lu et al., 2024b),
MMMU (Yue et al., 2024), ScienceQA (Saikh et al., 2022), OKVQA (Marino et al., 2019) (400
examples), VCR (Zellers et al., 2019), VisuLogic (Xu et al., 2025) and Super-Clevr (Li et al.,
2023b) datasets. These datasets span a range of reasoning problems, including logical, mathematical, and commonsense reasoning. Using the chain-of-thought (CoT) paradigm, we prompted
GPT-4.1 (Achiam et al., 2023) to decompose each main question (with its final answer) into subquestions, each targeting a subanswer and a specific function. The prompt encourages structured,
step-by-step reasoning, ensuring that each subquestion is clear, answerable, and sequentially dependent. This process yields a set of subquestion–answer–function (subQAF) triples for each QA pair:
subQAFs = _{_ ( _qi, ai, fi_ ) _}_ _[k]_ _i_ =1 [, where each contains a subquestion] _[ q][i]_ [, its concise answer] _[ a][i]_ [, and the]
corresponding function label _fi_ . The prompt for generating subquestions are list in Appendix A.6.


2.3 DATA FILTERING AND ANNOTATION


Recent advances enable leveraging large pre-trained models for dataset construction, thanks to their
reasoning capabilities and ability to generate high-quality annotations at scale (Wang et al., 2024b).
While our dataset is automatically generated using a VLM to minimize manual effort, we employ a
rigorous two-stage human verification pipeline to ensure data quality and mitigate hallucination. In
the first stage, three expert annotator independently evaluate whether each subquestion is logically
structured and consistent with natural human reasoning. QA pairs with incoherent or inconsistent


3


decompositions are moved out. In the second stage, annotators verify and, if necessary, relabel the
cognitive function associated with each subquestion to ensure accurate alignment with the intended
mental process. Subanswers are further cross-checked with GPT-o3 (OpenAI, 2024) and adjudicated
by humans in cases of disagreement (details in Appendix A.5). This multi-step verification ensures
that each retained subQAF triplet represents a coherent, interpretable reasoning step grounded in
core cognitive functions. After this refinement, our final dataset comprises 1,409 main QA pairs
and 5,744 validated subQAF triplets. The detailed statistics of the dataset including the number of
triplet in training and testing set and distributions of each function can be found in Appendix A.3.
We analyze in-group subquestion diversity, and the results in the Appendix A.2 show that the eight
functions exhibit broad and overlapping distributions in phrasing patterns and token lengths, indicating no systematic surface-form differences.


3 DETECTION OF COGNITIVE FUNCTIONS


Using the CogVision dataset, we adopt a probing-based framework (Alain & Bengio, 2016; Belinkov, 2022; Tenney et al., 2019) to identify which attention heads in VLMs are associated with
specific functions in reasoning process. Specifically, for each functional annotated subquestion, we
extract head activations (see Subsection 3.1), train classifiers and compute accuracies to identify
contributing heads (see Subsection 3.2). Unlike prior work focusing on a single label, our formulation captures many-to-many relationships between heads and cognitive functions, enabling a more
nuanced analysis of functional specialization and overlap within the model.


3.1 HEAD FEATURE EXTRACTION


Given a large VLM _M_, we generate an answer _a_ _[M]_ _i_ for each subquestion _qi_ derived from a main
question _Qi_ . To support coherent multi-step reasoning, we include preceding subquestions and
their answers as contextual input, emulating the incremental reasoning process observed in human
cognition.


During inference, each input token is first mapped into an embedding and then propagated through
the transformer’s layers. At each layer, attention and feedforward operations update the residual
stream, which is ultimately decoded into token predictions. For each generated token _i_, we extract
attention head outputs _Xi_ = _{x_ _[m]_ _l_ _| l ∈_ 1 _, . . ., Nl, m ∈_ 1 _, . . ., Nh}_ across all layers, where _x_ _[m]_ _l_
denotes the value vector from the _m_ -th head in layer _l_ projected into the residual stream, with _Nh_
the number of heads per layer and _Nl_ the total number of layers.


Let _Nt_ denote the number of tokens in the generated answer _a_ _[M]_ _i_ [. To isolate semantically informative]
content relevant to reasoning, we select the top- _k_ most important tokens, determined by prompting
Qwen3-30B LLM (Yang et al., 2025), yielding an index set _Ik_ with _|Ik|_ = _k_ . For each index
_j ∈Ik_, we extract the corresponding attention head activations _Xj_, and compute the averaged
activation feature for the _m_ -th head in layer _l_ as ¯ _x_ _[m]_ _l_ = _k_ [1] � _j ∈Ikxml_ [. This results in a full set of]

---

[Document 2]
Source: attention_functional_roles_raw_chunks_2k.jsonl
Chunk ID: attention_functional_roles_raw_chunks_2k_8
Content: An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu,
Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. _arXiv preprint_
_arXiv:2505.09388_, 2025.


Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens,
Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In _Proceedings of the IEEE/CVF_
_Conference on Computer Vision and Pattern Recognition_, pp. 9556–9567, 2024.


Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition: Visual
commonsense reasoning. In _Proceedings of the IEEE Conference on Computer Vision and Pattern_
_Recognition_, pp. 6720–6731, 2019.


Z Zheng, Y Wang, Y Huang, S Song, M Yang, B Tang, F Xiong, and Z Li. Attention heads of large
language models: A survey. arxiv 2024. _arXiv preprint arXiv:2409.03752_ .


Zhenhong Zhou, Haiyang Yu, Xinghua Zhang, Rongwu Xu, Fei Huang, Kun Wang, Yang Liu,
Junfeng Fang, and Yongbin Li. On the role of attention heads in large language model safety.
_arXiv preprint arXiv:2410.13708_, 2024.


Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. _arXiv preprint_
_arXiv:2304.10592_, 2023.


Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen
Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for
open-source multimodal models. _arXiv preprint arXiv:2504.10479_, 2025.


A APPENDIX


A.1 COGVISION FUNCTION DETAILS AND EXAMPLES


**Language Information Extraction and Understanding:** The ability to comprehend and extract
meaning from only language, including understanding word relationships, sentence structures, context, and intent within a given textual input.


13


**Low-Level Vision Reception:** The perception and interpretation of visual content, including recognizing low-level visual features such as number, color, shape, size and position.


**High-Level Vision Reception:** The perception and interpretation of visual content, including recognizing high-level visual features such as object recognition, the relationships, motion, spatial
arrangement, and scene-level understanding.


**Vision Knowledge Recall:** The access and application of long-term visual knowledge, such as
recognizing familiar objects, understanding occlusion, symmetry, physical structure, and part-whole
relationships (e.g., ”a cat has a tail”).


**Language Knowledge Recall:** The access and application of long-term domain-specific textual
knowledge, such as factual knowledge from science, history, or everyday concepts stored in memory.


**Math Reasoning:** The application of mathematical concepts and operations such as counting, comparison, arithmetic, and pattern-based quantitative reasoning.


**Inference:** The logical derivation of conclusions from given information, including deductive (guaranteed) reasoning and abductive (plausible) reasoning.


**Decision-Making:** The process of selecting the most appropriate option or answer based on prior
reasoning, evaluation of evidence, or predefined objectives.


Table 6 and Table 7 presents illustrative examples from the CogVision dataset. The main question
and its corresponding answer are taken from the original dataset. Based on an analysis of the main
question, a sequence of sub-questions, their answers, and associated cognitive function labels are
generated in order.


A.2 ANALYSES OF SURFACE-FORM VARIATION ACROSS COGNITIVE-FUNCTION GROUPS


We analyze the surface-form variation across cognitive-function groups. As shown in Figures 6
and 7, the eight functions exhibit wide and overlapping distributions in phrasing patterns and token
lengths, indicating no systematic surface-form differences. These results support that the cognitive
groups are not determined by trivial lexical or structural artifacts. About modality, as described
in Section 2.1, some functions (Low-level Visual Reception, High-level Visual Reception, Visual
Knowledge Recall) naturally involve vision, while others relate primarily to language. This modality
tendency is intrinsic to the underlying cognitive processes rather than an artifact of the pipeline.


Figure 6: Histogram of token length distribution for 8 functions.


14



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-13-0.png)
![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-14-0.png)

Figure 7: Word cloud distribution for 8 functions.


Table 6: One example from the CogVision dataset showing a main question, its final answer, and a
breakdown into subquestions with answers and their corresponding cognitive function labels.


**Example 1:**


**Main Question** In a case-control study, the results were shown in the table below. The OR was:
Choose one option from the following: A: 18 B: 16 C: 20D: 10


**Answer** B


**Subquestion** **Answer** **Cognitive Label**



1. What are the values in the 2x2

table for cases and controls with
and without a history of exposure
in the image?


2. What is the standard formula
to calculate the odds ratio (OR)
in a case-control 2x2 table?


3. What is the odds ratio (OR)
when you substitute the identified values into the formula?


4. Which option corresponds to
the calculated odds ratio?



Cases with exposure: 400, Cases without exposure: 100, Controls with exposure: 100,
Controls without exposure: 400


OR = (a*d) / (b*c), where a = cases with exposure, b = controls with exposure, c = cases
without exposure, d = controls without expo
sure


(400*400)/(100*100) = 160,000/10,000 =
16



B: 16 Decision-Making


15



High-Level Vision
Reception


Language Knowledge
Recall


Math Reasoning



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-14-1.png)
Table 7: One example from the CogQA dataset showing a main question, its final answer, and a
breakdown into subquestions with answers and their corresponding cognitive function labels.


**Example 2:**


**Main Question** How can you tell that this is a prokaryote or eukaryote cell? Choose one option
from the following: A: It is a prokaryote because it doesn’t have a nucleus B: It is a
eukaryotic cell because it has a cell wall and a nucleus C: It is eukaryotic because
it does not a nucleus nor a cell membrane D: It is prokaryote because it has a cell
wall


**Answer** B.


**Subquestion** **Answer** **Cognitive Label**



1. What visible features can be
observed in the cell image, such
as the presence of boundaries or
internal structures?


2. What do the observed
features (rectangular shape, defined boundaries, distinct internal
spots) correspond to in cellular
biology?


3. What cellular structures differentiate prokaryotic cells from eukaryotic cells?


4. Based on the cell’s visible
features and the definitions of
prokaryotic and eukaryotic cells,
which type of cell is shown in the
image?


A.3 COGVISION STATISTICS



Rectangular cells with defined boundaries
and distinct internal spots are visible.


Defined boundaries correspond to cell walls;
internal spots correspond to nuclei.

---

[Document 3]
Source: attention_functional_roles_raw_chunks_2k.jsonl
Chunk ID: attention_functional_roles_raw_chunks_2k_9
Content: **Answer** B.


**Subquestion** **Answer** **Cognitive Label**



1. What visible features can be
observed in the cell image, such
as the presence of boundaries or
internal structures?


2. What do the observed
features (rectangular shape, defined boundaries, distinct internal
spots) correspond to in cellular
biology?


3. What cellular structures differentiate prokaryotic cells from eukaryotic cells?


4. Based on the cell’s visible
features and the definitions of
prokaryotic and eukaryotic cells,
which type of cell is shown in the
image?


A.3 COGVISION STATISTICS



Rectangular cells with defined boundaries
and distinct internal spots are visible.


Defined boundaries correspond to cell walls;
internal spots correspond to nuclei.


Eukaryotic cells have nuclei and may have
cell walls; prokaryotic cells lack nuclei.



Low-Level Vision Reception


Vision Knowledge
Recall


Language Knowledge
Recall



B: It is a eukaryotic cell. Decision-Making



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-15-0.png)

The statistics for the CogVision dataset is shown in Table 8


16


Table 8: Dataset Statistics


**Metric** **Training** **Testing**


Main Questions 1,124 285
Sub-questions 4,603 1,141


**Cognitive Skills Distribution**


High-Level Vision Reception 1,262 (27.42%) 336 (29.45%)
Math Reasoning 570 (12.38%) 125 (10.96%)
Semantic Understanding 545 (11.84%) 152 (13.32%)
Inference 544 (11.82%) 126 (11.04%)
Decision-Making 454 (9.86%) 111 (9.73%)
Language Knowledge Recall 424 (9.21%) 102 (8.94%)
Vision Knowledge Recall 403 (8.76%) 96 (8.41%)
Low-Level Vision Reception 401 (8.71%) 92 (8.06%)


A.4 THE COGNITIVE FUNCTION DISTRIBUTION OF OTHER MODELS


We present the heatmaps for the remaining five models in this subsection. The results reveal a
notable universality in the sparsity patterns of attention heads across different architectures.


Figure 8: The existence of cognitive heads in Qwen2.5-VL-3B responsible for eight distinct functions in complex reasoning tasks. The x-axis represents the head index, while the y-axis indicates
the layer index.


17



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-16-0.png)
![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-17-0.png)

Figure 9: InternVL3-2B Heatmap


Figure 10: InternVL3-8B Heatmap


18



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-17-1.png)
![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-18-0.png)

Figure 11: gemma-3n-e2b-it Heatmap


Figure 12: gemma-3n-e4b-it Heatmap


A.5 ANNOTATIONS


To ensure the quality and reliability of the decomposed subQAF triplets in the CogVision dataset,
we design a rigorous multi-stage annotation pipeline, combining expert review and model-based
verification. The goal is to verify the logical validity of subquestions, the correctness of their associated cognitive function labels, and the accuracy of the answers. Notably, for each subquestion, we
aim to align it with a single primary cognitive function. However, certain queries—such as “What is
the solvent volume and how many particles in each solution?”—may involve multiple abilities (e.g.,
object recognition and counting). In such cases, we assign the subquestion to its dominant function
in CogVision.


19



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-18-1.png)
**Stage 1: Validating Subquestion Decomposition** In the first stage, we evaluate whether the generated subquestions are logically sound and align with natural human reasoning. For each QA pair,
three expert annotators (with backgrounds in linguistics or cognitive science) independently assess
the validity of each subquestion. A subquestion is marked true if it meaningfully contributes to
answering the main question and follows a logical reasoning trajectory. Otherwise, it is marked
false.


We apply the following filtering criteria:


    - **AI-Human Agreement** : If any annotator considers fewer than 60% of the subquestions
valid, the entire QA decomposition is discarded.

    - **Inter-Annotator Agreement** : A subquestion is deemed invalid if at least two annotators
mark it as false. If over 40% of the subquestions in a QA pair are invalid under this rule,
the whole QA pair is removed.


This filtering ensures that the retained QA decompositions follow coherent, cognitively plausible
reasoning chains.


**Stage 2: Verifying Cognitive Function Labels** In the second stage, annotators evaluate the correctness of the function label _fi_ assigned to each subQAF triplet ( _qi, ai, fi_ ). Three annotators independently mark each label as true or false. When discrepancies occur, annotators collaboratively reassign the correct cognitive label to ensure alignment with the underlying mental operation.


This step ensures that the categorization of subquestions accurately reflects established distinctions
between information retrieval, semantic understanding, logical reasoning, and other cognitive pro
cesses.


**Stage 3: Answer Verification via Model and Human Review** In the final stage, we verify the
correctness of each answer _ai_ using both automated and manual procedures. We employ the GPTo3 model (OpenAI, 2024), known for its logical reasoning capabilities, to re-evaluate GPT-4.1generated answers, and approximately 38.78% were found to be in disagreement. If GPT-o3 disagrees with GPT-4.1, it provides an alternative answer. A human annotator then compares both
answers and resolves discrepancies by supplying the correct one when necessary. Given the generally objective nature of answers, only one annotator is required for this task.


**Annotation Outcome** Following this multi-stage process, we retain 1,409 validated QA pairs,
yielding a total of 5,744 high-quality subQAF triplets.


A.6 PROMPT FOR GENERATING COGVISION


We decompose the main question into subquestions through a two-step process: first, we prompt
GPT-4.1 to generate a chain-of-thought (CoT) for the main question; second, we use the main question together with the CoT to guide the model in generating subquestions.


Here is the prompt for generating subquestions:


20



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-19-0.png)
![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-20-0.png)



21


A.7 PROMPT FOR QUESTION ASKING



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-21-0.png)

A.8 ABLATION STUDY


In the main experiments, we use the top-k generated tokens and average their multi-head attention
vectors. Here are some examples of selected top-k tokens by llm:



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-21-1.png)
","GPT-4.1 was used in the data collection process for the CogVision dataset. Specifically, it was prompted to:
1. Generate a chain-of-thought (CoT) for each main question.
2. Decompose each main question (along with its final answer and the generated CoT) into subquestions, subanswers, and assign a specific cognitive function label to each, thereby yielding subquestion-answer-function (subQAF) triples.",110,3951,4062,,3,,2026-01-02T20:15:04.403125
"What specific ""activation function"" is used in the ""Feed Forward Network"" of the Transformer?",Entity-Specific Attribute Retrieval,"[Document 1]
Source: attention_is_all_you_need_raw_chunks_2k.jsonl
Chunk ID: attention_is_all_you_need_raw_chunks_2k_1
Content: [10], consuming the previously generated symbols as additional input when generating the next.


2


![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_is_all_you_need.pdf-2-0.png)

Figure 1: The Transformer - model architecture.


The Transformer follows this overall architecture using stacked self-attention and point-wise, fully
connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,
respectively.


**3.1** **Encoder and Decoder Stacks**


**Encoder:** The encoder is composed of a stack of _N_ = 6 identical layers. Each layer has two
sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network. We employ a residual connection [11] around each of
the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is
LayerNorm( _x_ + Sublayer( _x_ )), where Sublayer( _x_ ) is the function implemented by the sub-layer
itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding
layers, produce outputs of dimension _d_ model = 512.


**Decoder:** The decoder is also composed of a stack of _N_ = 6 identical layers. In addition to the two
sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head
attention over the output of the encoder stack. Similar to the encoder, we employ residual connections
around each of the sub-layers, followed by layer normalization. We also modify the self-attention
sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This
masking, combined with fact that the output embeddings are offset by one position, ensures that the
predictions for position _i_ can depend only on the known outputs at positions less than _i_ .


**3.2** **Attention**


An attention function can be described as mapping a query and a set of key-value pairs to an output,
where the query, keys, values, and output are all vectors. The output is computed as a weighted sum


3


Scaled Dot-Product Attention Multi-Head Attention


Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several
attention layers running in parallel.


of the values, where the weight assigned to each value is computed by a compatibility function of the
query with the corresponding key.


**3.2.1** **Scaled Dot-Product Attention**


We call our particular attention ""Scaled Dot-Product Attention"" (Figure 2). The input consists of
queries and keys of dimension _dk_, and values of dimension _dv_ . We compute the dot products of the
query with all keys, divide each by _[√]_ _dk_, and apply a softmax function to obtain the weights on the
values.


In practice, we compute the attention function on a set of queries simultaneously, packed together
into a matrix _Q_ . The keys and values are also packed together into matrices _K_ and _V_ . We compute
the matrix of outputs as:


Attention( _Q, K, V_ ) = softmax( _[Q][K]_ _[T]_ ) _V_ (1)

~~_√_~~ _dk_


The two most commonly used attention functions are additive attention [2], and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor
of ~~_√_~~ 1 _dk_ . Additive attention computes the compatibility function using a feed-forward network with
a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is
much faster and more space-efficient in practice, since it can be implemented using highly optimized
matrix multiplication code.


While for small values of _dk_ the two mechanisms perform similarly, additive attention outperforms
dot product attention without scaling for larger values of _dk_ [3]. We suspect that for large values of
_dk_, the dot products grow large in magnitude, pushing the softmax function into regions where it has
extremely small gradients [4] . To counteract this effect, we scale the dot products by ~~_√_~~ 1 _dk_ .


**3.2.2** **Multi-Head Attention**


Instead of performing a single attention function with _d_ model-dimensional keys, values and queries,
we found it beneficial to linearly project the queries, keys and values _h_ times with different, learned
linear projections to _dk_, _dk_ and _dv_ dimensions, respectively. On each of these projected versions of
queries, keys and values we then perform the attention function in parallel, yielding _dv_ -dimensional


4To illustrate why the dot products get large, assume that the components of _q_ and _k_ are independent random
variables with mean 0 and variance 1. Then their dot product, _q · k_ = [�] _[d]_ _i_ =1 _[k]_ _[q][i][k][i]_ [, has mean][ 0][ and variance] _[ d][k]_ [.]


4



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_is_all_you_need.pdf-3-0.png)

![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_is_all_you_need.pdf-3-1.png)
output values. These are concatenated and once again projected, resulting in the final values, as
depicted in Figure 2.


Multi-head attention allows the model to jointly attend to information from different representation
subspaces at different positions. With a single attention head, averaging inhibits this.


MultiHead( _Q, K, V_ ) = Concat(head1 _, ...,_ headh) _W_ _[O]_

where headi = Attention( _QWi_ _[Q][, KW][ K]_ _i_ _[, V W][ V]_ _i_ [)]


Where the projections are parameter matrices _Wi_ _[Q]_ _∈_ R _[d]_ [model] _[×][d][k]_, _Wi_ _[K]_ _∈_ R _[d]_ [model] _[×][d][k]_, _Wi_ _[V]_ _∈_ R _[d]_ [model] _[×][d][v]_
and _W_ _[O]_ _∈_ R _[hd][v][×][d]_ [model] .


In this work we employ _h_ = 8 parallel attention layers, or heads. For each of these we use
_dk_ = _dv_ = _d_ model _/h_ = 64. Due to the reduced dimension of each head, the total computational cost
is similar to that of single-head attention with full dimensionality.


**3.2.3** **Applications of Attention in our Model**


The Transformer uses multi-head attention in three different ways:


    - In ""encoder-decoder attention"" layers, the queries come from the previous decoder layer,
and the memory keys and values come from the output of the encoder. This allows every
position in the decoder to attend over all positions in the input sequence. This mimics the
typical encoder-decoder attention mechanisms in sequence-to-sequence models such as

[38, 2, 9].


    - The encoder contains self-attention layers. In a self-attention layer all of the keys, values
and queries come from the same place, in this case, the output of the previous layer in the
encoder. Each position in the encoder can attend to all positions in the previous layer of the
encoder.


    - Similarly, self-attention layers in the decoder allow each position in the decoder to attend to
all positions in the decoder up to and including that position. We need to prevent leftward
information flow in the decoder to preserve the auto-regressive property. We implement this
inside of scaled dot-product attention by masking out (setting to _−∞_ ) all values in the input
of the softmax which correspond to illegal connections. See Figure 2.

---

[Document 2]
Source: attention_is_all_you_need_raw_chunks_2k.jsonl
Chunk ID: attention_is_all_you_need_raw_chunks_2k_0
Content: Provided proper attribution is provided, Google hereby grants permission to
reproduce the tables and figures in this paper solely for use in journalistic or
scholarly works.

## **Attention Is All You Need**



**Niki Parmar** _[∗]_
Google Research
```
nikip@google.com

```


**Ashish Vaswani** _[∗]_
Google Brain
```
avaswani@google.com

```

**Llion Jones** _[∗]_
Google Research
```
 llion@google.com

```


**Noam Shazeer** _[∗]_
Google Brain
```
noam@google.com

```


**Jakob Uszkoreit** _[∗]_
Google Research
```
usz@google.com

```


**Aidan N. Gomez** _[∗†]_
University of Toronto
```
aidan@cs.toronto.edu

```


**Łukasz Kaiser** _[∗]_
Google Brain
```
lukaszkaiser@google.com

```


**Illia Polosukhin** _[∗‡]_

```
             illia.polosukhin@gmail.com

```

**Abstract**


The dominant sequence transduction models are based on complex recurrent or
convolutional neural networks that include an encoder and a decoder. The best
performing models also connect the encoder and decoder through an attention
mechanism. We propose a new simple network architecture, the Transformer,
based solely on attention mechanisms, dispensing with recurrence and convolutions
entirely. Experiments on two machine translation tasks show these models to
be superior in quality while being more parallelizable and requiring significantly
less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including
ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,
our model establishes a new single-model state-of-the-art BLEU score of 41.8 after
training for 3.5 days on eight GPUs, a small fraction of the training costs of the
best models from the literature. We show that the Transformer generalizes well to
other tasks by applying it successfully to English constituency parsing both with
large and limited training data.


_∗_ Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started
the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and
has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head
attention and the parameter-free position representation and became the other person involved in nearly every
detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and
tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and
efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and
implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating
our research.

_†_ Work performed while at Google Brain.

_‡_ Work performed while at Google Research.


31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.


**1** **Introduction**


Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks
in particular, have been firmly established as state of the art approaches in sequence modeling and
transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous
efforts have since continued to push the boundaries of recurrent language models and encoder-decoder
architectures [38, 24, 15].


Recurrent models typically factor computation along the symbol positions of the input and output
sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden
states _ht_, as a function of the previous hidden state _ht−_ 1 and the input for position _t_ . This inherently
sequential nature precludes parallelization within training examples, which becomes critical at longer
sequence lengths, as memory constraints limit batching across examples. Recent work has achieved
significant improvements in computational efficiency through factorization tricks [21] and conditional
computation [32], while also improving model performance in case of the latter. The fundamental
constraint of sequential computation, however, remains.


Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in
the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms
are used in conjunction with a recurrent network.


In this work we propose the Transformer, a model architecture eschewing recurrence and instead
relying entirely on an attention mechanism to draw global dependencies between input and output.
The Transformer allows for significantly more parallelization and can reach a new state of the art in
translation quality after being trained for as little as twelve hours on eight P100 GPUs.


**2** **Background**


The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU

[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building
block, computing hidden representations in parallel for all input and output positions. In these models,
the number of operations required to relate signals from two arbitrary input or output positions grows
in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes
it more difficult to learn dependencies between distant positions [12]. In the Transformer this is
reduced to a constant number of operations, albeit at the cost of reduced effective resolution due
to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as
described in section 3.2.


Self-attention, sometimes called intra-attention is an attention mechanism relating different positions
of a single sequence in order to compute a representation of the sequence. Self-attention has been
used successfully in a variety of tasks including reading comprehension, abstractive summarization,
textual entailment and learning task-independent sentence representations [4, 27, 28, 22].


End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering and
language modeling tasks [34].


To the best of our knowledge, however, the Transformer is the first transduction model relying
entirely on self-attention to compute representations of its input and output without using sequencealigned RNNs or convolution. In the following sections, we will describe the Transformer, motivate
self-attention and discuss its advantages over models such as [17, 18] and [9].


**3** **Model Architecture**


Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].
Here, the encoder maps an input sequence of symbol representations ( _x_ 1 _, ..., xn_ ) to a sequence
of continuous representations **z** = ( _z_ 1 _, ..., zn_ ). Given **z**, the decoder then generates an output
sequence ( _y_ 1 _, ..., ym_ ) of symbols one element at a time. At each step the model is auto-regressive

[10], consuming the previously generated symbols as additional input when generating the next.


2


![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_is_all_you_need.pdf-2-0.png)

Figure 1: The Transformer - model architecture.


The Transformer follows this overall architecture using stacked self-attention and point-wise, fully
connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,
respectively.


**3.1** **Encoder and Decoder Stacks**

---

[Document 3]
Source: attention_is_all_you_need_raw_chunks_2k.jsonl
Chunk ID: attention_is_all_you_need_raw_chunks_2k_2
Content: [38, 2, 9].


    - The encoder contains self-attention layers. In a self-attention layer all of the keys, values
and queries come from the same place, in this case, the output of the previous layer in the
encoder. Each position in the encoder can attend to all positions in the previous layer of the
encoder.


    - Similarly, self-attention layers in the decoder allow each position in the decoder to attend to
all positions in the decoder up to and including that position. We need to prevent leftward
information flow in the decoder to preserve the auto-regressive property. We implement this
inside of scaled dot-product attention by masking out (setting to _−∞_ ) all values in the input
of the softmax which correspond to illegal connections. See Figure 2.


**3.3** **Position-wise Feed-Forward Networks**


In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully
connected feed-forward network, which is applied to each position separately and identically. This
consists of two linear transformations with a ReLU activation in between.


FFN( _x_ ) = max(0 _, xW_ 1 + _b_ 1) _W_ 2 + _b_ 2 (2)


While the linear transformations are the same across different positions, they use different parameters
from layer to layer. Another way of describing this is as two convolutions with kernel size 1.
The dimensionality of input and output is _d_ model = 512, and the inner-layer has dimensionality
_dff_ = 2048.


**3.4** **Embeddings and Softmax**


Similarly to other sequence transduction models, we use learned embeddings to convert the input
tokens and output tokens to vectors of dimension _d_ model. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In
our model, we share the same weight matrix between the two embedding layers and the pre-softmax
linear transformation, similar to [30]. In the embedding layers, we multiply those weights by _[√]_ _d_ model.


5


Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations
for different layer types. _n_ is the sequence length, _d_ is the representation dimension, _k_ is the kernel
size of convolutions and _r_ the size of the neighborhood in restricted self-attention.


Layer Type Complexity per Layer Sequential Maximum Path Length
Operations
Self-Attention _O_ ( _n_ [2] _· d_ ) _O_ (1) _O_ (1)
Recurrent _O_ ( _n · d_ [2] ) _O_ ( _n_ ) _O_ ( _n_ )
Convolutional _O_ ( _k · n · d_ [2] ) _O_ (1) _O_ ( _logk_ ( _n_ ))
Self-Attention (restricted) _O_ ( _r · n · d_ ) _O_ (1) _O_ ( _n/r_ )


**3.5** **Positional Encoding**


Since our model contains no recurrence and no convolution, in order for the model to make use of the
order of the sequence, we must inject some information about the relative or absolute position of the
tokens in the sequence. To this end, we add ""positional encodings"" to the input embeddings at the
bottoms of the encoder and decoder stacks. The positional encodings have the same dimension _d_ model
as the embeddings, so that the two can be summed. There are many choices of positional encodings,
learned and fixed [9].


In this work, we use sine and cosine functions of different frequencies:


_PE_ ( _pos,_ 2 _i_ ) = _sin_ ( _pos/_ 10000 [2] _[i/d]_ [model] )

_PE_ ( _pos,_ 2 _i_ +1) = _cos_ ( _pos/_ 10000 [2] _[i/d]_ [model] )


where _pos_ is the position and _i_ is the dimension. That is, each dimension of the positional encoding
corresponds to a sinusoid. The wavelengths form a geometric progression from 2 _π_ to 10000 _·_ 2 _π_ . We
chose this function because we hypothesized it would allow the model to easily learn to attend by
relative positions, since for any fixed offset _k_, _PEpos_ + _k_ can be represented as a linear function of
_PEpos_ .


We also experimented with using learned positional embeddings [9] instead, and found that the two
versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version
because it may allow the model to extrapolate to sequence lengths longer than the ones encountered
during training.


**4** **Why Self-Attention**


In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations
( _x_ 1 _, ..., xn_ ) to another sequence of equal length ( _z_ 1 _, ..., zn_ ), with _xi, zi ∈_ R _[d]_, such as a hidden
layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we
consider three desiderata.


One is the total computational complexity per layer. Another is the amount of computation that can
be parallelized, as measured by the minimum number of sequential operations required.


The third is the path length between long-range dependencies in the network. Learning long-range
dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the
ability to learn such dependencies is the length of the paths forward and backward signals have to
traverse in the network. The shorter these paths between any combination of positions in the input
and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare
the maximum path length between any two input and output positions in networks composed of the
different layer types.


As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially
executed operations, whereas a recurrent layer requires _O_ ( _n_ ) sequential operations. In terms of
computational complexity, self-attention layers are faster than recurrent layers when the sequence


6


length _n_ is smaller than the representation dimensionality _d_, which is most often the case with
sentence representations used by state-of-the-art models in machine translations, such as word-piece

[38] and byte-pair [31] representations. To improve computational performance for tasks involving
very long sequences, self-attention could be restricted to considering only a neighborhood of size _r_ in
the input sequence centered around the respective output position. This would increase the maximum
path length to _O_ ( _n/r_ ). We plan to investigate this approach further in future work.


A single convolutional layer with kernel width _k < n_ does not connect all pairs of input and output
positions. Doing so requires a stack of _O_ ( _n/k_ ) convolutional layers in the case of contiguous kernels,
or _O_ ( _logk_ ( _n_ )) in the case of dilated convolutions [18], increasing the length of the longest paths
between any two positions in the network. Convolutional layers are generally more expensive than
recurrent layers, by a factor of _k_ . Separable convolutions [6], however, decrease the complexity
considerably, to _O_ ( _k · n · d_ + _n · d_ [2] ). Even with _k_ = _n_, however, the complexity of a separable
convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,
the approach we take in our model.


As side benefit, self-attention could yield more interpretable models. We inspect attention distributions
from our models and present and discuss examples in the appendix. Not only do individual attention
heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic
and semantic structure of the sentences.


**5** **Training**


This section describes the training regime for our models.


**5.1** **Training Data and Batching**
","The Feed Forward Network in the Transformer uses a ReLU (Rectified Linear Unit) activation function. This is described as ""two linear transformations with a ReLU activation in between,"" and mathematically represented as FFN( _x_ ) = max(0 _, xW_ 1 + _b_ 1) _W_ 2 + _b_ 2.",107,3048,3156,,3,,2026-01-02T20:15:07.562486
"What is the ""sampling rate"" associated with the ""PRONOSTIA dataset"" in the NAC experiments?",Entity-Specific Attribute Retrieval,"[Document 1]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_7
Content: compact architecture suitable for resource-constrained devices, thereby supporting localized safety.
We utilized three benchmark datasets: (i) PRONOSTIA
(Nectoux et al., 2012), (ii) XJTU-SY (Wang et al., 2018),
and (iii) HUST (Thuan & Hong, 2023). Training is performed on PRONOSTIA, while XJTU-SY and HUST are

used to assess cross-validation. We used the Score met
ric (Nectoux et al., 2012) to assess the performance. We
evaluate generalization using _Bearing 1_ from the first operating condition of each dataset. Figure 3 visualizes the
expected degradation alongside the outputs of NAC. On the
PRONOSTIA dataset, ContiFormer performed the best with



**3.4. Run Time and Memory Experiments**


We evaluate computational requirements on fixed-length
sequences of 1024 steps, 64-dimensional features, 4 heads,
and a Batch size of 1. Each model is run for ten for
ward passes on Google Colab T4-GPU, and we report the
mean runtime with standard deviation, throughput, and peak
memory usage. NAC occupies an intermediate position
in runtime relative to several CT-RNN models, including
GRU-ODE, CfC, and LTC. In terms of memory consumption, NAC uses significantly less memory than mTAN, with
NAC-2k being the least memory-consuming among the CTAttention models. Reducing NAC sparsity from 90% to 20%
has minimal effect on memory, decreasing usage slightly



7


**Neuronal Attention Circuit (NAC) for Representation Learning**



_Table 2._ Run-Time and Memory Benchmark Results


**Run-Time** **Throughput** **Peak Memory**
**Model**
(s) (seq/s) (MB)


RNN 1 _._ 8392 _[±]_ [0] _[.]_ [1933] 0.544 0.29
CT-RNN 7 _._ 1097 _[±]_ [0] _[.]_ [3048] 0.141 0.67
LSTM 2 _._ 6241 _[±]_ [0] _[.]_ [2906] 0.381 0.42
PhasedLSTM 4 _._ 9812 _[±]_ [0] _[.]_ [272] 0.201 0.80
GRU 3 _._ 216 _[±]_ [0] _[.]_ [2566] 0.311 0.54
GRU-ODE 12 _._ 2498 _[±]_ [0] _[.]_ [0525] 0.082 0.64
mmRNN 7 _._ 5852 _[±]_ [0] _[.]_ [2785] 0.132 0.96
LTC 14 _._ 643 _[±]_ [0] _[.]_ [2445] 0.068 0.99
CfC 6 _._ 0988 _[±]_ [0] _[.]_ [2135] 0.164 0.76


Attention 0 _._ 0016 _[±]_ [0] _[.]_ [0001] 625.00 16.86
MHA 0 _._ 0041 _[±]_ [0] _[.]_ [0001] 243.90 69.05
mTAN 0 _._ 0272 _[±]_ [0] _[.]_ [0054] 36.76 790.16
ODEFormer 0 _._ 0317 _[±]_ [0] _[.]_ [0016] 31.55 67.71
CTA 8 _._ 5275 _[±]_ [0] _[.]_ [2355] 0.117 1.43
ContiFormer 0 _._ 066 _[±]_ [0] _[.]_ [0075] 15.15 67.71


NAC-2k 7 _._ 3071 _[±]_ [0] _[.]_ [1547] 0.137 44.75
NAC-32k 7 _._ 2313 _[±]_ [0] _[.]_ [219] 0.138 549.86
NAC-PW 8 _._ 5649 _[±]_ [0] _[.]_ [0203] 0.117 5042.09
NAC-FC 0 _._ 0195 ~~_[±]_~~ ~~[0]~~ ~~_[.]_~~ ~~[0002]~~ 51.28 29.92
NAC-02s 7 _._ 252 _[±]_ [0] _[.]_ [2018] 0.138 151.54
NAC-09s 7 _._ 222 _[±]_ [0] _[.]_ [176] 0.139 150.85
NAC-Exact/05s/8k 7 _._ 4101 ~~_[±]_~~ ~~[0]~~ ~~_[.]_~~ ~~[1586]~~ 0.135 151.50
NAC-Euler 7 _._ 3367 _[±]_ [0] _[.]_ [1719] 0.136 152.22
NAC-Steady 7 _._ 2942 _[±]_ [0] _[.]_ [1451] 0.137 150.86


from 151.54 MB to 150.85 MB. In constrast, decreasing
the Top- _K_ selection from _PW_ to _k_ = 2 drastically reduces
memory consumption from 5042 MB to 44.75 MB, demonstrating the flexibility of NAC.
**Interpreting the Results:** From the experiments, we observe that increasing the sparsity of the NAC layer improves
the robustness of the system and leads to higher overall accuracy. Similarly, increasing the Top- _K_ interactions enhances
accuracy too; however, the benefits diminish as memory
consumption grows. Using Exact mode, Top- _K_ =8 with 50%
sparsity achieves the best balance between accuracy and
efficiency. Steady mode is the fastest, while Euler mode
handles adaptive temporal dynamics.


**4. Discussions**



This research is part of ongoing work on biologically plausible attention mechanisms and represents a pioneering step,
with limitations to be addressed in future work.

**Architectural improvement:** Currently, NAC uses predetermined wiring (AutoNCP) requiring three inputs: number of units (sensory + interneuron + motor), output motor
neurons, and sparsity, with typically 60% of units assigned
to sensory neurons. To integrate with the attention mechanism while preserving wiring, sensory units for _NN_ sensory
are set as unitssensory = � _d_ model0 _.−_ 6 0 _._ 5 � and backbone units as



are set as unitssensory = � _d_ model0 _.−_ 6 0 _._ 5 � and backbone units as

units _backbone_ = _d_ model + � _d_ 0model _._ 6 �, where _⌈·⌉_ and _⌊·⌋_ denote



units _backbone_ = _d_ model + � _d_ 0model _._ 6 �, where _⌈·⌉_ and _⌊·⌋_ denote

the ceiling and floor functions, respectively. This results
in a larger overall architectural size and increased runtime.



Future work will support user-defined NCPs configurations
or randomized wiring to enable more efficient architectures.
**Learnable sparse Top-** _**K**_ **selection:** Sparse Top- _K_ attention can miss important context, is sensitive to _k_, and may
be harder to optimize. A further limitation is that it still
computes the full _QK_ _[⊤]_ matrix, which can dominate the
cost for very long sequences. Future work includes adaptive
or learnable Top- _K_ selection, improved key scoring, and
hardware-aware optimization to strengthen accuracy and
robustness.


**5. Conclusion**

---

[Document 2]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_6
Content: MHA 95.94 _[±]_ [0.15] 88.36 _[±]_ [1.06] 79.99 _[±]_ [0.49] 0.0185 _[±]_ [0.0017] 45.36 _[±]_ [5.16] 37.31 _[±]_ [12.20] 41.40 _[±]_ [7.72]

mTAN 95.97 _[±]_ [0.25] 88.08 _[±]_ [0.94] 80.86 _[±]_ [0.22] 0.0178 _[±]_ [0.0005] 44.41 _[±]_ [7.15] 41.34 _[±]_ [3.72] 66.29 _[±]_ [4.25]

CTA 95.86 _[±]_ [0.14] 88.10 _[±]_ [1.10] 80.54 _[±]_ [0.40] 0.0197 _[±]_ [0.0016] 39.16 _[±]_ [3.54] **25.86** _[±]_ [1.47] 38.41 _[±]_ [4.51]

ODEFormer 95.62 _[±]_ [0.20] 88.25 _[±]_ [0.66] 80.54 _[±]_ [0.40] 0.0190 _[±]_ [0.0012] 42.42 _[±]_ [6.98] 35.63 _[±]_ [9.24] 40.60 _[±]_ [6.83]

ContiFormer **96.04** _[±]_ **[0.23]** 81.28 _[±]_ [0.85] 80.47 _[±]_ [0.50] **0.0174** _[±]_ **[0.01]** **27.82** _[±]_ **[7.09]** 34.71 _[±]_ [4.98] 43.81 _[±]_ [10.18]


NAC-2k 95.73 _[±]_ [0.07] 88.84% _[±]_ [0.81] 80.59 _[±]_ [0.46] 0.0208 _[±]_ [0.0015] 43.78 _[±]_ [2.71] 37.43 _[±]_ [9.28] 40.51 _[±]_ [6.61]

NAC-32k 95.15 _[±]_ [0.11] 88.80 _[±]_ [0.76] 80.38 _[±]_ [0.16] **0.0170** _[±]_ **[0.0007]** 49.53 _[±]_ [4.89] 32.45 _[±]_ [10.84] 39.17 _[±]_ [12.23]

NAC-PW **96.64** _[±]_ [0.12] **89.15** _[±]_ **[1.01]** **80.72** _[±]_ **[0.41]** 0.0177 _[±]_ [0.0008] **37.50** _[±]_ **[2.56]** 28.01 _[±]_ [4.93] **30.14** _[±]_ **[6.87]**


NAC-FC 95.31 _[±]_ [0.07] 88.45 _[±]_ [0.91] 80.49 _[±]_ [0.46] 0.0192 _[±]_ [0.0012] 40.36 _[±]_ [6.09] **24.89** _[±]_ **[5.30]** **35.35** _[±]_ **[6.64]**

NAC-02s 95.31 _[±]_ [0.07] 88.84 _[±]_ [1.33] 80.47 _[±]_ [0.27] 0.0188 _[±]_ [0.0013] 39.43 _[±]_ [5.94] 35.59 _[±]_ [3.86] 38.90 _[±]_ [6.43]

NAC-09s 95.86 _[±]_ [0.11] 88.61% _[±]_ [1.25] 80.43% _[±]_ [0.17] 0.0188 _[±]_ [0.0013] 47.29 _[±]_ [5.52] 40.40 _[±]_ [8.85] 44.39 _[±]_ [6.82]


NAC-Exact/05s/8k **96.12** _[±]_ [0.11] **89.01** _[±]_ **[1.01]** 80.59 _[±]_ [1.82] **0.0173** _[±]_ **[0.0006]** **37.75** _[±]_ **[4.72]** **19.87** _[±]_ **[1.75]** **27.82** _[±]_ **[7.09]**

NAC-Euler 95.67 _[±]_ [0.26] 88.52 _[±]_ [0.68] **80.61** _[±]_ [0.28] 0.0181 _[±]_ [0.0017] 42.08 _[±]_ [6.14] 28.46 _[±]_ [8.18] 39.32 _[±]_ [9.15]

NAC-Steady 95.75 _[±]_ [0.28] 88.36 _[±]_ [1.05] **80.62** _[±]_ **[0.26]** 0.0181 _[±]_ [0.0012] 40.95 _[±]_ [5.77] 26.76 _[±]_ [7.36] 37.12 _[±]_ [12.43]


**Note:** (↑) higher is better; (↓) lower is better.


the lowest score of 27.82. NAC-Exact/05s/8k and NAC-PW

achieved nearly identical scores, obtaining 37.75 and 37.50
on average, respectively. On the XJTU-SY dataset, NACExact/05s/8k has the lowest score of 19.87. NAC-FC ranked

second with a score of 24.89, followed by NAC-PW in third
place with an average score of 28.01. A similar trend was
observed on the HUST dataset, where NAC-Exact/05s/8k
achieved first place with a score of 27.82, NAC-PW ranked
second with 30.14, and NAC-FC ranked third with 35.35.
These results demonstrated the strong cross-validation capability of NAC.



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/neuronal_attention_circuits.pdf-6-0.png)

_Figure 3._ Degradation Estimation Results.


compact architecture suitable for resource-constrained devices, thereby supporting localized safety.
We utilized three benchmark datasets: (i) PRONOSTIA
(Nectoux et al., 2012), (ii) XJTU-SY (Wang et al., 2018),
and (iii) HUST (Thuan & Hong, 2023). Training is performed on PRONOSTIA, while XJTU-SY and HUST are

used to assess cross-validation. We used the Score met
ric (Nectoux et al., 2012) to assess the performance. We
evaluate generalization using _Bearing 1_ from the first operating condition of each dataset. Figure 3 visualizes the
expected degradation alongside the outputs of NAC. On the
PRONOSTIA dataset, ContiFormer performed the best with



**3.4. Run Time and Memory Experiments**

---

[Document 3]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_15
Content: 15


**Neuronal Attention Circuit (NAC) for Representation Learning**


_Figure 5._ Saliency maps for OpenAI CarRacing


_Figure 6._ Saliency maps for Udacity Simulator



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/neuronal_attention_circuits.pdf-15-0.png)

![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/neuronal_attention_circuits.pdf-15-1.png)

and Steady) maintains focus on the road’s horizon, while
other models either focus on the sides or remain largely
unresponsive to the task. Figure 6 also presents the saliency
maps for the Udacity Simulator. In this case, NAC-Exact,
CTA produces the most accurate visual maps, maintaining
attention on the road’s horizon, followed by ContiFormer
and mTAN, which achieve comparable performance. Attention and PhasedLSTM also generate reasonable saliency
maps, although their focus is more dispersed across the
scene. In contrast, other models either fail to identify relevant regions, producing blurry maps, or focus solely on
one side of the road. These results demonstrate the NAC’s

ability to understand the underlying task.


D.3.4. INDUSTRY 4.0


**Dataset Explanation and Curation:** _PRONOSTIA dataset_
is a widely recognized benchmark dataset in the field of
condition monitoring and degradation estimation of rollingelement bearings. Nectiux et al.(Nectoux et al., 2012) developed this dataset as part of the PRONOSTIA experimental
platform. The dataset comprises 16 complete run-to-failure
experiments performed under accelerated wear conditions



**Algorithm 3** Time–frequency Representation Algorithm


**Require:** windowed signal _Iw_, critical frequency _fc_, operating
frequency _fo_, sampling period _Tsampling_, windowed physical
constraints ( _tw, Tw_ )
_amin_ = _fmax·Tfsamplingc_ _[,]_ _amax_ = _f_ min _·Tsamplingfc_ [,]
_ascale ∈_ [ _amin, amax_ ]
_IT F R ←{}_
**for** ( _iw, tn, Tn_ ) in ( _Iw, tw, Tw_ ): **do**

Wavelets: Γ _iw_ ( _a, b_ ) = � _−∞∞_ _[i][w][ψ][∗]_ [�] _[t][−]_ _a_ _[b]_ � _dt._

Energy: _E_ = [�] _[M]_ _m_ =1 _[|]_ [Γ] _[iw]_ [(] _[a, b]_ [)] _[|]_ [2]

Dominant frequency: _fd_ = _a_ scale [arg max( _E_ )].
Entropy: _h_ = _−_ [�] _[M]_ _i_ = _m_ _[P]_ [(] _[i][w]_ [(] _[t]_ [)) log] _[ P]_ [(] _[i][w]_ [(] _[t]_ [))][.]

Kurtosis: _K_ = [E][[(] _[i][w]_ [(] _σ_ _[t]_ [4][)] _[−][µ]_ [)][4][]] .

Skewness: _sk_ = [E][[(] _[i][w]_ [(] _σ_ _[t]_ [3][)] _[−][µ]_ [)][3][]] .

1 _M_
mean: _µ_ = _M_ � _m_ =1 _[i][w]_ [(] _[m]_ [)][.]


1 _M_

standard deviation: _σ_ = ~~�~~ _M_ ~~�~~ _i_ =1 [(] _[i][w]_ [(] _[m]_ [)] _[ −]_ _[µ]_ [)][2][.]

_Xn ←_ [log( _E_ ) _, fd, h, K, sk, µ, σ_ ]
**end for**
**return** _IT F R_ = _Concat_ ( _X_ 1 _, X_ 2 _. . . XNs_ _, tn, Tn_ )



16


**Neuronal Attention Circuit (NAC) for Representation Learning**


_Table 4._ Summary of Key Hyperparameters of All Experiments


**Param.** **MNIST** **PAR** **CarRacing** **Udacity** **RUL EST.**


Conv layers 2× **1D** (64@5) **1D** (64@5, 64@3) 3×TD- **2D** (10–30@3–5) 5× **2D** (24–64@5–3, ELU) 2× **1D** (32@3, 16@2)
NAC 64-d, 8h 32-d, 4h 64-d, 16h 100-d, 16h 16-d, 8h
Dense 32–10(SM) 32–11(SM) 64–5(SM) 64–1(Lin) 1(Lin)
Dropout – – 0.2 0.5 –
Opt. AdamW AdamW Adam AdamW AdamW
LR 0.001 0.001 0.0001 0.001 0.001

Loss SCE SCE SCE MSE MSE

Metric Acc Acc Acc MAE Score

Batch 32 20 32 – 32

Epochs 150 500 100 10 150


**Note:** SCE = Sparse Categorical Crossentropy; Acc = Accuracy; MAE = Mean Absolute Error; MSE = Mean Squared Error; SM =
softmax; Lin = Linear; TD = TimeDistributed; Conv1D/2D = Conv1D/2D; _d_ = model dimension; _h_ = attention heads.

**Baselines Hyperparameters Clarification:** All (CT & DT) RNNs use the same number of hidden units as NAC’s _d_ model, and all (DT &
CT) Attention use the same _d_ model and _heads_ as NAC. The other layers, including 1D/2D, Dense, and the remaining hyperparameters, are
the same during our tests.


_Table 5._ Data distributions of PRONOSTIA, XJTU-SY, and HUST datasets.


**Dataset** **Condition** **Frequency** **Radial Load** **Speed** **Train** **Test**

_Condition 1_ 100 Hz 4 kN 1800 rpm 4 _∼_ 7 1 _∼_ 3
**PRONOSTIA** _Condition 2_ 100 Hz 4.2 kN 1650 rpm      - 1 _∼_ 7
_Condition 3_ 100 Hz 5 kN 1500 rpm                - 1 _∼_ 3

_Condition 1_ 35 Hz 4 kN 2100 rpm                - 1 _∼_ 5
**XJTU-SY** _Condition 2_ 37.5 Hz 4.2 kN 2250 rpm       - 1 _∼_ 5
_Condition 3_ 40 Hz 5 kN 2400 rpm                - 1 _∼_ 5

_Condition 1_                - 0 W                -                - 1 _∼_ 5

**HUST** _Condition 2_       - 200 W       -       - 1 _∼_ 5

_Condition 3_                - 400 W                -                - 1 _∼_ 5
","Based on the provided context, the PRONOSTIA dataset has a ""Frequency"" of **100 Hz** for all its operating conditions (Condition 1, Condition 2, and Condition 3). This ""Frequency"" is associated with the data distribution of the dataset.",115,4335,4451,,3,,2026-01-02T20:15:12.015760
"Identify the ""rank r"" parameter value used for ""LoRA"" fine-tuning in the SWAA experiments.",Entity-Specific Attribute Retrieval,"[Document 1]
Source: sliding_window_attention_annotated_chunks_2k.jsonl
Chunk ID: sliding_window_attention_annotated_chunks_2k_2
Content: **3.4** **Fine-tuning**


This is the most natural way to mitigate traininginference mismatch. Apparently, the model should
be fine-tuned while SWA is applied, so that the
model’s parameters can be trained to better adapt
to SWA. Meanwhile, the training data must be longcontext tasks, where SWA actually works.
However, most available long-context datasets
only contain brief ground-truth answers, lacking the reasoning steps required for fine-tuning
a ""thinking"" model. Since our goal is to _restore_
the model’s original capabilities under SWA rather
than teach it new ones, instead of directly using the
original dataset, we adopt an approach similar to
self-distillation (Yang et al., 2024). Specifically, we
utilize the original full-attention model to generate
new answers for the dataset’s questions, and these
generated answers are then filtered for correctness
using GPT-5-Mini (OpenAI, 2025), to make up our
training dataset. For each question, we sample 4
answers with temperature 1, because we find this
strategy is slightly better than generating only one
answer with temperature 0.


**4** **Experiment Setup**


We organize our experiments around three research
questions:


**RQ1: Is SWA adaptation feasible without any**
**additional training?** We evaluate whether
an FA LLM can be adapted to SWA using
only inference-time modifications, and which
combinations of techniques are necessary.


**RQ2: How much does fine-tuning with SWA im-**
**prove performance?** We study the effect of
SWA-aware fine-tuning on long-context performance and identify which components of
SWAA are still required.



**RQ3: Which SWAA configurations achieves the**
**optimal performance-efficiency trade-offs?**
We evaluate how different SWAA configurations trade off accuracy against inference la
tency.


**4.1** **Models**


Our primary experiments use Qwen3-4B-Thinking
and Qwen3-4B-Instruct (Team, 2025b). The Thinking variant enforces chain-of-thought (CoT) style
reasoning, whereas the Instruct variant usually just
answers briefly. To ensure generality, we additionally evaluate Qwen3-30B-A3B-Thinking, Qwen330B-A3B-Instruct (Team, 2025b), and Llama3.18B-Instruct (Touvron et al., 2023).

All models are served with vLLM in float16

precision using a batch size of 64. We use greedy
decoding (temperature = 0) for all evaluations. In
preliminary experiments, we observed that vLLM
yields slightly lower (about 1% to 5%) scores
than HuggingFace Transformers due to precisionrelated discrepancies.


**4.2** **Evaluation Dataset**


SWA is identical to full attention when the context

length is within the window size. Even if the model
is fine-tuned, we can pre-calculate the prompt
length and simply disable the LoRA adapters for
short prompts to get completely the same response
as the original model. Therefore, our experiments
focus exclusively on long-context benchmarks with
inputs exceeding 16k tokens, as re-evaluating models on standard short-context benchmarks (e.g.,
MMLU (Hendrycks et al., 2021), GPQA (Rein
et al., 2023)) is completely unnecessary.
Since we find other long-context benchmarks are
either too easy or too difficult for 4B-level models (see Appendix B), we ultimately select LongMemEval (Wu et al., 2024), a benchmark consisting of various types of long-context QA tasks with
moderate difficulty, although it is originally designed for agent memory system evaluation. Its
context length is controllable by selecting a specific number of chat sessions to concatenate as the
context from a pool of hundreds of sessions (a session contains the chat history between user and
assistant within a day). To create a moderately difficult and discriminative evaluation, we construct
**LongMemEval_24k** by sampling 10 sessions, resulting in 500 samples ranging from 16k to 32k
with an average context length of 24k.



4


For additional validation of generalizability, as
shown in Appendix D, we also experiment on
LongBench-V2 (Bai et al., 2024b), a more modern and challenging benchmark that requires deep
reasoning across various real-world tasks.


**4.3** **Training Details**


For the fine-tuning dataset, we initially considered LongAlign (Bai et al., 2024a), a widely used
long-context fine-tuning dataset for adapt a regularlength model to long-context tasks. However, since

_∼_
its sample count ( 10,000) is insufficient, we incorporate an additional 6,000 samples from Fusangv1-long (Pan, 2024), a more comprehensive corpus
of over 40,000 long-context samples that includes
LongAlign as a subset.
We perform SWA-aware fine-tuning using LoRA
(Hu et al., 2022). Unless otherwise noted, we use
rank _r_ = 16 and _α_ = 128, and apply LoRA only
to the query, key, and value projection modules.
We adopt this parameter-efficient setting because
full-parameter fine-tuning often leads to overfitting
and degradation of the model’s original capabilities
in our preliminary experiments. We use a learning
rate of 1e-4 with a cosine decay schedule. Models
are fine-tuned for a single epoch on the sampled
long-context dataset since we observe no meaningful gains from additional epochs (see Appendix F).
Once training takes approximately 12 hours on an
8*H20 GPU server for Qwen3-4B and 30 hours for
Qwen3-30B-A3B.


**5** **Experiment Results**


**5.1** **SWA Adaptation Without Fine-tuning**


We first study SWA adaptation without any additional training. Table 1 reports LongMemEval_24k
accuracy for Qwen3-4B-Thinking (""think"") and
Qwen3-4B-Instruct (""non-think"") under different
combinations of SWAA components. In most settings, we use an aggressive 2k window to amplify
the impact of SWA. The configurations are ranked
by the number of methods applied (0, 1, 2, or 3 of
Interleaving Layers, Keep First and FA Decode).
Rows 1 (original model) and 2 (naive SWA) serve
as upper and lower baselines, respectively. In the
column ""FA layers"", the value records which layers use full attention, and [] means all the layers
use SWA, i.e., this method is not enabled. In the
column ""keep first"", the value is _k_ in Keep First
_k_ Tokens. When comparing results, an accuracy
difference of less than 5% is usually considered



statistically insignificant. From the results, we find
that:


**Naive SWA is not viable.** Naively replacing FA
with a 2k sliding window attention (row 1) drops
accuracy significantly to 3 _._ 2 and 11 _._ 0, respectively.
Even with an 8k window (row 2), accuracy only recovers to 13 _._ 2 and 19 _._ 8, far below the FA baseline.


**Single method helps, but cannot close the gap.**
Each method—Keep First, FA Decode, or Interleaving Layers—improves over naive SWA (rows 3–6),
yet each alone recovers only a small fraction of the
FA gap and remains well below the baseline. In
short, no single method is sufficient.


**Combinations exhibit strong synergy.** Recipes
that combine multiple methods deliver large gains:


  - **FA Decode + Keep First** _k_ substantially improves over naive SWA (rows 7–9), recovering roughly half to two-thirds of the gap on
the thinking model as _k_ increases. However,
increasing _k_ from 100 to 1000 yields almost
no improvement, indicating that _k_ does not
need to be exceedingly large.


  - **Interleaving Layers + FA Decode** is
markedly stronger (row 13), recovering most
of the gap for the thinking model.


  - **FA Decode + Interleaving Layers + Keep**
**First** _k_ perform best (rows 18). The thinking
model recovers close to 90% of the FA gap
even at 2k window.

---

[Document 2]
Source: sliding_window_attention_annotated_chunks_2k.jsonl
Chunk ID: sliding_window_attention_annotated_chunks_2k_3
Content: **Combinations exhibit strong synergy.** Recipes
that combine multiple methods deliver large gains:


  - **FA Decode + Keep First** _k_ substantially improves over naive SWA (rows 7–9), recovering roughly half to two-thirds of the gap on
the thinking model as _k_ increases. However,
increasing _k_ from 100 to 1000 yields almost
no improvement, indicating that _k_ does not
need to be exceedingly large.


  - **Interleaving Layers + FA Decode** is
markedly stronger (row 13), recovering most
of the gap for the thinking model.


  - **FA Decode + Interleaving Layers + Keep**
**First** _k_ perform best (rows 18). The thinking
model recovers close to 90% of the FA gap
even at 2k window.


**CoT synergizes with FA Decode.** Under recipes
that include FA Decode, the thinking model consistently benefits more than the non-thinking model
(rows 13 and 18), suggesting that CoT synergizes
with FA Decode: preserving global attention at
decoding time enables longer reasoning traces to
capitalize on context processed by SWA, confirming our hypothesis in Section 3.1.


**Sliding window size affects, but is not the deci-**
**sive role.** With FA Decode + Keep First _k_, accuracy improves as the window increases (rows 7,
14, 15), though benefits are modest until 8k. When
added with interleaving FA layers, moving from a
2k to a 4k window is enough to close the remaining
gap on the thinking model (row 22 matches the FA
baseline), indicating that FA Decode and Interleaving are the dominant drivers, and larger windows
mainly smooth residual error.



5


Table 1: Qwen3-4B-Thinking and Qwen3-4B-Instruct results on LongMemEval without SFT


**No.** **window size** **FA layers** **keep first** **FA decode** **Acc think** **Acc non-think**


0 Full [] 0 False **73.0** **62.0**
1 2k [] 0 False 3.2 11.0
2 8k [] 0 False 13.2 19.8


3 2k [] 10 False 16.0 15.6
4 2k [] 0 True 11.8 14.2
5 2k [1, 3, 5, ...] 0 False 13.4 18.4
6 8k [] 0 True 26.2 25.0


7 2k [] 10 True 38.2 20.6
8 2k [] 100 True 50.0 17.8
9 2k [] 1000 True 50.0 20.2
10 2k [0, 2, 4, ...] 10 False 17.0 14.8
11 2k [0, 2, 4, ...] 0 True 32.2 26.0
12 2k [1, 3, 5, ...] 10 False 25.8 36.4
13 2k [1, 3, 5, ...] 0 True 59.2 34.8
14 4k [] 10 True 38.0 24.4
15 8k [] 10 True 49.2 35.2


16 2k [0, 2, 4, ...] 10 True 36.0 17.2
17 2k [1, 3, 5, ...] 10 True 65.0 **53.6**
18 2k [1, 3, 5, ...] 100 True **68.8** 50.6
19 2k [1, 5, 9, ...] 10 True 53.2 31.4
20 2k [1, 9, 17, ...] 10 True 36.4 18.8
21 2k [3, 7, 11, ...] 10 True 54.2 34.6
22 4k [1, 3, 5, ...] 100 True **73.0** **54.2**
23 8k [1, 3, 5, ...] 100 True **71.6** **56.6**



**SWA/FA layer selection has large impacts.** Selecting less layers for FA, e.g. only one fourth or
eighth (row 19, 20), though more efficient, will
significantly decrease the improvement brought by
Interleaving Layers. More importantly, for Qwen34B, configuring _odd-numbered_ layers with full attention is significantly better than _even-numbered_
layers (row 11, 13, 16, 17). However, surprisingly,
this result is reversed for Qwen3-30B-A3B and
Llama3.1-8B-Instruct (row 10 and 11 in Table 4
and 5). This suggests that layer functionalities differ across model families and sizes, necessitating
model-specific layer selection strategies, as discussed in Section 3.2.


Therefore, we answer RQ1: adapting an FA
LLM to SWA is feasible even without any training. But it requires specific combinations of at
least 2 methods, which could be less efficient for
inference.



**5.2** **SWA Adaptation With Fine-tuning**


We next evaluate SWA-aware supervised finetuning, which is expected to provide higher improvement. Table 2 reports LongMemEval_24k
accuracy after SFT under various SWAA configurations. The original full-attention model is also
fine-tuned as a baseline (row 0). Since training is
relatively time-consuming, we only test a representative subset of configurations. Our findings are as
follows:


**Fine-tuning substantially lifts all SWA config-**
**urations.** Comparing all the fine-tuning results
with non-training ones, it is clear that fine-tuning
consistently provides great improvement. However,
simply fine-tuning with naive SWA remains insufficient, only achieving 18.8% and 23.8% accuracy
(row 1).


**FA Decode and Interleaving Layers emerge as**
**dominant components.** After SFT, FA Decode
and Interleaving Layers provide the largest gains.



6


Table 2: Qwen3-4B-Thinking and Qwen3-4B-Instruct results on LongMemEval with SFT


**No.** **window size** **FA layers** **keep first** **FA decode** **Acc think** **Acc non-think**


0 Full [] 0 True **74.6** **63.4**
1 2k [] 0 False 18.8 23.8


2 2k [] 10 False 15.6 /
3 2k [] 0 True 57.9 42.0
4 2k [1, 3, 5, ...] 0 False 63.6 54.6
5 4k [] 0 True 62.6 /


6 2k [] 10 True 56.7 /
7 2k [] 100 True 62.2 42.6
8 2k [1, 3, 5, ...] 0 True **73.2** **58.8**
9 2k [0, 2, 4, ...] 0 True **66.0** /
10 2k [1, 5, 9, ...] 0 True **68.8** 47.0


11 2k [1, 3, 5, ...] 100 True **73.2** **61.4**


Table 3: Recommended SWA adaptation recipes for different needs and scenarios. � means optional.


**Training** ? **Thinking Model?** **Prefer?** **FA Decode** **Interleaving Layers** **Keep First**


No No Any � � �
No Yes Efficiency � � �
No Yes Accuracy � � �


Yes Any Efficiency � � �
Yes Any Accuracy � � �



Using FA Decode alone (row 3) or using Interleaving Layers alone (row 4) both get high accuracy.
And combining both (row 8) further boosts performance to 73 _._ 2 (think) and 58 _._ 8 (non-think), nearly
matching the full-attention SFT baseline in row 0.


**Keep First becomes optional rather than essen-**
**tial.** Before fine-tuning, Keep First is crucial for
stability under SWA. But after SFT, it only provides
minor additional improvement. With FA Decode,
adding _k_ = 100 (row 7) improves over _k_ = 0
(row 3) only 4.5%, and if further combining FA
layers, it almost offers no improvements (row 11
and row 8).


**Effect of sliding window size.** Row 3 and 5
shows that increasing the window from 2k to 4k
with FA Decode improves thinking-model accuracy from 57 _._ 9 to 62 _._ 6. This mirrors the non-SFT
trend that larger windows help, but the dominant
improvements still come from FA Decode and Interleaving Layers.

---

[Document 3]
Source: sliding_window_attention_annotated_chunks_2k.jsonl
Chunk ID: sliding_window_attention_annotated_chunks_2k_1
Content: LightTransfer (Zhang et al., 2024) is a promising
attempt at adapting existing models to SWA without pretraining, which has the same motivation as
ours. But it may generalize poorly across model
families (see Appendix G).


**2.2** **Linear Attention**


An alternative approach involves reformulating
the attention mechanism entirely to achieve linear, _O_ ( _N_ ), complexity. This includes methods
such as RNN-like linear attention transformers

(Katharopoulos et al., 2020; Peng et al., 2023;
Sun et al., 2023) and structured state-space models
(SSMs) like Mamba (Gu and Dao, 2023). Many
works such as Jamba and Nemotron-Flash(Lieber
et al., 2024; Linsong Chu et al., 2024; Team et al.,
2025; Fu et al., 2025a) interleave linear attention
layers with traditional attention layers to create hybrid model structures. While promising, they represent a fundamental architectural departure from the
standard Transformer, and their performance is generally weaker than traditional Transformer-based

LLMs.



2


**3** **Candidate Methods for SWA**

**Adaptation**


As established, a naive application of SWA leads
to severe long-context performance degradation.
Therefore, we investigate five methods that can potentially facilitate SWA adaptation from distinct
perspectives. However, every method except finetuning has some drawbacks to LLM inference efficiency, as discussed in Appendix A. Therefore,
although these methods are not mutually exclusive,
we should not indiscriminately adopt all of them.
Instead, we must evaluate various combinations to
identify the optimal trade-off between performance
and efficiency.


**3.1** **Full Attention Decode**


This method applies SWA **only** to the prefilling
stage. During the decoding (auto-regressive generation) stage, each token still employs full attention,
allowing access to all previous tokens in the context. The resulting attention mask is depicted in
Figure 1a.
This novel approach, first proposed in our study,
is inspired by human reading comprehension: humans typically scan a passage casually (prefilling)
before thinking deeply to formulate an answer (decoding) for a specific problem. We term this strategy ""reading casually, thinking carefully."" In our
design, the SWA-constrained prefilling stage corresponds to casual reading, while the full-attention
decoding stage enables careful thinking. This analogy also suggests that Chain-of-Thought (CoT)
during decoding may be particularly beneficial, as
extended generation (i.e. ""thinking"") could compensate for the insufficient contextual information
gathered during the prefilling stage.


(a) FA Decode (b) Keep First


Figure 1: (a) Attention mask for FA Decode. SWA is
used for prompt tokens (prefill), and full attention is
used for generated tokens (decode). (b) Attention mask
for SWA combined with Keep First _k_ Tokens.



**3.1.1** **Keep First k Tokens**

Xiao et al. demonstrate that models pretrained with
full attention allocate a disproportionate amount
of attention to the initial tokens (""attention sink""),
and removing the visibility of these tokens causes
performance collapse. Their solution, streaming
attention, involves permanently retaining the attention to these ""sink"" tokens while using SWA, which
successfully maintains the stability of the attention
distribution and the model’s output. Our method
is basically the same: as shown in Figure 1b, any
subsequent token can attend to its local window
**and** the initial _k_ tokens.

Notably, however, our method extends beyond
its original version. Streaming attention operates
only at the KV cache level; specifically, the KV
cache of the sink tokens is retained during decoding, while the prefilling stage is not modified or accelerated at all. In contrast, we directly customize
the Flash-Attention-2 (Dao, 2024) kernel to implement such attention mask, thereby accelerating
prefilling via SWA as well, and eliminating the
need to modify KV cache.


**3.2** **Interleaving Layers**


This method retains full attention on a subset of

layers while applying SWA to the remainder, providing a simple hybrid mechanism to balance the
performance of full attention with the efficiency
of pure SWA. A common strategy involves designating one in every _n_ layers to use full attention
(e.g., layers 0, 2, 4, . . . retain full attention, while all
others use SWA). For example, Gemma-2 (Team,
2024a) uses SWA only for layers [1, 3, 5, . . . ], and
Gemma-3 (Team, 2025a) uses SWA only for layers

[5, 11, 17, ...].
However, for an FA-pretrained model, layers
may exhibit distinct behaviors, suggesting it may
not be optimal to simply assign SWA to layers [1,
3, 5, ...]. Instead, it might be preferable to use
statistical features to select ""lazy"" (mostly focusing
on just recent tokens) layers, as adopted by LightTransfer (Zhang et al., 2024). However, we find
that LightTransfer is not consistently superior in
practice (see Appendix G). Therefore, for simplicity, we still choose the simplest fixed-interval layer
selections in our experiments, such as [0, 2, 4, . . . ]
and [1, 3, 5, ...].


**3.3** **Chain-of-Thought**


Chain-of-Thought (CoT) (Wei et al., 2022) is a
widely used technique for improving model ac


3



```
### Figure - Sliding Window Attention
![Sliding Window Attention](sliding_window_attention.pdf-2-0.png)

**Caption:**  
This figure illustrates the sliding window attention mechanism in a transformer model. The ""Prefill"" region represents the initial input, while the ""Decode"" region shows the subsequent processing.

**Description:**
- The image depicts a grid representing a sequence of data, with a sliding window moving across it.
- The green area indicates the window's current position, highlighting the portion of the sequence being attended to.
- The blue area represents the decoded output based on the attended input.
- The window progressively moves across the sequence, enabling contextual understanding.
```

```
### Figure - Sliding Window Attention
![Sliding Window Attention](sliding_window_attention.pdf-2-1.png)

**Caption:**  
The figure illustrates the sliding window attention mechanism, where attention is retained over a limited window. Attention decreases as the distance from the current position increases.

**Description:**
- The image depicts a grid representing a sequence of tokens.
- The blue squares indicate attention retained across the sliding window.
- The green squares represent attention calculated within the sliding window.
- Attention decreases progressively as the distance 'k' from the current position increases, reflecting a decaying attention mechanism.
- The sliding window size is decreasing as it moves along the sequence.
curacy via reasoning. With the advent of ""thinking"" models, such as DeepSeek-R1 (DeepSeek-AI,
2025a), CoT has evolved from a prompting strategy
to an intrinsic LLM capability. However, whether
CoT has a specific impact in SWA scenarios remains uninvestigated. We explore this by comparing a thinking model with its non-thinking variant,
e.g., Qwen3-4B-Thinking and Qwen3-4B-Instruct,
to verify the effect of CoT on SWA adaptation,
which can produce more notable differences compared to simply adding CoT prompting to the same
model.


**3.4** **Fine-tuning**
","For LoRA fine-tuning in the SWAA experiments, the ""rank r"" parameter value used is 16.",140,1590,1731,,3,,2026-01-02T20:15:13.748561
"What ""model scales"" of the ""Qwen"" family were tested in the ""Investigating the Functional Roles..."" paper?",Entity-Specific Attribute Retrieval,"[Document 1]
Source: attention_functional_roles_raw_chunks_2k.jsonl
Chunk ID: attention_functional_roles_raw_chunks_2k_3
Content: 5


![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-5-0.png)

Figure 2: The existence of cognitive heads in Qwen2.5-VL-7B responsible for eight distinct functions in complex reasoning tasks. The x-axis represents the head index, while the y-axis indicates
the layer index. The values denote head importance scores, capped at a cutoff of 0.60.


Figure 3: Pearson Correlation between different functions across two models.


the ROUGE score (Chin-Yew, 2004) or the semantic similarity score surpasses 0.6. This provides a
comprehensive evaluation of performance degradation.


We gradually mask out the number of cognitive heads and and observe how model behavior changes.
As shown in Fig 4, randomly masking up to 10% of heads has minimal impact on the overall performance of Qwen2.5-VL-3B. In contrast, masking a similar number of cognitive heads leads to a
substantial drop across multiple functions. Notably, when more than 25% of heads are randomly
masked, performance also declines sharply, as this begins to include functionally critical heads.
These results further highlight the sparsity and importance of functional heads.


For each function, we select the top 10% of heads with the highest accuracy as cognitive heads. As
shown in Table 1, masking cognitive heads lead to a substantial decline in performance, whereas
masking an equal number of random heads results in only minor degradation across all VLMs. In
some cases, masking identified cognitive heads reduces accuracy to zero, indicating that the model
cannot perform the corresponding function without them. The t-test analysis (Appendix A.9) shows
that the difference between cognitive masking and random masking is statistically significant, with
_p ≪_ 0 _._ 05 in nearly all cases. To further validate their functional roles, we mask heads associated
with one function (e.g., language knowledge recall) while evaluating performance on a different
function (e.g., vision knowledge recall). As shown in Fig 5, masking the relevant functional heads
yields a significantly larger performance drop than masking unrelated heads, confirming their functional specialization.


In addition to masking, we also conduct activation patching, where the activations of cognitive heads
associated with one function are replaced by those from another function using two strategies: ran

6



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-5-1.png)
![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-6-0.png)

Figure 4: The performance of Qwen2.5-VL-3B after masking out top K cognitive heads vs K random
heads on inference, high-level visual reception, low-level visual reception, and decion-making.


Table 1: Intervention results (mean accuracy over 5 runs %) of cognitive heads vs. random heads
across 8 functions: **Low-level** Visual Reception, **High-level** Visual Reception, Vision Knowledge
**Recall**, Language **Info** rmation Extraction and Understanding, Language Knowledge **Recall**, **Math**
Reasoning, **Inference**, and **Decision** Making. For model names, Intern2B: InternVL3-2B, Intern8B:
InternVL3-8B, gemma2B: gemma-3n-e2b-it, gemma4B: gemma-3n-e4b-it, Qwen3B: Qwen2.5VL-3B-Instruct, Qwen7B: Qwen2.5-VL-7B-Instruct. Lower values indicate more effective intervention outcomes, suggesting that the corresponding heads play a greater role in the cognitive function.







|VisionmainlyFunctions LanguagemainlyFunctions<br>Model Inter Head<br>Low-Level High -Le vel Recall Info Recall Math Inference Decision<br>llm acc llm acc llm acc llm acc llm acc llm acc llm acc llm acc|Col2|
|---|---|
|Intern2B<br>random<br>cognitive|75.61 82.44<br>87.5<br>88.75 89.15<br>86.1<br>57.84 69.19 84.06 84.64 81.29 90.97 75.06 74.12 67.22 71.67<br>**60.24 62.68 75.71 76.61 73.05 64.58** 66.76 68.92** 44.93 46.38 61.29 64.52 71.76 64.71 48.06 52.22**|
|Intern8B<br>random<br>cognitive|92.2<br>93.17 88.47 91.25 87.61 82.25 59.41 69.12 87.59 89.37 79.25 84.15 82.64 85.05 66.27 73.49<br>**68.78 78.05 56.94 65.97 71.69 70.56**<br>**8.82**<br>**19.12 74.68 77.22 20.75**<br>**56.6**<br>**43.96 42.86** 66.27** 66.27**|
|gemma2B<br>random<br>cognitive|58.37 76.33 57.53 67.64 54.57 62.57 55.07 60.82 81.05 82.11 23.12<br>55.0<br>57.44 63.49<br>30.0<br>66.94<br>**48.98**<br>**55.1**<br>**55.06 63.48**<br>**2.86**<br>**8.57**<br>**30.27 38.49**<br>**50.0**<br>**47.37 11.25 36.88 36.98 52.09 19.44 54.17**|
|gemma4B<br>random<br>cognitive|36.0<br>48.73 29.22 38.65 33.52 34.08 25.12 27.56 40.27 47.84 27.91 44.19<br>57.3<br>57.08 18.29 34.57<br>**29.09 41.82 10.88 21.24**<br>**5.63**<br>**14.08**<br>**22.2**<br>**36.34**<br>**9.46**<br>**13.51** 27.91** 53.49 36.18 32.81**<br>**4.29**<br>**15.71**|
|Qwen3B<br>random<br>cognitive|70.97 82.58 82.42 86.06 88.48 86.36 52.22 58.89 85.14 86.57 65.85 89.76 85.32 90.13 63.44 71.88<br>**12.9**<br>**12.9**<br>**12.12 16.67 77.88 77.88** 55.56 62.96** 61.43 68.57**<br>**0.0**<br>**0.0**<br>**1.27**<br>**1.27**<br>**1.56**<br>**4.69**|
|Qwen7B<br>random<br>cognitive|83.2<br>88.8<br>84.57 89.51 79.43 80.29 75.08 79.38 90.13<br>86.4<br>67.84 72.94 80.67 83.33 75.14 79.19<br>**30.0**<br>**38.0**<br>**73.21 73.83 21.43 22.86 15.38 33.85**<br>**84.0**<br>**78.67** 68.63 72.55 85.56 81.11** 25.68 27.03**|


dom activation and mean activation. In the random activation setting, activations are substituted
with those from a randomly selected subquestion belonging to a different function. In the mean
activation setting, activations are replaced with the average activation computed over all subquestions associated with another function (details in Appendix A.10). As shown in Table 2, both types
of activation patching result in substantial performance degradation for cognitive heads, consistent
with the effects observed under masking interventions.

---

[Document 2]
Source: attention_functional_roles_raw_chunks_2k.jsonl
Chunk ID: attention_functional_roles_raw_chunks_2k_11
Content: Figure 14: Pearson Correlation of different K of TopK token across eight functions for Qwen2.5VL-3B-Instruct.


Figure 15: Pearson Correlation of different LLM selection across eight functions for Qwen2.5-VL3B-Instruct.


24



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-23-1.png)
Table 10: Examples of topk token selection using different LLMs.


Question: What are the notable visual features of the couch and love seat in the image, such
as their shape, trim, and upholstery?
Answer: The couch and love seat in the image are patterned after a classic, elegant style with
intrica te detailing and a neutral color palette.
LLM top5 token selection:
Qwen3-30B: classic, elegant, patterned, detailing, neutral
GPT5: classic, elegant, patterned, detailing, neutral
Llama3.3-70B: classic, elegant, patterned, detailing, color
Question: What familiar object matches the shape and features of the blue item shown in the
image?
Answer: The blue item is a fire hydrant.

LLM top5 token selection:
Qwen3-30B: fire, hydrant
GPT5: fire, hydrant
Llama3.3-70B: fire, hydrant


Figure 16: Pearson Correlation of different classification layer initialization seed across eight functions for Qwen2.5-VL-3B-Instruct.


**Sparse consistency across models:** The relatively high Pearson correlation coefficients between
models in Fig. 17 indicate that different models exhibit consistent sparsity patterns for different
functions.


Table 11 reports the t-test results comparing random heads and cognitive heads, showing that the
differences are statistically significant, (p-value _≪_ 0.05).


Table 11: Welch t-test between random heads and cognitive heads.



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-24-0.png)





|Models|Vision mainly Cognitive Functions Language mainly Cognitive Functions|
|---|---|
|Models|Low-Level<br>High-Level<br>Recall<br>Info<br>Recall<br>Math<br>Inference<br>Decision|
|Models|p-value (t-test)<br>p-value (t-test)<br>p-value (t-test)<br>p-value (t-test)<br>p-value (t-test)<br>p-value (t-test)<br>p-value (t-test)<br>p-value (t-test)|
|Qwen2.5-VL-3B-Instruct<br>Qwen2.5-VL-3B-Instruct<br>InternVL3-2B<br>InternVL3-8B<br>Gemma3-2B<br>Gemma3-4B|1.83e-03<br>5.71e-05<br>0.43<br>0.66<br>2.84e-03<br>4.11e-03<br>8.47e-06<br>1.39e-04<br>8.27e-05<br>0.10<br>3.01e-03<br>2.91e-05<br>7.70e-03<br>0.76<br>0.46<br>7.71e-05<br>0.02<br>0.05<br>8.58e-03<br>0.88<br>4.97e-03<br>1.49e-03<br>0.18<br>3.78e-03<br>3.98e-07<br>4.86e-06<br>2.80e-04<br>3.15e-06<br>1.19e-03<br>1.86e-05<br>1.01e-03<br>0.19<br>1.84e-06<br>0.19<br>2.32e-03<br>3.87e-03<br>1.98e-04<br>1.86e-04<br>9.43e-03<br>6.65e-05<br>5.33e-05<br>0.04<br>0.01<br>0.07<br>2.28e-04<br>0.55<br>7.19e-03<br>4.15e-05|


25


![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-25-0.png)

Figure 17: Pearson Correlation betweeen different models across eight functions.


Figure 18: The importance (average attention weight) of visual modality for different functional
heads (Top 30).


A.10 DETAILS OF ACTIVATION PATCHING


In both the _random_ and _mean_ activation-patch methods, we replace the cognitive heads using patches
from another function. To minimize correlation between functions, we use language-related functions when patching vision functions, and vice versa. Table 12 shows the corresponding function
mappings.


Table 12: Function-to-Function Mapping Used for Activation Patching


**Original Function** **Replacement Function**


Vision Knowledge Recall Decision-Making
Language Knowledge Recall High-Level Vision Reception
Semantic Understanding High-Level Vision Reception
Math Reasoning Vision Knowledge Recall
Low-Level Vision Reception Inference
Inference Vision Knowledge Recall
High-Level Vision Reception Semantic Understanding
Decision-Making Vision Knowledge Recall


26



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-25-1.png)
A.11 NEGATIVE AND POSITIVE INTERVENTION CASES


The examples shown below illustrates a negative intervention case, where the model initially gives
an correct answer but produces the incorrect one (The model fails to answer in this case) after
intervention.



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-26-0.png)



The examples shown below illustrates the positive intervention success cases and failure case, where
the model initially gives an incorrect answer (marked in red) but produces the correct one (marked
in green) after intervention. This shows that adjusting the activation of cognitive heads along their
corresponding functional directions enhances performance on visual reasoning tasks. However, in
complex scenes, positive intervention may also have negative effects by assigning more weight to
unimportant elements, for example, focusing on tools and materials while overlooking the toilet
settings in the example below.



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-26-2.png)



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-26-1.png)

27


![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-27-1.png)

28



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-27-0.png)

![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-27-2.png)

![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-27-3.png)
![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-28-1.png)

29

---

[Document 3]
Source: attention_functional_roles_raw_chunks_2k.jsonl
Chunk ID: attention_functional_roles_raw_chunks_2k_10
Content: 21


A.7 PROMPT FOR QUESTION ASKING



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-21-0.png)

A.8 ABLATION STUDY


In the main experiments, we use the top-k generated tokens and average their multi-head attention
vectors. Here are some examples of selected top-k tokens by llm:



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-21-1.png)



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-21-2.png)



We also explore alternative strategies for extracting representations, including using the first meaningful token, first token, last token and with or without layerbias. The corresponding results are
shown in Table 9.


Sensitivity to the choice of k: We vary _k ∈{_ 1 _,_ 3 _,_ 5 _}_ and compute Pearson correlations of the
resulting attention-head heatmaps. Figure 14 shows that the heatmaps remain highly correlated
across choices of k, demonstrating that our method is robust to the exact number of selected tokens.
This robustness arises because (1) the activation patterns associated with answering a subquestion
are reflected across multiple output tokens, and (2) VLM outputs are short, reducing variance from
token choice.


22


Table 9: Ablation experiment of topK tokens, first meaningful token, first token and last token, and
with layerbias vs without layerbias on Qwen2.5-VL-3B-Instruct.


Vision mainly Cognitive Functions Language mainly Cognitive Functions



Token LayerBias Inter ~~H~~ ead



Low-Level High-Level Recall Info Recall Math Inference Decision


llm acc llm acc llm acc llm acc llm acc llm acc llm acc llm acc



random 80.65 90.32 87.88 93.18 92.42 90.91 59.26 75.93 87.14 85.71 65.85 87.80 81.01 87.34 40.63 73.44
First without
cognitive 48.39 54.84 45.24 48.23 80.30 83.33 50.00 72.22 **0.00** **0.01** 82.93 95.12 55.70 63.29 64.06 70.31


random 51.61 67.74 81.82 88.64 92.42 95.45 64.81 81.48 88.57 85.71 78.05 92.68 83.54 87.34 65.63 75.00
with
cognitive 45.16 48.39 56.06 66.67 86.36 86.36 48.15 62.96 74.29 81.43 78.05 90.24 51.90 56.96 68.75 75.00


random 80.65 93.54 90.15 90.91 89.39 90.91 70.37 77.78 87.14 91.43 63.41 82.93 70.89 79.75 63.19 73.75
Last without
cognitive 12.90 12.90 66.94 67.12 86.36 86.36 38.89 64.81 15.71 18.57 82.93 95.12 6.33 7.59 48.44 50.00


random 90.32 100.0 89.39 93.18 95.45 92.42 61.11 72.22 38.57 38.57 85.37 92.68 84.81 81.61 75.00 78.13
with
cognitive 67.74 64.52 89.39 93.18 67.74 64.52 **0.06** **0.02** 68.57 64.29 82.93 95.12 43.04 46.84 65.63 70.31


random 77.42 80.65 81.06 83.33 84.85 86.36 68.52 59.26 75.71 88.57 51.22 65.85 77.22 79.75 73.44 70.32
Meaning ~~f~~ irst without cognitive 77.42 87.10 50.76 54.55 84.85 75.76 50.00 53.70 2.85 4.23 85.37 95.68 72.15 79.75 57.81 75.00


random 90.32 93.55 86.36 87.88 84.85 86.36 77.78 83.33 81.43 87.14 65.85 82.93 70.89 62.03 68.75 67.19
with
cognitive 67.74 64.52 68.18 72.73 84.85 75.76 59.26 62.96 2.85 4.23 7.31 9.76 72.15 79.75 48.44 57.82


random 83.87 90.32 82.58 83.33 84.85 86.36 64.81 70.37 87.14 94.29 78.05 92.68 58.23 91.14 78.13 90.63
TopK without cognitive 12.90 67.74 40.15 45.45 **27.27 28.79** 55.56 46.30 38.57 42.86 78.05 85.37 29.11 32.91 57.81 75.00


random 87.10 96.77 82.58 83.33 86.36 84.85 59.26 55.56 85.71 85.71 82.93 87.80 91.14 86.08 81.25 82.81
with
cognitive **6.45** **6.45** **16.67 18.94** 75.76 75.76 62.96 81.48 57.14 62.86 **2.43** **2.43** **0.00** **0.00** **3.13** **4.69**


Figure 13: Pearson Correlation of different prompt types across eight functions for Qwen2.5-VL3B-Instruct. Full Prompt: The prompt used in the main results, with given CoT, Main Question,
Current Question. w/o Main Question: Prompt without main question involved.


Sensitivity to the choice of LLM: We further experimented with alternative LLMs for token selection. As shown in the examples in Table 10, different LLMs consistently select highly similar
semantic tokens. The Pearson correlations of the resulting attention-head heatmaps (Fig. 15) are
likewise very high (almost 1), indicating that modern LLMs share a strong and consistent ability to
identify key semantic units.


**Prompt format:** We also examined the influence of the main question and contextual input (preceding subquestions and their answers). Fig. 13 shows that head importance maps vary noticeably
across these changes, highlighting the importance of including both the main question and contextual information when identifying cognitive heads.


A.9 MORE RESULTS


We conducted probing experiments using 3 random seeds. The results in Fig. 16 demonstrate that
our probing method is highly stable across seeds, with Pearson correlations of the heatmaps reaching
1 for all eight functions in Qwen2.5-VL-3B-Instruct.


23



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-22-0.png)
![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-23-0.png)

Figure 14: Pearson Correlation of different K of TopK token across eight functions for Qwen2.5VL-3B-Instruct.


Figure 15: Pearson Correlation of different LLM selection across eight functions for Qwen2.5-VL3B-Instruct.


24



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-23-1.png)
Table 10: Examples of topk token selection using different LLMs.
","The ""Investigating the Functional Roles..."" paper tested the following ""model scales"" of the ""Qwen"" family:

*   **Qwen2.5-VL-3B-Instruct** (referred to as Qwen3B)
*   **Qwen2.5-VL-7B-Instruct** (referred to as Qwen7B)

These models are explicitly mentioned in Table 1 and various figure captions throughout the documents.",109,3502,3612,,3,,2026-01-02T20:15:17.361424
"What is the ""dimension d_model"" used for the base model in ""Attention Is All You Need""?",Entity-Specific Attribute Retrieval,"[Document 1]
Source: attention_is_all_you_need_raw_chunks_2k.jsonl
Chunk ID: attention_is_all_you_need_raw_chunks_2k_1
Content: [10], consuming the previously generated symbols as additional input when generating the next.


2


![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_is_all_you_need.pdf-2-0.png)

Figure 1: The Transformer - model architecture.


The Transformer follows this overall architecture using stacked self-attention and point-wise, fully
connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,
respectively.


**3.1** **Encoder and Decoder Stacks**


**Encoder:** The encoder is composed of a stack of _N_ = 6 identical layers. Each layer has two
sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network. We employ a residual connection [11] around each of
the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is
LayerNorm( _x_ + Sublayer( _x_ )), where Sublayer( _x_ ) is the function implemented by the sub-layer
itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding
layers, produce outputs of dimension _d_ model = 512.


**Decoder:** The decoder is also composed of a stack of _N_ = 6 identical layers. In addition to the two
sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head
attention over the output of the encoder stack. Similar to the encoder, we employ residual connections
around each of the sub-layers, followed by layer normalization. We also modify the self-attention
sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This
masking, combined with fact that the output embeddings are offset by one position, ensures that the
predictions for position _i_ can depend only on the known outputs at positions less than _i_ .


**3.2** **Attention**


An attention function can be described as mapping a query and a set of key-value pairs to an output,
where the query, keys, values, and output are all vectors. The output is computed as a weighted sum


3


Scaled Dot-Product Attention Multi-Head Attention


Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several
attention layers running in parallel.


of the values, where the weight assigned to each value is computed by a compatibility function of the
query with the corresponding key.


**3.2.1** **Scaled Dot-Product Attention**


We call our particular attention ""Scaled Dot-Product Attention"" (Figure 2). The input consists of
queries and keys of dimension _dk_, and values of dimension _dv_ . We compute the dot products of the
query with all keys, divide each by _[√]_ _dk_, and apply a softmax function to obtain the weights on the
values.


In practice, we compute the attention function on a set of queries simultaneously, packed together
into a matrix _Q_ . The keys and values are also packed together into matrices _K_ and _V_ . We compute
the matrix of outputs as:


Attention( _Q, K, V_ ) = softmax( _[Q][K]_ _[T]_ ) _V_ (1)

~~_√_~~ _dk_


The two most commonly used attention functions are additive attention [2], and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor
of ~~_√_~~ 1 _dk_ . Additive attention computes the compatibility function using a feed-forward network with
a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is
much faster and more space-efficient in practice, since it can be implemented using highly optimized
matrix multiplication code.


While for small values of _dk_ the two mechanisms perform similarly, additive attention outperforms
dot product attention without scaling for larger values of _dk_ [3]. We suspect that for large values of
_dk_, the dot products grow large in magnitude, pushing the softmax function into regions where it has
extremely small gradients [4] . To counteract this effect, we scale the dot products by ~~_√_~~ 1 _dk_ .


**3.2.2** **Multi-Head Attention**


Instead of performing a single attention function with _d_ model-dimensional keys, values and queries,
we found it beneficial to linearly project the queries, keys and values _h_ times with different, learned
linear projections to _dk_, _dk_ and _dv_ dimensions, respectively. On each of these projected versions of
queries, keys and values we then perform the attention function in parallel, yielding _dv_ -dimensional


4To illustrate why the dot products get large, assume that the components of _q_ and _k_ are independent random
variables with mean 0 and variance 1. Then their dot product, _q · k_ = [�] _[d]_ _i_ =1 _[k]_ _[q][i][k][i]_ [, has mean][ 0][ and variance] _[ d][k]_ [.]


4



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_is_all_you_need.pdf-3-0.png)

![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_is_all_you_need.pdf-3-1.png)
output values. These are concatenated and once again projected, resulting in the final values, as
depicted in Figure 2.


Multi-head attention allows the model to jointly attend to information from different representation
subspaces at different positions. With a single attention head, averaging inhibits this.


MultiHead( _Q, K, V_ ) = Concat(head1 _, ...,_ headh) _W_ _[O]_

where headi = Attention( _QWi_ _[Q][, KW][ K]_ _i_ _[, V W][ V]_ _i_ [)]


Where the projections are parameter matrices _Wi_ _[Q]_ _∈_ R _[d]_ [model] _[×][d][k]_, _Wi_ _[K]_ _∈_ R _[d]_ [model] _[×][d][k]_, _Wi_ _[V]_ _∈_ R _[d]_ [model] _[×][d][v]_
and _W_ _[O]_ _∈_ R _[hd][v][×][d]_ [model] .


In this work we employ _h_ = 8 parallel attention layers, or heads. For each of these we use
_dk_ = _dv_ = _d_ model _/h_ = 64. Due to the reduced dimension of each head, the total computational cost
is similar to that of single-head attention with full dimensionality.


**3.2.3** **Applications of Attention in our Model**


The Transformer uses multi-head attention in three different ways:


    - In ""encoder-decoder attention"" layers, the queries come from the previous decoder layer,
and the memory keys and values come from the output of the encoder. This allows every
position in the decoder to attend over all positions in the input sequence. This mimics the
typical encoder-decoder attention mechanisms in sequence-to-sequence models such as

[38, 2, 9].


    - The encoder contains self-attention layers. In a self-attention layer all of the keys, values
and queries come from the same place, in this case, the output of the previous layer in the
encoder. Each position in the encoder can attend to all positions in the previous layer of the
encoder.


    - Similarly, self-attention layers in the decoder allow each position in the decoder to attend to
all positions in the decoder up to and including that position. We need to prevent leftward
information flow in the decoder to preserve the auto-regressive property. We implement this
inside of scaled dot-product attention by masking out (setting to _−∞_ ) all values in the input
of the softmax which correspond to illegal connections. See Figure 2.

---

[Document 2]
Source: attention_is_all_you_need_raw_chunks_2k.jsonl
Chunk ID: attention_is_all_you_need_raw_chunks_2k_2
Content: [38, 2, 9].


    - The encoder contains self-attention layers. In a self-attention layer all of the keys, values
and queries come from the same place, in this case, the output of the previous layer in the
encoder. Each position in the encoder can attend to all positions in the previous layer of the
encoder.


    - Similarly, self-attention layers in the decoder allow each position in the decoder to attend to
all positions in the decoder up to and including that position. We need to prevent leftward
information flow in the decoder to preserve the auto-regressive property. We implement this
inside of scaled dot-product attention by masking out (setting to _−∞_ ) all values in the input
of the softmax which correspond to illegal connections. See Figure 2.


**3.3** **Position-wise Feed-Forward Networks**


In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully
connected feed-forward network, which is applied to each position separately and identically. This
consists of two linear transformations with a ReLU activation in between.


FFN( _x_ ) = max(0 _, xW_ 1 + _b_ 1) _W_ 2 + _b_ 2 (2)


While the linear transformations are the same across different positions, they use different parameters
from layer to layer. Another way of describing this is as two convolutions with kernel size 1.
The dimensionality of input and output is _d_ model = 512, and the inner-layer has dimensionality
_dff_ = 2048.


**3.4** **Embeddings and Softmax**


Similarly to other sequence transduction models, we use learned embeddings to convert the input
tokens and output tokens to vectors of dimension _d_ model. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In
our model, we share the same weight matrix between the two embedding layers and the pre-softmax
linear transformation, similar to [30]. In the embedding layers, we multiply those weights by _[√]_ _d_ model.


5


Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations
for different layer types. _n_ is the sequence length, _d_ is the representation dimension, _k_ is the kernel
size of convolutions and _r_ the size of the neighborhood in restricted self-attention.


Layer Type Complexity per Layer Sequential Maximum Path Length
Operations
Self-Attention _O_ ( _n_ [2] _· d_ ) _O_ (1) _O_ (1)
Recurrent _O_ ( _n · d_ [2] ) _O_ ( _n_ ) _O_ ( _n_ )
Convolutional _O_ ( _k · n · d_ [2] ) _O_ (1) _O_ ( _logk_ ( _n_ ))
Self-Attention (restricted) _O_ ( _r · n · d_ ) _O_ (1) _O_ ( _n/r_ )


**3.5** **Positional Encoding**


Since our model contains no recurrence and no convolution, in order for the model to make use of the
order of the sequence, we must inject some information about the relative or absolute position of the
tokens in the sequence. To this end, we add ""positional encodings"" to the input embeddings at the
bottoms of the encoder and decoder stacks. The positional encodings have the same dimension _d_ model
as the embeddings, so that the two can be summed. There are many choices of positional encodings,
learned and fixed [9].


In this work, we use sine and cosine functions of different frequencies:


_PE_ ( _pos,_ 2 _i_ ) = _sin_ ( _pos/_ 10000 [2] _[i/d]_ [model] )

_PE_ ( _pos,_ 2 _i_ +1) = _cos_ ( _pos/_ 10000 [2] _[i/d]_ [model] )


where _pos_ is the position and _i_ is the dimension. That is, each dimension of the positional encoding
corresponds to a sinusoid. The wavelengths form a geometric progression from 2 _π_ to 10000 _·_ 2 _π_ . We
chose this function because we hypothesized it would allow the model to easily learn to attend by
relative positions, since for any fixed offset _k_, _PEpos_ + _k_ can be represented as a linear function of
_PEpos_ .


We also experimented with using learned positional embeddings [9] instead, and found that the two
versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version
because it may allow the model to extrapolate to sequence lengths longer than the ones encountered
during training.


**4** **Why Self-Attention**


In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations
( _x_ 1 _, ..., xn_ ) to another sequence of equal length ( _z_ 1 _, ..., zn_ ), with _xi, zi ∈_ R _[d]_, such as a hidden
layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we
consider three desiderata.


One is the total computational complexity per layer. Another is the amount of computation that can
be parallelized, as measured by the minimum number of sequential operations required.


The third is the path length between long-range dependencies in the network. Learning long-range
dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the
ability to learn such dependencies is the length of the paths forward and backward signals have to
traverse in the network. The shorter these paths between any combination of positions in the input
and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare
the maximum path length between any two input and output positions in networks composed of the
different layer types.


As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially
executed operations, whereas a recurrent layer requires _O_ ( _n_ ) sequential operations. In terms of
computational complexity, self-attention layers are faster than recurrent layers when the sequence


6


length _n_ is smaller than the representation dimensionality _d_, which is most often the case with
sentence representations used by state-of-the-art models in machine translations, such as word-piece

[38] and byte-pair [31] representations. To improve computational performance for tasks involving
very long sequences, self-attention could be restricted to considering only a neighborhood of size _r_ in
the input sequence centered around the respective output position. This would increase the maximum
path length to _O_ ( _n/r_ ). We plan to investigate this approach further in future work.


A single convolutional layer with kernel width _k < n_ does not connect all pairs of input and output
positions. Doing so requires a stack of _O_ ( _n/k_ ) convolutional layers in the case of contiguous kernels,
or _O_ ( _logk_ ( _n_ )) in the case of dilated convolutions [18], increasing the length of the longest paths
between any two positions in the network. Convolutional layers are generally more expensive than
recurrent layers, by a factor of _k_ . Separable convolutions [6], however, decrease the complexity
considerably, to _O_ ( _k · n · d_ + _n · d_ [2] ). Even with _k_ = _n_, however, the complexity of a separable
convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,
the approach we take in our model.


As side benefit, self-attention could yield more interpretable models. We inspect attention distributions
from our models and present and discuss examples in the appendix. Not only do individual attention
heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic
and semantic structure of the sentences.


**5** **Training**


This section describes the training regime for our models.


**5.1** **Training Data and Batching**

---

[Document 3]
Source: attention_is_all_you_need_raw_chunks_2k.jsonl
Chunk ID: attention_is_all_you_need_raw_chunks_2k_0
Content: Provided proper attribution is provided, Google hereby grants permission to
reproduce the tables and figures in this paper solely for use in journalistic or
scholarly works.

## **Attention Is All You Need**



**Niki Parmar** _[∗]_
Google Research
```
nikip@google.com

```


**Ashish Vaswani** _[∗]_
Google Brain
```
avaswani@google.com

```

**Llion Jones** _[∗]_
Google Research
```
 llion@google.com

```


**Noam Shazeer** _[∗]_
Google Brain
```
noam@google.com

```


**Jakob Uszkoreit** _[∗]_
Google Research
```
usz@google.com

```


**Aidan N. Gomez** _[∗†]_
University of Toronto
```
aidan@cs.toronto.edu

```


**Łukasz Kaiser** _[∗]_
Google Brain
```
lukaszkaiser@google.com

```


**Illia Polosukhin** _[∗‡]_

```
             illia.polosukhin@gmail.com

```

**Abstract**


The dominant sequence transduction models are based on complex recurrent or
convolutional neural networks that include an encoder and a decoder. The best
performing models also connect the encoder and decoder through an attention
mechanism. We propose a new simple network architecture, the Transformer,
based solely on attention mechanisms, dispensing with recurrence and convolutions
entirely. Experiments on two machine translation tasks show these models to
be superior in quality while being more parallelizable and requiring significantly
less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including
ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,
our model establishes a new single-model state-of-the-art BLEU score of 41.8 after
training for 3.5 days on eight GPUs, a small fraction of the training costs of the
best models from the literature. We show that the Transformer generalizes well to
other tasks by applying it successfully to English constituency parsing both with
large and limited training data.


_∗_ Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started
the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and
has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head
attention and the parameter-free position representation and became the other person involved in nearly every
detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and
tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and
efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and
implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating
our research.

_†_ Work performed while at Google Brain.

_‡_ Work performed while at Google Research.


31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.


**1** **Introduction**


Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks
in particular, have been firmly established as state of the art approaches in sequence modeling and
transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous
efforts have since continued to push the boundaries of recurrent language models and encoder-decoder
architectures [38, 24, 15].


Recurrent models typically factor computation along the symbol positions of the input and output
sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden
states _ht_, as a function of the previous hidden state _ht−_ 1 and the input for position _t_ . This inherently
sequential nature precludes parallelization within training examples, which becomes critical at longer
sequence lengths, as memory constraints limit batching across examples. Recent work has achieved
significant improvements in computational efficiency through factorization tricks [21] and conditional
computation [32], while also improving model performance in case of the latter. The fundamental
constraint of sequential computation, however, remains.


Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in
the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms
are used in conjunction with a recurrent network.


In this work we propose the Transformer, a model architecture eschewing recurrence and instead
relying entirely on an attention mechanism to draw global dependencies between input and output.
The Transformer allows for significantly more parallelization and can reach a new state of the art in
translation quality after being trained for as little as twelve hours on eight P100 GPUs.


**2** **Background**


The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU

[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building
block, computing hidden representations in parallel for all input and output positions. In these models,
the number of operations required to relate signals from two arbitrary input or output positions grows
in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes
it more difficult to learn dependencies between distant positions [12]. In the Transformer this is
reduced to a constant number of operations, albeit at the cost of reduced effective resolution due
to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as
described in section 3.2.


Self-attention, sometimes called intra-attention is an attention mechanism relating different positions
of a single sequence in order to compute a representation of the sequence. Self-attention has been
used successfully in a variety of tasks including reading comprehension, abstractive summarization,
textual entailment and learning task-independent sentence representations [4, 27, 28, 22].


End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering and
language modeling tasks [34].


To the best of our knowledge, however, the Transformer is the first transduction model relying
entirely on self-attention to compute representations of its input and output without using sequencealigned RNNs or convolution. In the following sections, we will describe the Transformer, motivate
self-attention and discuss its advantages over models such as [17, 18] and [9].


**3** **Model Architecture**


Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].
Here, the encoder maps an input sequence of symbol representations ( _x_ 1 _, ..., xn_ ) to a sequence
of continuous representations **z** = ( _z_ 1 _, ..., zn_ ). Given **z**, the decoder then generates an output
sequence ( _y_ 1 _, ..., ym_ ) of symbols one element at a time. At each step the model is auto-regressive

[10], consuming the previously generated symbols as additional input when generating the next.


2


![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_is_all_you_need.pdf-2-0.png)

Figure 1: The Transformer - model architecture.


The Transformer follows this overall architecture using stacked self-attention and point-wise, fully
connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,
respectively.


**3.1** **Encoder and Decoder Stacks**
","The ""dimension d_model"" used for the base model in ""Attention Is All You Need"" is 512. This dimension is used for the outputs of all sub-layers, embedding layers, and positional encodings.",99,2644,2744,,3,,2026-01-02T20:15:20.106274
"Which ""baseline models"" were used for comparison in the ""Lane-Keeping of Autonomous Vehicles"" experiment for NAC?",Entity-Specific Attribute Retrieval,"[Document 1]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_5
Content: **3.2. Lane-Keeping of Autonomous Vehicles**


Lane keeping in autonomous vehicles (AVs) is a fundamental problem in robotics and AI. Early works (Tiang et al.,
2018; Park et al., 2021) primarily emphasized accuracy,
often relying on large models. More recent research (Lechner et al., 2020; Razzaq & Hongwei, 2023) has shifted toward designing compact architectures suitable for resourceconstrained devices. The goal of this experiment is to create
a long causal structure between the road’s horizon and the



corresponding steering commands. To evaluate, we used
two widely adopted simulation environments: (i) OpenAI
CarRacing (Brockman et al., 2016); and (ii) the Udacity SelfDriving Car Simulator (uda). In OpenAI CarRacing, the
task is to classify steering actions from a predefined action
set. In contrast, the Udacity Simulator requires predicting a
continuous trajectory of steering values. We implemented
the AI models proposed by (Razzaq & Hongwei, 2023),
replacing the recurrent layer with NAC and its counterparts.
All models achieved around 80% accuracy on average in
the CarRacing benchmark. Notably, NAC-PW performed
the best, reaching the highest accuracy of 80.72%, followed
by NAC-Steady, ranked second with 80.62%. LSTM and
GRU took third position, achieving 80.60% on average.
In the Udacity benchmark, NAC-32k performed the best,
achieving the lowest MSE of 0.0170. NAC-Exact followed
with 0.0173, and ContiFormer ranked third with 0.0174.
To visualize saliency maps for these experiments, refer to
Appendix D.3.3. Experimental videos are available for the
[OpenAI CarRacing [click here] and for the Udacity Simula-](https://www.youtube.com/watch?v=kwTNU8aV8-I)
[tor [click here].](https://www.youtube.com/watch?v=mMRVsNUQ8i0)


**3.3. Industry 4.0**


Industry 4.0 has transformed manufacturing, making prognostic health management (PHM) systems essential. A key
PHM task is estimating the remaining useful life (RUL) of
components, particularly rolling element bearings (REB),
which account for 40–50% of machine failures (Ding et al.,
2021; Zhuang et al., 2021). The objective is to learn degradation features from one operating condition of a dataset
and generalize to unseen conditions within the same dataset.
Furthermore, the model should provide accurate RUL estimation on entirely different datasets, while maintaining a



6


**Neuronal Attention Circuit (NAC) for Representation Learning**


_Table 1._ Model Performance Across All Categories and Datasets


**Irregular Time-Series** **Lane-Keeeping of AVs** **Industry 4.0**
**Model**

**E-MNIST (↑)** **PAR (↑)** **CarRacing (↑)** **Udacity (↓)** **PRONOSTIA (↓)** **XJTU-SY (↓)** **HUST (↓)**


RNN 95.59 _[±]_ [0.37] 88.77 _[±]_ [0.58] 78.90 _[±]_ [3.35] 0.0210 _[±]_ [0.0014] 42.05 _[±]_ [7.49] 31.07 _[±]_ [6.63] 42.22 _[±]_ [8.16]

LSTM 95.88 _[±]_ [0.23] 88.36 _[±]_ [0.79] 80.60 _[±]_ [0.12] 0.0181 _[±]_ [0.0014] 41.87 _[±]_ [2.88] 31.99 _[±]_ [8.32] 44.09 _[±]_ [2.14]

GRU 95.85 _[±]_ [0.22] 88.68 _[±]_ [1.35] 80.60 _[±]_ [0.22] 0.0206 _[±]_ [0.0014] 44.22 _[±]_ [4.60] 26.65 _[±]_ [4.49] 41.86 _[±]_ [7.96]

CT-RNN 95.18 _[±]_ [0.20] 88.71 _[±]_ [0.87] 80.21 _[±]_ [0.27] 0.0206 _[±]_ [0.0013] 44.32 _[±]_ [8.69] 26.01 _[±]_ [8.74] 39.99 _[±]_ [6.33]

GRU-ODE **96.04** _[±]_ **[0.13]** **89.01** _[±]_ **[1.55]** 80.29 _[±]_ [0.72] 0.0188 _[±]_ [0.0016] 45.11 _[±]_ [3.19] 31.20 _[±]_ [8.69] 43.91 _[±]_ [7.10]

PhasedLSTM 95.79 _[±]_ [0.14] 88.93 _[±]_ [1.08] 80.35 _[±]_ [0.38] 0.0186 _[±]_ [0.0015] 44.15 _[±]_ [4.80] 35.49 _[±]_ [5.54] 38.66 _[±]_ [5.55]

mmRNN 95.74 _[±]_ [0.27] 88.48 _[±]_ [0.46] 80.13 _[±]_ [0.54] 0.0205 _[±]_ [0.0027] 48.50 _[±]_ [4.60] 27.84 _[±]_ [4.05] 40.11 _[±]_ [9.56]

LTC 95.25 _[±]_ [0.00] 88.12 _[±]_ [0.68] 76.37 _[±]_ [3.01] 0.0245 _[±]_ [0.0024] 48.14 _[±]_ [5.01] 36.83 _[±]_ [8.57] 61.82 _[±]_ [15.64]

CfC 94.16 _[±]_ [0.49] 88.60 _[±]_ [0.34] 80.59 _[±]_ [0.33] 0.0198 _[±]_ [0.0022] 47.78 _[±]_ [3.54] 35.51 _[±]_ [3.94] 54.09 _[±]_ [10.13]

Attention 95.68 _[±]_ [0.23] 88.29 _[±]_ [0.98] 80.40 _[±]_ [0.26] 0.0193 _[±]_ [0.0009] 41.89 _[±]_ [6.98] 26.29 _[±]_ [4.06] 40.28 _[±]_ [4.23]

MHA 95.94 _[±]_ [0.15] 88.36 _[±]_ [1.06] 79.99 _[±]_ [0.49] 0.0185 _[±]_ [0.0017] 45.36 _[±]_ [5.16] 37.31 _[±]_ [12.20] 41.40 _[±]_ [7.72]

mTAN 95.97 _[±]_ [0.25] 88.08 _[±]_ [0.94] 80.86 _[±]_ [0.22] 0.0178 _[±]_ [0.0005] 44.41 _[±]_ [7.15] 41.34 _[±]_ [3.72] 66.29 _[±]_ [4.25]

---

[Document 2]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_4
Content: **2.3. NAC as Universal Approximator**


We now establish the universal approximation capability of
NAC by extending the classical Universal Approximation
Theorem (UAT) (Nishijima, 2021) to the proposed mechanism. For brevity, we consider a network with a single
NAC layer processing fixed-dimensional inputs, though the
argument generalizes to sequences.


**Theorem 2** (Universal Approximation by NAC) **.** _Let K ⊂_
R _[n]_ _be a compact set and f_ : _K →_ R _[m]_ _be a continuous_
_function. For any ϵ >_ 0 _, there exists a neural network_
_consisting of a single NAC layer, with sufficiently large_
_model dimension dmodel, number of heads H, sparsity s,_
_and nonlinear activations, such that the network’s output_
_g_ : R _[n]_ _→_ R _[m]_ _satisfies_


sup _∥f_ ( _x_ ) _−_ _g_ ( _x_ ) _∥_ _< ϵ._ (20)
_x∈K_


_The proof is provided in Appendix B.3._


**3. Evaluation**


We evaluate the proposed architecture against a range of
baselines, including (DT & CT) RNN, (DT & CT) attention,
and multiple NAC ablation configurations. Experiments
are conducted across diverse domains, including irregular
time-series modeling, lane keeping of autonomous vehicles,
and Industry 4.0 prognostics. All results are obtained using 5-fold cross-validation, where models are trained using
BPTT (see Appendix C.2) on each fold and evaluated across
all folds. We report the mean ( _µ_ ) and standard deviation ( _σ_ )
to capture variability and quantify uncertainty in the predictions. Table 1 provides results for all experiments, and the
details of the baselines, ablation, environment utilized, the
data curation and preprocessing, and neural network architectures for all experiments are provided in the Appendix
D.3.


**3.1. Irregular Time-series**


We evaluate the proposed architecture on two irregular timeseries datasets: (i) Event-based MNIST; and (ii) Person
Activity Recognition (PAR).



For finite _t_, the exponential factor (1 _−_ _e_ _[−][ω][τ][ t]_ ) regulates the
buildup of attention, giving _ωτ_ a temporal gating role. Normalizing across all keys via _softmax_ yields attention weights
_αt_ = softmax( _at_ ), defining a valid probability distribution
where _ϕ_ amplifies or suppresses content alignments, and _ωτ_
shapes both the speed and saturation of these preferences.
As _t →∞_, the trajectory converges to the steady state


_a_ _[∗]_ _t_ [=] _[ϕ]_ _≈_ _[q][⊤][k]_ _,_ (18)

_ωτ_ ~~_√_~~ _dk_


which is analogous to scaled-dot attention under specific
parameterization when the backbone _NN_ backbone is configured as a linear projection such that _ϕ_ ( **u** ) = _q_ _[⊤]_ _k_ and
_ωτ_ ( _u_ ) = _[√]_ _dk_ (e.g., by setting NCP weights to emulate
bilinear forms and disabling nonlinearities). In general, the
nonlinear backbone allows for more expressive similarities,
with the approximation holding when trained to mimic dot
products.
**Attention output:** Finally, the attention output is computed
by integrating the attention weights with the value matrix:


NAC( _q, k, v_ ) = _αtvtdt_ (19)
� _T_


In practice, the integration is approximated using a Riemannstyle approach, where the weighted elements are computed
by multiplying each _vt_ with its corresponding _αt_ . These are
then summed and multiplied by a fixed pseudo-time step
_δt_, chosen as a scalar (typically between 0.5–1.0) hyperparameter during layer initialization. This yields a continuous
analogue of standard weighted sums, giving finer resolution
of the attention trajectory without altering the underlying
values. Sensitivity to attention output w.r.t _δt_ is visualized
in Appendix D.2.


2.2.1. EXTENSION TO MULTI-HEAD


To scale this mechanism to multi-head attention, we project
the input sequence into _H_ independent subspaces (heads)
of dimension _d_ model _/H_, yielding query, key, and value tensors ( _q_ [(] _[h]_ [)] _, k_ [(] _[h]_ [)] _, v_ [(] _[h]_ [)] ) for _h ∈{_ 1 _, . . ., H}_ . For each head,
pairwise logits are computed according to Eqns. 2,3 or 18,



5


**Neuronal Attention Circuit (NAC) for Representation Learning**


_Figure 2._ Illustration of the architecture of **(a)** Neuronal Attention Circuit mechanism ; **(b)** Multi-Head Extension



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/neuronal_attention_circuits.pdf-5-0.png)

**Event-based MNIST:** Event-based MNIST is the trans
formation of the widely recognized MNIST dataset with
irregular sampling added originally proposed in (Lechner
& Hasani, 2022). The transformation was done in two
steps: (i) flattening each 28×28 image into a time series
of length 784, and (ii) encoding the binary time series into
an event-based format by collapsing consecutive identical
values (e.g., 1,1,1,1 → (1, t=4)). This representation requires models to handle temporal dependencies effectively.
NAC-PW achieved first place with an accuracy of 96.64%,
followed by NAC-Exact/05s/8k at 96.12%. GRU-ODE and
ContiFormer ranked third with 96.04%.

**Person Activity Recognition (PAR):** We employed the
Localized Person Activity dataset from UC Irvine (Vidulin
et al., 2010). The dataset contains data from five participants,
each equipped with inertial measurement sensors sampled
every 211 ms. The goal of this experiment is to predict a
person’s activity from a set of predefined actions, making it
a classification task. All models performed well on this task,
with NAC-PW achieving 89.15% accuracy and taking first
place. NAC-Exact/05s/8k and GRU-ODE ranked second
with 89.01% accuracy, while NAC-02s ranked third with
88.84% mean accuracy.


**3.2. Lane-Keeping of Autonomous Vehicles**


Lane keeping in autonomous vehicles (AVs) is a fundamental problem in robotics and AI. Early works (Tiang et al.,
2018; Park et al., 2021) primarily emphasized accuracy,
often relying on large models. More recent research (Lechner et al., 2020; Razzaq & Hongwei, 2023) has shifted toward designing compact architectures suitable for resourceconstrained devices. The goal of this experiment is to create
a long causal structure between the road’s horizon and the

---

[Document 3]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_14
Content: **D.2. Ablations Details**


The brief descriptions of variants and ablation are also divided into four subcategories:
**Top-** _**K**_ **Ablations:** _NAC-2k_ uses Top- _K_ =2 to compute the
logits and _NAC-32k_ uses Top- _K_ =32. All variants use the
exact computation mode with 50% sparsity.
**Sparsity Ablations:** _NAC-02s_ uses 20% sparsity to compute the logits and _NAC-09s_ uses 90%. _NAC-PW_ employs
full pairwise (non-sparse) concatenation for input curation.
_NAC-FC_ replaces the sparse NCP gating mechanism with
a simple fully connected layer. All variants use the exact
computation mode with Top- _K_ =8.
**Modes variants:** _NAC-Euler_ computes attention logits using the explicit Euler integration method. _NAC-Steady_ derives attention logits from the steady-state solution of the
exact formulation. _NAC-Exact/05s/8k_ computes attention
logits using the closed-form exact solution. It also overlaps
with other ablations, so we combined it into a single one.
All modes use Top- _K_ =8, 50% sparsity and _δt_ =1.0. The
sensitivity of NAC to _δt_ is visualized in Figure 4


_Figure 4._ Effect of _δt_ on output of NAC.



14



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/neuronal_attention_circuits.pdf-13-0.png)
**Neuronal Attention Circuit (NAC) for Representation Learning**



**D.3. Experimental Details**


D.3.1. EVENT-BASED MNIST


**Dataset Explanation and Curation:** The MNIST dataset,
introduced by (Deng, 2012), is a widely used benchmark for
computer vision and image classification tasks. It consists
of 70,000 grayscale images of handwritten digits (0–9), each
of size 28 _×_ 28 pixels, split into 60,000 training and 10,000
testing samples.
**Preprocessing:** We follow the preprocessing pipeline described in (Lechner & Hasani, 2022), which proceeds as
follows. First, a threshold is applied to convert the 8-bit
pixel values into binary values, with 128 as the threshold
on a scale from 0 (minimum intensity) to 255 (maximum
intensity). Second, each 28 _×_ 28 image is reshaped into a
one-dimensional time series of length 784. Third, the binary
time series is encoded in an event-based format, eliminating
consecutive occurrences of the same value; for example, the
sequence [1 _,_ 1 _,_ 1 _,_ 1] is transformed into (1 _, t_ = 4). This encoding introduces a temporal dimension and compresses the
sequences from 784 to an average of 53 time steps. Finally,
to facilitate efficient batching and training, each sequence
is padded to a fixed length of 256, and the time dimension
is normalized such that each symbol corresponds to one
unit of time. The resulting dataset defines a per-sequence
classification problem on irregularly sampled time series.
**Neural Network Architecture:** We develop an end-to-end
hybrid neural network by combining compact convolutional
layers with NAC or counterparts baselines for fair comparison. Detailed hyperparameters and architectural specifications are provided in Table 4.


D.3.2. PERSON ACTIVITY RECOGNITION (PAR)


**Dataset Explanation and Curation:** We used the
Localized Person Activity Recognition dataset provided by
UC Irvine (Vidulin et al., 2010). The dataset comprises
25 recordings of human participants performing different
physical activities. The eleven possible activities are
“walking,” “falling,” “lying down,” “lying,” “sitting down,”
“sitting,” “standing up from lying,” “on all fours,” “sitting
on the ground,” “standing up from sitting,” and “standing
up from sitting on the ground.” The objective of this
experiment is to recognize the participant’s activity from
inertial sensors, formulating the task as a per-time-step
classification problem. The input data consist of sensor
readings from four inertial measurement units placed on
participants’ arms and feet. While the sensors are sampled
at a fixed interval of 211 ms, recordings exhibit different
phase shifts and are thus treated as irregularly sampled time
series.

**Preprocessing:** We first separated each participant’s
recordings based on sequence identity and calculated
elapsed time in seconds using the sampling period. To



mitigate class imbalance, we removed excess samples from
overrepresented classes to match the size of the smallest
class. Subsequently, the data were normalized using a
standard scaler. Finally, the dataset was split into a 90:10
ratio for training and testing.
**Neural Network Architecture:** Following the approach in
Section D.3.1, we developed an end-to-end hybrid neural
network combining convolutional heads with NAC or other
baselines. Hyperparameter details are summarized in Table
4.


D.3.3. AUTONOMOUS VEHICLE


**Dataset Explanation and Curation:** We followed the data
collection methodology described in (Razzaq & Hongwei,
2023). For OpenAI-CarRacing, a PPO-trained agent (5M
timesteps) was used to record 20 episodes, yielding approximately 48,174 RGB images of size 92 _×_ 92 _×_ 3 with
corresponding action labels across five discrete actions (noact, move left, forward, move right, stop). The dataset was
split with 10% reserved for testing and the remaining 90%
for training. For the Udacity simulator, we manually controlled the vehicle for 50 minutes, producing 15647 RGB
images of size 320 _×_ 160 _×_ 3, captured from three camera
streams (left, center, right) along with their corresponding
continuous steering values. This dataset was split into 20%
testing and 80% training.
**Preprocessing:** No preprocessing was applied to the
OpenAI-CarRacing dataset. For the Udacity simulator, we
followed the preprocessing steps in (Shibuya, 2017). Each
image was first cropped to remove irrelevant regions and
resized to 66 _×_ 120 _×_ 3. Images were then converted from
RGB to YUV color space to match the network input. To improve robustness, data augmentation techniques, including
random flips, translations, shadow overlays, and brightness
variations, were applied to simulate lateral shifts and diverse
lighting conditions.
**Neural Network Architecture:** For OpenAI-CarRacing,
we modified the neural network architecture proposed in
(Razzaq & Hongwei, 2023), which combines compact CNN
layers for spatial feature extraction with LNNs to capture
temporal dynamics. In our implementation, the LNN layers
were replaced with NAC and its comparable alternatives
for fair evaluation. Full hyperparameter configurations are
provided in Table 4. For the Udacity simulator, we modified
the network proposed in (Bojarski et al., 2016) by replacing
three latent MLP layers with NAC and its counterparts. Full
hyperparameters for this configuration are summarized in
Table 4.

**Saliency Maps:** A saliency map visualizes the regions of
the input that a model attends to when making decisions.
Figure 5 shows the saliency maps for the OpenAI CarRacing environment. We observe that only NAC (Steady, Euler,



15


**Neuronal Attention Circuit (NAC) for Representation Learning**


_Figure 5._ Saliency maps for OpenAI CarRacing


_Figure 6._ Saliency maps for Udacity Simulator



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/neuronal_attention_circuits.pdf-15-0.png)

![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/neuronal_attention_circuits.pdf-15-1.png)
","The ""baseline models"" used for comparison in the ""Lane-Keeping of Autonomous Vehicles"" experiment for NAC, as listed in Table 1 and mentioned in the text, include:

*   RNN
*   LSTM
*   GRU
*   CT-RNN
*   GRU-ODE
*   PhasedLSTM
*   mmRNN
*   LTC
*   CfC
*   Attention
*   MHA
*   mTAN
*   ContiFormer",106,7535,7642,,3,,2026-01-02T20:15:27.749719
"What ""window size"" was primarily used for the ""aggressive"" SWA settings in the SWAA experiments?",Entity-Specific Attribute Retrieval,"[Document 1]
Source: sliding_window_attention_annotated_chunks_2k.jsonl
Chunk ID: sliding_window_attention_annotated_chunks_2k_0
Content: ## **Sliding Window Attention Adaptation**

Yijiong Yu [a], Jiale Liu [b], Qingyun Wu [b], Huazheng Wang [a], and Ji Pei [c]


aOregon State University, {yuyiji, huazheng.wang}@oregonstate.edu
bPenn State University, {jiale.liu, qingyun.wu}@psu.edu
cDeepSolution, research@deepsolution.chat



**Abstract**


The self-attention mechanism in Transformer
based Large Language Models (LLMs) scales
quadratically with input length, making longcontext inference expensive. Sliding window attention (SWA) reduces this cost to linear complexity, but naively enabling complete
SWA at inference-time for models pretrained
with full attention (FA) causes severe longcontext performance degradation due to training–inference mismatch. This makes us wonder: _Can FA-pretrained LLMs be well adapted_
_to SWA without pretraining?_ We investigate
this by proposing Sliding Window Attention
Adaptation (SWAA), a set of practical recipes
that combine five methods for better adaptation: (i) applying SWA only during prefilling;
(ii) preserving “sink” tokens; (iii) interleaving
FA/SWA layers; (iv) chain-of-thought (CoT);
and (v) fine-tuning. Our experiments show that
SWA adaptation is feasible while non-trivial:
no single method suffices, yet specific synergistic combinations effectively recover the original
long-context performance. We further analyze
the performance-efficiency trade-offs of different SWAA configurations and provide recommended recipes for diverse scenarios. Our code
[is available at github.](https://github.com/yuyijiong/sliding-window-attention-adaptation)


**1** **Introduction**


Transformer-based Large Language Models
(LLMs) (Vaswani et al., 2017) demonstrate
remarkable capabilities, but their self-attention
scales quadratically with the input sequence length,
making long context processing inefficient. Sliding
Window Attention (SWA), the most straightforward and widely adopted sparse attention
pattern, which restricts each token’s attention to a
fixed-size local window, reduces the computational
complexity to linearity, along with some other
benefits (see Appendix A).
To apply SWA to LLMs, typical solutions involve training a model with SWA from scratch, but



are prohibitively costly and cannot match the performance of state-of-the-art full-causal-attention

models like Qwen3 (Team, 2025b), mainly due
to the inability to reproduce pretraining data.
Training-free methods like streaming attention
(Xiao et al., 2024) can stabilize LLM outputs by retaining “sink tokens” while applying SWA, which
greatly improve efficiency but inevitably suffer
from severe long-context performance degradation
possibly due to the inaccessibility of distant tokens’
information (Xiao, 2025). This motivates a critical,
unexplored question: _Can a full-attention model_
_be adapted to sliding window attention at low cost_
_while maintaining long-context performance?_
We answer Yes to this question by proposing
Sliding Window Attention Adaptation(SWAA), a
set of recipes for adapting FA-pretrained models to
SWA, which requires neither costly pretraining nor
modifications to the standard Transformer architecture. Specifically, it systematically combines five
practical and composable methods:


1. **Full Attention (FA) Decode** : applying SWA
only during the prefilling stage while switching back to full attention for decoding.


2. **Keep First** _k_ **Tokens** : explicitly preserving
attention to the first _k_ “sink” tokens.


3. **Interleaving FA/SWA layers** : mix fullattention and SWA layers (e.g., assigning
SWA to half layers).


4. **Chain-of-Thought (CoT)** : enforcing an explicit ""thinking"" process during decoding.


5. **Fine-tuning with SWA** : lightweight SWAaware supervised fine-tuning on long-context
data.


Among these, FA Decode is a novel method we
introduce. Keep First _k_ Tokens and FA/SWA Interleaving have been proven effective in prior work



1


(Xiao et al., 2024; Team, 2024a; Zhang et al., 2024),
while CoT and fine-tuning are common LLM techniques. However, how these methods should be
combined to be actually effective for SWA adaptation remains unexplored.
Therefore, in our experiments, we evaluate
SWAA on Qwen3 (Team, 2025b) and Llama3.1
(Team, 2024b) across several long-context benchmarks, measuring both performance and efficiency
under a wide range of SWAA recipes. First, we
find that each method makes a distinct contribution,
but no single ingredient suffices to make SWA competitive with full attention. Second, we show that
specific synergistic combinations of methods can
recover a large fraction of the original long-context
performance. Third, we analyze the performanceefficiency trade-offs of different SWAA recipes and
identify some recommended configurations suitable for different deployment scenarios.
Rather than proposing a single globally optimal
configuration, we view SWAA as a flexible toolkit
of practical recipes: practitioners can select SWAA
recipes that match their accuracy and efficiency
constraints, or compose their own SWA adaptation
strategies by combining the available ingredients.
Our key contributions are:


1. We perform the first systematic study on adapting FA-pretrained LLMs to SWA without pretraining, revealing novel insights about how
SWA impacts LLMs and providing a foundation for future research in efficient sparse
attention.


2. We propose SWAA, a set of practical
SWA adaptation recipes that offer a robust
performance-efficiency balance for various
use cases, accelerating LLM inference from
the bottom level.


3. We implement our methods with FlashAttention (Dao, 2024) and vLLM (Kwon
et al., 2023), making it plug-and-play and userfriendly for practical deployment.


**2** **Related Works**


The _O_ ( _N_ [2] ) complexity of self-attention in Transformers (Vaswani et al., 2017) has spurred a wide
field of research about more efficient language
model architectures. Among the two most popular technological routes are sparse attention and
linear attention.



**2.1** **Sparse Attention**


Our work falls in this category. Sliding Window
Attention (SWA) represents the most basic form
of local sparse attention, yet its performance is inherently limited. Therefore, model architectures
such as Longformer (Beltagy et al., 2020), BigBird
(Zaheer et al., 2020), and RATTENTION (Wang
et al., 2025) combine local SWA on most tokens
with special global attention on specific tokens to
create a more powerful, albeit still sparse, pattern.
Popular LLMs like Gemma2 (Team, 2024a) adopt
SWA in half of their layers to balance the efficiency
of SWA and peformance of FA. Sliding Window
Attention Training (SWAT) (Fu et al., 2025b) introduces architectural changes, such as sigmoid
activation and balanced position embeddings, to
stabilize SWA performance. More advanced methods like Deepseek-sparse-attention (Yuan et al.,
2025; DeepSeek-AI, 2025b), although achieving
excellent quality, involve more complicated implementation and optimization due to semantic-aware
attention operations (e.g., selecting the most important tokens based on attention weights). Regardless,
almost all of the above methods require pretraining with a specific sparse pattern, which is costly
and fails to leverage the advantages of existing pretrained models.


LightTransfer (Zhang et al., 2024) is a promising
attempt at adapting existing models to SWA without pretraining, which has the same motivation as
ours. But it may generalize poorly across model
families (see Appendix G).


**2.2** **Linear Attention**


An alternative approach involves reformulating
the attention mechanism entirely to achieve linear, _O_ ( _N_ ), complexity. This includes methods
such as RNN-like linear attention transformers

---

[Document 2]
Source: sliding_window_attention_annotated_chunks_2k.jsonl
Chunk ID: sliding_window_attention_annotated_chunks_2k_3
Content: **Combinations exhibit strong synergy.** Recipes
that combine multiple methods deliver large gains:


  - **FA Decode + Keep First** _k_ substantially improves over naive SWA (rows 7–9), recovering roughly half to two-thirds of the gap on
the thinking model as _k_ increases. However,
increasing _k_ from 100 to 1000 yields almost
no improvement, indicating that _k_ does not
need to be exceedingly large.


  - **Interleaving Layers + FA Decode** is
markedly stronger (row 13), recovering most
of the gap for the thinking model.


  - **FA Decode + Interleaving Layers + Keep**
**First** _k_ perform best (rows 18). The thinking
model recovers close to 90% of the FA gap
even at 2k window.


**CoT synergizes with FA Decode.** Under recipes
that include FA Decode, the thinking model consistently benefits more than the non-thinking model
(rows 13 and 18), suggesting that CoT synergizes
with FA Decode: preserving global attention at
decoding time enables longer reasoning traces to
capitalize on context processed by SWA, confirming our hypothesis in Section 3.1.


**Sliding window size affects, but is not the deci-**
**sive role.** With FA Decode + Keep First _k_, accuracy improves as the window increases (rows 7,
14, 15), though benefits are modest until 8k. When
added with interleaving FA layers, moving from a
2k to a 4k window is enough to close the remaining
gap on the thinking model (row 22 matches the FA
baseline), indicating that FA Decode and Interleaving are the dominant drivers, and larger windows
mainly smooth residual error.



5


Table 1: Qwen3-4B-Thinking and Qwen3-4B-Instruct results on LongMemEval without SFT


**No.** **window size** **FA layers** **keep first** **FA decode** **Acc think** **Acc non-think**


0 Full [] 0 False **73.0** **62.0**
1 2k [] 0 False 3.2 11.0
2 8k [] 0 False 13.2 19.8


3 2k [] 10 False 16.0 15.6
4 2k [] 0 True 11.8 14.2
5 2k [1, 3, 5, ...] 0 False 13.4 18.4
6 8k [] 0 True 26.2 25.0


7 2k [] 10 True 38.2 20.6
8 2k [] 100 True 50.0 17.8
9 2k [] 1000 True 50.0 20.2
10 2k [0, 2, 4, ...] 10 False 17.0 14.8
11 2k [0, 2, 4, ...] 0 True 32.2 26.0
12 2k [1, 3, 5, ...] 10 False 25.8 36.4
13 2k [1, 3, 5, ...] 0 True 59.2 34.8
14 4k [] 10 True 38.0 24.4
15 8k [] 10 True 49.2 35.2


16 2k [0, 2, 4, ...] 10 True 36.0 17.2
17 2k [1, 3, 5, ...] 10 True 65.0 **53.6**
18 2k [1, 3, 5, ...] 100 True **68.8** 50.6
19 2k [1, 5, 9, ...] 10 True 53.2 31.4
20 2k [1, 9, 17, ...] 10 True 36.4 18.8
21 2k [3, 7, 11, ...] 10 True 54.2 34.6
22 4k [1, 3, 5, ...] 100 True **73.0** **54.2**
23 8k [1, 3, 5, ...] 100 True **71.6** **56.6**



**SWA/FA layer selection has large impacts.** Selecting less layers for FA, e.g. only one fourth or
eighth (row 19, 20), though more efficient, will
significantly decrease the improvement brought by
Interleaving Layers. More importantly, for Qwen34B, configuring _odd-numbered_ layers with full attention is significantly better than _even-numbered_
layers (row 11, 13, 16, 17). However, surprisingly,
this result is reversed for Qwen3-30B-A3B and
Llama3.1-8B-Instruct (row 10 and 11 in Table 4
and 5). This suggests that layer functionalities differ across model families and sizes, necessitating
model-specific layer selection strategies, as discussed in Section 3.2.


Therefore, we answer RQ1: adapting an FA
LLM to SWA is feasible even without any training. But it requires specific combinations of at
least 2 methods, which could be less efficient for
inference.



**5.2** **SWA Adaptation With Fine-tuning**


We next evaluate SWA-aware supervised finetuning, which is expected to provide higher improvement. Table 2 reports LongMemEval_24k
accuracy after SFT under various SWAA configurations. The original full-attention model is also
fine-tuned as a baseline (row 0). Since training is
relatively time-consuming, we only test a representative subset of configurations. Our findings are as
follows:


**Fine-tuning substantially lifts all SWA config-**
**urations.** Comparing all the fine-tuning results
with non-training ones, it is clear that fine-tuning
consistently provides great improvement. However,
simply fine-tuning with naive SWA remains insufficient, only achieving 18.8% and 23.8% accuracy
(row 1).


**FA Decode and Interleaving Layers emerge as**
**dominant components.** After SFT, FA Decode
and Interleaving Layers provide the largest gains.



6


Table 2: Qwen3-4B-Thinking and Qwen3-4B-Instruct results on LongMemEval with SFT


**No.** **window size** **FA layers** **keep first** **FA decode** **Acc think** **Acc non-think**


0 Full [] 0 True **74.6** **63.4**
1 2k [] 0 False 18.8 23.8


2 2k [] 10 False 15.6 /
3 2k [] 0 True 57.9 42.0
4 2k [1, 3, 5, ...] 0 False 63.6 54.6
5 4k [] 0 True 62.6 /


6 2k [] 10 True 56.7 /
7 2k [] 100 True 62.2 42.6
8 2k [1, 3, 5, ...] 0 True **73.2** **58.8**
9 2k [0, 2, 4, ...] 0 True **66.0** /
10 2k [1, 5, 9, ...] 0 True **68.8** 47.0


11 2k [1, 3, 5, ...] 100 True **73.2** **61.4**


Table 3: Recommended SWA adaptation recipes for different needs and scenarios. � means optional.


**Training** ? **Thinking Model?** **Prefer?** **FA Decode** **Interleaving Layers** **Keep First**


No No Any � � �
No Yes Efficiency � � �
No Yes Accuracy � � �


Yes Any Efficiency � � �
Yes Any Accuracy � � �



Using FA Decode alone (row 3) or using Interleaving Layers alone (row 4) both get high accuracy.
And combining both (row 8) further boosts performance to 73 _._ 2 (think) and 58 _._ 8 (non-think), nearly
matching the full-attention SFT baseline in row 0.


**Keep First becomes optional rather than essen-**
**tial.** Before fine-tuning, Keep First is crucial for
stability under SWA. But after SFT, it only provides
minor additional improvement. With FA Decode,
adding _k_ = 100 (row 7) improves over _k_ = 0
(row 3) only 4.5%, and if further combining FA
layers, it almost offers no improvements (row 11
and row 8).


**Effect of sliding window size.** Row 3 and 5
shows that increasing the window from 2k to 4k
with FA Decode improves thinking-model accuracy from 57 _._ 9 to 62 _._ 6. This mirrors the non-SFT
trend that larger windows help, but the dominant
improvements still come from FA Decode and Interleaving Layers.

---

[Document 3]
Source: sliding_window_attention_annotated_chunks_2k.jsonl
Chunk ID: sliding_window_attention_annotated_chunks_2k_2
Content: **3.4** **Fine-tuning**


This is the most natural way to mitigate traininginference mismatch. Apparently, the model should
be fine-tuned while SWA is applied, so that the
model’s parameters can be trained to better adapt
to SWA. Meanwhile, the training data must be longcontext tasks, where SWA actually works.
However, most available long-context datasets
only contain brief ground-truth answers, lacking the reasoning steps required for fine-tuning
a ""thinking"" model. Since our goal is to _restore_
the model’s original capabilities under SWA rather
than teach it new ones, instead of directly using the
original dataset, we adopt an approach similar to
self-distillation (Yang et al., 2024). Specifically, we
utilize the original full-attention model to generate
new answers for the dataset’s questions, and these
generated answers are then filtered for correctness
using GPT-5-Mini (OpenAI, 2025), to make up our
training dataset. For each question, we sample 4
answers with temperature 1, because we find this
strategy is slightly better than generating only one
answer with temperature 0.


**4** **Experiment Setup**


We organize our experiments around three research
questions:


**RQ1: Is SWA adaptation feasible without any**
**additional training?** We evaluate whether
an FA LLM can be adapted to SWA using
only inference-time modifications, and which
combinations of techniques are necessary.


**RQ2: How much does fine-tuning with SWA im-**
**prove performance?** We study the effect of
SWA-aware fine-tuning on long-context performance and identify which components of
SWAA are still required.



**RQ3: Which SWAA configurations achieves the**
**optimal performance-efficiency trade-offs?**
We evaluate how different SWAA configurations trade off accuracy against inference la
tency.


**4.1** **Models**


Our primary experiments use Qwen3-4B-Thinking
and Qwen3-4B-Instruct (Team, 2025b). The Thinking variant enforces chain-of-thought (CoT) style
reasoning, whereas the Instruct variant usually just
answers briefly. To ensure generality, we additionally evaluate Qwen3-30B-A3B-Thinking, Qwen330B-A3B-Instruct (Team, 2025b), and Llama3.18B-Instruct (Touvron et al., 2023).

All models are served with vLLM in float16

precision using a batch size of 64. We use greedy
decoding (temperature = 0) for all evaluations. In
preliminary experiments, we observed that vLLM
yields slightly lower (about 1% to 5%) scores
than HuggingFace Transformers due to precisionrelated discrepancies.


**4.2** **Evaluation Dataset**


SWA is identical to full attention when the context

length is within the window size. Even if the model
is fine-tuned, we can pre-calculate the prompt
length and simply disable the LoRA adapters for
short prompts to get completely the same response
as the original model. Therefore, our experiments
focus exclusively on long-context benchmarks with
inputs exceeding 16k tokens, as re-evaluating models on standard short-context benchmarks (e.g.,
MMLU (Hendrycks et al., 2021), GPQA (Rein
et al., 2023)) is completely unnecessary.
Since we find other long-context benchmarks are
either too easy or too difficult for 4B-level models (see Appendix B), we ultimately select LongMemEval (Wu et al., 2024), a benchmark consisting of various types of long-context QA tasks with
moderate difficulty, although it is originally designed for agent memory system evaluation. Its
context length is controllable by selecting a specific number of chat sessions to concatenate as the
context from a pool of hundreds of sessions (a session contains the chat history between user and
assistant within a day). To create a moderately difficult and discriminative evaluation, we construct
**LongMemEval_24k** by sampling 10 sessions, resulting in 500 samples ranging from 16k to 32k
with an average context length of 24k.



4


For additional validation of generalizability, as
shown in Appendix D, we also experiment on
LongBench-V2 (Bai et al., 2024b), a more modern and challenging benchmark that requires deep
reasoning across various real-world tasks.


**4.3** **Training Details**


For the fine-tuning dataset, we initially considered LongAlign (Bai et al., 2024a), a widely used
long-context fine-tuning dataset for adapt a regularlength model to long-context tasks. However, since

_∼_
its sample count ( 10,000) is insufficient, we incorporate an additional 6,000 samples from Fusangv1-long (Pan, 2024), a more comprehensive corpus
of over 40,000 long-context samples that includes
LongAlign as a subset.
We perform SWA-aware fine-tuning using LoRA
(Hu et al., 2022). Unless otherwise noted, we use
rank _r_ = 16 and _α_ = 128, and apply LoRA only
to the query, key, and value projection modules.
We adopt this parameter-efficient setting because
full-parameter fine-tuning often leads to overfitting
and degradation of the model’s original capabilities
in our preliminary experiments. We use a learning
rate of 1e-4 with a cosine decay schedule. Models
are fine-tuned for a single epoch on the sampled
long-context dataset since we observe no meaningful gains from additional epochs (see Appendix F).
Once training takes approximately 12 hours on an
8*H20 GPU server for Qwen3-4B and 30 hours for
Qwen3-30B-A3B.


**5** **Experiment Results**


**5.1** **SWA Adaptation Without Fine-tuning**


We first study SWA adaptation without any additional training. Table 1 reports LongMemEval_24k
accuracy for Qwen3-4B-Thinking (""think"") and
Qwen3-4B-Instruct (""non-think"") under different
combinations of SWAA components. In most settings, we use an aggressive 2k window to amplify
the impact of SWA. The configurations are ranked
by the number of methods applied (0, 1, 2, or 3 of
Interleaving Layers, Keep First and FA Decode).
Rows 1 (original model) and 2 (naive SWA) serve
as upper and lower baselines, respectively. In the
column ""FA layers"", the value records which layers use full attention, and [] means all the layers
use SWA, i.e., this method is not enabled. In the
column ""keep first"", the value is _k_ in Keep First
_k_ Tokens. When comparing results, an accuracy
difference of less than 5% is usually considered



statistically insignificant. From the results, we find
that:


**Naive SWA is not viable.** Naively replacing FA
with a 2k sliding window attention (row 1) drops
accuracy significantly to 3 _._ 2 and 11 _._ 0, respectively.
Even with an 8k window (row 2), accuracy only recovers to 13 _._ 2 and 19 _._ 8, far below the FA baseline.


**Single method helps, but cannot close the gap.**
Each method—Keep First, FA Decode, or Interleaving Layers—improves over naive SWA (rows 3–6),
yet each alone recovers only a small fraction of the
FA gap and remains well below the baseline. In
short, no single method is sufficient.


**Combinations exhibit strong synergy.** Recipes
that combine multiple methods deliver large gains:


  - **FA Decode + Keep First** _k_ substantially improves over naive SWA (rows 7–9), recovering roughly half to two-thirds of the gap on
the thinking model as _k_ increases. However,
increasing _k_ from 100 to 1000 yields almost
no improvement, indicating that _k_ does not
need to be exceedingly large.


  - **Interleaving Layers + FA Decode** is
markedly stronger (row 13), recovering most
of the gap for the thinking model.


  - **FA Decode + Interleaving Layers + Keep**
**First** _k_ perform best (rows 18). The thinking
model recovers close to 90% of the FA gap
even at 2k window.
","In most settings for the SWAA experiments, an ""aggressive 2k window"" size was primarily used to amplify the impact of SWA.",111,2068,2180,,3,,2026-01-02T20:15:29.931962
"Who are the ""correspondence authors"" listed for the ""Neuronal Attention Circuit"" paper?",Entity-Specific Attribute Retrieval,"[Document 1]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_8
Content: the ceiling and floor functions, respectively. This results
in a larger overall architectural size and increased runtime.



Future work will support user-defined NCPs configurations
or randomized wiring to enable more efficient architectures.
**Learnable sparse Top-** _**K**_ **selection:** Sparse Top- _K_ attention can miss important context, is sensitive to _k_, and may
be harder to optimize. A further limitation is that it still
computes the full _QK_ _[⊤]_ matrix, which can dominate the
cost for very long sequences. Future work includes adaptive
or learnable Top- _K_ selection, improved key scoring, and
hardware-aware optimization to strengthen accuracy and
robustness.


**5. Conclusion**


In this paper, we introduce the Neuronal Attention Circuit
(NAC), a biologically inspired attention mechanism that
reformulates attention logits as the solution to a first-order
ODE modulated by nonlinear, interlinked gates derived from
repurposing _C.elegans_ nematode NCPs. NAC bridges discrete attention with continuous-time dynamics, enabling
adaptive temporal processing without the limitations inherent to traditional scaled dot-product attention. Based on the
solution to ODE, we introduce three computational modes:
(i) Euler, based on _explicit Euler_ integration; (ii) Exact, providing closed-form solutions; and (iii) Steady, approximating equilibrium states. In addition, a sparse Top- _K_ pairwise
concatenation scheme is introduced to mitigate the memory intensity. Theoretically, we establish NAC’s log-state
stability, exponential error bounds, and universal approximation, thereby providing rigorous guarantees of convergence
and expressiveness. Empirical evaluations demonstrate that
NAC achieves state-of-the-art performance across diverse
tasks, including irregularly sampled time-series benchmarks,
autonomous vehicle lane-keeping, and industrial prognostics. Moreover, NAC occupies an intermediate position
between CT-RNNs and CT-Attention, offering robust temporal modeling while requiring less runtime than CT-RNNs
and less memory than CT-Attention models.


**Reproducibility Statement**


The code for reproducibility is available at
[https://github.com/itxwaleedrazzaq/](https://github.com/itxwaleedrazzaq/neuronal_attention_circuit)

[neuronal_attention_circuit](https://github.com/itxwaleedrazzaq/neuronal_attention_circuit)


**Impact Statement**


The work addresses the growing field of continuous-time
attention and pioneers a biologically plausible mechanism.
It encourages research into sparse, adaptive networks that
resemble natural wiring. From a societal perspective, it
supports more robust AI in resource-limited settings, but it
also raises ethical concerns when applied to areas such as
surveillance or autonomous systems.



8


**Neuronal Attention Circuit (NAC) for Representation Learning**



**References**


Introduction to self-driving cars. URL
[https://www.udacity.com/course/](https://www.udacity.com/course/intro-to-self-driving-cars--nd113)
[intro-to-self-driving-cars--nd113.](https://www.udacity.com/course/intro-to-self-driving-cars--nd113)


Aguiar-Conraria, L. and Soares, M. J. The continuous
wavelet transform: Moving beyond uni-and bivariate analysis. _Journal of economic surveys_, 28(2):344–375, 2014.


Beltagy, I., Peters, M. E., and Cohan, A. Longformer: The long-document transformer. _arXiv preprint_
_arXiv:2004.05150_, 2020.


Bojarski, M., Del Testa, D., Dworakowski, D., Firner, B.,
Flepp, B., Goyal, P., Jackel, L. D., Monfort, M., Muller,
U., Zhang, J., et al. End to end learning for self-driving
cars. _arXiv preprint arXiv:1604.07316_, 2016.


Brockman, G., Cheung, V., Pettersson, L., Schneider, J.,
Schulman, J., Tang, J., and Zaremba, W. Openai gym.
_arXiv preprint arXiv:1606.01540_, 2016.


Cao, Y., Li, S., Petzold, L., and Serban, R. Adjoint sensitivity analysis for differential-algebraic equations: The
adjoint dae system and its numerical solution. _SIAM_
_journal on scientific computing_, 24(3):1076–1089, 2003.


Chen, R. T., Rubanova, Y., Bettencourt, J., and Duvenaud,
D. K. Neural ordinary differential equations. _Advances_
_in neural information processing systems_, 31, 2018.


Chen, Y., Ren, K., Wang, Y., Fang, Y., Sun, W., and Li, D.
Contiformer: Continuous-time transformer for irregular
time series modeling. _Advances in Neural Information_
_Processing Systems_, 36:47143–47175, 2023.


Chien, J.-T. and Chen, Y.-H. Continuous-time attention for
sequential learning. In _Proceedings of the AAAI confer-_
_ence on artificial intelligence_, volume 35, pp. 7116–7124,
2021.


Cho, K., Van Merrienboer, B., Gulcehre, C., Bahdanau,¨
D., Bougares, F., Schwenk, H., and Bengio, Y. Learning phrase representations using rnn encoder-decoder
for statistical machine translation. _arXiv preprint_
_arXiv:1406.1078_, 2014.


d’Ascoli, S., Becker, S., Mathis, A., Schwaller, P., and
Kilbertus, N. Odeformer: Symbolic regression of dynamical systems with transformers. _arXiv preprint_
_arXiv:2310.05573_, 2023.


De Brouwer, E., Simm, J., Arany, A., and Moreau, Y.
Gru-ode-bayes: Continuous modeling of sporadicallyobserved time series. _Advances in neural information_
_processing systems_, 32, 2019.



Deng, L. The mnist database of handwritten digit images
for machine learning research [best of the web]. _IEEE_
_signal processing magazine_, 29(6):141–142, 2012.


Ding, Y., Jia, M., Miao, Q., and Huang, P. Remaining
useful life estimation using deep metric transfer learning
for kernel regression. _Reliability Engineering & System_
_Safety_, 212:107583, 2021.


Hasani, R., Lechner, M., Amini, A., Rus, D., and Grosu,
R. Liquid time-constant networks. In _Proceedings of the_
_AAAI Conference on Artificial Intelligence_, volume 35,
pp. 7657–7666, 2021.


Hasani, R., Lechner, M., Amini, A., Liebenwein, L., Ray,
A., Tschaikowski, M., Teschl, G., and Rus, D. Closed
form continuous-time neural networks. _Nature Machine_

_Intelligence_, 4(11):992–1003, 2022.


Hochreiter, S. The vanishing gradient problem during learning recurrent neural nets and problem solutions. _Interna-_
_tional Journal of Uncertainty, Fuzziness and Knowledge-_
_Based Systems_, 6(02):107–116, 1998.


Hochreiter, S. and Schmidhuber, J. Long short-term memory.
_Neural computation_, 9(8):1735–1780, 1997.


Hong, H. S. and Thuan, N. Hust bearing: a practical dataset
for ball bearing fault diagnosis. _Mendeley Data_, 3, 2023.


John, F. On integration of parabolic equations by difference methods: I. linear and quasi-linear equations for the
infinite interval. _Communications on Pure and Applied_
_Mathematics_, 5(2):155–211, 1952.

---

[Document 2]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_9
Content: Hochreiter, S. The vanishing gradient problem during learning recurrent neural nets and problem solutions. _Interna-_
_tional Journal of Uncertainty, Fuzziness and Knowledge-_
_Based Systems_, 6(02):107–116, 1998.


Hochreiter, S. and Schmidhuber, J. Long short-term memory.
_Neural computation_, 9(8):1735–1780, 1997.


Hong, H. S. and Thuan, N. Hust bearing: a practical dataset
for ball bearing fault diagnosis. _Mendeley Data_, 3, 2023.


John, F. On integration of parabolic equations by difference methods: I. linear and quasi-linear equations for the
infinite interval. _Communications on Pure and Applied_
_Mathematics_, 5(2):155–211, 1952.


Jordan, M. I. Serial order: A parallel distributed processing
approach. In _Advances in psychology_, volume 121, pp.
471–495. Elsevier, 1997.


Lechner, M. and Hasani, R. Mixed-memory rnns for learning long-term dependencies in irregularly sampled time
series. 2022.


Lechner, M., Hasani, R. M., and Grosu, R. Neuronal circuit
policies. _arXiv preprint arXiv:1803.08554_, 2018.


Lechner, M., Hasani, R., Amini, A., Henzinger, T. A., Rus,
D., and Grosu, R. Neural circuit policies enabling auditable autonomy. _Nature Machine Intelligence_, 2(10):
642–652, 2020.


LeCun, Y., Touresky, D., Hinton, G., and Sejnowski, T. A
theoretical framework for back-propagation. In _Proceed-_
_ings of the 1988 connectionist models summer school_,
volume 1, pp. 21–28, 1988.


Lin, J. and Qu, L. Feature extraction based on morlet
wavelet and its application for mechanical fault diagnosis.
_Journal of sound and vibration_, 234(1):135–148, 2000.



9


**Neuronal Attention Circuit (NAC) for Representation Learning**



Nectoux, P., Gouriveau, R., Medjaher, K., Ramasso, E.,
Chebel-Morello, B., Zerhouni, N., and Varnier, C. Pronostia: An experimental platform for bearings accelerated
degradation tests. In _IEEE International Conference on_
_Prognostics and Health Management, PHM’12._, pp. 1–8.
IEEE Catalog Number: CPF12PHM-CDR, 2012.


Neil, D., Pfeiffer, M., and Liu, S.-C. Phased lstm: Accelerating recurrent network training for long or event-based
sequences. _Advances in neural information processing_
_systems_, 29, 2016.


Nishijima, T. Universal approximation theorem for neural
networks. _arXiv preprint arXiv:2102.10993_, 2021.


Park, M., Kim, H., and Park, S. A convolutional neural
network-based end-to-end self-driving using lidar and
camera fusion: Analysis perspectives in a real-world environment. _Electronics_, 10(21):2608, 2021.


Razzaq, W. and Hongwei, M. Neural circuit policies imposing visual perceptual autonomy. _Neural Processing_
_Letters_, 55(7):9101–9116, 2023.


Razzaq, W. and Zhao, Y.-B. Carle: a hybrid deep-shallow
learning framework for robust and explainable rul estimation of rolling element bearings. _Soft Computing_, 29(23):
6269–6292, 2025a.


Razzaq, W. and Zhao, Y.-B. Developing distance-aware uncertainty quantification methods in physics-guided neural
networks for reliable bearing health prediction, 2025b.
[URL https://arxiv.org/abs/2512.08499.](https://arxiv.org/abs/2512.08499)


Roy, A., Saffar, M., Vaswani, A., and Grangier, D. Efficient
content-based sparse attention with routing transformers. _Transactions of the Association for Computational_
_Linguistics_, 9:53–68, 2021.


Rubanova, Y., Chen, R. T., and Duvenaud, D. K. Latent
ordinary differential equations for irregularly-sampled
time series. _Advances in neural information processing_
_systems_, 32, 2019.


Rumelhart, D. E., Hinton, G. E., and Williams, R. J. Learning internal representations by error propagation. Technical report, 1985.


Shibuya, N. Car behavioral cloning, 2017. URL
[https://github.com/naokishibuya/](https://github.com/naokishibuya/car-behavioral-cloning)
[car-behavioral-cloning.](https://github.com/naokishibuya/car-behavioral-cloning) Accessed: 202510-05.


Shukla, S. N. and Marlin, B. M. Multi-time attention networks for irregularly sampled time series. _arXiv preprint_
_arXiv:2101.10318_, 2021.



Stinchcomb, M. Multilayered feedforward networks are
universal approximators. _Neural Networks_, 2:356–359,
1989.


Tay, Y., Bahri, D., Yang, L., Metzler, D., and Juan, D.-C.
Sparse sinkhorn attention. In _International conference on_
_machine learning_, pp. 9438–9447. PMLR, 2020.


Thuan, N. D. and Hong, H. S. Hust bearing: a practical
dataset for ball bearing fault diagnosis. _BMC research_
_notes_, 16(1):138, 2023.


Tiang, Y., Gelernter, J. R., Wang, X., Chen, W., Gao, J.,
Zhang, Y., and Li, X. Lane marking detection via fast
end-to-end deep convolutional neural network that is our
patch proposal network (ppn). 2018.


Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. _Advances in neural information_
_processing systems_, 30, 2017.


Vidulin, V., Lustrek, M., Kaluza, B., Piltaver, R.,
and Krivec, J. Localization Data for Person Activity. UCI Machine Learning Repository, 2010. DOI:
https://doi.org/10.24432/C57G8X.


Wang, B., Lei, Y., Li, N., et al. Xjtu-sy bearing datasets.
_GitHub, GitHub Repository_, 2018.


Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Alberti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q.,
Yang, L., et al. Big bird: Transformers for longer sequences. _Advances in neural information processing_
_systems_, 33:17283–17297, 2020.


Zhuang, J., Dvornek, N., Li, X., Tatikonda, S., Papademetris,
X., and Duncan, J. Adaptive checkpoint adjoint method
for gradient estimation in neural ode. In _International_
_Conference on Machine Learning_, pp. 11639–11649.
PMLR, 2020.


Zhuang, J., Jia, M., Ding, Y., and Ding, P. Temporal
convolution-based transferable cross-domain adaptation
approach for remaining useful life estimation under variable failure behaviors. _Reliability Engineering & System_
_Safety_, 216:107946, 2021.



10


**Neuronal Attention Circuit (NAC) for Representation Learning**



**Appendix**


**A. Preliminaries**


**A.1. Attention Mechanism**


Attention mechanisms have become a cornerstone in

---

[Document 3]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_10
Content: Zhuang, J., Jia, M., Ding, Y., and Ding, P. Temporal
convolution-based transferable cross-domain adaptation
approach for remaining useful life estimation under variable failure behaviors. _Reliability Engineering & System_
_Safety_, 216:107946, 2021.



10


**Neuronal Attention Circuit (NAC) for Representation Learning**



**Appendix**


**A. Preliminaries**


**A.1. Attention Mechanism**


Attention mechanisms have become a cornerstone in

modern neural architectures, enabling models to dynamically focus on relevant parts of the input. The concept
was first introduced in the context of neural machine
translation, where it allowed the decoder to weight encoder
outputs according to their importance for generating each
target token. Formally, given a query vector _q ∈_ R _[d]_, key
vectors _K_ = [ _k_ 1 _, k_ 2 _, . . ., kn_ ] _∈_ R _[n][×][d]_, and value vectors
_V_ = [ _v_ 1 _, v_ 2 _, . . ., vn_ ] _∈_ R _[n][×][d]_, the attention mechanism can
be expressed in two steps:


1. Compute the scaled dot attention logits:


_ai_ = _[q][T][ k][i]_ (21)
~~_√_~~ _d_


2. Normalize the logits to get attention weights and compute the output:


_e_ _[a][i]_
_αi_ = softmax( _ai_ ) = ~~_n_~~ (22)
~~�~~ _j_ =1 _[e][a][j]_



exhibit a resting potential at _−_ 70 mV and an activation potential near _−_ 20 mV. Similarly, each _Nm_ is composed of
two subneurons, _Mp_ and _Mn_, and is driven by a controllable variable _y_, which also maps to a biologically plausible
range [ _−_ 70 mV _, −_ 20 mV]. The connections in the NCP
architecture are designed to reflect the biological sparsity
and abstraction of neural circuits. Specifically, connections
from _Ns_ to _Ni_ are feedforward, while those between _Nc_
and _Nm_ are highly recurrent (Lechner et al., 2018). Figure
1(a) illustrates the connectome of NCPs.


**B. Proofs**


In this section, we provide all the proofs.


**B.1. Deriving Closed-form (Exact) Solution**


Although _ϕ_ and _ωτ_ are nonlinear functions of the input
**u** = [ **q** ; **k** ], we derive closed-form solution by treating them
as locally constant over the pseudo-time integration interval
for each query–key pair based on frozen-coefficient approximation (John, 1952). This is accurate whenever the interval
is short or when input variations are slow compared with
the relaxation rate _ωτ_ . Under approximation assumption,
rewrite Eqn. 1 as


_dadtt_ [+] _[ ω][τ]_ _[a][t]_ [ =] _[ ϕ.]_ (24)


This is now a linear first-order ODE. The integrating factor
is



Attention( _q, k, v_ ) =



_n_
� _αivi_ (23)


_i_ =1



Here, _ai_ is the raw attention logit between the query and
each key, and the scaling factor _√d_ prevents large dot prod
ucts from destabilizing the softmax (Vaswani et al., 2017).


**A.2. Neuronal Circuit Policies (NCPs)**


NCPs represent a biologically inspired framework for developing interpretable neural control agents by adapting the
tap-withdrawal circuit found in the nematode _C. elegans_
(Lechner et al., 2018). Unlike traditional spiking neural
networks, the majority of neurons in this circuit exhibit
electronic dynamics, characterized by the passive flow of
electrical charges, resulting in graded potentials. NCPs
are structured as a four-layer hierarchical architecture comprising sensory neurons ( _Ns_ ), interneurons ( _Ni_ ), command
neurons ( _Nc_ ), and motor neurons ( _Nm_ ). The _Ns_ perceive
and respond to external stimulus inputs and are responsible for the initial signal transduction. Each _Ns_ consists
of subneurons _Sp_ and _Sn_ and a system variable _x_ . The
activation of _Sp_ and _Sn_ depends upon the sign of _x_ : _Sp_
becomes activated for _x >_ 0, whereas _Sn_ becomes activated for _x <_ 0. The variable _x_ is mapped to the membrane
potential range of [ _−_ 70 mV _, −_ 20 mV], which is consistent
with the biophysical behavior of nerve cells, which typically



� _ωτ dt_

_µ_ = _e_ � �


Multiply both sides by _µ_ ( _t_ ):



Substitute back:


_e_ _[ω][τ][ t]_ _at −_ _a_ 0 = _ϕ ·_ _[e][ω][τ][ t][ −]_ [1] _._ (30)

_ωτ_



= _e_ _[ω][τ][ t]_ _._ (25)




_[da][t]_
_e_ _[ω][τ][ t]_ _dt_ [+] _[ ω][τ]_ _[e][ω][τ][ t][a][t]_ [ =] _[ ϕe][ω][τ][ t][.]_ (26)


Recognize the left-hand side as the derivative of _e_ _[ω][τ][ t]_ _at_ :


_d_
� _e_ _[ω][τ][ t]_ _at_ � = _ϕe_ _[ω][τ][ t]_ _._ (27)
_dt_


Integrate from 0 to _t_ :


_t_
_e_ _[ω][τ][ t]_ _at −_ _e_ [0] _a_ 0 = _ϕ_ _e_ _[ω][τ][ s]_ _ds._ (28)
�0


Compute the integral (since _ωτ ̸_ = 0):



�0 _t_



_e_ _[ω][τ][ s]_ _ds_ = [1]
0 _ωτ_



_ωτ_



� _e_ _[ω][τ][ t]_ _−_ 1� _._ (29)



11


Rearrange:



**Neuronal Attention Circuit (NAC) for Representation Learning**


CASE 2: MULTIPLE CONNECTIONS ( _M >_ 1).



_e_ _[ω][τ][ t]_ _at_ = _a_ 0 + _[ϕ]_

_ωτ_


Divide both sides by _e_ _[ω][τ][ t]_ :


_at_ = _a_ 0 _e_ _[−][ω][τ][ t]_ + _[ϕ]_

_ωτ_



� _e_ _[ω][τ][ t]_ _−_ 1� _._ (31)


�1 _−_ _e_ _[−][ω][τ][ t]_ [�] _._ (32)



The ODE is



with per-connection equilibria _Aj_ = _ϕj/fj_ . The effective
equilibrium is



_da_

_dt_ [=] _[ −]_ � � _[M]_



_da_



� _fj_ � _a_ +

_j_ =1



_M_
� _fjAj,_ (37)

_j_ =1



Set _a_ _[∗]_ := _[ϕ]_ . Then _at_ = _a_ _[∗]_ + ( _a_ 0 _−_ _a_ _[∗]_ ) _e_ _[−][ω][τ][ t]_, proved.

_ωτ_


**B.2. Proof of Theorem 1**
","The provided context does not list the ""correspondence authors"" for the ""Neuronal Attention Circuit"" paper.",95,4042,4138,,3,,2026-01-02T20:15:34.070892
"What specific ""GPU hardware"" was used to train the ""base models"" in ""Attention Is All You Need""?",Entity-Specific Attribute Retrieval,"[Document 1]
Source: attention_is_all_you_need_raw_chunks_2k.jsonl
Chunk ID: attention_is_all_you_need_raw_chunks_2k_3
Content: As side benefit, self-attention could yield more interpretable models. We inspect attention distributions
from our models and present and discuss examples in the appendix. Not only do individual attention
heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic
and semantic structure of the sentences.


**5** **Training**


This section describes the training regime for our models.


**5.1** **Training Data and Batching**


We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million
sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared sourcetarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT
2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece
vocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training
batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000
target tokens.


**5.2** **Hardware and Schedule**


We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using
the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We
trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the
bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps
(3.5 days).


**5.3** **Optimizer**


We used the Adam optimizer [20] with _β_ 1 = 0 _._ 9, _β_ 2 = 0 _._ 98 and _ϵ_ = 10 _[−]_ [9] . We varied the learning
rate over the course of training, according to the formula:


_lrate_ = _d_ _[−]_ model [0] _[.]_ [5] _[·]_ [ min(] _[step]_ [_] _[num][−]_ [0] _[.]_ [5] _[, step]_ [_] _[num][ ·][ warmup]_ [_] _[steps][−]_ [1] _[.]_ [5][)] (3)


This corresponds to increasing the learning rate linearly for the first _warmup_ _ _steps_ training steps,
and decreasing it thereafter proportionally to the inverse square root of the step number. We used
_warmup_ _ _steps_ = 4000.


**5.4** **Regularization**


We employ three types of regularization during training:


7


Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the
English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.


BLEU Training Cost (FLOPs)
Model

EN-DE EN-FR EN-DE EN-FR
ByteNet [18] 23.75
Deep-Att + PosUnk [39] 39.2 1 _._ 0 _·_ 10 [20]

GNMT + RL [38] 24.6 39.92 2 _._ 3 _·_ 10 [19] 1 _._ 4 _·_ 10 [20]

ConvS2S [9] 25.16 40.46 9 _._ 6 _·_ 10 [18] 1 _._ 5 _·_ 10 [20]

MoE [32] 26.03 40.56 2 _._ 0 _·_ 10 [19] 1 _._ 2 _·_ 10 [20]

Deep-Att + PosUnk Ensemble [39] 40.4 8 _._ 0 _·_ 10 [20]

GNMT + RL Ensemble [38] 26.30 41.16 1 _._ 8 _·_ 10 [20] 1 _._ 1 _·_ 10 [21]

ConvS2S Ensemble [9] 26.36 **41.29** 7 _._ 7 _·_ 10 [19] 1 _._ 2 _·_ 10 [21]

Transformer (base model) 27.3 38.1 **3** _**.**_ **3** _**·**_ **10** **[18]**

Transformer (big) **28.4** **41.8** 2 _._ 3 _·_ 10 [19]


**Residual Dropout** We apply dropout [33] to the output of each sub-layer, before it is added to the
sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the
positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of
_Pdrop_ = 0 _._ 1.


**Label Smoothing** During training, we employed label smoothing of value _ϵls_ = 0 _._ 1 [36]. This
hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.


**6** **Results**


**6.1** **Machine Translation**


On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)
in Table 2) outperforms the best previously reported models (including ensembles) by more than 2 _._ 0
BLEU, establishing a new state-of-the-art BLEU score of 28 _._ 4. The configuration of this model is
listed in the bottom line of Table 3. Training took 3 _._ 5 days on 8 P100 GPUs. Even our base model
surpasses all previously published models and ensembles, at a fraction of the training cost of any of
the competitive models.


On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41 _._ 0,
outperforming all of the previously published single models, at less than 1 _/_ 4 the training cost of the
previous state-of-the-art model. The Transformer (big) model trained for English-to-French used
dropout rate _Pdrop_ = 0 _._ 1, instead of 0 _._ 3.


For the base models, we used a single model obtained by averaging the last 5 checkpoints, which
were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We
used beam search with a beam size of 4 and length penalty _α_ = 0 _._ 6 [38]. These hyperparameters
were chosen after experimentation on the development set. We set the maximum output length during
inference to input length + 50, but terminate early when possible [38].


Table 2 summarizes our results and compares our translation quality and training costs to other model
architectures from the literature. We estimate the number of floating point operations used to train a
model by multiplying the training time, the number of GPUs used, and an estimate of the sustained
single-precision floating-point capacity of each GPU [5] .


**6.2** **Model Variations**


To evaluate the importance of different components of the Transformer, we varied our base model
in different ways, measuring the change in performance on English-to-German translation on the


5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.


8


Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base
model. All metrics are on the English-to-German translation development set, newstest2013. Listed
perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to
per-word perplexities.

---

[Document 2]
Source: attention_is_all_you_need_raw_chunks_2k.jsonl
Chunk ID: attention_is_all_you_need_raw_chunks_2k_5
Content: We are excited about the future of attention-based models and plan to apply them to other tasks. We
plan to extend the Transformer to problems involving input and output modalities other than text and
to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs
such as images, audio and video. Making generation less sequential is another research goals of ours.


The code we used to train and evaluate our models is available at `[https://github.com/](https://github.com/tensorflow/tensor2tensor)`
`[tensorflow/tensor2tensor](https://github.com/tensorflow/tensor2tensor)` .


**Acknowledgements** We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful
comments, corrections and inspiration.


**References**


[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. _arXiv preprint_
_[arXiv:1607.06450](http://arxiv.org/abs/1607.06450)_, 2016.


[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. _CoRR_, abs/1409.0473, 2014.


[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural
machine translation architectures. _CoRR_, abs/1703.03906, 2017.


[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine
reading. _[arXiv preprint arXiv:1601.06733](http://arxiv.org/abs/1601.06733)_, 2016.


10


[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,
and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical
machine translation. _CoRR_, abs/1406.1078, 2014.


[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. _arXiv_
_[preprint arXiv:1610.02357](http://arxiv.org/abs/1610.02357)_, 2016.


[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation
of gated recurrent neural networks on sequence modeling. _CoRR_, abs/1412.3555, 2014.


[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural
network grammars. In _Proc. of NAACL_, 2016.


[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional sequence to sequence learning. _[arXiv preprint arXiv:1705.03122v2](http://arxiv.org/abs/1705.03122)_, 2017.


[10] Alex Graves. Generating sequences with recurrent neural networks. _arXiv preprint_
_[arXiv:1308.0850](http://arxiv.org/abs/1308.0850)_, 2013.


[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE Conference on Computer Vision and Pattern_
_Recognition_, pages 770–778, 2016.


[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in
recurrent nets: the difficulty of learning long-term dependencies, 2001.


[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. _Neural computation_,
9(8):1735–1780, 1997.


[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations
across languages. In _Proceedings of the 2009 Conference on Empirical Methods in Natural_
_Language Processing_, pages 832–841. ACL, August 2009.


[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring
the limits of language modeling. _[arXiv preprint arXiv:1602.02410](http://arxiv.org/abs/1602.02410)_, 2016.


[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In _Advances in Neural_
_Information Processing Systems, (NIPS)_, 2016.


[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In _International Conference_
_on Learning Representations (ICLR)_, 2016.


[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Koray Kavukcuoglu. Neural machine translation in linear time. _[arXiv preprint arXiv:1610.10099v2](http://arxiv.org/abs/1610.10099)_,
2017.


[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.
In _International Conference on Learning Representations_, 2017.


[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In _ICLR_, 2015.


[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. _arXiv preprint_
_[arXiv:1703.10722](http://arxiv.org/abs/1703.10722)_, 2017.


[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen
Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. _arXiv preprint_
_[arXiv:1703.03130](http://arxiv.org/abs/1703.03130)_, 2017.


[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task
sequence to sequence learning. _[arXiv preprint arXiv:1511.06114](http://arxiv.org/abs/1511.06114)_, 2015.


[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attentionbased neural machine translation. _[arXiv preprint arXiv:1508.04025](http://arxiv.org/abs/1508.04025)_, 2015.


11


[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated
corpus of english: The penn treebank. _Computational linguistics_, 19(2):313–330, 1993.


[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In
_Proceedings of the Human Language Technology Conference of the NAACL, Main Conference_,
pages 152–159. ACL, June 2006.


[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention
model. In _Empirical Methods in Natural Language Processing_, 2016.


[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive
summarization. _[arXiv preprint arXiv:1705.04304](http://arxiv.org/abs/1705.04304)_, 2017.

---

[Document 3]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_7
Content: compact architecture suitable for resource-constrained devices, thereby supporting localized safety.
We utilized three benchmark datasets: (i) PRONOSTIA
(Nectoux et al., 2012), (ii) XJTU-SY (Wang et al., 2018),
and (iii) HUST (Thuan & Hong, 2023). Training is performed on PRONOSTIA, while XJTU-SY and HUST are

used to assess cross-validation. We used the Score met
ric (Nectoux et al., 2012) to assess the performance. We
evaluate generalization using _Bearing 1_ from the first operating condition of each dataset. Figure 3 visualizes the
expected degradation alongside the outputs of NAC. On the
PRONOSTIA dataset, ContiFormer performed the best with



**3.4. Run Time and Memory Experiments**


We evaluate computational requirements on fixed-length
sequences of 1024 steps, 64-dimensional features, 4 heads,
and a Batch size of 1. Each model is run for ten for
ward passes on Google Colab T4-GPU, and we report the
mean runtime with standard deviation, throughput, and peak
memory usage. NAC occupies an intermediate position
in runtime relative to several CT-RNN models, including
GRU-ODE, CfC, and LTC. In terms of memory consumption, NAC uses significantly less memory than mTAN, with
NAC-2k being the least memory-consuming among the CTAttention models. Reducing NAC sparsity from 90% to 20%
has minimal effect on memory, decreasing usage slightly



7


**Neuronal Attention Circuit (NAC) for Representation Learning**



_Table 2._ Run-Time and Memory Benchmark Results


**Run-Time** **Throughput** **Peak Memory**
**Model**
(s) (seq/s) (MB)


RNN 1 _._ 8392 _[±]_ [0] _[.]_ [1933] 0.544 0.29
CT-RNN 7 _._ 1097 _[±]_ [0] _[.]_ [3048] 0.141 0.67
LSTM 2 _._ 6241 _[±]_ [0] _[.]_ [2906] 0.381 0.42
PhasedLSTM 4 _._ 9812 _[±]_ [0] _[.]_ [272] 0.201 0.80
GRU 3 _._ 216 _[±]_ [0] _[.]_ [2566] 0.311 0.54
GRU-ODE 12 _._ 2498 _[±]_ [0] _[.]_ [0525] 0.082 0.64
mmRNN 7 _._ 5852 _[±]_ [0] _[.]_ [2785] 0.132 0.96
LTC 14 _._ 643 _[±]_ [0] _[.]_ [2445] 0.068 0.99
CfC 6 _._ 0988 _[±]_ [0] _[.]_ [2135] 0.164 0.76


Attention 0 _._ 0016 _[±]_ [0] _[.]_ [0001] 625.00 16.86
MHA 0 _._ 0041 _[±]_ [0] _[.]_ [0001] 243.90 69.05
mTAN 0 _._ 0272 _[±]_ [0] _[.]_ [0054] 36.76 790.16
ODEFormer 0 _._ 0317 _[±]_ [0] _[.]_ [0016] 31.55 67.71
CTA 8 _._ 5275 _[±]_ [0] _[.]_ [2355] 0.117 1.43
ContiFormer 0 _._ 066 _[±]_ [0] _[.]_ [0075] 15.15 67.71


NAC-2k 7 _._ 3071 _[±]_ [0] _[.]_ [1547] 0.137 44.75
NAC-32k 7 _._ 2313 _[±]_ [0] _[.]_ [219] 0.138 549.86
NAC-PW 8 _._ 5649 _[±]_ [0] _[.]_ [0203] 0.117 5042.09
NAC-FC 0 _._ 0195 ~~_[±]_~~ ~~[0]~~ ~~_[.]_~~ ~~[0002]~~ 51.28 29.92
NAC-02s 7 _._ 252 _[±]_ [0] _[.]_ [2018] 0.138 151.54
NAC-09s 7 _._ 222 _[±]_ [0] _[.]_ [176] 0.139 150.85
NAC-Exact/05s/8k 7 _._ 4101 ~~_[±]_~~ ~~[0]~~ ~~_[.]_~~ ~~[1586]~~ 0.135 151.50
NAC-Euler 7 _._ 3367 _[±]_ [0] _[.]_ [1719] 0.136 152.22
NAC-Steady 7 _._ 2942 _[±]_ [0] _[.]_ [1451] 0.137 150.86


from 151.54 MB to 150.85 MB. In constrast, decreasing
the Top- _K_ selection from _PW_ to _k_ = 2 drastically reduces
memory consumption from 5042 MB to 44.75 MB, demonstrating the flexibility of NAC.
**Interpreting the Results:** From the experiments, we observe that increasing the sparsity of the NAC layer improves
the robustness of the system and leads to higher overall accuracy. Similarly, increasing the Top- _K_ interactions enhances
accuracy too; however, the benefits diminish as memory
consumption grows. Using Exact mode, Top- _K_ =8 with 50%
sparsity achieves the best balance between accuracy and
efficiency. Steady mode is the fastest, while Euler mode
handles adaptive temporal dynamics.


**4. Discussions**



This research is part of ongoing work on biologically plausible attention mechanisms and represents a pioneering step,
with limitations to be addressed in future work.

**Architectural improvement:** Currently, NAC uses predetermined wiring (AutoNCP) requiring three inputs: number of units (sensory + interneuron + motor), output motor
neurons, and sparsity, with typically 60% of units assigned
to sensory neurons. To integrate with the attention mechanism while preserving wiring, sensory units for _NN_ sensory
are set as unitssensory = � _d_ model0 _.−_ 6 0 _._ 5 � and backbone units as



are set as unitssensory = � _d_ model0 _.−_ 6 0 _._ 5 � and backbone units as

units _backbone_ = _d_ model + � _d_ 0model _._ 6 �, where _⌈·⌉_ and _⌊·⌋_ denote



units _backbone_ = _d_ model + � _d_ 0model _._ 6 �, where _⌈·⌉_ and _⌊·⌋_ denote

the ceiling and floor functions, respectively. This results
in a larger overall architectural size and increased runtime.



Future work will support user-defined NCPs configurations
or randomized wiring to enable more efficient architectures.
**Learnable sparse Top-** _**K**_ **selection:** Sparse Top- _K_ attention can miss important context, is sensitive to _k_, and may
be harder to optimize. A further limitation is that it still
computes the full _QK_ _[⊤]_ matrix, which can dominate the
cost for very long sequences. Future work includes adaptive
or learnable Top- _K_ selection, improved key scoring, and
hardware-aware optimization to strengthen accuracy and
robustness.


**5. Conclusion**
","The ""base models"" in ""Attention Is All You Need"" were trained on one machine with 8 NVIDIA P100 GPUs.",104,1704,1808,,3,,2026-01-02T20:15:35.880215
"What is the ""Pearson correlation"" value observed between ""cognitive masking"" and ""random masking"" effects in VLMs?",Entity-Specific Attribute Retrieval,"[Document 1]
Source: attention_functional_roles_raw_chunks_2k.jsonl
Chunk ID: attention_functional_roles_raw_chunks_2k_3
Content: 5


![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-5-0.png)

Figure 2: The existence of cognitive heads in Qwen2.5-VL-7B responsible for eight distinct functions in complex reasoning tasks. The x-axis represents the head index, while the y-axis indicates
the layer index. The values denote head importance scores, capped at a cutoff of 0.60.


Figure 3: Pearson Correlation between different functions across two models.


the ROUGE score (Chin-Yew, 2004) or the semantic similarity score surpasses 0.6. This provides a
comprehensive evaluation of performance degradation.


We gradually mask out the number of cognitive heads and and observe how model behavior changes.
As shown in Fig 4, randomly masking up to 10% of heads has minimal impact on the overall performance of Qwen2.5-VL-3B. In contrast, masking a similar number of cognitive heads leads to a
substantial drop across multiple functions. Notably, when more than 25% of heads are randomly
masked, performance also declines sharply, as this begins to include functionally critical heads.
These results further highlight the sparsity and importance of functional heads.


For each function, we select the top 10% of heads with the highest accuracy as cognitive heads. As
shown in Table 1, masking cognitive heads lead to a substantial decline in performance, whereas
masking an equal number of random heads results in only minor degradation across all VLMs. In
some cases, masking identified cognitive heads reduces accuracy to zero, indicating that the model
cannot perform the corresponding function without them. The t-test analysis (Appendix A.9) shows
that the difference between cognitive masking and random masking is statistically significant, with
_p ≪_ 0 _._ 05 in nearly all cases. To further validate their functional roles, we mask heads associated
with one function (e.g., language knowledge recall) while evaluating performance on a different
function (e.g., vision knowledge recall). As shown in Fig 5, masking the relevant functional heads
yields a significantly larger performance drop than masking unrelated heads, confirming their functional specialization.


In addition to masking, we also conduct activation patching, where the activations of cognitive heads
associated with one function are replaced by those from another function using two strategies: ran

6



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-5-1.png)
![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-6-0.png)

Figure 4: The performance of Qwen2.5-VL-3B after masking out top K cognitive heads vs K random
heads on inference, high-level visual reception, low-level visual reception, and decion-making.


Table 1: Intervention results (mean accuracy over 5 runs %) of cognitive heads vs. random heads
across 8 functions: **Low-level** Visual Reception, **High-level** Visual Reception, Vision Knowledge
**Recall**, Language **Info** rmation Extraction and Understanding, Language Knowledge **Recall**, **Math**
Reasoning, **Inference**, and **Decision** Making. For model names, Intern2B: InternVL3-2B, Intern8B:
InternVL3-8B, gemma2B: gemma-3n-e2b-it, gemma4B: gemma-3n-e4b-it, Qwen3B: Qwen2.5VL-3B-Instruct, Qwen7B: Qwen2.5-VL-7B-Instruct. Lower values indicate more effective intervention outcomes, suggesting that the corresponding heads play a greater role in the cognitive function.







|VisionmainlyFunctions LanguagemainlyFunctions<br>Model Inter Head<br>Low-Level High -Le vel Recall Info Recall Math Inference Decision<br>llm acc llm acc llm acc llm acc llm acc llm acc llm acc llm acc|Col2|
|---|---|
|Intern2B<br>random<br>cognitive|75.61 82.44<br>87.5<br>88.75 89.15<br>86.1<br>57.84 69.19 84.06 84.64 81.29 90.97 75.06 74.12 67.22 71.67<br>**60.24 62.68 75.71 76.61 73.05 64.58** 66.76 68.92** 44.93 46.38 61.29 64.52 71.76 64.71 48.06 52.22**|
|Intern8B<br>random<br>cognitive|92.2<br>93.17 88.47 91.25 87.61 82.25 59.41 69.12 87.59 89.37 79.25 84.15 82.64 85.05 66.27 73.49<br>**68.78 78.05 56.94 65.97 71.69 70.56**<br>**8.82**<br>**19.12 74.68 77.22 20.75**<br>**56.6**<br>**43.96 42.86** 66.27** 66.27**|
|gemma2B<br>random<br>cognitive|58.37 76.33 57.53 67.64 54.57 62.57 55.07 60.82 81.05 82.11 23.12<br>55.0<br>57.44 63.49<br>30.0<br>66.94<br>**48.98**<br>**55.1**<br>**55.06 63.48**<br>**2.86**<br>**8.57**<br>**30.27 38.49**<br>**50.0**<br>**47.37 11.25 36.88 36.98 52.09 19.44 54.17**|
|gemma4B<br>random<br>cognitive|36.0<br>48.73 29.22 38.65 33.52 34.08 25.12 27.56 40.27 47.84 27.91 44.19<br>57.3<br>57.08 18.29 34.57<br>**29.09 41.82 10.88 21.24**<br>**5.63**<br>**14.08**<br>**22.2**<br>**36.34**<br>**9.46**<br>**13.51** 27.91** 53.49 36.18 32.81**<br>**4.29**<br>**15.71**|
|Qwen3B<br>random<br>cognitive|70.97 82.58 82.42 86.06 88.48 86.36 52.22 58.89 85.14 86.57 65.85 89.76 85.32 90.13 63.44 71.88<br>**12.9**<br>**12.9**<br>**12.12 16.67 77.88 77.88** 55.56 62.96** 61.43 68.57**<br>**0.0**<br>**0.0**<br>**1.27**<br>**1.27**<br>**1.56**<br>**4.69**|
|Qwen7B<br>random<br>cognitive|83.2<br>88.8<br>84.57 89.51 79.43 80.29 75.08 79.38 90.13<br>86.4<br>67.84 72.94 80.67 83.33 75.14 79.19<br>**30.0**<br>**38.0**<br>**73.21 73.83 21.43 22.86 15.38 33.85**<br>**84.0**<br>**78.67** 68.63 72.55 85.56 81.11** 25.68 27.03**|


dom activation and mean activation. In the random activation setting, activations are substituted
with those from a randomly selected subquestion belonging to a different function. In the mean
activation setting, activations are replaced with the average activation computed over all subquestions associated with another function (details in Appendix A.10). As shown in Table 2, both types
of activation patching result in substantial performance degradation for cognitive heads, consistent
with the effects observed under masking interventions.

---

[Document 2]
Source: attention_functional_roles_raw_chunks_2k.jsonl
Chunk ID: attention_functional_roles_raw_chunks_2k_4
Content: dom activation and mean activation. In the random activation setting, activations are substituted
with those from a randomly selected subquestion belonging to a different function. In the mean
activation setting, activations are replaced with the average activation computed over all subquestions associated with another function (details in Appendix A.10). As shown in Table 2, both types
of activation patching result in substantial performance degradation for cognitive heads, consistent
with the effects observed under masking interventions.


Table 2: Ablation study of different activation-masking methods on Qwen2.5-VL-3B. Random:
random activation. Mean: mean activation. Scalar: masking.







|VisionmainlyCognitiveFunctions LanguagemainlyCognitiveFunctions<br>Method Inter Head<br>Low-Level High -Level Recall Info Recall Math Inference Decision<br>llm acc llm acc llm acc llm acc llm acc llm acc llm acc llm acc|Col2|
|---|---|
|Random<br>cognitive|**0.00 0.00** 20.45 23.48** 62.12 65.15** 35.79 57.41 12.86 24.29 17.07 17.07 14.32 14.32 6.25 9.38|
|Mean<br>cognitive|3.80 3.80 17.39 19.91 66.67 65.15** 35.19 55.56 10.00 21.43** 37.17 34.15<br>3.80<br>3.80<br>6.25 9.38|
|Scalar<br>cognitive|6.45 6.45** 16.67 18.94** 75.76 75.76 62.96 81.48 57.14 62.86<br>**2.43**<br>**2.43**<br>**0.00**<br>**0.00**<br>**3.13 4.69**|


We further examine how vision-related and language-related heads attend to their respective modalities. For the top-30 heads of each function, we compute the average attention weight on visual tokens
across the test set. As shown in Appendix A.9, vision-related heads (e.g., high-level and low-level


7


![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-7-0.png)

Figure 5: The performance of VLMs on target functions after masking out top K cognitive heads on
intervention functions. The scores are based on LLM-Judge.


Table 3: Study on the influence of low-level cognitive heads for high-order function on Qwen2.5VL-3B. The score is measured based on LLM-judge. We only evaluate subquestions that the model
originally answered correctly. This filtering ensures that any observed drop in performance is caused
solely by the intervention. Notably, the model’s own generated outputs are used as inputs for subsequent subquestions.


Vision Recall Language Recall Info. Low-Level High Level Math Decision Inference


✗ ✓ ✓ ✓ ✓ 50 _._ 00 _↓_ 50.00 54 _._ 55 _↓_ 45.45 54 _._ 17 _↓_ 45.83
✓ ✗ ✓ ✓ ✓ 16 _._ 67 _↓_ 83.33 56 _._ 25 _↓_ 43.75 65 _._ 22 _↓_ 34.78
✓ ✓ ✗ ✓ ✓ 22 _._ 22 _↓_ 77.78 57 _._ 89 _↓_ 42.11 51 _._ 61 _↓_ 48.39
✓ ✓ ✓ ✗ ✓ 27 _._ 27 _↓_ 72.73 72 _._ 73 _↓_ 27.27 59 _._ 09 _↓_ 40.91
✓ ✓ ✓ ✓ ✗ 33 _._ 96 _↓_ 66.04 64 _._ 29 _↓_ 35.71 53 _._ 95 _↓_ 46.05


visual reception) predominantly focus on image tokens, capturing spatial and object-level information, whereas language-related heads (e.g., language knowledge recall) concentrate on text tokens.
Interestingly, Qwen and Intern models allocate more attention to text, while Gemma emphasizes
vision, revealing family-specific modality preferences. We also observe heads with cross-modal attention that respond to both visual and textual tokens, likely mediating interactions between visual
perception and linguistic reasoning. These findings suggest that functional specialization in VLMs
is complemented by selective cross-modal integration, enabling coherent multimodal reasoning.


4.3 RELATIONSHIP AMONG COGNITIVE HEADS


While cognitive heads are specialized for distinct functions, understanding their relationships is
crucial for revealing how complex reasoning emerges from their cooperation.


**Heads Across Functions** The neural system is inherently complex, with individual neurons often
participating in multiple functions (Mante et al., 2013). We observe a similar phenomenon in VLMs:
certain functional heads overlap, with a single head participating in multiple cognitive roles (e.g., in
the Qwen2.5-VL-7B model, 18% of cognitive heads across eight functions participate in more than
one function). In our probing-based method, we quantify and rank the accuracy of attention heads for
each cognitive function. A head that ranks highly for one function may also exhibit non-negligible
importance for others, leading to the phenomenon of ”Heads Across Functions”. Notably, even if
a head ranks in the top 10% for multiple cognitive functions, our ranking still reveals a primary
function for which it is most diagnostic. In summary, functional heads serve not only as specialized
units but also as integrative components that bridge multiple reasoning processes.


**Hierarchical Structure** Humans often solve complex tasks through step-by-step reasoning, where
former functions, such as low-level visual reception, support higher-level processes like inference
and decision making. The CogVision dataset reflects this hierarchy: under CoT, early subquestions
focus on information extraction, while later ones require more complex reasoning. Leveraging this


8


structure, we test whether VLMs exhibit similar functional dependencies by masking attention heads
associated with early-stage functions and observing the impact on subsequent reasoning steps. As
shown in Table 3, masking vision or language knowledge recall heads significantly impairs laterstage performance, particularly in decision making. These results suggest that VLMs exhibit an
emergent hierarchical organization, where early cognitive functions support more advanced reasoning. The prompt used for VLMs can be found in Appendix A.7.


4.4 INFLUENCE OF FUNCTIONAL HEADS ON DOWNSTREAM TASKS


In this section, we investigate how functional heads influence downstream tasks through both negative interventions (masking out function heads) and positive interventions (shifting heads toward
specific functions).


**Negative Intervention:** We randomly
sample 200 question for two VQA bench- Table 4: Negative Intervention on Visual Question Anmarks, OK-VQA and Clevr, both reason- swering task (OK-VQA and Clevr-Math). The scores
ing tasks. We perform negative interven- are based on LLM-Judge.
tion by masking high-level visual reception heads on OK-VQA and math reason- Dataset Inter ~~H~~ ead Model

leads to a significant performance drop after **0.00** **59.00** 29.00 **13.00** 14.00 **13.00**
across all models. Further analysis in Ap
|Dataset Inter Head|Model|
|---|---|
|Dataset<br>Inter~~ H~~ead|Qwen3B Qwen7B Intern2B Intern8B gemma2B gemma4B|
|OK-VQA<br>before<br>after|54.00<br>55.00<br>46.00<br>51.00<br>21.00<br>20.00<br>**7.00**<br>55.00<br>**44.00**<br>53.00<br>**18.00**<br>21.00|
|Clevr-Mathbefore<br>after|94.00<br>70.00<br>20.00<br>93.00<br>14.00<br>14.00<br>**0.00**<br>**59.00**<br>29.00<br>**13.00**<br>14.00<br>**13.00**|

---

[Document 3]
Source: attention_functional_roles_raw_chunks_2k.jsonl
Chunk ID: attention_functional_roles_raw_chunks_2k_11
Content: Figure 14: Pearson Correlation of different K of TopK token across eight functions for Qwen2.5VL-3B-Instruct.


Figure 15: Pearson Correlation of different LLM selection across eight functions for Qwen2.5-VL3B-Instruct.


24



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-23-1.png)
Table 10: Examples of topk token selection using different LLMs.


Question: What are the notable visual features of the couch and love seat in the image, such
as their shape, trim, and upholstery?
Answer: The couch and love seat in the image are patterned after a classic, elegant style with
intrica te detailing and a neutral color palette.
LLM top5 token selection:
Qwen3-30B: classic, elegant, patterned, detailing, neutral
GPT5: classic, elegant, patterned, detailing, neutral
Llama3.3-70B: classic, elegant, patterned, detailing, color
Question: What familiar object matches the shape and features of the blue item shown in the
image?
Answer: The blue item is a fire hydrant.

LLM top5 token selection:
Qwen3-30B: fire, hydrant
GPT5: fire, hydrant
Llama3.3-70B: fire, hydrant


Figure 16: Pearson Correlation of different classification layer initialization seed across eight functions for Qwen2.5-VL-3B-Instruct.


**Sparse consistency across models:** The relatively high Pearson correlation coefficients between
models in Fig. 17 indicate that different models exhibit consistent sparsity patterns for different
functions.


Table 11 reports the t-test results comparing random heads and cognitive heads, showing that the
differences are statistically significant, (p-value _≪_ 0.05).


Table 11: Welch t-test between random heads and cognitive heads.



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-24-0.png)





|Models|Vision mainly Cognitive Functions Language mainly Cognitive Functions|
|---|---|
|Models|Low-Level<br>High-Level<br>Recall<br>Info<br>Recall<br>Math<br>Inference<br>Decision|
|Models|p-value (t-test)<br>p-value (t-test)<br>p-value (t-test)<br>p-value (t-test)<br>p-value (t-test)<br>p-value (t-test)<br>p-value (t-test)<br>p-value (t-test)|
|Qwen2.5-VL-3B-Instruct<br>Qwen2.5-VL-3B-Instruct<br>InternVL3-2B<br>InternVL3-8B<br>Gemma3-2B<br>Gemma3-4B|1.83e-03<br>5.71e-05<br>0.43<br>0.66<br>2.84e-03<br>4.11e-03<br>8.47e-06<br>1.39e-04<br>8.27e-05<br>0.10<br>3.01e-03<br>2.91e-05<br>7.70e-03<br>0.76<br>0.46<br>7.71e-05<br>0.02<br>0.05<br>8.58e-03<br>0.88<br>4.97e-03<br>1.49e-03<br>0.18<br>3.78e-03<br>3.98e-07<br>4.86e-06<br>2.80e-04<br>3.15e-06<br>1.19e-03<br>1.86e-05<br>1.01e-03<br>0.19<br>1.84e-06<br>0.19<br>2.32e-03<br>3.87e-03<br>1.98e-04<br>1.86e-04<br>9.43e-03<br>6.65e-05<br>5.33e-05<br>0.04<br>0.01<br>0.07<br>2.28e-04<br>0.55<br>7.19e-03<br>4.15e-05|


25


![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-25-0.png)

Figure 17: Pearson Correlation betweeen different models across eight functions.


Figure 18: The importance (average attention weight) of visual modality for different functional
heads (Top 30).


A.10 DETAILS OF ACTIVATION PATCHING


In both the _random_ and _mean_ activation-patch methods, we replace the cognitive heads using patches
from another function. To minimize correlation between functions, we use language-related functions when patching vision functions, and vice versa. Table 12 shows the corresponding function
mappings.


Table 12: Function-to-Function Mapping Used for Activation Patching


**Original Function** **Replacement Function**


Vision Knowledge Recall Decision-Making
Language Knowledge Recall High-Level Vision Reception
Semantic Understanding High-Level Vision Reception
Math Reasoning Vision Knowledge Recall
Low-Level Vision Reception Inference
Inference Vision Knowledge Recall
High-Level Vision Reception Semantic Understanding
Decision-Making Vision Knowledge Recall


26



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-25-1.png)
A.11 NEGATIVE AND POSITIVE INTERVENTION CASES


The examples shown below illustrates a negative intervention case, where the model initially gives
an correct answer but produces the incorrect one (The model fails to answer in this case) after
intervention.



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-26-0.png)



The examples shown below illustrates the positive intervention success cases and failure case, where
the model initially gives an incorrect answer (marked in red) but produces the correct one (marked
in green) after intervention. This shows that adjusting the activation of cognitive heads along their
corresponding functional directions enhances performance on visual reasoning tasks. However, in
complex scenes, positive intervention may also have negative effects by assigning more weight to
unimportant elements, for example, focusing on tools and materials while overlooking the toilet
settings in the example below.



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-26-2.png)



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-26-1.png)

27


![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-27-1.png)

28



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-27-0.png)

![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-27-2.png)

![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-27-3.png)
![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-28-1.png)

29
","The provided context discusses Pearson Correlation in several figures (Figure 3, Figure 14, Figure 15, Figure 16, Figure 17) and compares the effects of cognitive masking and random masking using performance metrics and t-tests (Table 1, Table 11). However, it does not provide a specific Pearson correlation value observed directly between ""cognitive masking"" and ""random masking"" effects in VLMs.",116,4473,4590,,3,,2026-01-02T20:15:40.470962
"Compare the computational complexity of the Neuronal Attention Circuit (NAC) with the standard self-attention mechanism described in ""Attention Is All You Need"".",Multi-Document Aggregation,"[Document 1]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_7
Content: compact architecture suitable for resource-constrained devices, thereby supporting localized safety.
We utilized three benchmark datasets: (i) PRONOSTIA
(Nectoux et al., 2012), (ii) XJTU-SY (Wang et al., 2018),
and (iii) HUST (Thuan & Hong, 2023). Training is performed on PRONOSTIA, while XJTU-SY and HUST are

used to assess cross-validation. We used the Score met
ric (Nectoux et al., 2012) to assess the performance. We
evaluate generalization using _Bearing 1_ from the first operating condition of each dataset. Figure 3 visualizes the
expected degradation alongside the outputs of NAC. On the
PRONOSTIA dataset, ContiFormer performed the best with



**3.4. Run Time and Memory Experiments**


We evaluate computational requirements on fixed-length
sequences of 1024 steps, 64-dimensional features, 4 heads,
and a Batch size of 1. Each model is run for ten for
ward passes on Google Colab T4-GPU, and we report the
mean runtime with standard deviation, throughput, and peak
memory usage. NAC occupies an intermediate position
in runtime relative to several CT-RNN models, including
GRU-ODE, CfC, and LTC. In terms of memory consumption, NAC uses significantly less memory than mTAN, with
NAC-2k being the least memory-consuming among the CTAttention models. Reducing NAC sparsity from 90% to 20%
has minimal effect on memory, decreasing usage slightly



7


**Neuronal Attention Circuit (NAC) for Representation Learning**



_Table 2._ Run-Time and Memory Benchmark Results


**Run-Time** **Throughput** **Peak Memory**
**Model**
(s) (seq/s) (MB)


RNN 1 _._ 8392 _[±]_ [0] _[.]_ [1933] 0.544 0.29
CT-RNN 7 _._ 1097 _[±]_ [0] _[.]_ [3048] 0.141 0.67
LSTM 2 _._ 6241 _[±]_ [0] _[.]_ [2906] 0.381 0.42
PhasedLSTM 4 _._ 9812 _[±]_ [0] _[.]_ [272] 0.201 0.80
GRU 3 _._ 216 _[±]_ [0] _[.]_ [2566] 0.311 0.54
GRU-ODE 12 _._ 2498 _[±]_ [0] _[.]_ [0525] 0.082 0.64
mmRNN 7 _._ 5852 _[±]_ [0] _[.]_ [2785] 0.132 0.96
LTC 14 _._ 643 _[±]_ [0] _[.]_ [2445] 0.068 0.99
CfC 6 _._ 0988 _[±]_ [0] _[.]_ [2135] 0.164 0.76


Attention 0 _._ 0016 _[±]_ [0] _[.]_ [0001] 625.00 16.86
MHA 0 _._ 0041 _[±]_ [0] _[.]_ [0001] 243.90 69.05
mTAN 0 _._ 0272 _[±]_ [0] _[.]_ [0054] 36.76 790.16
ODEFormer 0 _._ 0317 _[±]_ [0] _[.]_ [0016] 31.55 67.71
CTA 8 _._ 5275 _[±]_ [0] _[.]_ [2355] 0.117 1.43
ContiFormer 0 _._ 066 _[±]_ [0] _[.]_ [0075] 15.15 67.71


NAC-2k 7 _._ 3071 _[±]_ [0] _[.]_ [1547] 0.137 44.75
NAC-32k 7 _._ 2313 _[±]_ [0] _[.]_ [219] 0.138 549.86
NAC-PW 8 _._ 5649 _[±]_ [0] _[.]_ [0203] 0.117 5042.09
NAC-FC 0 _._ 0195 ~~_[±]_~~ ~~[0]~~ ~~_[.]_~~ ~~[0002]~~ 51.28 29.92
NAC-02s 7 _._ 252 _[±]_ [0] _[.]_ [2018] 0.138 151.54
NAC-09s 7 _._ 222 _[±]_ [0] _[.]_ [176] 0.139 150.85
NAC-Exact/05s/8k 7 _._ 4101 ~~_[±]_~~ ~~[0]~~ ~~_[.]_~~ ~~[1586]~~ 0.135 151.50
NAC-Euler 7 _._ 3367 _[±]_ [0] _[.]_ [1719] 0.136 152.22
NAC-Steady 7 _._ 2942 _[±]_ [0] _[.]_ [1451] 0.137 150.86


from 151.54 MB to 150.85 MB. In constrast, decreasing
the Top- _K_ selection from _PW_ to _k_ = 2 drastically reduces
memory consumption from 5042 MB to 44.75 MB, demonstrating the flexibility of NAC.
**Interpreting the Results:** From the experiments, we observe that increasing the sparsity of the NAC layer improves
the robustness of the system and leads to higher overall accuracy. Similarly, increasing the Top- _K_ interactions enhances
accuracy too; however, the benefits diminish as memory
consumption grows. Using Exact mode, Top- _K_ =8 with 50%
sparsity achieves the best balance between accuracy and
efficiency. Steady mode is the fastest, while Euler mode
handles adaptive temporal dynamics.


**4. Discussions**



This research is part of ongoing work on biologically plausible attention mechanisms and represents a pioneering step,
with limitations to be addressed in future work.

**Architectural improvement:** Currently, NAC uses predetermined wiring (AutoNCP) requiring three inputs: number of units (sensory + interneuron + motor), output motor
neurons, and sparsity, with typically 60% of units assigned
to sensory neurons. To integrate with the attention mechanism while preserving wiring, sensory units for _NN_ sensory
are set as unitssensory = � _d_ model0 _.−_ 6 0 _._ 5 � and backbone units as



are set as unitssensory = � _d_ model0 _.−_ 6 0 _._ 5 � and backbone units as

units _backbone_ = _d_ model + � _d_ 0model _._ 6 �, where _⌈·⌉_ and _⌊·⌋_ denote



units _backbone_ = _d_ model + � _d_ 0model _._ 6 �, where _⌈·⌉_ and _⌊·⌋_ denote

the ceiling and floor functions, respectively. This results
in a larger overall architectural size and increased runtime.



Future work will support user-defined NCPs configurations
or randomized wiring to enable more efficient architectures.
**Learnable sparse Top-** _**K**_ **selection:** Sparse Top- _K_ attention can miss important context, is sensitive to _k_, and may
be harder to optimize. A further limitation is that it still
computes the full _QK_ _[⊤]_ matrix, which can dominate the
cost for very long sequences. Future work includes adaptive
or learnable Top- _K_ selection, improved key scoring, and
hardware-aware optimization to strengthen accuracy and
robustness.


**5. Conclusion**

---

[Document 2]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_8
Content: the ceiling and floor functions, respectively. This results
in a larger overall architectural size and increased runtime.



Future work will support user-defined NCPs configurations
or randomized wiring to enable more efficient architectures.
**Learnable sparse Top-** _**K**_ **selection:** Sparse Top- _K_ attention can miss important context, is sensitive to _k_, and may
be harder to optimize. A further limitation is that it still
computes the full _QK_ _[⊤]_ matrix, which can dominate the
cost for very long sequences. Future work includes adaptive
or learnable Top- _K_ selection, improved key scoring, and
hardware-aware optimization to strengthen accuracy and
robustness.


**5. Conclusion**


In this paper, we introduce the Neuronal Attention Circuit
(NAC), a biologically inspired attention mechanism that
reformulates attention logits as the solution to a first-order
ODE modulated by nonlinear, interlinked gates derived from
repurposing _C.elegans_ nematode NCPs. NAC bridges discrete attention with continuous-time dynamics, enabling
adaptive temporal processing without the limitations inherent to traditional scaled dot-product attention. Based on the
solution to ODE, we introduce three computational modes:
(i) Euler, based on _explicit Euler_ integration; (ii) Exact, providing closed-form solutions; and (iii) Steady, approximating equilibrium states. In addition, a sparse Top- _K_ pairwise
concatenation scheme is introduced to mitigate the memory intensity. Theoretically, we establish NAC’s log-state
stability, exponential error bounds, and universal approximation, thereby providing rigorous guarantees of convergence
and expressiveness. Empirical evaluations demonstrate that
NAC achieves state-of-the-art performance across diverse
tasks, including irregularly sampled time-series benchmarks,
autonomous vehicle lane-keeping, and industrial prognostics. Moreover, NAC occupies an intermediate position
between CT-RNNs and CT-Attention, offering robust temporal modeling while requiring less runtime than CT-RNNs
and less memory than CT-Attention models.


**Reproducibility Statement**


The code for reproducibility is available at
[https://github.com/itxwaleedrazzaq/](https://github.com/itxwaleedrazzaq/neuronal_attention_circuit)

[neuronal_attention_circuit](https://github.com/itxwaleedrazzaq/neuronal_attention_circuit)


**Impact Statement**


The work addresses the growing field of continuous-time
attention and pioneers a biologically plausible mechanism.
It encourages research into sparse, adaptive networks that
resemble natural wiring. From a societal perspective, it
supports more robust AI in resource-limited settings, but it
also raises ethical concerns when applied to areas such as
surveillance or autonomous systems.



8


**Neuronal Attention Circuit (NAC) for Representation Learning**



**References**


Introduction to self-driving cars. URL
[https://www.udacity.com/course/](https://www.udacity.com/course/intro-to-self-driving-cars--nd113)
[intro-to-self-driving-cars--nd113.](https://www.udacity.com/course/intro-to-self-driving-cars--nd113)


Aguiar-Conraria, L. and Soares, M. J. The continuous
wavelet transform: Moving beyond uni-and bivariate analysis. _Journal of economic surveys_, 28(2):344–375, 2014.


Beltagy, I., Peters, M. E., and Cohan, A. Longformer: The long-document transformer. _arXiv preprint_
_arXiv:2004.05150_, 2020.


Bojarski, M., Del Testa, D., Dworakowski, D., Firner, B.,
Flepp, B., Goyal, P., Jackel, L. D., Monfort, M., Muller,
U., Zhang, J., et al. End to end learning for self-driving
cars. _arXiv preprint arXiv:1604.07316_, 2016.


Brockman, G., Cheung, V., Pettersson, L., Schneider, J.,
Schulman, J., Tang, J., and Zaremba, W. Openai gym.
_arXiv preprint arXiv:1606.01540_, 2016.


Cao, Y., Li, S., Petzold, L., and Serban, R. Adjoint sensitivity analysis for differential-algebraic equations: The
adjoint dae system and its numerical solution. _SIAM_
_journal on scientific computing_, 24(3):1076–1089, 2003.


Chen, R. T., Rubanova, Y., Bettencourt, J., and Duvenaud,
D. K. Neural ordinary differential equations. _Advances_
_in neural information processing systems_, 31, 2018.


Chen, Y., Ren, K., Wang, Y., Fang, Y., Sun, W., and Li, D.
Contiformer: Continuous-time transformer for irregular
time series modeling. _Advances in Neural Information_
_Processing Systems_, 36:47143–47175, 2023.


Chien, J.-T. and Chen, Y.-H. Continuous-time attention for
sequential learning. In _Proceedings of the AAAI confer-_
_ence on artificial intelligence_, volume 35, pp. 7116–7124,
2021.


Cho, K., Van Merrienboer, B., Gulcehre, C., Bahdanau,¨
D., Bougares, F., Schwenk, H., and Bengio, Y. Learning phrase representations using rnn encoder-decoder
for statistical machine translation. _arXiv preprint_
_arXiv:1406.1078_, 2014.


d’Ascoli, S., Becker, S., Mathis, A., Schwaller, P., and
Kilbertus, N. Odeformer: Symbolic regression of dynamical systems with transformers. _arXiv preprint_
_arXiv:2310.05573_, 2023.


De Brouwer, E., Simm, J., Arany, A., and Moreau, Y.
Gru-ode-bayes: Continuous modeling of sporadicallyobserved time series. _Advances in neural information_
_processing systems_, 32, 2019.



Deng, L. The mnist database of handwritten digit images
for machine learning research [best of the web]. _IEEE_
_signal processing magazine_, 29(6):141–142, 2012.


Ding, Y., Jia, M., Miao, Q., and Huang, P. Remaining
useful life estimation using deep metric transfer learning
for kernel regression. _Reliability Engineering & System_
_Safety_, 212:107583, 2021.


Hasani, R., Lechner, M., Amini, A., Rus, D., and Grosu,
R. Liquid time-constant networks. In _Proceedings of the_
_AAAI Conference on Artificial Intelligence_, volume 35,
pp. 7657–7666, 2021.


Hasani, R., Lechner, M., Amini, A., Liebenwein, L., Ray,
A., Tschaikowski, M., Teschl, G., and Rus, D. Closed
form continuous-time neural networks. _Nature Machine_

_Intelligence_, 4(11):992–1003, 2022.


Hochreiter, S. The vanishing gradient problem during learning recurrent neural nets and problem solutions. _Interna-_
_tional Journal of Uncertainty, Fuzziness and Knowledge-_
_Based Systems_, 6(02):107–116, 1998.


Hochreiter, S. and Schmidhuber, J. Long short-term memory.
_Neural computation_, 9(8):1735–1780, 1997.


Hong, H. S. and Thuan, N. Hust bearing: a practical dataset
for ball bearing fault diagnosis. _Mendeley Data_, 3, 2023.


John, F. On integration of parabolic equations by difference methods: I. linear and quasi-linear equations for the
infinite interval. _Communications on Pure and Applied_
_Mathematics_, 5(2):155–211, 1952.

---

[Document 3]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_0
Content: ## **Neuronal Attention Circuit (NAC) for Representation Learning**

**Waleed Razzaq** [1] **Izis Kankaraway** [1] **Yun-Bo Zhao** [1 2]



**Abstract**

Attention improves representation learning over
RNNs, but its discrete nature limits continuoustime (CT) modeling. We introduce Neuronal Attention Circuit (NAC), a novel, biologically plausible CT-Attention mechanism that reformulates

attention logits computation as the solution to a
linear first-order ODE with nonlinear interlinked
gates derived from repurposing _C. elegans_ Neuronal Circuit Policies (NCPs) wiring mechanism.
NAC replaces dense projections with sparse sensory gates for key-query projections and a sparse
backbone network with two heads for computing
_content-target_ and _learnable time-constant_ gates,
enabling efficient adaptive dynamics. NAC supports three attention logit computation modes: (i)
explicit Euler integration, (ii) exact closed-form
solution, and (iii) steady-state approximation. To
improve memory intensity, we implemented a
sparse Top- _K_ pairwise concatenation scheme that
selectively curates key-query interactions. We
provide rigorous theoretical guarantees, including
state stability, bounded approximation errors, and
universal approximation. Empirically, we implemented NAC in diverse domains, including irregular time-series classification, lane-keeping for
autonomous vehicles, and industrial prognostics.
We observed that NAC matches or outperforms
competing baselines in accuracy and occupies an
intermediate position in runtime and memory efficiency compared with several CT baselines.


**1. Introduction**


Learning representations of sequential data in temporal or
spatio-temporal domains is essential for capturing patterns
and enabling accurate forecasting. Discrete-time Recurrent neural networks (DT-RNNs) such as RNN (Rumelhart et al., 1985; Jordan, 1997), Long-short term memory


1Department of Automation, University of Science & Technology of China, Hefei, China [2] Institute of Artificial Intelligence,
Hefei Comprehensive National Science Center. Correspondence to:
Yun-Bo Zhao _<_ ybzhao@ustc.edu.cn _>_, Waleed Razzaq _<_ waleedrazzaq@mail.ustc.edu.cn _>_ .


_Preprint. December 12, 2025._



(LSTM) (Hochreiter & Schmidhuber, 1997), and Gated Recurrent Unit (GRU) (Cho et al., 2014) model sequential dependencies by iteratively updating hidden states to represent
or predict future elements in a sequence. While effective
for regularly sampled sequences, DT-RNNs face challenges
with irregularly sampled data because they assume uniform
time intervals. In addition, vanishing gradients can make
it difficult to capture long-term dependencies (Hochreiter,
1998).
Continuous-time RNNs (CT-RNNs) (Rubanova et al., 2019)
model hidden states as ordinary differential equations
(ODEs), allowing them to process inputs that arrive at arbitrary or irregular time intervals. Mixed-memory RNNs
(mmRNNs) (Lechner & Hasani, 2022) build on this idea
by separating memory compartments from time-continuous
states, helping maintain stable error propagation while capturing continuous-time dynamics. Liquid neural networks
(LNNs) (Hasani et al., 2021; 2022) take a biologically inspired approach by assigning variable time-constants to hidden states, improving adaptability and robustness, though
vanishing gradients can still pose challenges during training.
The attention mechanisms (Vaswani et al., 2017) mitigate
this limitation by treating all time steps equally and allowing models to focus on the most relevant observations. It
computes the similarity between queries ( _q_ ) and keys ( _k_ ),
scaling by the key dimension to keep gradients stable. MultiHead Attention (MHA) (Vaswani et al., 2017) extends this
by allowing the model to attend to different representation
subspaces in parallel. Variants like Sparse Attention (Tay
et al., 2020; Roy et al., 2021), BigBird (Zaheer et al., 2020),
and Longformer (Beltagy et al., 2020) modify the attention
pattern to reduce computational cost, particularly for long sequences, by attending only to selected positions rather than
all pairs. Even with these improvements, attention-based
methods still rely on discrete scaled dot-product operations,
limiting their ability to model continuous trajectories often
captured by CT counterparts.
Recent work has explored bridging this gap through NeuralODE (Chen et al., 2018) formulation. mTAN (Shukla &
Marlin, 2021) learns CT embeddings and uses time-based
attention to interpolate irregular observations into a fixedlength representation for downstream encoder-decoder modeling. ODEFormer (d’Ascoli et al., 2023) trains a sequenceto-sequence transformer on synthetic trajectories to directly
infer symbolic ODE systems from noisy, irregular data,
though it struggles with chaotic systems and generalization



1


**Neuronal Attention Circuit (NAC) for Representation Learning**



beyond observed conditions. Continuous-time Attention
(CTA) (Chien & Chen, 2021) embeds a continuous-time
attention mechanism within a Neural ODE, allowing attention weights and hidden states to evolve jointly over time.
Still, it remains computationally intensive and sensitive to
the accuracy of the ODE solver. ContiFormer (Chen et al.,
2023) builds a CT-transformer by pairing ODE-defined latent trajectories with a time-aware attention mechanism to
model dynamic relationships in data.
Despite these innovations, a persistent and underexplored
gap remains in developing a biologically plausible attention
mechanism that seamlessly integrates CT dynamics with the
abstraction of the brain’s connectome to handle irregular sequences without prohibitive computational costs. Building
on this, we propose a novel attention mechanism called the
_Neuronal Attention Circuit_ (NAC), in which attention logits
are computed as the solution to a first-order ODE modulated
by nonlinear, interlinked gates derived from repurposing
Neuronal Circuit Policies (NCPs) from the nervous system
of _C. elegans_ nematode (refer to Appendix A.2 for more
information). Unlike standard attention, which projects keyquery pairs through a dense layer, NAC employs a sensory
gate to transform input features and a backbone to model
nonlinear interactions, with multiple heads producing outputs structured for attention logits computation. Based on
the solutions to ODE, we define three computation modes:
(i) Exact, using the closed-form ODE solution; (ii) Euler,
approximating the solution via _explicit Euler_ integration;
and (iii) Steady, using only the steady-state solution, analogous to standard attention scores. To reduce computational
complexity, we implemented a sparse Top- _K_ pairwise concatenation algorithm that selectively curates key-query inputs. We evaluate NAC across multiple domains, including
irregularly sampled time series, autonomous vehicle lanekeeping, and Industry 4.0, comparing it to state-of-the-art
baselines. NAC consistently matches or outperforms these
models, while runtime and peak memory benchmarks place
it between CT-RNNs in terms of speed and CT-Attentions
in terms of memory requirements.


**2. Neuronal Attention Circuit (NAC)**


We propose a simple alternative formulation of the attention
logits _a_ (refer to Appendix A.1 for more information), interpreting them as the solution to a first-order linear ODE
modulated by nonlinear, interlinked gates:



from repurposing NCPs. We refer to this formulation as
the Neuronal Attention Circuit (NAC). It enables the logits
_at_ to evolve dynamically with input-dependent, variable
time constants, mirroring the adaptive temporal dynamics
found in _C. elegans_ nervous systems while improving
computational efficiency and expressiveness. Moreover, it
introduces continuous depth into the attention mechanism,
bridging discrete-layer computation with dynamic temporal
evolution.

**Motivation behind this formulation:** The proposed
formulation is loosely motivated by the input-dependent
time-constant mechanism of Liquid Neural Networks
(LNNs), a class of CT-RNNs inspired by biological nervous
systems and synaptic transmission. In this framework, the
dynamics of non-spiking neurons are described by a linear
ODE with nonlinear, interlinked gates: _ddt_ **xt** = **[x]** _τ_ **[t]** [+] **[ S][t]** _[,]_
","Based on the provided context:

**Standard Self-Attention (as described in ""Attention Is All You Need"" and represented by ""Attention"" and ""MHA"" in the benchmarks):**
*   **Runtime:** Standard ""Attention"" is very fast, with a reported runtime of 0.0016 seconds. Multi-Head Attention (MHA) is also fast, with a runtime of 0.0041 seconds.
*   **Memory:** Standard ""Attention"" uses 16.86 MB of peak memory, while MHA uses 69.05 MB.
*   **Mechanism:** It computes the similarity between queries and keys, scaling by the key dimension. It relies on discrete scaled dot-product operations.

**Neuronal Attention Circuit (NAC):**
*   **Runtime:** Most NAC variants (e.g., NAC-2k, NAC-32k, NAC-Euler, NAC-Steady) have significantly higher runtimes, typically ranging from 7 to 8.5 seconds. An exception is NAC-FC, which has a runtime of 0.0195 seconds, making it faster than most other NACs but still slower than standard ""Attention"" and MHA. The ""Steady mode"" is noted as the fastest among NAC's computational modes. The current architectural wiring of NAC also contributes to increased runtime.
*   **Memory:** NAC's memory consumption varies widely depending on the variant and configuration. NAC-FC uses 29.92 MB, and NAC-2k uses 44.75 MB, which are less than MHA. However, other variants like NAC-32k (549.86 MB) and NAC-PW (5042.09 MB) use substantially more memory. The sparse Top-K selection scheme can drastically reduce memory consumption (e.g., from 5042 MB for PW to 44.75 MB for k=2). NAC is generally noted to use significantly less memory than mTAN.
*   **Mechanism:** NAC reformulates attention logits as the solution to a first-order Ordinary Differential Equation (ODE), modulated by nonlinear, interlinked gates. It replaces dense projections with sparse sensory gates for key-query projections and a sparse backbone network. To mitigate memory intensity and reduce computational complexity, it employs a sparse Top-K pairwise concatenation scheme. However, a limitation of this sparse Top-K approach is that it still computes the full QKᵀ matrix, which can dominate the cost for very long sequences.

**Comparison Summary:**
In terms of empirical runtime, standard ""Attention"" and Multi-Head Attention are generally significantly faster than most NAC variants. While NAC-FC offers a competitive runtime, other NAC configurations are considerably slower. For memory consumption, NAC's performance is more varied; some NAC variants (like NAC-FC and NAC-2k) are more memory-efficient than MHA, while others (like NAC-PW and NAC-32k) require much more memory. NAC's sparse Top-K scheme aims to reduce complexity, but the underlying computation of the full QKᵀ matrix remains a potential bottleneck for long sequences. The current NAC architecture also contributes to a larger size and increased runtime.",124,13419,13545,,3,,2026-01-02T20:15:54.018842
"How does the ""Interleaving Layers"" strategy in SWAA compare to the hierarchical organization of functional heads discussed in the CogVision paper?",Multi-Document Aggregation,"[Document 1]
Source: sliding_window_attention_annotated_chunks_2k.jsonl
Chunk ID: sliding_window_attention_annotated_chunks_2k_3
Content: **Combinations exhibit strong synergy.** Recipes
that combine multiple methods deliver large gains:


  - **FA Decode + Keep First** _k_ substantially improves over naive SWA (rows 7–9), recovering roughly half to two-thirds of the gap on
the thinking model as _k_ increases. However,
increasing _k_ from 100 to 1000 yields almost
no improvement, indicating that _k_ does not
need to be exceedingly large.


  - **Interleaving Layers + FA Decode** is
markedly stronger (row 13), recovering most
of the gap for the thinking model.


  - **FA Decode + Interleaving Layers + Keep**
**First** _k_ perform best (rows 18). The thinking
model recovers close to 90% of the FA gap
even at 2k window.


**CoT synergizes with FA Decode.** Under recipes
that include FA Decode, the thinking model consistently benefits more than the non-thinking model
(rows 13 and 18), suggesting that CoT synergizes
with FA Decode: preserving global attention at
decoding time enables longer reasoning traces to
capitalize on context processed by SWA, confirming our hypothesis in Section 3.1.


**Sliding window size affects, but is not the deci-**
**sive role.** With FA Decode + Keep First _k_, accuracy improves as the window increases (rows 7,
14, 15), though benefits are modest until 8k. When
added with interleaving FA layers, moving from a
2k to a 4k window is enough to close the remaining
gap on the thinking model (row 22 matches the FA
baseline), indicating that FA Decode and Interleaving are the dominant drivers, and larger windows
mainly smooth residual error.



5


Table 1: Qwen3-4B-Thinking and Qwen3-4B-Instruct results on LongMemEval without SFT


**No.** **window size** **FA layers** **keep first** **FA decode** **Acc think** **Acc non-think**


0 Full [] 0 False **73.0** **62.0**
1 2k [] 0 False 3.2 11.0
2 8k [] 0 False 13.2 19.8


3 2k [] 10 False 16.0 15.6
4 2k [] 0 True 11.8 14.2
5 2k [1, 3, 5, ...] 0 False 13.4 18.4
6 8k [] 0 True 26.2 25.0


7 2k [] 10 True 38.2 20.6
8 2k [] 100 True 50.0 17.8
9 2k [] 1000 True 50.0 20.2
10 2k [0, 2, 4, ...] 10 False 17.0 14.8
11 2k [0, 2, 4, ...] 0 True 32.2 26.0
12 2k [1, 3, 5, ...] 10 False 25.8 36.4
13 2k [1, 3, 5, ...] 0 True 59.2 34.8
14 4k [] 10 True 38.0 24.4
15 8k [] 10 True 49.2 35.2


16 2k [0, 2, 4, ...] 10 True 36.0 17.2
17 2k [1, 3, 5, ...] 10 True 65.0 **53.6**
18 2k [1, 3, 5, ...] 100 True **68.8** 50.6
19 2k [1, 5, 9, ...] 10 True 53.2 31.4
20 2k [1, 9, 17, ...] 10 True 36.4 18.8
21 2k [3, 7, 11, ...] 10 True 54.2 34.6
22 4k [1, 3, 5, ...] 100 True **73.0** **54.2**
23 8k [1, 3, 5, ...] 100 True **71.6** **56.6**



**SWA/FA layer selection has large impacts.** Selecting less layers for FA, e.g. only one fourth or
eighth (row 19, 20), though more efficient, will
significantly decrease the improvement brought by
Interleaving Layers. More importantly, for Qwen34B, configuring _odd-numbered_ layers with full attention is significantly better than _even-numbered_
layers (row 11, 13, 16, 17). However, surprisingly,
this result is reversed for Qwen3-30B-A3B and
Llama3.1-8B-Instruct (row 10 and 11 in Table 4
and 5). This suggests that layer functionalities differ across model families and sizes, necessitating
model-specific layer selection strategies, as discussed in Section 3.2.


Therefore, we answer RQ1: adapting an FA
LLM to SWA is feasible even without any training. But it requires specific combinations of at
least 2 methods, which could be less efficient for
inference.



**5.2** **SWA Adaptation With Fine-tuning**


We next evaluate SWA-aware supervised finetuning, which is expected to provide higher improvement. Table 2 reports LongMemEval_24k
accuracy after SFT under various SWAA configurations. The original full-attention model is also
fine-tuned as a baseline (row 0). Since training is
relatively time-consuming, we only test a representative subset of configurations. Our findings are as
follows:


**Fine-tuning substantially lifts all SWA config-**
**urations.** Comparing all the fine-tuning results
with non-training ones, it is clear that fine-tuning
consistently provides great improvement. However,
simply fine-tuning with naive SWA remains insufficient, only achieving 18.8% and 23.8% accuracy
(row 1).


**FA Decode and Interleaving Layers emerge as**
**dominant components.** After SFT, FA Decode
and Interleaving Layers provide the largest gains.



6


Table 2: Qwen3-4B-Thinking and Qwen3-4B-Instruct results on LongMemEval with SFT


**No.** **window size** **FA layers** **keep first** **FA decode** **Acc think** **Acc non-think**


0 Full [] 0 True **74.6** **63.4**
1 2k [] 0 False 18.8 23.8


2 2k [] 10 False 15.6 /
3 2k [] 0 True 57.9 42.0
4 2k [1, 3, 5, ...] 0 False 63.6 54.6
5 4k [] 0 True 62.6 /


6 2k [] 10 True 56.7 /
7 2k [] 100 True 62.2 42.6
8 2k [1, 3, 5, ...] 0 True **73.2** **58.8**
9 2k [0, 2, 4, ...] 0 True **66.0** /
10 2k [1, 5, 9, ...] 0 True **68.8** 47.0


11 2k [1, 3, 5, ...] 100 True **73.2** **61.4**


Table 3: Recommended SWA adaptation recipes for different needs and scenarios. � means optional.


**Training** ? **Thinking Model?** **Prefer?** **FA Decode** **Interleaving Layers** **Keep First**


No No Any � � �
No Yes Efficiency � � �
No Yes Accuracy � � �


Yes Any Efficiency � � �
Yes Any Accuracy � � �



Using FA Decode alone (row 3) or using Interleaving Layers alone (row 4) both get high accuracy.
And combining both (row 8) further boosts performance to 73 _._ 2 (think) and 58 _._ 8 (non-think), nearly
matching the full-attention SFT baseline in row 0.


**Keep First becomes optional rather than essen-**
**tial.** Before fine-tuning, Keep First is crucial for
stability under SWA. But after SFT, it only provides
minor additional improvement. With FA Decode,
adding _k_ = 100 (row 7) improves over _k_ = 0
(row 3) only 4.5%, and if further combining FA
layers, it almost offers no improvements (row 11
and row 8).


**Effect of sliding window size.** Row 3 and 5
shows that increasing the window from 2k to 4k
with FA Decode improves thinking-model accuracy from 57 _._ 9 to 62 _._ 6. This mirrors the non-SFT
trend that larger windows help, but the dominant
improvements still come from FA Decode and Interleaving Layers.

---

[Document 2]
Source: attention_functional_roles_raw_chunks_2k.jsonl
Chunk ID: attention_functional_roles_raw_chunks_2k_1
Content: Furthermore, we validate the functional importance of these heads by showing that their removal
degrades performance on complex tasks and leads to specific error patterns, while their enhancement improves reasoning capabilities. Our findings provide compelling evidence that these attention
heads play a critical role in multimodal reasoning. This insight not only deepens our understanding of the internal organization of VLMs but also suggests potential avenues for designing more
interpretable and cognitive-inspired multimodal AI systems.


2 COGVISION


In this section, we present our dataset Cognitive Vision (CogVision) that contains cognitive process
in multimodal reasoning. CogVision contains 1,409 main questions and 5,744 subquestions. Each


2


example comprises a main question and answer, its subquestions and subanswers and an annotation
specifying the receptive or cognitive function required for each subquestion.


2.1 COGNITIVE FUNCTIONS


To systematically capture the cognitive processes involved in complex reasoning tasks, we consider eight functions related to complex multimodal reasoning. These functions are inspired by
established frameworks in cognitive science (Anderson, 2014; Diamond, 2013), which highlight the
importance of perception, working memory, and reasoning in human cognition.


The functions related to vision include:


    - **Low-level Visual Reception** : Recognizing basic visual features such as color, shape, size,
position, motion.

    - **High-level Visual Reception** : Integrating visual information to recognize objects, patterns,
and scene structure.

    - **Visual Knowledge Recall** : Applying long-term visual knowledge related to visual concepts and their properties.


The functions related to language include:


    - **Language Information Extraction and Understanding** : locating and understanding relevant information from an external source or prior context.

    - **Language Knowledge Recall** : Accessing domain-specific knowledge without visual input.

    - **Math Reasoning** : Performing counting, arithmetic, comparison, and logic-based operations.

    - **Inference** : deriving implicit information that is not directly stated.

    - **Decision-Making** : Selecting the best outcome among alternatives based on reasoning.


This categorization reflects a natural progression from basic information processing to complex cognitive integration. Both the human brain and VLMs encompass a wide range of functional modules.
Our focus in this work is specifically on reasoning-related cognitive functions. By identifying and
organizing these eight core reasoning functions, we can more clearly examine how VLMs handle
different types of thinking steps, in a way that is both systematic and easy to interpret. Detail descriptions of each function and examples in CogVision can be found in Appendix A.1.


2.2 DATA COLLECTIONS


Based on our categorization of reasoning functions, we sampled 2000 diverse questions from existing commonly used visual reasoning benchmarks, selecting 200 examples each from ClevrMath (Lindstr¨om & Abraham, 2022), MathVision (Wang et al., 2024a), MathVista (Lu et al., 2024b),
MMMU (Yue et al., 2024), ScienceQA (Saikh et al., 2022), OKVQA (Marino et al., 2019) (400
examples), VCR (Zellers et al., 2019), VisuLogic (Xu et al., 2025) and Super-Clevr (Li et al.,
2023b) datasets. These datasets span a range of reasoning problems, including logical, mathematical, and commonsense reasoning. Using the chain-of-thought (CoT) paradigm, we prompted
GPT-4.1 (Achiam et al., 2023) to decompose each main question (with its final answer) into subquestions, each targeting a subanswer and a specific function. The prompt encourages structured,
step-by-step reasoning, ensuring that each subquestion is clear, answerable, and sequentially dependent. This process yields a set of subquestion–answer–function (subQAF) triples for each QA pair:
subQAFs = _{_ ( _qi, ai, fi_ ) _}_ _[k]_ _i_ =1 [, where each contains a subquestion] _[ q][i]_ [, its concise answer] _[ a][i]_ [, and the]
corresponding function label _fi_ . The prompt for generating subquestions are list in Appendix A.6.


2.3 DATA FILTERING AND ANNOTATION


Recent advances enable leveraging large pre-trained models for dataset construction, thanks to their
reasoning capabilities and ability to generate high-quality annotations at scale (Wang et al., 2024b).
While our dataset is automatically generated using a VLM to minimize manual effort, we employ a
rigorous two-stage human verification pipeline to ensure data quality and mitigate hallucination. In
the first stage, three expert annotator independently evaluate whether each subquestion is logically
structured and consistent with natural human reasoning. QA pairs with incoherent or inconsistent


3


decompositions are moved out. In the second stage, annotators verify and, if necessary, relabel the
cognitive function associated with each subquestion to ensure accurate alignment with the intended
mental process. Subanswers are further cross-checked with GPT-o3 (OpenAI, 2024) and adjudicated
by humans in cases of disagreement (details in Appendix A.5). This multi-step verification ensures
that each retained subQAF triplet represents a coherent, interpretable reasoning step grounded in
core cognitive functions. After this refinement, our final dataset comprises 1,409 main QA pairs
and 5,744 validated subQAF triplets. The detailed statistics of the dataset including the number of
triplet in training and testing set and distributions of each function can be found in Appendix A.3.
We analyze in-group subquestion diversity, and the results in the Appendix A.2 show that the eight
functions exhibit broad and overlapping distributions in phrasing patterns and token lengths, indicating no systematic surface-form differences.


3 DETECTION OF COGNITIVE FUNCTIONS


Using the CogVision dataset, we adopt a probing-based framework (Alain & Bengio, 2016; Belinkov, 2022; Tenney et al., 2019) to identify which attention heads in VLMs are associated with
specific functions in reasoning process. Specifically, for each functional annotated subquestion, we
extract head activations (see Subsection 3.1), train classifiers and compute accuracies to identify
contributing heads (see Subsection 3.2). Unlike prior work focusing on a single label, our formulation captures many-to-many relationships between heads and cognitive functions, enabling a more
nuanced analysis of functional specialization and overlap within the model.


3.1 HEAD FEATURE EXTRACTION


Given a large VLM _M_, we generate an answer _a_ _[M]_ _i_ for each subquestion _qi_ derived from a main
question _Qi_ . To support coherent multi-step reasoning, we include preceding subquestions and
their answers as contextual input, emulating the incremental reasoning process observed in human
cognition.


During inference, each input token is first mapped into an embedding and then propagated through
the transformer’s layers. At each layer, attention and feedforward operations update the residual
stream, which is ultimately decoded into token predictions. For each generated token _i_, we extract
attention head outputs _Xi_ = _{x_ _[m]_ _l_ _| l ∈_ 1 _, . . ., Nl, m ∈_ 1 _, . . ., Nh}_ across all layers, where _x_ _[m]_ _l_
denotes the value vector from the _m_ -th head in layer _l_ projected into the residual stream, with _Nh_
the number of heads per layer and _Nl_ the total number of layers.


Let _Nt_ denote the number of tokens in the generated answer _a_ _[M]_ _i_ [. To isolate semantically informative]
content relevant to reasoning, we select the top- _k_ most important tokens, determined by prompting
Qwen3-30B LLM (Yang et al., 2025), yielding an index set _Ik_ with _|Ik|_ = _k_ . For each index
_j ∈Ik_, we extract the corresponding attention head activations _Xj_, and compute the averaged
activation feature for the _m_ -th head in layer _l_ as ¯ _x_ _[m]_ _l_ = _k_ [1] � _j ∈Ikxml_ [. This results in a full set of]

---

[Document 3]
Source: sliding_window_attention_annotated_chunks_2k.jsonl
Chunk ID: sliding_window_attention_annotated_chunks_2k_1
Content: LightTransfer (Zhang et al., 2024) is a promising
attempt at adapting existing models to SWA without pretraining, which has the same motivation as
ours. But it may generalize poorly across model
families (see Appendix G).


**2.2** **Linear Attention**


An alternative approach involves reformulating
the attention mechanism entirely to achieve linear, _O_ ( _N_ ), complexity. This includes methods
such as RNN-like linear attention transformers

(Katharopoulos et al., 2020; Peng et al., 2023;
Sun et al., 2023) and structured state-space models
(SSMs) like Mamba (Gu and Dao, 2023). Many
works such as Jamba and Nemotron-Flash(Lieber
et al., 2024; Linsong Chu et al., 2024; Team et al.,
2025; Fu et al., 2025a) interleave linear attention
layers with traditional attention layers to create hybrid model structures. While promising, they represent a fundamental architectural departure from the
standard Transformer, and their performance is generally weaker than traditional Transformer-based

LLMs.



2


**3** **Candidate Methods for SWA**

**Adaptation**


As established, a naive application of SWA leads
to severe long-context performance degradation.
Therefore, we investigate five methods that can potentially facilitate SWA adaptation from distinct
perspectives. However, every method except finetuning has some drawbacks to LLM inference efficiency, as discussed in Appendix A. Therefore,
although these methods are not mutually exclusive,
we should not indiscriminately adopt all of them.
Instead, we must evaluate various combinations to
identify the optimal trade-off between performance
and efficiency.


**3.1** **Full Attention Decode**


This method applies SWA **only** to the prefilling
stage. During the decoding (auto-regressive generation) stage, each token still employs full attention,
allowing access to all previous tokens in the context. The resulting attention mask is depicted in
Figure 1a.
This novel approach, first proposed in our study,
is inspired by human reading comprehension: humans typically scan a passage casually (prefilling)
before thinking deeply to formulate an answer (decoding) for a specific problem. We term this strategy ""reading casually, thinking carefully."" In our
design, the SWA-constrained prefilling stage corresponds to casual reading, while the full-attention
decoding stage enables careful thinking. This analogy also suggests that Chain-of-Thought (CoT)
during decoding may be particularly beneficial, as
extended generation (i.e. ""thinking"") could compensate for the insufficient contextual information
gathered during the prefilling stage.


(a) FA Decode (b) Keep First


Figure 1: (a) Attention mask for FA Decode. SWA is
used for prompt tokens (prefill), and full attention is
used for generated tokens (decode). (b) Attention mask
for SWA combined with Keep First _k_ Tokens.



**3.1.1** **Keep First k Tokens**

Xiao et al. demonstrate that models pretrained with
full attention allocate a disproportionate amount
of attention to the initial tokens (""attention sink""),
and removing the visibility of these tokens causes
performance collapse. Their solution, streaming
attention, involves permanently retaining the attention to these ""sink"" tokens while using SWA, which
successfully maintains the stability of the attention
distribution and the model’s output. Our method
is basically the same: as shown in Figure 1b, any
subsequent token can attend to its local window
**and** the initial _k_ tokens.

Notably, however, our method extends beyond
its original version. Streaming attention operates
only at the KV cache level; specifically, the KV
cache of the sink tokens is retained during decoding, while the prefilling stage is not modified or accelerated at all. In contrast, we directly customize
the Flash-Attention-2 (Dao, 2024) kernel to implement such attention mask, thereby accelerating
prefilling via SWA as well, and eliminating the
need to modify KV cache.


**3.2** **Interleaving Layers**


This method retains full attention on a subset of

layers while applying SWA to the remainder, providing a simple hybrid mechanism to balance the
performance of full attention with the efficiency
of pure SWA. A common strategy involves designating one in every _n_ layers to use full attention
(e.g., layers 0, 2, 4, . . . retain full attention, while all
others use SWA). For example, Gemma-2 (Team,
2024a) uses SWA only for layers [1, 3, 5, . . . ], and
Gemma-3 (Team, 2025a) uses SWA only for layers

[5, 11, 17, ...].
However, for an FA-pretrained model, layers
may exhibit distinct behaviors, suggesting it may
not be optimal to simply assign SWA to layers [1,
3, 5, ...]. Instead, it might be preferable to use
statistical features to select ""lazy"" (mostly focusing
on just recent tokens) layers, as adopted by LightTransfer (Zhang et al., 2024). However, we find
that LightTransfer is not consistently superior in
practice (see Appendix G). Therefore, for simplicity, we still choose the simplest fixed-interval layer
selections in our experiments, such as [0, 2, 4, . . . ]
and [1, 3, 5, ...].


**3.3** **Chain-of-Thought**


Chain-of-Thought (CoT) (Wei et al., 2022) is a
widely used technique for improving model ac


3



```
### Figure - Sliding Window Attention
![Sliding Window Attention](sliding_window_attention.pdf-2-0.png)

**Caption:**  
This figure illustrates the sliding window attention mechanism in a transformer model. The ""Prefill"" region represents the initial input, while the ""Decode"" region shows the subsequent processing.

**Description:**
- The image depicts a grid representing a sequence of data, with a sliding window moving across it.
- The green area indicates the window's current position, highlighting the portion of the sequence being attended to.
- The blue area represents the decoded output based on the attended input.
- The window progressively moves across the sequence, enabling contextual understanding.
```

```
### Figure - Sliding Window Attention
![Sliding Window Attention](sliding_window_attention.pdf-2-1.png)

**Caption:**  
The figure illustrates the sliding window attention mechanism, where attention is retained over a limited window. Attention decreases as the distance from the current position increases.

**Description:**
- The image depicts a grid representing a sequence of tokens.
- The blue squares indicate attention retained across the sliding window.
- The green squares represent attention calculated within the sliding window.
- Attention decreases progressively as the distance 'k' from the current position increases, reflecting a decaying attention mechanism.
- The sliding window size is decreasing as it moves along the sequence.
curacy via reasoning. With the advent of ""thinking"" models, such as DeepSeek-R1 (DeepSeek-AI,
2025a), CoT has evolved from a prompting strategy
to an intrinsic LLM capability. However, whether
CoT has a specific impact in SWA scenarios remains uninvestigated. We explore this by comparing a thinking model with its non-thinking variant,
e.g., Qwen3-4B-Thinking and Qwen3-4B-Instruct,
to verify the effect of CoT on SWA adaptation,
which can produce more notable differences compared to simply adding CoT prompting to the same
model.


**3.4** **Fine-tuning**
","The ""Interleaving Layers"" strategy in SWAA and the hierarchical organization of functional heads discussed in the CogVision paper both acknowledge functional differentiation within transformer models, but they operate at different granularities and serve different purposes.

**Interleaving Layers (SWAA):**
This strategy involves applying full attention to a **subset of layers** in a transformer model while using Sliding Window Attention (SWA) for the remaining layers. Its primary goal is to balance the performance benefits of full attention with the computational efficiency of SWA, particularly in long-context scenarios. The context notes that ""layer functionalities differ across model families and sizes,"" suggesting that selecting specific layers (e.g., odd-numbered layers for Qwen3-4B) for full attention is crucial and model-dependent. This implies an understanding that different layers contribute differently to the model's overall function, making a selective application of full attention beneficial.

**Hierarchical Organization of Functional Heads (CogVision):**
The CogVision paper identifies eight distinct ""cognitive functions"" (e.g., Low-level Visual Reception, High-level Visual Reception, Language Information Extraction and Understanding, Math Reasoning, Inference, Decision-Making). This categorization reflects a ""natural progression from basic information processing to complex cognitive integration."" The paper uses a probing framework to identify which **attention heads** in Vision-Language Models (VLMs) are associated with these specific functions. This suggests that individual attention heads exhibit ""functional specialization"" and that the cognitive processes they perform can be organized hierarchically, from basic perception to complex reasoning.

**Comparison:**
*   Both concepts are based on the idea that not all parts of a transformer model (layers or attention heads) are uniform; they have distinct roles or importance.
*   Both implicitly or explicitly leverage this functional differentiation.

**Contrast:**
*   **Unit of Analysis:** ""Interleaving Layers"" operates at the **layer level**, deciding which entire layers receive full attention. The CogVision paper focuses on the more granular **attention head level**, identifying specific cognitive functions performed by individual heads.
*   **Purpose:** ""Interleaving Layers"" is a practical adaptation strategy aimed at optimizing model performance and efficiency for long contexts. The CogVision paper aims to deepen the understanding of the *internal cognitive organization* of VLMs and their interpretability by mapping cognitive functions to specific heads.
*   **Nature of Hierarchy/Differentiation:** While ""Interleaving Layers"" recognizes that ""layer functionalities differ,"" it doesn't explicitly define a *cognitive hierarchy* among layers in the same way CogVision categorizes cognitive functions into a ""natural progression from basic information processing to complex cognitive integration."" The SWAA strategy empirically selects layers for full attention based on performance, whereas CogVision aims to uncover a more intrinsic, cognitively-inspired hierarchy of functions performed by heads.",117,13091,13209,,3,,2026-01-02T20:16:07.228589
"Discuss the differences in how ""Attention Is All You Need"" and ""Neuronal Attention Circuit"" handle positional information in sequence modeling.",Multi-Document Aggregation,"[Document 1]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_0
Content: ## **Neuronal Attention Circuit (NAC) for Representation Learning**

**Waleed Razzaq** [1] **Izis Kankaraway** [1] **Yun-Bo Zhao** [1 2]



**Abstract**

Attention improves representation learning over
RNNs, but its discrete nature limits continuoustime (CT) modeling. We introduce Neuronal Attention Circuit (NAC), a novel, biologically plausible CT-Attention mechanism that reformulates

attention logits computation as the solution to a
linear first-order ODE with nonlinear interlinked
gates derived from repurposing _C. elegans_ Neuronal Circuit Policies (NCPs) wiring mechanism.
NAC replaces dense projections with sparse sensory gates for key-query projections and a sparse
backbone network with two heads for computing
_content-target_ and _learnable time-constant_ gates,
enabling efficient adaptive dynamics. NAC supports three attention logit computation modes: (i)
explicit Euler integration, (ii) exact closed-form
solution, and (iii) steady-state approximation. To
improve memory intensity, we implemented a
sparse Top- _K_ pairwise concatenation scheme that
selectively curates key-query interactions. We
provide rigorous theoretical guarantees, including
state stability, bounded approximation errors, and
universal approximation. Empirically, we implemented NAC in diverse domains, including irregular time-series classification, lane-keeping for
autonomous vehicles, and industrial prognostics.
We observed that NAC matches or outperforms
competing baselines in accuracy and occupies an
intermediate position in runtime and memory efficiency compared with several CT baselines.


**1. Introduction**


Learning representations of sequential data in temporal or
spatio-temporal domains is essential for capturing patterns
and enabling accurate forecasting. Discrete-time Recurrent neural networks (DT-RNNs) such as RNN (Rumelhart et al., 1985; Jordan, 1997), Long-short term memory


1Department of Automation, University of Science & Technology of China, Hefei, China [2] Institute of Artificial Intelligence,
Hefei Comprehensive National Science Center. Correspondence to:
Yun-Bo Zhao _<_ ybzhao@ustc.edu.cn _>_, Waleed Razzaq _<_ waleedrazzaq@mail.ustc.edu.cn _>_ .


_Preprint. December 12, 2025._



(LSTM) (Hochreiter & Schmidhuber, 1997), and Gated Recurrent Unit (GRU) (Cho et al., 2014) model sequential dependencies by iteratively updating hidden states to represent
or predict future elements in a sequence. While effective
for regularly sampled sequences, DT-RNNs face challenges
with irregularly sampled data because they assume uniform
time intervals. In addition, vanishing gradients can make
it difficult to capture long-term dependencies (Hochreiter,
1998).
Continuous-time RNNs (CT-RNNs) (Rubanova et al., 2019)
model hidden states as ordinary differential equations
(ODEs), allowing them to process inputs that arrive at arbitrary or irregular time intervals. Mixed-memory RNNs
(mmRNNs) (Lechner & Hasani, 2022) build on this idea
by separating memory compartments from time-continuous
states, helping maintain stable error propagation while capturing continuous-time dynamics. Liquid neural networks
(LNNs) (Hasani et al., 2021; 2022) take a biologically inspired approach by assigning variable time-constants to hidden states, improving adaptability and robustness, though
vanishing gradients can still pose challenges during training.
The attention mechanisms (Vaswani et al., 2017) mitigate
this limitation by treating all time steps equally and allowing models to focus on the most relevant observations. It
computes the similarity between queries ( _q_ ) and keys ( _k_ ),
scaling by the key dimension to keep gradients stable. MultiHead Attention (MHA) (Vaswani et al., 2017) extends this
by allowing the model to attend to different representation
subspaces in parallel. Variants like Sparse Attention (Tay
et al., 2020; Roy et al., 2021), BigBird (Zaheer et al., 2020),
and Longformer (Beltagy et al., 2020) modify the attention
pattern to reduce computational cost, particularly for long sequences, by attending only to selected positions rather than
all pairs. Even with these improvements, attention-based
methods still rely on discrete scaled dot-product operations,
limiting their ability to model continuous trajectories often
captured by CT counterparts.
Recent work has explored bridging this gap through NeuralODE (Chen et al., 2018) formulation. mTAN (Shukla &
Marlin, 2021) learns CT embeddings and uses time-based
attention to interpolate irregular observations into a fixedlength representation for downstream encoder-decoder modeling. ODEFormer (d’Ascoli et al., 2023) trains a sequenceto-sequence transformer on synthetic trajectories to directly
infer symbolic ODE systems from noisy, irregular data,
though it struggles with chaotic systems and generalization



1


**Neuronal Attention Circuit (NAC) for Representation Learning**



beyond observed conditions. Continuous-time Attention
(CTA) (Chien & Chen, 2021) embeds a continuous-time
attention mechanism within a Neural ODE, allowing attention weights and hidden states to evolve jointly over time.
Still, it remains computationally intensive and sensitive to
the accuracy of the ODE solver. ContiFormer (Chen et al.,
2023) builds a CT-transformer by pairing ODE-defined latent trajectories with a time-aware attention mechanism to
model dynamic relationships in data.
Despite these innovations, a persistent and underexplored
gap remains in developing a biologically plausible attention
mechanism that seamlessly integrates CT dynamics with the
abstraction of the brain’s connectome to handle irregular sequences without prohibitive computational costs. Building
on this, we propose a novel attention mechanism called the
_Neuronal Attention Circuit_ (NAC), in which attention logits
are computed as the solution to a first-order ODE modulated
by nonlinear, interlinked gates derived from repurposing
Neuronal Circuit Policies (NCPs) from the nervous system
of _C. elegans_ nematode (refer to Appendix A.2 for more
information). Unlike standard attention, which projects keyquery pairs through a dense layer, NAC employs a sensory
gate to transform input features and a backbone to model
nonlinear interactions, with multiple heads producing outputs structured for attention logits computation. Based on
the solutions to ODE, we define three computation modes:
(i) Exact, using the closed-form ODE solution; (ii) Euler,
approximating the solution via _explicit Euler_ integration;
and (iii) Steady, using only the steady-state solution, analogous to standard attention scores. To reduce computational
complexity, we implemented a sparse Top- _K_ pairwise concatenation algorithm that selectively curates key-query inputs. We evaluate NAC across multiple domains, including
irregularly sampled time series, autonomous vehicle lanekeeping, and Industry 4.0, comparing it to state-of-the-art
baselines. NAC consistently matches or outperforms these
models, while runtime and peak memory benchmarks place
it between CT-RNNs in terms of speed and CT-Attentions
in terms of memory requirements.


**2. Neuronal Attention Circuit (NAC)**


We propose a simple alternative formulation of the attention
logits _a_ (refer to Appendix A.1 for more information), interpreting them as the solution to a first-order linear ODE
modulated by nonlinear, interlinked gates:



from repurposing NCPs. We refer to this formulation as
the Neuronal Attention Circuit (NAC). It enables the logits
_at_ to evolve dynamically with input-dependent, variable
time constants, mirroring the adaptive temporal dynamics
found in _C. elegans_ nervous systems while improving
computational efficiency and expressiveness. Moreover, it
introduces continuous depth into the attention mechanism,
bridging discrete-layer computation with dynamic temporal
evolution.

**Motivation behind this formulation:** The proposed
formulation is loosely motivated by the input-dependent
time-constant mechanism of Liquid Neural Networks
(LNNs), a class of CT-RNNs inspired by biological nervous
systems and synaptic transmission. In this framework, the
dynamics of non-spiking neurons are described by a linear
ODE with nonlinear, interlinked gates: _ddt_ **xt** = **[x]** _τ_ **[t]** [+] **[ S][t]** _[,]_

---

[Document 2]
Source: attention_is_all_you_need_raw_chunks_2k.jsonl
Chunk ID: attention_is_all_you_need_raw_chunks_2k_0
Content: Provided proper attribution is provided, Google hereby grants permission to
reproduce the tables and figures in this paper solely for use in journalistic or
scholarly works.

## **Attention Is All You Need**



**Niki Parmar** _[∗]_
Google Research
```
nikip@google.com

```


**Ashish Vaswani** _[∗]_
Google Brain
```
avaswani@google.com

```

**Llion Jones** _[∗]_
Google Research
```
 llion@google.com

```


**Noam Shazeer** _[∗]_
Google Brain
```
noam@google.com

```


**Jakob Uszkoreit** _[∗]_
Google Research
```
usz@google.com

```


**Aidan N. Gomez** _[∗†]_
University of Toronto
```
aidan@cs.toronto.edu

```


**Łukasz Kaiser** _[∗]_
Google Brain
```
lukaszkaiser@google.com

```


**Illia Polosukhin** _[∗‡]_

```
             illia.polosukhin@gmail.com

```

**Abstract**


The dominant sequence transduction models are based on complex recurrent or
convolutional neural networks that include an encoder and a decoder. The best
performing models also connect the encoder and decoder through an attention
mechanism. We propose a new simple network architecture, the Transformer,
based solely on attention mechanisms, dispensing with recurrence and convolutions
entirely. Experiments on two machine translation tasks show these models to
be superior in quality while being more parallelizable and requiring significantly
less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including
ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,
our model establishes a new single-model state-of-the-art BLEU score of 41.8 after
training for 3.5 days on eight GPUs, a small fraction of the training costs of the
best models from the literature. We show that the Transformer generalizes well to
other tasks by applying it successfully to English constituency parsing both with
large and limited training data.


_∗_ Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started
the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and
has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head
attention and the parameter-free position representation and became the other person involved in nearly every
detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and
tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and
efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and
implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating
our research.

_†_ Work performed while at Google Brain.

_‡_ Work performed while at Google Research.


31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.


**1** **Introduction**


Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks
in particular, have been firmly established as state of the art approaches in sequence modeling and
transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous
efforts have since continued to push the boundaries of recurrent language models and encoder-decoder
architectures [38, 24, 15].


Recurrent models typically factor computation along the symbol positions of the input and output
sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden
states _ht_, as a function of the previous hidden state _ht−_ 1 and the input for position _t_ . This inherently
sequential nature precludes parallelization within training examples, which becomes critical at longer
sequence lengths, as memory constraints limit batching across examples. Recent work has achieved
significant improvements in computational efficiency through factorization tricks [21] and conditional
computation [32], while also improving model performance in case of the latter. The fundamental
constraint of sequential computation, however, remains.


Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in
the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms
are used in conjunction with a recurrent network.


In this work we propose the Transformer, a model architecture eschewing recurrence and instead
relying entirely on an attention mechanism to draw global dependencies between input and output.
The Transformer allows for significantly more parallelization and can reach a new state of the art in
translation quality after being trained for as little as twelve hours on eight P100 GPUs.


**2** **Background**


The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU

[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building
block, computing hidden representations in parallel for all input and output positions. In these models,
the number of operations required to relate signals from two arbitrary input or output positions grows
in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes
it more difficult to learn dependencies between distant positions [12]. In the Transformer this is
reduced to a constant number of operations, albeit at the cost of reduced effective resolution due
to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as
described in section 3.2.


Self-attention, sometimes called intra-attention is an attention mechanism relating different positions
of a single sequence in order to compute a representation of the sequence. Self-attention has been
used successfully in a variety of tasks including reading comprehension, abstractive summarization,
textual entailment and learning task-independent sentence representations [4, 27, 28, 22].


End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering and
language modeling tasks [34].


To the best of our knowledge, however, the Transformer is the first transduction model relying
entirely on self-attention to compute representations of its input and output without using sequencealigned RNNs or convolution. In the following sections, we will describe the Transformer, motivate
self-attention and discuss its advantages over models such as [17, 18] and [9].


**3** **Model Architecture**


Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].
Here, the encoder maps an input sequence of symbol representations ( _x_ 1 _, ..., xn_ ) to a sequence
of continuous representations **z** = ( _z_ 1 _, ..., zn_ ). Given **z**, the decoder then generates an output
sequence ( _y_ 1 _, ..., ym_ ) of symbols one element at a time. At each step the model is auto-regressive

[10], consuming the previously generated symbols as additional input when generating the next.


2


![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_is_all_you_need.pdf-2-0.png)

Figure 1: The Transformer - model architecture.


The Transformer follows this overall architecture using stacked self-attention and point-wise, fully
connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,
respectively.


**3.1** **Encoder and Decoder Stacks**

---

[Document 3]
Source: attention_is_all_you_need_raw_chunks_2k.jsonl
Chunk ID: attention_is_all_you_need_raw_chunks_2k_2
Content: [38, 2, 9].


    - The encoder contains self-attention layers. In a self-attention layer all of the keys, values
and queries come from the same place, in this case, the output of the previous layer in the
encoder. Each position in the encoder can attend to all positions in the previous layer of the
encoder.


    - Similarly, self-attention layers in the decoder allow each position in the decoder to attend to
all positions in the decoder up to and including that position. We need to prevent leftward
information flow in the decoder to preserve the auto-regressive property. We implement this
inside of scaled dot-product attention by masking out (setting to _−∞_ ) all values in the input
of the softmax which correspond to illegal connections. See Figure 2.


**3.3** **Position-wise Feed-Forward Networks**


In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully
connected feed-forward network, which is applied to each position separately and identically. This
consists of two linear transformations with a ReLU activation in between.


FFN( _x_ ) = max(0 _, xW_ 1 + _b_ 1) _W_ 2 + _b_ 2 (2)


While the linear transformations are the same across different positions, they use different parameters
from layer to layer. Another way of describing this is as two convolutions with kernel size 1.
The dimensionality of input and output is _d_ model = 512, and the inner-layer has dimensionality
_dff_ = 2048.


**3.4** **Embeddings and Softmax**


Similarly to other sequence transduction models, we use learned embeddings to convert the input
tokens and output tokens to vectors of dimension _d_ model. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In
our model, we share the same weight matrix between the two embedding layers and the pre-softmax
linear transformation, similar to [30]. In the embedding layers, we multiply those weights by _[√]_ _d_ model.


5


Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations
for different layer types. _n_ is the sequence length, _d_ is the representation dimension, _k_ is the kernel
size of convolutions and _r_ the size of the neighborhood in restricted self-attention.


Layer Type Complexity per Layer Sequential Maximum Path Length
Operations
Self-Attention _O_ ( _n_ [2] _· d_ ) _O_ (1) _O_ (1)
Recurrent _O_ ( _n · d_ [2] ) _O_ ( _n_ ) _O_ ( _n_ )
Convolutional _O_ ( _k · n · d_ [2] ) _O_ (1) _O_ ( _logk_ ( _n_ ))
Self-Attention (restricted) _O_ ( _r · n · d_ ) _O_ (1) _O_ ( _n/r_ )


**3.5** **Positional Encoding**


Since our model contains no recurrence and no convolution, in order for the model to make use of the
order of the sequence, we must inject some information about the relative or absolute position of the
tokens in the sequence. To this end, we add ""positional encodings"" to the input embeddings at the
bottoms of the encoder and decoder stacks. The positional encodings have the same dimension _d_ model
as the embeddings, so that the two can be summed. There are many choices of positional encodings,
learned and fixed [9].


In this work, we use sine and cosine functions of different frequencies:


_PE_ ( _pos,_ 2 _i_ ) = _sin_ ( _pos/_ 10000 [2] _[i/d]_ [model] )

_PE_ ( _pos,_ 2 _i_ +1) = _cos_ ( _pos/_ 10000 [2] _[i/d]_ [model] )


where _pos_ is the position and _i_ is the dimension. That is, each dimension of the positional encoding
corresponds to a sinusoid. The wavelengths form a geometric progression from 2 _π_ to 10000 _·_ 2 _π_ . We
chose this function because we hypothesized it would allow the model to easily learn to attend by
relative positions, since for any fixed offset _k_, _PEpos_ + _k_ can be represented as a linear function of
_PEpos_ .


We also experimented with using learned positional embeddings [9] instead, and found that the two
versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version
because it may allow the model to extrapolate to sequence lengths longer than the ones encountered
during training.


**4** **Why Self-Attention**


In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations
( _x_ 1 _, ..., xn_ ) to another sequence of equal length ( _z_ 1 _, ..., zn_ ), with _xi, zi ∈_ R _[d]_, such as a hidden
layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we
consider three desiderata.


One is the total computational complexity per layer. Another is the amount of computation that can
be parallelized, as measured by the minimum number of sequential operations required.


The third is the path length between long-range dependencies in the network. Learning long-range
dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the
ability to learn such dependencies is the length of the paths forward and backward signals have to
traverse in the network. The shorter these paths between any combination of positions in the input
and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare
the maximum path length between any two input and output positions in networks composed of the
different layer types.


As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially
executed operations, whereas a recurrent layer requires _O_ ( _n_ ) sequential operations. In terms of
computational complexity, self-attention layers are faster than recurrent layers when the sequence


6


length _n_ is smaller than the representation dimensionality _d_, which is most often the case with
sentence representations used by state-of-the-art models in machine translations, such as word-piece

[38] and byte-pair [31] representations. To improve computational performance for tasks involving
very long sequences, self-attention could be restricted to considering only a neighborhood of size _r_ in
the input sequence centered around the respective output position. This would increase the maximum
path length to _O_ ( _n/r_ ). We plan to investigate this approach further in future work.


A single convolutional layer with kernel width _k < n_ does not connect all pairs of input and output
positions. Doing so requires a stack of _O_ ( _n/k_ ) convolutional layers in the case of contiguous kernels,
or _O_ ( _logk_ ( _n_ )) in the case of dilated convolutions [18], increasing the length of the longest paths
between any two positions in the network. Convolutional layers are generally more expensive than
recurrent layers, by a factor of _k_ . Separable convolutions [6], however, decrease the complexity
considerably, to _O_ ( _k · n · d_ + _n · d_ [2] ). Even with _k_ = _n_, however, the complexity of a separable
convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,
the approach we take in our model.


As side benefit, self-attention could yield more interpretable models. We inspect attention distributions
from our models and present and discuss examples in the appendix. Not only do individual attention
heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic
and semantic structure of the sentences.


**5** **Training**


This section describes the training regime for our models.


**5.1** **Training Data and Batching**
","""Attention Is All You Need"" (the Transformer) and ""Neuronal Attention Circuit"" (NAC) handle positional information differently due to their underlying architectures and goals.

1.  **Attention Is All You Need (Transformer):**
    *   Since the Transformer model contains no recurrence and no convolution, it explicitly injects positional information into the sequence.
    *   It does this by adding ""positional encodings"" to the input embeddings at the bottom of the encoder and decoder stacks.
    *   These positional encodings are typically generated using fixed sine and cosine functions of different frequencies, allowing the model to learn relative positions and potentially extrapolate to longer sequences. Learned positional embeddings were also experimented with, yielding similar results.

2.  **Neuronal Attention Circuit (NAC):**
    *   NAC is designed for continuous-time (CT) modeling and handling irregularly sampled data, a limitation of discrete-time attention mechanisms.
    *   Instead of explicit positional encodings, NAC reformulates attention logits computation as the solution to a linear first-order Ordinary Differential Equation (ODE).
    *   This formulation allows the attention logits to ""evolve dynamically with input-dependent, variable time constants,"" mirroring adaptive temporal dynamics found in biological nervous systems.
    *   This dynamic evolution, driven by the ODE and modulated by nonlinear, interlinked gates, inherently captures temporal and positional dependencies in a continuous manner, introducing ""continuous depth"" into the attention mechanism.",144,7883,8028,,3,,2026-01-02T20:16:15.258311
"Contrast the approach to long-context processing in ""Sliding Window Attention Adaptation"" with the ""sparse Top-K pairwise concatenation"" used in NAC.",Multi-Document Aggregation,"[Document 1]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_7
Content: compact architecture suitable for resource-constrained devices, thereby supporting localized safety.
We utilized three benchmark datasets: (i) PRONOSTIA
(Nectoux et al., 2012), (ii) XJTU-SY (Wang et al., 2018),
and (iii) HUST (Thuan & Hong, 2023). Training is performed on PRONOSTIA, while XJTU-SY and HUST are

used to assess cross-validation. We used the Score met
ric (Nectoux et al., 2012) to assess the performance. We
evaluate generalization using _Bearing 1_ from the first operating condition of each dataset. Figure 3 visualizes the
expected degradation alongside the outputs of NAC. On the
PRONOSTIA dataset, ContiFormer performed the best with



**3.4. Run Time and Memory Experiments**


We evaluate computational requirements on fixed-length
sequences of 1024 steps, 64-dimensional features, 4 heads,
and a Batch size of 1. Each model is run for ten for
ward passes on Google Colab T4-GPU, and we report the
mean runtime with standard deviation, throughput, and peak
memory usage. NAC occupies an intermediate position
in runtime relative to several CT-RNN models, including
GRU-ODE, CfC, and LTC. In terms of memory consumption, NAC uses significantly less memory than mTAN, with
NAC-2k being the least memory-consuming among the CTAttention models. Reducing NAC sparsity from 90% to 20%
has minimal effect on memory, decreasing usage slightly



7


**Neuronal Attention Circuit (NAC) for Representation Learning**



_Table 2._ Run-Time and Memory Benchmark Results


**Run-Time** **Throughput** **Peak Memory**
**Model**
(s) (seq/s) (MB)


RNN 1 _._ 8392 _[±]_ [0] _[.]_ [1933] 0.544 0.29
CT-RNN 7 _._ 1097 _[±]_ [0] _[.]_ [3048] 0.141 0.67
LSTM 2 _._ 6241 _[±]_ [0] _[.]_ [2906] 0.381 0.42
PhasedLSTM 4 _._ 9812 _[±]_ [0] _[.]_ [272] 0.201 0.80
GRU 3 _._ 216 _[±]_ [0] _[.]_ [2566] 0.311 0.54
GRU-ODE 12 _._ 2498 _[±]_ [0] _[.]_ [0525] 0.082 0.64
mmRNN 7 _._ 5852 _[±]_ [0] _[.]_ [2785] 0.132 0.96
LTC 14 _._ 643 _[±]_ [0] _[.]_ [2445] 0.068 0.99
CfC 6 _._ 0988 _[±]_ [0] _[.]_ [2135] 0.164 0.76


Attention 0 _._ 0016 _[±]_ [0] _[.]_ [0001] 625.00 16.86
MHA 0 _._ 0041 _[±]_ [0] _[.]_ [0001] 243.90 69.05
mTAN 0 _._ 0272 _[±]_ [0] _[.]_ [0054] 36.76 790.16
ODEFormer 0 _._ 0317 _[±]_ [0] _[.]_ [0016] 31.55 67.71
CTA 8 _._ 5275 _[±]_ [0] _[.]_ [2355] 0.117 1.43
ContiFormer 0 _._ 066 _[±]_ [0] _[.]_ [0075] 15.15 67.71


NAC-2k 7 _._ 3071 _[±]_ [0] _[.]_ [1547] 0.137 44.75
NAC-32k 7 _._ 2313 _[±]_ [0] _[.]_ [219] 0.138 549.86
NAC-PW 8 _._ 5649 _[±]_ [0] _[.]_ [0203] 0.117 5042.09
NAC-FC 0 _._ 0195 ~~_[±]_~~ ~~[0]~~ ~~_[.]_~~ ~~[0002]~~ 51.28 29.92
NAC-02s 7 _._ 252 _[±]_ [0] _[.]_ [2018] 0.138 151.54
NAC-09s 7 _._ 222 _[±]_ [0] _[.]_ [176] 0.139 150.85
NAC-Exact/05s/8k 7 _._ 4101 ~~_[±]_~~ ~~[0]~~ ~~_[.]_~~ ~~[1586]~~ 0.135 151.50
NAC-Euler 7 _._ 3367 _[±]_ [0] _[.]_ [1719] 0.136 152.22
NAC-Steady 7 _._ 2942 _[±]_ [0] _[.]_ [1451] 0.137 150.86


from 151.54 MB to 150.85 MB. In constrast, decreasing
the Top- _K_ selection from _PW_ to _k_ = 2 drastically reduces
memory consumption from 5042 MB to 44.75 MB, demonstrating the flexibility of NAC.
**Interpreting the Results:** From the experiments, we observe that increasing the sparsity of the NAC layer improves
the robustness of the system and leads to higher overall accuracy. Similarly, increasing the Top- _K_ interactions enhances
accuracy too; however, the benefits diminish as memory
consumption grows. Using Exact mode, Top- _K_ =8 with 50%
sparsity achieves the best balance between accuracy and
efficiency. Steady mode is the fastest, while Euler mode
handles adaptive temporal dynamics.


**4. Discussions**



This research is part of ongoing work on biologically plausible attention mechanisms and represents a pioneering step,
with limitations to be addressed in future work.

**Architectural improvement:** Currently, NAC uses predetermined wiring (AutoNCP) requiring three inputs: number of units (sensory + interneuron + motor), output motor
neurons, and sparsity, with typically 60% of units assigned
to sensory neurons. To integrate with the attention mechanism while preserving wiring, sensory units for _NN_ sensory
are set as unitssensory = � _d_ model0 _.−_ 6 0 _._ 5 � and backbone units as



are set as unitssensory = � _d_ model0 _.−_ 6 0 _._ 5 � and backbone units as

units _backbone_ = _d_ model + � _d_ 0model _._ 6 �, where _⌈·⌉_ and _⌊·⌋_ denote



units _backbone_ = _d_ model + � _d_ 0model _._ 6 �, where _⌈·⌉_ and _⌊·⌋_ denote

the ceiling and floor functions, respectively. This results
in a larger overall architectural size and increased runtime.



Future work will support user-defined NCPs configurations
or randomized wiring to enable more efficient architectures.
**Learnable sparse Top-** _**K**_ **selection:** Sparse Top- _K_ attention can miss important context, is sensitive to _k_, and may
be harder to optimize. A further limitation is that it still
computes the full _QK_ _[⊤]_ matrix, which can dominate the
cost for very long sequences. Future work includes adaptive
or learnable Top- _K_ selection, improved key scoring, and
hardware-aware optimization to strengthen accuracy and
robustness.


**5. Conclusion**

---

[Document 2]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_8
Content: the ceiling and floor functions, respectively. This results
in a larger overall architectural size and increased runtime.



Future work will support user-defined NCPs configurations
or randomized wiring to enable more efficient architectures.
**Learnable sparse Top-** _**K**_ **selection:** Sparse Top- _K_ attention can miss important context, is sensitive to _k_, and may
be harder to optimize. A further limitation is that it still
computes the full _QK_ _[⊤]_ matrix, which can dominate the
cost for very long sequences. Future work includes adaptive
or learnable Top- _K_ selection, improved key scoring, and
hardware-aware optimization to strengthen accuracy and
robustness.


**5. Conclusion**


In this paper, we introduce the Neuronal Attention Circuit
(NAC), a biologically inspired attention mechanism that
reformulates attention logits as the solution to a first-order
ODE modulated by nonlinear, interlinked gates derived from
repurposing _C.elegans_ nematode NCPs. NAC bridges discrete attention with continuous-time dynamics, enabling
adaptive temporal processing without the limitations inherent to traditional scaled dot-product attention. Based on the
solution to ODE, we introduce three computational modes:
(i) Euler, based on _explicit Euler_ integration; (ii) Exact, providing closed-form solutions; and (iii) Steady, approximating equilibrium states. In addition, a sparse Top- _K_ pairwise
concatenation scheme is introduced to mitigate the memory intensity. Theoretically, we establish NAC’s log-state
stability, exponential error bounds, and universal approximation, thereby providing rigorous guarantees of convergence
and expressiveness. Empirical evaluations demonstrate that
NAC achieves state-of-the-art performance across diverse
tasks, including irregularly sampled time-series benchmarks,
autonomous vehicle lane-keeping, and industrial prognostics. Moreover, NAC occupies an intermediate position
between CT-RNNs and CT-Attention, offering robust temporal modeling while requiring less runtime than CT-RNNs
and less memory than CT-Attention models.


**Reproducibility Statement**


The code for reproducibility is available at
[https://github.com/itxwaleedrazzaq/](https://github.com/itxwaleedrazzaq/neuronal_attention_circuit)

[neuronal_attention_circuit](https://github.com/itxwaleedrazzaq/neuronal_attention_circuit)


**Impact Statement**


The work addresses the growing field of continuous-time
attention and pioneers a biologically plausible mechanism.
It encourages research into sparse, adaptive networks that
resemble natural wiring. From a societal perspective, it
supports more robust AI in resource-limited settings, but it
also raises ethical concerns when applied to areas such as
surveillance or autonomous systems.



8


**Neuronal Attention Circuit (NAC) for Representation Learning**



**References**


Introduction to self-driving cars. URL
[https://www.udacity.com/course/](https://www.udacity.com/course/intro-to-self-driving-cars--nd113)
[intro-to-self-driving-cars--nd113.](https://www.udacity.com/course/intro-to-self-driving-cars--nd113)


Aguiar-Conraria, L. and Soares, M. J. The continuous
wavelet transform: Moving beyond uni-and bivariate analysis. _Journal of economic surveys_, 28(2):344–375, 2014.


Beltagy, I., Peters, M. E., and Cohan, A. Longformer: The long-document transformer. _arXiv preprint_
_arXiv:2004.05150_, 2020.


Bojarski, M., Del Testa, D., Dworakowski, D., Firner, B.,
Flepp, B., Goyal, P., Jackel, L. D., Monfort, M., Muller,
U., Zhang, J., et al. End to end learning for self-driving
cars. _arXiv preprint arXiv:1604.07316_, 2016.


Brockman, G., Cheung, V., Pettersson, L., Schneider, J.,
Schulman, J., Tang, J., and Zaremba, W. Openai gym.
_arXiv preprint arXiv:1606.01540_, 2016.


Cao, Y., Li, S., Petzold, L., and Serban, R. Adjoint sensitivity analysis for differential-algebraic equations: The
adjoint dae system and its numerical solution. _SIAM_
_journal on scientific computing_, 24(3):1076–1089, 2003.


Chen, R. T., Rubanova, Y., Bettencourt, J., and Duvenaud,
D. K. Neural ordinary differential equations. _Advances_
_in neural information processing systems_, 31, 2018.


Chen, Y., Ren, K., Wang, Y., Fang, Y., Sun, W., and Li, D.
Contiformer: Continuous-time transformer for irregular
time series modeling. _Advances in Neural Information_
_Processing Systems_, 36:47143–47175, 2023.


Chien, J.-T. and Chen, Y.-H. Continuous-time attention for
sequential learning. In _Proceedings of the AAAI confer-_
_ence on artificial intelligence_, volume 35, pp. 7116–7124,
2021.


Cho, K., Van Merrienboer, B., Gulcehre, C., Bahdanau,¨
D., Bougares, F., Schwenk, H., and Bengio, Y. Learning phrase representations using rnn encoder-decoder
for statistical machine translation. _arXiv preprint_
_arXiv:1406.1078_, 2014.


d’Ascoli, S., Becker, S., Mathis, A., Schwaller, P., and
Kilbertus, N. Odeformer: Symbolic regression of dynamical systems with transformers. _arXiv preprint_
_arXiv:2310.05573_, 2023.


De Brouwer, E., Simm, J., Arany, A., and Moreau, Y.
Gru-ode-bayes: Continuous modeling of sporadicallyobserved time series. _Advances in neural information_
_processing systems_, 32, 2019.



Deng, L. The mnist database of handwritten digit images
for machine learning research [best of the web]. _IEEE_
_signal processing magazine_, 29(6):141–142, 2012.


Ding, Y., Jia, M., Miao, Q., and Huang, P. Remaining
useful life estimation using deep metric transfer learning
for kernel regression. _Reliability Engineering & System_
_Safety_, 212:107583, 2021.


Hasani, R., Lechner, M., Amini, A., Rus, D., and Grosu,
R. Liquid time-constant networks. In _Proceedings of the_
_AAAI Conference on Artificial Intelligence_, volume 35,
pp. 7657–7666, 2021.


Hasani, R., Lechner, M., Amini, A., Liebenwein, L., Ray,
A., Tschaikowski, M., Teschl, G., and Rus, D. Closed
form continuous-time neural networks. _Nature Machine_

_Intelligence_, 4(11):992–1003, 2022.


Hochreiter, S. The vanishing gradient problem during learning recurrent neural nets and problem solutions. _Interna-_
_tional Journal of Uncertainty, Fuzziness and Knowledge-_
_Based Systems_, 6(02):107–116, 1998.


Hochreiter, S. and Schmidhuber, J. Long short-term memory.
_Neural computation_, 9(8):1735–1780, 1997.


Hong, H. S. and Thuan, N. Hust bearing: a practical dataset
for ball bearing fault diagnosis. _Mendeley Data_, 3, 2023.


John, F. On integration of parabolic equations by difference methods: I. linear and quasi-linear equations for the
infinite interval. _Communications on Pure and Applied_
_Mathematics_, 5(2):155–211, 1952.

---

[Document 3]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_3
Content: Scores: _S ←_ _Q · K_ _[⊤]_

Effective Top- _K_ : _K_ eff _←_ min( _K, Tk_ )
Indices: _I_ topk _←_ top ~~k~~ ( _S, K_ eff)
Gather: _K_ selected _←_ gather( _K, I_ topk) _∈_ R _[B][×][H][×][T][q]_ _[×][K]_ [eff] _[×][D]_

Tiled: _Q_ tiled _←_ tile( _Q, K_ eff) _∈_ R _[B][×][H][×][T][q]_ _[×][K]_ [eff] _[×][D]_

Concatenate: _U_ topk _←_ [ _Q_ tiled; _K_ selected ] _∈_ R _[B][×][H][×][T][q]_ _[×][K]_ [eff] _[×]_ [2] _[D]_

**return** _U_ topk


**2.2. Designing the Neural Network**


We now outline the design of a neural network layer guided
by the preceding analysis. The process involves five steps:
(i) repurposing NCPs; (ii) input curation; (iii) construction
of the time vector ( _t_ ); (iv) computing attention logits and
weights; and (v) generating the attention output. Figure 2
provides a graphical overview of NAC.
**Repurposing NCPs:** We repurpose the NCPs framework
by converting its fixed, biologically derived wiring (see Figure 1(a)) into a flexible recurrent architecture that allows
configurable input–output mappings. Instead of enforcing
a static connectome, our approach exposes adjacency matrices as modifiable structures defining sparse input and
recurrent connections. This enables selective information

routing across neuron groups while retaining the original circuit topology. Decoupling wiring specifications from model
instantiation allows dynamic connectivity adjustments to
accommodate different input modalities without full retraining. Algorithm 1 summarizes the steps for repurposing the
NCPs wiring mechanism. Key features include group-wise
masking for neuron isolation, adaptive remapping of inputs
and outputs for task-specific adaptation, and tunable sparsity
_s_ to balance expressiveness and efficiency.
In our implementation, the sensory neuron gate ( _NN_ sensory)
projects the _q_, _k_, and _v_ representations (see Figure 1(b)).
This enables sensory neurons to maintain structured, contextaware representations rather than collapsing inputs into fully



connected layers. As a result, the network preserves locality
and modularity, which improves information routing.


_NN_ sensory = NCPCell( _G_ input = [ _Ns_ ] _, G_ output = [ _Ns_ ] _,_

_D_ = [ _Ni, Nc, Nm_ ] _, s_ )
(13)
The inter-to-motor pathways form a backbone network
( _NN_ backbone) with branches that compute _ϕ_ and _ωτ_ (see
Figure 1(c)). Instead of learning _ϕ_ and _ωτ_ independently,
this backbone allows the model to learn shared representations, enabling multiple benefits: (i) separate head layers
enable the system to capture temporal and structural dependencies independently; (ii) accelerates convergence during
training.


_NN_ backbone = NCPCell( _G_ input = [ _Ni_ ] _, G_ output = [ _Nm_ ] _,_

_D_ = [ _Ns_ ] _, s_ )
(14)
The output heads are defined as:


_ϕ_ = _σ_ ( _NN_ backbone( **u** )) (15)

_ωτ_ = softplus( _NN_ backbone( **u** )) + _ε,_ _ε >_ 0 (16)


Here, _ϕ_ serves as a _content–target gate_ head, where the
sigmoid function _σ_ ( _·_ ) determines the target signal strength.
In contrast, _ωτ_ is a strictly positive _time–constant gate_ head
that controls the rate of convergence and the steady-state
amplitude. Conceptually, this parallels recurrent gating: _ϕ_
regulates _what_ content to emphasize, while _ωτ_ governs _how_
_quickly_ and _to what extent_ it is expressed.
**Input Curation:** We experimented with different
strategies for constructing query–key inputs. Initially, we implemented full pairwise concatenation,
where queries _Q ∈_ R _[B][×][H][×][T][q][×][D]_ are combined with
all keys _K_ _∈_ R _[B][×][H][×][T][k][×][D]_ to form a joint tensor
_U ∈_ R _[B][×][H][×][T][q][×][T][k][×]_ [2] _[D]_ . While this preserved complete
feature information and enabled expressive, learnable
similarity functions, it was memory-intensive, making it impractical for longer sequences. To mitigate this, we applied
a sparse Top- _K_ optimization: for each query, we compute
pairwise scores _S_ = _Q · K_ _[⊤]_ _∈_ R _[B][×][H][×][T][q][×][T][k]_, select the
Top- _K_ eff = min( _K, Tk_ ) keys, and construct concatenated
pairs _U_ topk _∈_ R _[B][×][H][×][T][q][×][K]_ [eff] _[×]_ [2] _[D]_ . This approach preserves
the most relevant interactions while substantially reducing
memory requirements in the concatenation and subsequent
backbone processing stages, allowing the method to scale
linearly with the sequence length in those components.
However, the initial computation of _S_ remains quadratic
(see Appendix C.3). Algorithm 2 outlines the steps required
for input curation.
**Time Vector:** NAC builds on continuous-depth models
as (Hasani et al., 2022) that adapt their temporal dynamics to the task. It constructs an internal, normalized
pseudo-time vector _t_ pseudo using a sigmoidal transformation,
_t_ pseudo = _σ_ ( _ta · t_ + _tb_ ), where _ta_ and _tb_ are learnable affine



4


**Neuronal Attention Circuit (NAC) for Representation Learning**



parameters and _σ_ is the sigmoid function. For time-varying
datasets (e.g., irregularly sampled series), each time point
_t_ is derived from the sample’s timestamp, while for tasks
without meaningful timing, _t_ is set to 1. The resulting _t_ pseudo
lies in [0 _,_ 1] and provides a smooth, bounded representation
of time for modulating the network’s dynamics.
**Attention logits and weights:** Starting from Eqn. 3,
consider the trajectory of a query–key pair with initial
condition _a_ 0 = 0:



_at_ = _[ϕ]_

_ωτ_



�1 _−_ _e_ _[−][ω][τ][ t]_ [�] _,_ (17)



followed by the _softmax_ normalization to calculate attention weights. The resulting attention weights _αt_ [(] _[h]_ [)] are then
used to integrate with the value vector _v_ [(] _[h]_ [)], producing headspecific attention outputs. Finally, these outputs are concatenated and linearly projected back into the model dimension.
This formulation ensures that each head learns distinct dynamic compatibilities governed by its own parameterization
of _ϕ_ and _ωτ_, while the aggregation across heads preserves
the expressive capacity of the standard multi-head attention
mechanism.


**2.3. NAC as Universal Approximator**


We now establish the universal approximation capability of
NAC by extending the classical Universal Approximation
Theorem (UAT) (Nishijima, 2021) to the proposed mechanism. For brevity, we consider a network with a single
NAC layer processing fixed-dimensional inputs, though the
argument generalizes to sequences.
","The provided context describes the ""sparse Top-K pairwise concatenation"" approach used in NAC but does not contain any information about ""Sliding Window Attention Adaptation.""

Regarding NAC's ""sparse Top-K pairwise concatenation"":
*   **Approach:** To mitigate the memory intensity of full pairwise concatenation (which is impractical for longer sequences), NAC computes pairwise scores for each query, selects the Top-K effective keys, and then concatenates the tiled query with these selected keys.
*   **Benefits:** This method substantially reduces memory requirements in the concatenation and subsequent backbone processing stages, allowing those components to scale linearly with the sequence length.
*   **Limitations:** The initial computation of the full QKᵀ matrix still remains quadratic, which can dominate the cost for very long sequences. Additionally, sparse Top-K attention can miss important context and is sensitive to the choice of 'k'. Future work aims to address this with adaptive or learnable Top-K selection.",119,6451,6571,,3,,2026-01-02T20:16:21.830197
"How do the findings regarding ""sparse functional organization"" in VLMs (CogVision paper) align with the sparsity principles used in the NAC architecture?",Multi-Document Aggregation,"[Document 1]
Source: attention_functional_roles_raw_chunks_2k.jsonl
Chunk ID: attention_functional_roles_raw_chunks_2k_1
Content: Furthermore, we validate the functional importance of these heads by showing that their removal
degrades performance on complex tasks and leads to specific error patterns, while their enhancement improves reasoning capabilities. Our findings provide compelling evidence that these attention
heads play a critical role in multimodal reasoning. This insight not only deepens our understanding of the internal organization of VLMs but also suggests potential avenues for designing more
interpretable and cognitive-inspired multimodal AI systems.


2 COGVISION


In this section, we present our dataset Cognitive Vision (CogVision) that contains cognitive process
in multimodal reasoning. CogVision contains 1,409 main questions and 5,744 subquestions. Each


2


example comprises a main question and answer, its subquestions and subanswers and an annotation
specifying the receptive or cognitive function required for each subquestion.


2.1 COGNITIVE FUNCTIONS


To systematically capture the cognitive processes involved in complex reasoning tasks, we consider eight functions related to complex multimodal reasoning. These functions are inspired by
established frameworks in cognitive science (Anderson, 2014; Diamond, 2013), which highlight the
importance of perception, working memory, and reasoning in human cognition.


The functions related to vision include:


    - **Low-level Visual Reception** : Recognizing basic visual features such as color, shape, size,
position, motion.

    - **High-level Visual Reception** : Integrating visual information to recognize objects, patterns,
and scene structure.

    - **Visual Knowledge Recall** : Applying long-term visual knowledge related to visual concepts and their properties.


The functions related to language include:


    - **Language Information Extraction and Understanding** : locating and understanding relevant information from an external source or prior context.

    - **Language Knowledge Recall** : Accessing domain-specific knowledge without visual input.

    - **Math Reasoning** : Performing counting, arithmetic, comparison, and logic-based operations.

    - **Inference** : deriving implicit information that is not directly stated.

    - **Decision-Making** : Selecting the best outcome among alternatives based on reasoning.


This categorization reflects a natural progression from basic information processing to complex cognitive integration. Both the human brain and VLMs encompass a wide range of functional modules.
Our focus in this work is specifically on reasoning-related cognitive functions. By identifying and
organizing these eight core reasoning functions, we can more clearly examine how VLMs handle
different types of thinking steps, in a way that is both systematic and easy to interpret. Detail descriptions of each function and examples in CogVision can be found in Appendix A.1.


2.2 DATA COLLECTIONS


Based on our categorization of reasoning functions, we sampled 2000 diverse questions from existing commonly used visual reasoning benchmarks, selecting 200 examples each from ClevrMath (Lindstr¨om & Abraham, 2022), MathVision (Wang et al., 2024a), MathVista (Lu et al., 2024b),
MMMU (Yue et al., 2024), ScienceQA (Saikh et al., 2022), OKVQA (Marino et al., 2019) (400
examples), VCR (Zellers et al., 2019), VisuLogic (Xu et al., 2025) and Super-Clevr (Li et al.,
2023b) datasets. These datasets span a range of reasoning problems, including logical, mathematical, and commonsense reasoning. Using the chain-of-thought (CoT) paradigm, we prompted
GPT-4.1 (Achiam et al., 2023) to decompose each main question (with its final answer) into subquestions, each targeting a subanswer and a specific function. The prompt encourages structured,
step-by-step reasoning, ensuring that each subquestion is clear, answerable, and sequentially dependent. This process yields a set of subquestion–answer–function (subQAF) triples for each QA pair:
subQAFs = _{_ ( _qi, ai, fi_ ) _}_ _[k]_ _i_ =1 [, where each contains a subquestion] _[ q][i]_ [, its concise answer] _[ a][i]_ [, and the]
corresponding function label _fi_ . The prompt for generating subquestions are list in Appendix A.6.


2.3 DATA FILTERING AND ANNOTATION


Recent advances enable leveraging large pre-trained models for dataset construction, thanks to their
reasoning capabilities and ability to generate high-quality annotations at scale (Wang et al., 2024b).
While our dataset is automatically generated using a VLM to minimize manual effort, we employ a
rigorous two-stage human verification pipeline to ensure data quality and mitigate hallucination. In
the first stage, three expert annotator independently evaluate whether each subquestion is logically
structured and consistent with natural human reasoning. QA pairs with incoherent or inconsistent


3


decompositions are moved out. In the second stage, annotators verify and, if necessary, relabel the
cognitive function associated with each subquestion to ensure accurate alignment with the intended
mental process. Subanswers are further cross-checked with GPT-o3 (OpenAI, 2024) and adjudicated
by humans in cases of disagreement (details in Appendix A.5). This multi-step verification ensures
that each retained subQAF triplet represents a coherent, interpretable reasoning step grounded in
core cognitive functions. After this refinement, our final dataset comprises 1,409 main QA pairs
and 5,744 validated subQAF triplets. The detailed statistics of the dataset including the number of
triplet in training and testing set and distributions of each function can be found in Appendix A.3.
We analyze in-group subquestion diversity, and the results in the Appendix A.2 show that the eight
functions exhibit broad and overlapping distributions in phrasing patterns and token lengths, indicating no systematic surface-form differences.


3 DETECTION OF COGNITIVE FUNCTIONS


Using the CogVision dataset, we adopt a probing-based framework (Alain & Bengio, 2016; Belinkov, 2022; Tenney et al., 2019) to identify which attention heads in VLMs are associated with
specific functions in reasoning process. Specifically, for each functional annotated subquestion, we
extract head activations (see Subsection 3.1), train classifiers and compute accuracies to identify
contributing heads (see Subsection 3.2). Unlike prior work focusing on a single label, our formulation captures many-to-many relationships between heads and cognitive functions, enabling a more
nuanced analysis of functional specialization and overlap within the model.


3.1 HEAD FEATURE EXTRACTION


Given a large VLM _M_, we generate an answer _a_ _[M]_ _i_ for each subquestion _qi_ derived from a main
question _Qi_ . To support coherent multi-step reasoning, we include preceding subquestions and
their answers as contextual input, emulating the incremental reasoning process observed in human
cognition.


During inference, each input token is first mapped into an embedding and then propagated through
the transformer’s layers. At each layer, attention and feedforward operations update the residual
stream, which is ultimately decoded into token predictions. For each generated token _i_, we extract
attention head outputs _Xi_ = _{x_ _[m]_ _l_ _| l ∈_ 1 _, . . ., Nl, m ∈_ 1 _, . . ., Nh}_ across all layers, where _x_ _[m]_ _l_
denotes the value vector from the _m_ -th head in layer _l_ projected into the residual stream, with _Nh_
the number of heads per layer and _Nl_ the total number of layers.


Let _Nt_ denote the number of tokens in the generated answer _a_ _[M]_ _i_ [. To isolate semantically informative]
content relevant to reasoning, we select the top- _k_ most important tokens, determined by prompting
Qwen3-30B LLM (Yang et al., 2025), yielding an index set _Ik_ with _|Ik|_ = _k_ . For each index
_j ∈Ik_, we extract the corresponding attention head activations _Xj_, and compute the averaged
activation feature for the _m_ -th head in layer _l_ as ¯ _x_ _[m]_ _l_ = _k_ [1] � _j ∈Ikxml_ [. This results in a full set of]

---

[Document 2]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_3
Content: Scores: _S ←_ _Q · K_ _[⊤]_

Effective Top- _K_ : _K_ eff _←_ min( _K, Tk_ )
Indices: _I_ topk _←_ top ~~k~~ ( _S, K_ eff)
Gather: _K_ selected _←_ gather( _K, I_ topk) _∈_ R _[B][×][H][×][T][q]_ _[×][K]_ [eff] _[×][D]_

Tiled: _Q_ tiled _←_ tile( _Q, K_ eff) _∈_ R _[B][×][H][×][T][q]_ _[×][K]_ [eff] _[×][D]_

Concatenate: _U_ topk _←_ [ _Q_ tiled; _K_ selected ] _∈_ R _[B][×][H][×][T][q]_ _[×][K]_ [eff] _[×]_ [2] _[D]_

**return** _U_ topk


**2.2. Designing the Neural Network**


We now outline the design of a neural network layer guided
by the preceding analysis. The process involves five steps:
(i) repurposing NCPs; (ii) input curation; (iii) construction
of the time vector ( _t_ ); (iv) computing attention logits and
weights; and (v) generating the attention output. Figure 2
provides a graphical overview of NAC.
**Repurposing NCPs:** We repurpose the NCPs framework
by converting its fixed, biologically derived wiring (see Figure 1(a)) into a flexible recurrent architecture that allows
configurable input–output mappings. Instead of enforcing
a static connectome, our approach exposes adjacency matrices as modifiable structures defining sparse input and
recurrent connections. This enables selective information

routing across neuron groups while retaining the original circuit topology. Decoupling wiring specifications from model
instantiation allows dynamic connectivity adjustments to
accommodate different input modalities without full retraining. Algorithm 1 summarizes the steps for repurposing the
NCPs wiring mechanism. Key features include group-wise
masking for neuron isolation, adaptive remapping of inputs
and outputs for task-specific adaptation, and tunable sparsity
_s_ to balance expressiveness and efficiency.
In our implementation, the sensory neuron gate ( _NN_ sensory)
projects the _q_, _k_, and _v_ representations (see Figure 1(b)).
This enables sensory neurons to maintain structured, contextaware representations rather than collapsing inputs into fully



connected layers. As a result, the network preserves locality
and modularity, which improves information routing.


_NN_ sensory = NCPCell( _G_ input = [ _Ns_ ] _, G_ output = [ _Ns_ ] _,_

_D_ = [ _Ni, Nc, Nm_ ] _, s_ )
(13)
The inter-to-motor pathways form a backbone network
( _NN_ backbone) with branches that compute _ϕ_ and _ωτ_ (see
Figure 1(c)). Instead of learning _ϕ_ and _ωτ_ independently,
this backbone allows the model to learn shared representations, enabling multiple benefits: (i) separate head layers
enable the system to capture temporal and structural dependencies independently; (ii) accelerates convergence during
training.


_NN_ backbone = NCPCell( _G_ input = [ _Ni_ ] _, G_ output = [ _Nm_ ] _,_

_D_ = [ _Ns_ ] _, s_ )
(14)
The output heads are defined as:


_ϕ_ = _σ_ ( _NN_ backbone( **u** )) (15)

_ωτ_ = softplus( _NN_ backbone( **u** )) + _ε,_ _ε >_ 0 (16)


Here, _ϕ_ serves as a _content–target gate_ head, where the
sigmoid function _σ_ ( _·_ ) determines the target signal strength.
In contrast, _ωτ_ is a strictly positive _time–constant gate_ head
that controls the rate of convergence and the steady-state
amplitude. Conceptually, this parallels recurrent gating: _ϕ_
regulates _what_ content to emphasize, while _ωτ_ governs _how_
_quickly_ and _to what extent_ it is expressed.
**Input Curation:** We experimented with different
strategies for constructing query–key inputs. Initially, we implemented full pairwise concatenation,
where queries _Q ∈_ R _[B][×][H][×][T][q][×][D]_ are combined with
all keys _K_ _∈_ R _[B][×][H][×][T][k][×][D]_ to form a joint tensor
_U ∈_ R _[B][×][H][×][T][q][×][T][k][×]_ [2] _[D]_ . While this preserved complete
feature information and enabled expressive, learnable
similarity functions, it was memory-intensive, making it impractical for longer sequences. To mitigate this, we applied
a sparse Top- _K_ optimization: for each query, we compute
pairwise scores _S_ = _Q · K_ _[⊤]_ _∈_ R _[B][×][H][×][T][q][×][T][k]_, select the
Top- _K_ eff = min( _K, Tk_ ) keys, and construct concatenated
pairs _U_ topk _∈_ R _[B][×][H][×][T][q][×][K]_ [eff] _[×]_ [2] _[D]_ . This approach preserves
the most relevant interactions while substantially reducing
memory requirements in the concatenation and subsequent
backbone processing stages, allowing the method to scale
linearly with the sequence length in those components.
However, the initial computation of _S_ remains quadratic
(see Appendix C.3). Algorithm 2 outlines the steps required
for input curation.
**Time Vector:** NAC builds on continuous-depth models
as (Hasani et al., 2022) that adapt their temporal dynamics to the task. It constructs an internal, normalized
pseudo-time vector _t_ pseudo using a sigmoidal transformation,
_t_ pseudo = _σ_ ( _ta · t_ + _tb_ ), where _ta_ and _tb_ are learnable affine



4


**Neuronal Attention Circuit (NAC) for Representation Learning**



parameters and _σ_ is the sigmoid function. For time-varying
datasets (e.g., irregularly sampled series), each time point
_t_ is derived from the sample’s timestamp, while for tasks
without meaningful timing, _t_ is set to 1. The resulting _t_ pseudo
lies in [0 _,_ 1] and provides a smooth, bounded representation
of time for modulating the network’s dynamics.
**Attention logits and weights:** Starting from Eqn. 3,
consider the trajectory of a query–key pair with initial
condition _a_ 0 = 0:



_at_ = _[ϕ]_

_ωτ_



�1 _−_ _e_ _[−][ω][τ][ t]_ [�] _,_ (17)



followed by the _softmax_ normalization to calculate attention weights. The resulting attention weights _αt_ [(] _[h]_ [)] are then
used to integrate with the value vector _v_ [(] _[h]_ [)], producing headspecific attention outputs. Finally, these outputs are concatenated and linearly projected back into the model dimension.
This formulation ensures that each head learns distinct dynamic compatibilities governed by its own parameterization
of _ϕ_ and _ωτ_, while the aggregation across heads preserves
the expressive capacity of the standard multi-head attention
mechanism.


**2.3. NAC as Universal Approximator**


We now establish the universal approximation capability of
NAC by extending the classical Universal Approximation
Theorem (UAT) (Nishijima, 2021) to the proposed mechanism. For brevity, we consider a network with a single
NAC layer processing fixed-dimensional inputs, though the
argument generalizes to sequences.

---

[Document 3]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_14
Content: **D.2. Ablations Details**


The brief descriptions of variants and ablation are also divided into four subcategories:
**Top-** _**K**_ **Ablations:** _NAC-2k_ uses Top- _K_ =2 to compute the
logits and _NAC-32k_ uses Top- _K_ =32. All variants use the
exact computation mode with 50% sparsity.
**Sparsity Ablations:** _NAC-02s_ uses 20% sparsity to compute the logits and _NAC-09s_ uses 90%. _NAC-PW_ employs
full pairwise (non-sparse) concatenation for input curation.
_NAC-FC_ replaces the sparse NCP gating mechanism with
a simple fully connected layer. All variants use the exact
computation mode with Top- _K_ =8.
**Modes variants:** _NAC-Euler_ computes attention logits using the explicit Euler integration method. _NAC-Steady_ derives attention logits from the steady-state solution of the
exact formulation. _NAC-Exact/05s/8k_ computes attention
logits using the closed-form exact solution. It also overlaps
with other ablations, so we combined it into a single one.
All modes use Top- _K_ =8, 50% sparsity and _δt_ =1.0. The
sensitivity of NAC to _δt_ is visualized in Figure 4


_Figure 4._ Effect of _δt_ on output of NAC.



14



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/neuronal_attention_circuits.pdf-13-0.png)
**Neuronal Attention Circuit (NAC) for Representation Learning**



**D.3. Experimental Details**


D.3.1. EVENT-BASED MNIST


**Dataset Explanation and Curation:** The MNIST dataset,
introduced by (Deng, 2012), is a widely used benchmark for
computer vision and image classification tasks. It consists
of 70,000 grayscale images of handwritten digits (0–9), each
of size 28 _×_ 28 pixels, split into 60,000 training and 10,000
testing samples.
**Preprocessing:** We follow the preprocessing pipeline described in (Lechner & Hasani, 2022), which proceeds as
follows. First, a threshold is applied to convert the 8-bit
pixel values into binary values, with 128 as the threshold
on a scale from 0 (minimum intensity) to 255 (maximum
intensity). Second, each 28 _×_ 28 image is reshaped into a
one-dimensional time series of length 784. Third, the binary
time series is encoded in an event-based format, eliminating
consecutive occurrences of the same value; for example, the
sequence [1 _,_ 1 _,_ 1 _,_ 1] is transformed into (1 _, t_ = 4). This encoding introduces a temporal dimension and compresses the
sequences from 784 to an average of 53 time steps. Finally,
to facilitate efficient batching and training, each sequence
is padded to a fixed length of 256, and the time dimension
is normalized such that each symbol corresponds to one
unit of time. The resulting dataset defines a per-sequence
classification problem on irregularly sampled time series.
**Neural Network Architecture:** We develop an end-to-end
hybrid neural network by combining compact convolutional
layers with NAC or counterparts baselines for fair comparison. Detailed hyperparameters and architectural specifications are provided in Table 4.


D.3.2. PERSON ACTIVITY RECOGNITION (PAR)


**Dataset Explanation and Curation:** We used the
Localized Person Activity Recognition dataset provided by
UC Irvine (Vidulin et al., 2010). The dataset comprises
25 recordings of human participants performing different
physical activities. The eleven possible activities are
“walking,” “falling,” “lying down,” “lying,” “sitting down,”
“sitting,” “standing up from lying,” “on all fours,” “sitting
on the ground,” “standing up from sitting,” and “standing
up from sitting on the ground.” The objective of this
experiment is to recognize the participant’s activity from
inertial sensors, formulating the task as a per-time-step
classification problem. The input data consist of sensor
readings from four inertial measurement units placed on
participants’ arms and feet. While the sensors are sampled
at a fixed interval of 211 ms, recordings exhibit different
phase shifts and are thus treated as irregularly sampled time
series.

**Preprocessing:** We first separated each participant’s
recordings based on sequence identity and calculated
elapsed time in seconds using the sampling period. To



mitigate class imbalance, we removed excess samples from
overrepresented classes to match the size of the smallest
class. Subsequently, the data were normalized using a
standard scaler. Finally, the dataset was split into a 90:10
ratio for training and testing.
**Neural Network Architecture:** Following the approach in
Section D.3.1, we developed an end-to-end hybrid neural
network combining convolutional heads with NAC or other
baselines. Hyperparameter details are summarized in Table
4.


D.3.3. AUTONOMOUS VEHICLE


**Dataset Explanation and Curation:** We followed the data
collection methodology described in (Razzaq & Hongwei,
2023). For OpenAI-CarRacing, a PPO-trained agent (5M
timesteps) was used to record 20 episodes, yielding approximately 48,174 RGB images of size 92 _×_ 92 _×_ 3 with
corresponding action labels across five discrete actions (noact, move left, forward, move right, stop). The dataset was
split with 10% reserved for testing and the remaining 90%
for training. For the Udacity simulator, we manually controlled the vehicle for 50 minutes, producing 15647 RGB
images of size 320 _×_ 160 _×_ 3, captured from three camera
streams (left, center, right) along with their corresponding
continuous steering values. This dataset was split into 20%
testing and 80% training.
**Preprocessing:** No preprocessing was applied to the
OpenAI-CarRacing dataset. For the Udacity simulator, we
followed the preprocessing steps in (Shibuya, 2017). Each
image was first cropped to remove irrelevant regions and
resized to 66 _×_ 120 _×_ 3. Images were then converted from
RGB to YUV color space to match the network input. To improve robustness, data augmentation techniques, including
random flips, translations, shadow overlays, and brightness
variations, were applied to simulate lateral shifts and diverse
lighting conditions.
**Neural Network Architecture:** For OpenAI-CarRacing,
we modified the neural network architecture proposed in
(Razzaq & Hongwei, 2023), which combines compact CNN
layers for spatial feature extraction with LNNs to capture
temporal dynamics. In our implementation, the LNN layers
were replaced with NAC and its comparable alternatives
for fair evaluation. Full hyperparameter configurations are
provided in Table 4. For the Udacity simulator, we modified
the network proposed in (Bojarski et al., 2016) by replacing
three latent MLP layers with NAC and its counterparts. Full
hyperparameters for this configuration are summarized in
Table 4.

**Saliency Maps:** A saliency map visualizes the regions of
the input that a model attends to when making decisions.
Figure 5 shows the saliency maps for the OpenAI CarRacing environment. We observe that only NAC (Steady, Euler,



15


**Neuronal Attention Circuit (NAC) for Representation Learning**


_Figure 5._ Saliency maps for OpenAI CarRacing


_Figure 6._ Saliency maps for Udacity Simulator



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/neuronal_attention_circuits.pdf-15-0.png)

![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/neuronal_attention_circuits.pdf-15-1.png)
","The CogVision paper's findings regarding ""sparse functional organization"" in VLMs align with the sparsity principles in the NAC architecture through the concept of specialized, non-uniform engagement of components for specific tasks or information processing.

1.  **Sparse Functional Organization in VLMs (CogVision):** The CogVision paper identifies that specific attention heads in VLMs are ""associated with specific functions in reasoning process."" It highlights ""functional specialization and overlap"" within the model, where certain heads contribute to particular cognitive functions (e.g., Low-level Visual Reception, Language Knowledge Recall). This suggests a form of sparse functional organization where not all attention heads are equally involved in all cognitive processes, but rather some are specialized for certain roles.

2.  **Sparsity Principles in NAC Architecture:** The NAC architecture explicitly incorporates sparsity as a design principle.
    *   It repurposes NCPs by converting fixed wiring into a flexible recurrent architecture with ""adjacency matrices as modifiable structures defining sparse input and recurrent connections."" This enables ""selective information routing across neuron groups"" and includes ""tunable sparsity _s_.""
    *   For input curation, NAC uses a ""sparse Top-K optimization"" where it selects only the ""Top-Keff"" most relevant keys for each query, preserving ""the most relevant interactions"" while reducing memory requirements.
    *   It also employs a ""sparse NCP gating mechanism"" as opposed to fully connected layers.

**Alignment:** Both concepts leverage the idea that not all components need to be fully connected or equally active for all operations. The CogVision paper *observes* a specialized (sparse) functional organization in existing VLMs, where specific attention heads are functionally specialized. The NAC architecture, conversely, *designs* its network with explicit sparsity (tunable sparsity, sparse Top-K optimization, sparse gating) to achieve efficiency, selective information routing, and potentially foster such functional specialization by design.",124,8173,8298,,3,,2026-01-02T20:16:30.130722
"Compare the ""Keep First k Tokens"" method in SWAA with the ""positional encoding"" mechanism in the original Transformer paper in terms of preserving global context.",Multi-Document Aggregation,"[Document 1]
Source: sliding_window_attention_annotated_chunks_2k.jsonl
Chunk ID: sliding_window_attention_annotated_chunks_2k_1
Content: LightTransfer (Zhang et al., 2024) is a promising
attempt at adapting existing models to SWA without pretraining, which has the same motivation as
ours. But it may generalize poorly across model
families (see Appendix G).


**2.2** **Linear Attention**


An alternative approach involves reformulating
the attention mechanism entirely to achieve linear, _O_ ( _N_ ), complexity. This includes methods
such as RNN-like linear attention transformers

(Katharopoulos et al., 2020; Peng et al., 2023;
Sun et al., 2023) and structured state-space models
(SSMs) like Mamba (Gu and Dao, 2023). Many
works such as Jamba and Nemotron-Flash(Lieber
et al., 2024; Linsong Chu et al., 2024; Team et al.,
2025; Fu et al., 2025a) interleave linear attention
layers with traditional attention layers to create hybrid model structures. While promising, they represent a fundamental architectural departure from the
standard Transformer, and their performance is generally weaker than traditional Transformer-based

LLMs.



2


**3** **Candidate Methods for SWA**

**Adaptation**


As established, a naive application of SWA leads
to severe long-context performance degradation.
Therefore, we investigate five methods that can potentially facilitate SWA adaptation from distinct
perspectives. However, every method except finetuning has some drawbacks to LLM inference efficiency, as discussed in Appendix A. Therefore,
although these methods are not mutually exclusive,
we should not indiscriminately adopt all of them.
Instead, we must evaluate various combinations to
identify the optimal trade-off between performance
and efficiency.


**3.1** **Full Attention Decode**


This method applies SWA **only** to the prefilling
stage. During the decoding (auto-regressive generation) stage, each token still employs full attention,
allowing access to all previous tokens in the context. The resulting attention mask is depicted in
Figure 1a.
This novel approach, first proposed in our study,
is inspired by human reading comprehension: humans typically scan a passage casually (prefilling)
before thinking deeply to formulate an answer (decoding) for a specific problem. We term this strategy ""reading casually, thinking carefully."" In our
design, the SWA-constrained prefilling stage corresponds to casual reading, while the full-attention
decoding stage enables careful thinking. This analogy also suggests that Chain-of-Thought (CoT)
during decoding may be particularly beneficial, as
extended generation (i.e. ""thinking"") could compensate for the insufficient contextual information
gathered during the prefilling stage.


(a) FA Decode (b) Keep First


Figure 1: (a) Attention mask for FA Decode. SWA is
used for prompt tokens (prefill), and full attention is
used for generated tokens (decode). (b) Attention mask
for SWA combined with Keep First _k_ Tokens.



**3.1.1** **Keep First k Tokens**

Xiao et al. demonstrate that models pretrained with
full attention allocate a disproportionate amount
of attention to the initial tokens (""attention sink""),
and removing the visibility of these tokens causes
performance collapse. Their solution, streaming
attention, involves permanently retaining the attention to these ""sink"" tokens while using SWA, which
successfully maintains the stability of the attention
distribution and the model’s output. Our method
is basically the same: as shown in Figure 1b, any
subsequent token can attend to its local window
**and** the initial _k_ tokens.

Notably, however, our method extends beyond
its original version. Streaming attention operates
only at the KV cache level; specifically, the KV
cache of the sink tokens is retained during decoding, while the prefilling stage is not modified or accelerated at all. In contrast, we directly customize
the Flash-Attention-2 (Dao, 2024) kernel to implement such attention mask, thereby accelerating
prefilling via SWA as well, and eliminating the
need to modify KV cache.


**3.2** **Interleaving Layers**


This method retains full attention on a subset of

layers while applying SWA to the remainder, providing a simple hybrid mechanism to balance the
performance of full attention with the efficiency
of pure SWA. A common strategy involves designating one in every _n_ layers to use full attention
(e.g., layers 0, 2, 4, . . . retain full attention, while all
others use SWA). For example, Gemma-2 (Team,
2024a) uses SWA only for layers [1, 3, 5, . . . ], and
Gemma-3 (Team, 2025a) uses SWA only for layers

[5, 11, 17, ...].
However, for an FA-pretrained model, layers
may exhibit distinct behaviors, suggesting it may
not be optimal to simply assign SWA to layers [1,
3, 5, ...]. Instead, it might be preferable to use
statistical features to select ""lazy"" (mostly focusing
on just recent tokens) layers, as adopted by LightTransfer (Zhang et al., 2024). However, we find
that LightTransfer is not consistently superior in
practice (see Appendix G). Therefore, for simplicity, we still choose the simplest fixed-interval layer
selections in our experiments, such as [0, 2, 4, . . . ]
and [1, 3, 5, ...].


**3.3** **Chain-of-Thought**


Chain-of-Thought (CoT) (Wei et al., 2022) is a
widely used technique for improving model ac


3



```
### Figure - Sliding Window Attention
![Sliding Window Attention](sliding_window_attention.pdf-2-0.png)

**Caption:**  
This figure illustrates the sliding window attention mechanism in a transformer model. The ""Prefill"" region represents the initial input, while the ""Decode"" region shows the subsequent processing.

**Description:**
- The image depicts a grid representing a sequence of data, with a sliding window moving across it.
- The green area indicates the window's current position, highlighting the portion of the sequence being attended to.
- The blue area represents the decoded output based on the attended input.
- The window progressively moves across the sequence, enabling contextual understanding.
```

```
### Figure - Sliding Window Attention
![Sliding Window Attention](sliding_window_attention.pdf-2-1.png)

**Caption:**  
The figure illustrates the sliding window attention mechanism, where attention is retained over a limited window. Attention decreases as the distance from the current position increases.

**Description:**
- The image depicts a grid representing a sequence of tokens.
- The blue squares indicate attention retained across the sliding window.
- The green squares represent attention calculated within the sliding window.
- Attention decreases progressively as the distance 'k' from the current position increases, reflecting a decaying attention mechanism.
- The sliding window size is decreasing as it moves along the sequence.
curacy via reasoning. With the advent of ""thinking"" models, such as DeepSeek-R1 (DeepSeek-AI,
2025a), CoT has evolved from a prompting strategy
to an intrinsic LLM capability. However, whether
CoT has a specific impact in SWA scenarios remains uninvestigated. We explore this by comparing a thinking model with its non-thinking variant,
e.g., Qwen3-4B-Thinking and Qwen3-4B-Instruct,
to verify the effect of CoT on SWA adaptation,
which can produce more notable differences compared to simply adding CoT prompting to the same
model.


**3.4** **Fine-tuning**

---

[Document 2]
Source: sliding_window_attention_annotated_chunks_2k.jsonl
Chunk ID: sliding_window_attention_annotated_chunks_2k_0
Content: ## **Sliding Window Attention Adaptation**

Yijiong Yu [a], Jiale Liu [b], Qingyun Wu [b], Huazheng Wang [a], and Ji Pei [c]


aOregon State University, {yuyiji, huazheng.wang}@oregonstate.edu
bPenn State University, {jiale.liu, qingyun.wu}@psu.edu
cDeepSolution, research@deepsolution.chat



**Abstract**


The self-attention mechanism in Transformer
based Large Language Models (LLMs) scales
quadratically with input length, making longcontext inference expensive. Sliding window attention (SWA) reduces this cost to linear complexity, but naively enabling complete
SWA at inference-time for models pretrained
with full attention (FA) causes severe longcontext performance degradation due to training–inference mismatch. This makes us wonder: _Can FA-pretrained LLMs be well adapted_
_to SWA without pretraining?_ We investigate
this by proposing Sliding Window Attention
Adaptation (SWAA), a set of practical recipes
that combine five methods for better adaptation: (i) applying SWA only during prefilling;
(ii) preserving “sink” tokens; (iii) interleaving
FA/SWA layers; (iv) chain-of-thought (CoT);
and (v) fine-tuning. Our experiments show that
SWA adaptation is feasible while non-trivial:
no single method suffices, yet specific synergistic combinations effectively recover the original
long-context performance. We further analyze
the performance-efficiency trade-offs of different SWAA configurations and provide recommended recipes for diverse scenarios. Our code
[is available at github.](https://github.com/yuyijiong/sliding-window-attention-adaptation)


**1** **Introduction**


Transformer-based Large Language Models
(LLMs) (Vaswani et al., 2017) demonstrate
remarkable capabilities, but their self-attention
scales quadratically with the input sequence length,
making long context processing inefficient. Sliding
Window Attention (SWA), the most straightforward and widely adopted sparse attention
pattern, which restricts each token’s attention to a
fixed-size local window, reduces the computational
complexity to linearity, along with some other
benefits (see Appendix A).
To apply SWA to LLMs, typical solutions involve training a model with SWA from scratch, but



are prohibitively costly and cannot match the performance of state-of-the-art full-causal-attention

models like Qwen3 (Team, 2025b), mainly due
to the inability to reproduce pretraining data.
Training-free methods like streaming attention
(Xiao et al., 2024) can stabilize LLM outputs by retaining “sink tokens” while applying SWA, which
greatly improve efficiency but inevitably suffer
from severe long-context performance degradation
possibly due to the inaccessibility of distant tokens’
information (Xiao, 2025). This motivates a critical,
unexplored question: _Can a full-attention model_
_be adapted to sliding window attention at low cost_
_while maintaining long-context performance?_
We answer Yes to this question by proposing
Sliding Window Attention Adaptation(SWAA), a
set of recipes for adapting FA-pretrained models to
SWA, which requires neither costly pretraining nor
modifications to the standard Transformer architecture. Specifically, it systematically combines five
practical and composable methods:


1. **Full Attention (FA) Decode** : applying SWA
only during the prefilling stage while switching back to full attention for decoding.


2. **Keep First** _k_ **Tokens** : explicitly preserving
attention to the first _k_ “sink” tokens.


3. **Interleaving FA/SWA layers** : mix fullattention and SWA layers (e.g., assigning
SWA to half layers).


4. **Chain-of-Thought (CoT)** : enforcing an explicit ""thinking"" process during decoding.


5. **Fine-tuning with SWA** : lightweight SWAaware supervised fine-tuning on long-context
data.


Among these, FA Decode is a novel method we
introduce. Keep First _k_ Tokens and FA/SWA Interleaving have been proven effective in prior work



1


(Xiao et al., 2024; Team, 2024a; Zhang et al., 2024),
while CoT and fine-tuning are common LLM techniques. However, how these methods should be
combined to be actually effective for SWA adaptation remains unexplored.
Therefore, in our experiments, we evaluate
SWAA on Qwen3 (Team, 2025b) and Llama3.1
(Team, 2024b) across several long-context benchmarks, measuring both performance and efficiency
under a wide range of SWAA recipes. First, we
find that each method makes a distinct contribution,
but no single ingredient suffices to make SWA competitive with full attention. Second, we show that
specific synergistic combinations of methods can
recover a large fraction of the original long-context
performance. Third, we analyze the performanceefficiency trade-offs of different SWAA recipes and
identify some recommended configurations suitable for different deployment scenarios.
Rather than proposing a single globally optimal
configuration, we view SWAA as a flexible toolkit
of practical recipes: practitioners can select SWAA
recipes that match their accuracy and efficiency
constraints, or compose their own SWA adaptation
strategies by combining the available ingredients.
Our key contributions are:


1. We perform the first systematic study on adapting FA-pretrained LLMs to SWA without pretraining, revealing novel insights about how
SWA impacts LLMs and providing a foundation for future research in efficient sparse
attention.


2. We propose SWAA, a set of practical
SWA adaptation recipes that offer a robust
performance-efficiency balance for various
use cases, accelerating LLM inference from
the bottom level.


3. We implement our methods with FlashAttention (Dao, 2024) and vLLM (Kwon
et al., 2023), making it plug-and-play and userfriendly for practical deployment.


**2** **Related Works**


The _O_ ( _N_ [2] ) complexity of self-attention in Transformers (Vaswani et al., 2017) has spurred a wide
field of research about more efficient language
model architectures. Among the two most popular technological routes are sparse attention and
linear attention.



**2.1** **Sparse Attention**


Our work falls in this category. Sliding Window
Attention (SWA) represents the most basic form
of local sparse attention, yet its performance is inherently limited. Therefore, model architectures
such as Longformer (Beltagy et al., 2020), BigBird
(Zaheer et al., 2020), and RATTENTION (Wang
et al., 2025) combine local SWA on most tokens
with special global attention on specific tokens to
create a more powerful, albeit still sparse, pattern.
Popular LLMs like Gemma2 (Team, 2024a) adopt
SWA in half of their layers to balance the efficiency
of SWA and peformance of FA. Sliding Window
Attention Training (SWAT) (Fu et al., 2025b) introduces architectural changes, such as sigmoid
activation and balanced position embeddings, to
stabilize SWA performance. More advanced methods like Deepseek-sparse-attention (Yuan et al.,
2025; DeepSeek-AI, 2025b), although achieving
excellent quality, involve more complicated implementation and optimization due to semantic-aware
attention operations (e.g., selecting the most important tokens based on attention weights). Regardless,
almost all of the above methods require pretraining with a specific sparse pattern, which is costly
and fails to leverage the advantages of existing pretrained models.


LightTransfer (Zhang et al., 2024) is a promising
attempt at adapting existing models to SWA without pretraining, which has the same motivation as
ours. But it may generalize poorly across model
families (see Appendix G).


**2.2** **Linear Attention**


An alternative approach involves reformulating
the attention mechanism entirely to achieve linear, _O_ ( _N_ ), complexity. This includes methods
such as RNN-like linear attention transformers

---

[Document 3]
Source: sliding_window_attention_annotated_chunks_2k.jsonl
Chunk ID: sliding_window_attention_annotated_chunks_2k_3
Content: **Combinations exhibit strong synergy.** Recipes
that combine multiple methods deliver large gains:


  - **FA Decode + Keep First** _k_ substantially improves over naive SWA (rows 7–9), recovering roughly half to two-thirds of the gap on
the thinking model as _k_ increases. However,
increasing _k_ from 100 to 1000 yields almost
no improvement, indicating that _k_ does not
need to be exceedingly large.


  - **Interleaving Layers + FA Decode** is
markedly stronger (row 13), recovering most
of the gap for the thinking model.


  - **FA Decode + Interleaving Layers + Keep**
**First** _k_ perform best (rows 18). The thinking
model recovers close to 90% of the FA gap
even at 2k window.


**CoT synergizes with FA Decode.** Under recipes
that include FA Decode, the thinking model consistently benefits more than the non-thinking model
(rows 13 and 18), suggesting that CoT synergizes
with FA Decode: preserving global attention at
decoding time enables longer reasoning traces to
capitalize on context processed by SWA, confirming our hypothesis in Section 3.1.


**Sliding window size affects, but is not the deci-**
**sive role.** With FA Decode + Keep First _k_, accuracy improves as the window increases (rows 7,
14, 15), though benefits are modest until 8k. When
added with interleaving FA layers, moving from a
2k to a 4k window is enough to close the remaining
gap on the thinking model (row 22 matches the FA
baseline), indicating that FA Decode and Interleaving are the dominant drivers, and larger windows
mainly smooth residual error.



5


Table 1: Qwen3-4B-Thinking and Qwen3-4B-Instruct results on LongMemEval without SFT


**No.** **window size** **FA layers** **keep first** **FA decode** **Acc think** **Acc non-think**


0 Full [] 0 False **73.0** **62.0**
1 2k [] 0 False 3.2 11.0
2 8k [] 0 False 13.2 19.8


3 2k [] 10 False 16.0 15.6
4 2k [] 0 True 11.8 14.2
5 2k [1, 3, 5, ...] 0 False 13.4 18.4
6 8k [] 0 True 26.2 25.0


7 2k [] 10 True 38.2 20.6
8 2k [] 100 True 50.0 17.8
9 2k [] 1000 True 50.0 20.2
10 2k [0, 2, 4, ...] 10 False 17.0 14.8
11 2k [0, 2, 4, ...] 0 True 32.2 26.0
12 2k [1, 3, 5, ...] 10 False 25.8 36.4
13 2k [1, 3, 5, ...] 0 True 59.2 34.8
14 4k [] 10 True 38.0 24.4
15 8k [] 10 True 49.2 35.2


16 2k [0, 2, 4, ...] 10 True 36.0 17.2
17 2k [1, 3, 5, ...] 10 True 65.0 **53.6**
18 2k [1, 3, 5, ...] 100 True **68.8** 50.6
19 2k [1, 5, 9, ...] 10 True 53.2 31.4
20 2k [1, 9, 17, ...] 10 True 36.4 18.8
21 2k [3, 7, 11, ...] 10 True 54.2 34.6
22 4k [1, 3, 5, ...] 100 True **73.0** **54.2**
23 8k [1, 3, 5, ...] 100 True **71.6** **56.6**



**SWA/FA layer selection has large impacts.** Selecting less layers for FA, e.g. only one fourth or
eighth (row 19, 20), though more efficient, will
significantly decrease the improvement brought by
Interleaving Layers. More importantly, for Qwen34B, configuring _odd-numbered_ layers with full attention is significantly better than _even-numbered_
layers (row 11, 13, 16, 17). However, surprisingly,
this result is reversed for Qwen3-30B-A3B and
Llama3.1-8B-Instruct (row 10 and 11 in Table 4
and 5). This suggests that layer functionalities differ across model families and sizes, necessitating
model-specific layer selection strategies, as discussed in Section 3.2.


Therefore, we answer RQ1: adapting an FA
LLM to SWA is feasible even without any training. But it requires specific combinations of at
least 2 methods, which could be less efficient for
inference.



**5.2** **SWA Adaptation With Fine-tuning**


We next evaluate SWA-aware supervised finetuning, which is expected to provide higher improvement. Table 2 reports LongMemEval_24k
accuracy after SFT under various SWAA configurations. The original full-attention model is also
fine-tuned as a baseline (row 0). Since training is
relatively time-consuming, we only test a representative subset of configurations. Our findings are as
follows:


**Fine-tuning substantially lifts all SWA config-**
**urations.** Comparing all the fine-tuning results
with non-training ones, it is clear that fine-tuning
consistently provides great improvement. However,
simply fine-tuning with naive SWA remains insufficient, only achieving 18.8% and 23.8% accuracy
(row 1).


**FA Decode and Interleaving Layers emerge as**
**dominant components.** After SFT, FA Decode
and Interleaving Layers provide the largest gains.



6


Table 2: Qwen3-4B-Thinking and Qwen3-4B-Instruct results on LongMemEval with SFT


**No.** **window size** **FA layers** **keep first** **FA decode** **Acc think** **Acc non-think**


0 Full [] 0 True **74.6** **63.4**
1 2k [] 0 False 18.8 23.8


2 2k [] 10 False 15.6 /
3 2k [] 0 True 57.9 42.0
4 2k [1, 3, 5, ...] 0 False 63.6 54.6
5 4k [] 0 True 62.6 /


6 2k [] 10 True 56.7 /
7 2k [] 100 True 62.2 42.6
8 2k [1, 3, 5, ...] 0 True **73.2** **58.8**
9 2k [0, 2, 4, ...] 0 True **66.0** /
10 2k [1, 5, 9, ...] 0 True **68.8** 47.0


11 2k [1, 3, 5, ...] 100 True **73.2** **61.4**


Table 3: Recommended SWA adaptation recipes for different needs and scenarios. � means optional.


**Training** ? **Thinking Model?** **Prefer?** **FA Decode** **Interleaving Layers** **Keep First**


No No Any � � �
No Yes Efficiency � � �
No Yes Accuracy � � �


Yes Any Efficiency � � �
Yes Any Accuracy � � �



Using FA Decode alone (row 3) or using Interleaving Layers alone (row 4) both get high accuracy.
And combining both (row 8) further boosts performance to 73 _._ 2 (think) and 58 _._ 8 (non-think), nearly
matching the full-attention SFT baseline in row 0.


**Keep First becomes optional rather than essen-**
**tial.** Before fine-tuning, Keep First is crucial for
stability under SWA. But after SFT, it only provides
minor additional improvement. With FA Decode,
adding _k_ = 100 (row 7) improves over _k_ = 0
(row 3) only 4.5%, and if further combining FA
layers, it almost offers no improvements (row 11
and row 8).


**Effect of sliding window size.** Row 3 and 5
shows that increasing the window from 2k to 4k
with FA Decode improves thinking-model accuracy from 57 _._ 9 to 62 _._ 6. This mirrors the non-SFT
trend that larger windows help, but the dominant
improvements still come from FA Decode and Interleaving Layers.
","The ""Keep First k Tokens"" method in SWAA preserves global context by permanently retaining attention to the initial `k` ""sink"" tokens. This means that any subsequent token can attend to its local window *and* these initial `k` tokens, which helps maintain the stability of the attention distribution and the model's output, especially when using Sliding Window Attention (SWA). This explicit preservation of attention to initial tokens is crucial for preventing performance degradation caused by the inaccessibility of distant tokens.

The provided context mentions ""Transformer-based Large Language Models (LLMs) (Vaswani et al., 2017),"" which refers to the original Transformer paper. However, it does not contain information describing the ""positional encoding"" mechanism from that paper or how it preserves global context. Therefore, a comparison of ""Keep First k Tokens"" with ""positional encoding"" in terms of preserving global context cannot be fully made based on the provided documents.",129,4837,4967,,3,,2026-01-02T20:16:35.099955
"How does the ""Full Attention Decode"" method in SWAA relate to the ""decoder"" structure described in ""Attention Is All You Need""?",Multi-Document Aggregation,"[Document 1]
Source: sliding_window_attention_annotated_chunks_2k.jsonl
Chunk ID: sliding_window_attention_annotated_chunks_2k_1
Content: LightTransfer (Zhang et al., 2024) is a promising
attempt at adapting existing models to SWA without pretraining, which has the same motivation as
ours. But it may generalize poorly across model
families (see Appendix G).


**2.2** **Linear Attention**


An alternative approach involves reformulating
the attention mechanism entirely to achieve linear, _O_ ( _N_ ), complexity. This includes methods
such as RNN-like linear attention transformers

(Katharopoulos et al., 2020; Peng et al., 2023;
Sun et al., 2023) and structured state-space models
(SSMs) like Mamba (Gu and Dao, 2023). Many
works such as Jamba and Nemotron-Flash(Lieber
et al., 2024; Linsong Chu et al., 2024; Team et al.,
2025; Fu et al., 2025a) interleave linear attention
layers with traditional attention layers to create hybrid model structures. While promising, they represent a fundamental architectural departure from the
standard Transformer, and their performance is generally weaker than traditional Transformer-based

LLMs.



2


**3** **Candidate Methods for SWA**

**Adaptation**


As established, a naive application of SWA leads
to severe long-context performance degradation.
Therefore, we investigate five methods that can potentially facilitate SWA adaptation from distinct
perspectives. However, every method except finetuning has some drawbacks to LLM inference efficiency, as discussed in Appendix A. Therefore,
although these methods are not mutually exclusive,
we should not indiscriminately adopt all of them.
Instead, we must evaluate various combinations to
identify the optimal trade-off between performance
and efficiency.


**3.1** **Full Attention Decode**


This method applies SWA **only** to the prefilling
stage. During the decoding (auto-regressive generation) stage, each token still employs full attention,
allowing access to all previous tokens in the context. The resulting attention mask is depicted in
Figure 1a.
This novel approach, first proposed in our study,
is inspired by human reading comprehension: humans typically scan a passage casually (prefilling)
before thinking deeply to formulate an answer (decoding) for a specific problem. We term this strategy ""reading casually, thinking carefully."" In our
design, the SWA-constrained prefilling stage corresponds to casual reading, while the full-attention
decoding stage enables careful thinking. This analogy also suggests that Chain-of-Thought (CoT)
during decoding may be particularly beneficial, as
extended generation (i.e. ""thinking"") could compensate for the insufficient contextual information
gathered during the prefilling stage.


(a) FA Decode (b) Keep First


Figure 1: (a) Attention mask for FA Decode. SWA is
used for prompt tokens (prefill), and full attention is
used for generated tokens (decode). (b) Attention mask
for SWA combined with Keep First _k_ Tokens.



**3.1.1** **Keep First k Tokens**

Xiao et al. demonstrate that models pretrained with
full attention allocate a disproportionate amount
of attention to the initial tokens (""attention sink""),
and removing the visibility of these tokens causes
performance collapse. Their solution, streaming
attention, involves permanently retaining the attention to these ""sink"" tokens while using SWA, which
successfully maintains the stability of the attention
distribution and the model’s output. Our method
is basically the same: as shown in Figure 1b, any
subsequent token can attend to its local window
**and** the initial _k_ tokens.

Notably, however, our method extends beyond
its original version. Streaming attention operates
only at the KV cache level; specifically, the KV
cache of the sink tokens is retained during decoding, while the prefilling stage is not modified or accelerated at all. In contrast, we directly customize
the Flash-Attention-2 (Dao, 2024) kernel to implement such attention mask, thereby accelerating
prefilling via SWA as well, and eliminating the
need to modify KV cache.


**3.2** **Interleaving Layers**


This method retains full attention on a subset of

layers while applying SWA to the remainder, providing a simple hybrid mechanism to balance the
performance of full attention with the efficiency
of pure SWA. A common strategy involves designating one in every _n_ layers to use full attention
(e.g., layers 0, 2, 4, . . . retain full attention, while all
others use SWA). For example, Gemma-2 (Team,
2024a) uses SWA only for layers [1, 3, 5, . . . ], and
Gemma-3 (Team, 2025a) uses SWA only for layers

[5, 11, 17, ...].
However, for an FA-pretrained model, layers
may exhibit distinct behaviors, suggesting it may
not be optimal to simply assign SWA to layers [1,
3, 5, ...]. Instead, it might be preferable to use
statistical features to select ""lazy"" (mostly focusing
on just recent tokens) layers, as adopted by LightTransfer (Zhang et al., 2024). However, we find
that LightTransfer is not consistently superior in
practice (see Appendix G). Therefore, for simplicity, we still choose the simplest fixed-interval layer
selections in our experiments, such as [0, 2, 4, . . . ]
and [1, 3, 5, ...].


**3.3** **Chain-of-Thought**


Chain-of-Thought (CoT) (Wei et al., 2022) is a
widely used technique for improving model ac


3



```
### Figure - Sliding Window Attention
![Sliding Window Attention](sliding_window_attention.pdf-2-0.png)

**Caption:**  
This figure illustrates the sliding window attention mechanism in a transformer model. The ""Prefill"" region represents the initial input, while the ""Decode"" region shows the subsequent processing.

**Description:**
- The image depicts a grid representing a sequence of data, with a sliding window moving across it.
- The green area indicates the window's current position, highlighting the portion of the sequence being attended to.
- The blue area represents the decoded output based on the attended input.
- The window progressively moves across the sequence, enabling contextual understanding.
```

```
### Figure - Sliding Window Attention
![Sliding Window Attention](sliding_window_attention.pdf-2-1.png)

**Caption:**  
The figure illustrates the sliding window attention mechanism, where attention is retained over a limited window. Attention decreases as the distance from the current position increases.

**Description:**
- The image depicts a grid representing a sequence of tokens.
- The blue squares indicate attention retained across the sliding window.
- The green squares represent attention calculated within the sliding window.
- Attention decreases progressively as the distance 'k' from the current position increases, reflecting a decaying attention mechanism.
- The sliding window size is decreasing as it moves along the sequence.
curacy via reasoning. With the advent of ""thinking"" models, such as DeepSeek-R1 (DeepSeek-AI,
2025a), CoT has evolved from a prompting strategy
to an intrinsic LLM capability. However, whether
CoT has a specific impact in SWA scenarios remains uninvestigated. We explore this by comparing a thinking model with its non-thinking variant,
e.g., Qwen3-4B-Thinking and Qwen3-4B-Instruct,
to verify the effect of CoT on SWA adaptation,
which can produce more notable differences compared to simply adding CoT prompting to the same
model.


**3.4** **Fine-tuning**

---

[Document 2]
Source: sliding_window_attention_annotated_chunks_2k.jsonl
Chunk ID: sliding_window_attention_annotated_chunks_2k_0
Content: ## **Sliding Window Attention Adaptation**

Yijiong Yu [a], Jiale Liu [b], Qingyun Wu [b], Huazheng Wang [a], and Ji Pei [c]


aOregon State University, {yuyiji, huazheng.wang}@oregonstate.edu
bPenn State University, {jiale.liu, qingyun.wu}@psu.edu
cDeepSolution, research@deepsolution.chat



**Abstract**


The self-attention mechanism in Transformer
based Large Language Models (LLMs) scales
quadratically with input length, making longcontext inference expensive. Sliding window attention (SWA) reduces this cost to linear complexity, but naively enabling complete
SWA at inference-time for models pretrained
with full attention (FA) causes severe longcontext performance degradation due to training–inference mismatch. This makes us wonder: _Can FA-pretrained LLMs be well adapted_
_to SWA without pretraining?_ We investigate
this by proposing Sliding Window Attention
Adaptation (SWAA), a set of practical recipes
that combine five methods for better adaptation: (i) applying SWA only during prefilling;
(ii) preserving “sink” tokens; (iii) interleaving
FA/SWA layers; (iv) chain-of-thought (CoT);
and (v) fine-tuning. Our experiments show that
SWA adaptation is feasible while non-trivial:
no single method suffices, yet specific synergistic combinations effectively recover the original
long-context performance. We further analyze
the performance-efficiency trade-offs of different SWAA configurations and provide recommended recipes for diverse scenarios. Our code
[is available at github.](https://github.com/yuyijiong/sliding-window-attention-adaptation)


**1** **Introduction**


Transformer-based Large Language Models
(LLMs) (Vaswani et al., 2017) demonstrate
remarkable capabilities, but their self-attention
scales quadratically with the input sequence length,
making long context processing inefficient. Sliding
Window Attention (SWA), the most straightforward and widely adopted sparse attention
pattern, which restricts each token’s attention to a
fixed-size local window, reduces the computational
complexity to linearity, along with some other
benefits (see Appendix A).
To apply SWA to LLMs, typical solutions involve training a model with SWA from scratch, but



are prohibitively costly and cannot match the performance of state-of-the-art full-causal-attention

models like Qwen3 (Team, 2025b), mainly due
to the inability to reproduce pretraining data.
Training-free methods like streaming attention
(Xiao et al., 2024) can stabilize LLM outputs by retaining “sink tokens” while applying SWA, which
greatly improve efficiency but inevitably suffer
from severe long-context performance degradation
possibly due to the inaccessibility of distant tokens’
information (Xiao, 2025). This motivates a critical,
unexplored question: _Can a full-attention model_
_be adapted to sliding window attention at low cost_
_while maintaining long-context performance?_
We answer Yes to this question by proposing
Sliding Window Attention Adaptation(SWAA), a
set of recipes for adapting FA-pretrained models to
SWA, which requires neither costly pretraining nor
modifications to the standard Transformer architecture. Specifically, it systematically combines five
practical and composable methods:


1. **Full Attention (FA) Decode** : applying SWA
only during the prefilling stage while switching back to full attention for decoding.


2. **Keep First** _k_ **Tokens** : explicitly preserving
attention to the first _k_ “sink” tokens.


3. **Interleaving FA/SWA layers** : mix fullattention and SWA layers (e.g., assigning
SWA to half layers).


4. **Chain-of-Thought (CoT)** : enforcing an explicit ""thinking"" process during decoding.


5. **Fine-tuning with SWA** : lightweight SWAaware supervised fine-tuning on long-context
data.


Among these, FA Decode is a novel method we
introduce. Keep First _k_ Tokens and FA/SWA Interleaving have been proven effective in prior work



1


(Xiao et al., 2024; Team, 2024a; Zhang et al., 2024),
while CoT and fine-tuning are common LLM techniques. However, how these methods should be
combined to be actually effective for SWA adaptation remains unexplored.
Therefore, in our experiments, we evaluate
SWAA on Qwen3 (Team, 2025b) and Llama3.1
(Team, 2024b) across several long-context benchmarks, measuring both performance and efficiency
under a wide range of SWAA recipes. First, we
find that each method makes a distinct contribution,
but no single ingredient suffices to make SWA competitive with full attention. Second, we show that
specific synergistic combinations of methods can
recover a large fraction of the original long-context
performance. Third, we analyze the performanceefficiency trade-offs of different SWAA recipes and
identify some recommended configurations suitable for different deployment scenarios.
Rather than proposing a single globally optimal
configuration, we view SWAA as a flexible toolkit
of practical recipes: practitioners can select SWAA
recipes that match their accuracy and efficiency
constraints, or compose their own SWA adaptation
strategies by combining the available ingredients.
Our key contributions are:


1. We perform the first systematic study on adapting FA-pretrained LLMs to SWA without pretraining, revealing novel insights about how
SWA impacts LLMs and providing a foundation for future research in efficient sparse
attention.


2. We propose SWAA, a set of practical
SWA adaptation recipes that offer a robust
performance-efficiency balance for various
use cases, accelerating LLM inference from
the bottom level.


3. We implement our methods with FlashAttention (Dao, 2024) and vLLM (Kwon
et al., 2023), making it plug-and-play and userfriendly for practical deployment.


**2** **Related Works**


The _O_ ( _N_ [2] ) complexity of self-attention in Transformers (Vaswani et al., 2017) has spurred a wide
field of research about more efficient language
model architectures. Among the two most popular technological routes are sparse attention and
linear attention.



**2.1** **Sparse Attention**


Our work falls in this category. Sliding Window
Attention (SWA) represents the most basic form
of local sparse attention, yet its performance is inherently limited. Therefore, model architectures
such as Longformer (Beltagy et al., 2020), BigBird
(Zaheer et al., 2020), and RATTENTION (Wang
et al., 2025) combine local SWA on most tokens
with special global attention on specific tokens to
create a more powerful, albeit still sparse, pattern.
Popular LLMs like Gemma2 (Team, 2024a) adopt
SWA in half of their layers to balance the efficiency
of SWA and peformance of FA. Sliding Window
Attention Training (SWAT) (Fu et al., 2025b) introduces architectural changes, such as sigmoid
activation and balanced position embeddings, to
stabilize SWA performance. More advanced methods like Deepseek-sparse-attention (Yuan et al.,
2025; DeepSeek-AI, 2025b), although achieving
excellent quality, involve more complicated implementation and optimization due to semantic-aware
attention operations (e.g., selecting the most important tokens based on attention weights). Regardless,
almost all of the above methods require pretraining with a specific sparse pattern, which is costly
and fails to leverage the advantages of existing pretrained models.


LightTransfer (Zhang et al., 2024) is a promising
attempt at adapting existing models to SWA without pretraining, which has the same motivation as
ours. But it may generalize poorly across model
families (see Appendix G).


**2.2** **Linear Attention**


An alternative approach involves reformulating
the attention mechanism entirely to achieve linear, _O_ ( _N_ ), complexity. This includes methods
such as RNN-like linear attention transformers

---

[Document 3]
Source: sliding_window_attention_annotated_chunks_2k.jsonl
Chunk ID: sliding_window_attention_annotated_chunks_2k_3
Content: **Combinations exhibit strong synergy.** Recipes
that combine multiple methods deliver large gains:


  - **FA Decode + Keep First** _k_ substantially improves over naive SWA (rows 7–9), recovering roughly half to two-thirds of the gap on
the thinking model as _k_ increases. However,
increasing _k_ from 100 to 1000 yields almost
no improvement, indicating that _k_ does not
need to be exceedingly large.


  - **Interleaving Layers + FA Decode** is
markedly stronger (row 13), recovering most
of the gap for the thinking model.


  - **FA Decode + Interleaving Layers + Keep**
**First** _k_ perform best (rows 18). The thinking
model recovers close to 90% of the FA gap
even at 2k window.


**CoT synergizes with FA Decode.** Under recipes
that include FA Decode, the thinking model consistently benefits more than the non-thinking model
(rows 13 and 18), suggesting that CoT synergizes
with FA Decode: preserving global attention at
decoding time enables longer reasoning traces to
capitalize on context processed by SWA, confirming our hypothesis in Section 3.1.


**Sliding window size affects, but is not the deci-**
**sive role.** With FA Decode + Keep First _k_, accuracy improves as the window increases (rows 7,
14, 15), though benefits are modest until 8k. When
added with interleaving FA layers, moving from a
2k to a 4k window is enough to close the remaining
gap on the thinking model (row 22 matches the FA
baseline), indicating that FA Decode and Interleaving are the dominant drivers, and larger windows
mainly smooth residual error.



5


Table 1: Qwen3-4B-Thinking and Qwen3-4B-Instruct results on LongMemEval without SFT


**No.** **window size** **FA layers** **keep first** **FA decode** **Acc think** **Acc non-think**


0 Full [] 0 False **73.0** **62.0**
1 2k [] 0 False 3.2 11.0
2 8k [] 0 False 13.2 19.8


3 2k [] 10 False 16.0 15.6
4 2k [] 0 True 11.8 14.2
5 2k [1, 3, 5, ...] 0 False 13.4 18.4
6 8k [] 0 True 26.2 25.0


7 2k [] 10 True 38.2 20.6
8 2k [] 100 True 50.0 17.8
9 2k [] 1000 True 50.0 20.2
10 2k [0, 2, 4, ...] 10 False 17.0 14.8
11 2k [0, 2, 4, ...] 0 True 32.2 26.0
12 2k [1, 3, 5, ...] 10 False 25.8 36.4
13 2k [1, 3, 5, ...] 0 True 59.2 34.8
14 4k [] 10 True 38.0 24.4
15 8k [] 10 True 49.2 35.2


16 2k [0, 2, 4, ...] 10 True 36.0 17.2
17 2k [1, 3, 5, ...] 10 True 65.0 **53.6**
18 2k [1, 3, 5, ...] 100 True **68.8** 50.6
19 2k [1, 5, 9, ...] 10 True 53.2 31.4
20 2k [1, 9, 17, ...] 10 True 36.4 18.8
21 2k [3, 7, 11, ...] 10 True 54.2 34.6
22 4k [1, 3, 5, ...] 100 True **73.0** **54.2**
23 8k [1, 3, 5, ...] 100 True **71.6** **56.6**



**SWA/FA layer selection has large impacts.** Selecting less layers for FA, e.g. only one fourth or
eighth (row 19, 20), though more efficient, will
significantly decrease the improvement brought by
Interleaving Layers. More importantly, for Qwen34B, configuring _odd-numbered_ layers with full attention is significantly better than _even-numbered_
layers (row 11, 13, 16, 17). However, surprisingly,
this result is reversed for Qwen3-30B-A3B and
Llama3.1-8B-Instruct (row 10 and 11 in Table 4
and 5). This suggests that layer functionalities differ across model families and sizes, necessitating
model-specific layer selection strategies, as discussed in Section 3.2.


Therefore, we answer RQ1: adapting an FA
LLM to SWA is feasible even without any training. But it requires specific combinations of at
least 2 methods, which could be less efficient for
inference.



**5.2** **SWA Adaptation With Fine-tuning**


We next evaluate SWA-aware supervised finetuning, which is expected to provide higher improvement. Table 2 reports LongMemEval_24k
accuracy after SFT under various SWAA configurations. The original full-attention model is also
fine-tuned as a baseline (row 0). Since training is
relatively time-consuming, we only test a representative subset of configurations. Our findings are as
follows:


**Fine-tuning substantially lifts all SWA config-**
**urations.** Comparing all the fine-tuning results
with non-training ones, it is clear that fine-tuning
consistently provides great improvement. However,
simply fine-tuning with naive SWA remains insufficient, only achieving 18.8% and 23.8% accuracy
(row 1).


**FA Decode and Interleaving Layers emerge as**
**dominant components.** After SFT, FA Decode
and Interleaving Layers provide the largest gains.



6


Table 2: Qwen3-4B-Thinking and Qwen3-4B-Instruct results on LongMemEval with SFT


**No.** **window size** **FA layers** **keep first** **FA decode** **Acc think** **Acc non-think**


0 Full [] 0 True **74.6** **63.4**
1 2k [] 0 False 18.8 23.8


2 2k [] 10 False 15.6 /
3 2k [] 0 True 57.9 42.0
4 2k [1, 3, 5, ...] 0 False 63.6 54.6
5 4k [] 0 True 62.6 /


6 2k [] 10 True 56.7 /
7 2k [] 100 True 62.2 42.6
8 2k [1, 3, 5, ...] 0 True **73.2** **58.8**
9 2k [0, 2, 4, ...] 0 True **66.0** /
10 2k [1, 5, 9, ...] 0 True **68.8** 47.0


11 2k [1, 3, 5, ...] 100 True **73.2** **61.4**


Table 3: Recommended SWA adaptation recipes for different needs and scenarios. � means optional.


**Training** ? **Thinking Model?** **Prefer?** **FA Decode** **Interleaving Layers** **Keep First**


No No Any � � �
No Yes Efficiency � � �
No Yes Accuracy � � �


Yes Any Efficiency � � �
Yes Any Accuracy � � �



Using FA Decode alone (row 3) or using Interleaving Layers alone (row 4) both get high accuracy.
And combining both (row 8) further boosts performance to 73 _._ 2 (think) and 58 _._ 8 (non-think), nearly
matching the full-attention SFT baseline in row 0.


**Keep First becomes optional rather than essen-**
**tial.** Before fine-tuning, Keep First is crucial for
stability under SWA. But after SFT, it only provides
minor additional improvement. With FA Decode,
adding _k_ = 100 (row 7) improves over _k_ = 0
(row 3) only 4.5%, and if further combining FA
layers, it almost offers no improvements (row 11
and row 8).


**Effect of sliding window size.** Row 3 and 5
shows that increasing the window from 2k to 4k
with FA Decode improves thinking-model accuracy from 57 _._ 9 to 62 _._ 6. This mirrors the non-SFT
trend that larger windows help, but the dominant
improvements still come from FA Decode and Interleaving Layers.
","The provided context describes ""Full Attention Decode"" as a method within SWAA where Sliding Window Attention (SWA) is applied only during the prefilling stage. During the subsequent decoding (auto-regressive generation) stage, each token uses full attention, allowing access to all previous tokens in the context (Document 1, Document 2).

The context mentions ""Transformer-based Large Language Models (LLMs) (Vaswani et al., 2017)"" which refers to the ""Attention Is All You Need"" paper (Document 1, Document 2). However, the provided documents do not describe the specific ""decoder"" structure from ""Attention Is All You Need"" or explicitly relate ""Full Attention Decode"" to that particular structure.",122,4189,4313,,3,,2026-01-02T20:16:39.414631
"Evaluate the ""Chain-of-Thought"" (CoT) usage in SWAA against the CoT-based subquestion decomposition in the CogVision dataset.",Multi-Document Aggregation,"[Document 1]
Source: sliding_window_attention_annotated_chunks_2k.jsonl
Chunk ID: sliding_window_attention_annotated_chunks_2k_1
Content: LightTransfer (Zhang et al., 2024) is a promising
attempt at adapting existing models to SWA without pretraining, which has the same motivation as
ours. But it may generalize poorly across model
families (see Appendix G).


**2.2** **Linear Attention**


An alternative approach involves reformulating
the attention mechanism entirely to achieve linear, _O_ ( _N_ ), complexity. This includes methods
such as RNN-like linear attention transformers

(Katharopoulos et al., 2020; Peng et al., 2023;
Sun et al., 2023) and structured state-space models
(SSMs) like Mamba (Gu and Dao, 2023). Many
works such as Jamba and Nemotron-Flash(Lieber
et al., 2024; Linsong Chu et al., 2024; Team et al.,
2025; Fu et al., 2025a) interleave linear attention
layers with traditional attention layers to create hybrid model structures. While promising, they represent a fundamental architectural departure from the
standard Transformer, and their performance is generally weaker than traditional Transformer-based

LLMs.



2


**3** **Candidate Methods for SWA**

**Adaptation**


As established, a naive application of SWA leads
to severe long-context performance degradation.
Therefore, we investigate five methods that can potentially facilitate SWA adaptation from distinct
perspectives. However, every method except finetuning has some drawbacks to LLM inference efficiency, as discussed in Appendix A. Therefore,
although these methods are not mutually exclusive,
we should not indiscriminately adopt all of them.
Instead, we must evaluate various combinations to
identify the optimal trade-off between performance
and efficiency.


**3.1** **Full Attention Decode**


This method applies SWA **only** to the prefilling
stage. During the decoding (auto-regressive generation) stage, each token still employs full attention,
allowing access to all previous tokens in the context. The resulting attention mask is depicted in
Figure 1a.
This novel approach, first proposed in our study,
is inspired by human reading comprehension: humans typically scan a passage casually (prefilling)
before thinking deeply to formulate an answer (decoding) for a specific problem. We term this strategy ""reading casually, thinking carefully."" In our
design, the SWA-constrained prefilling stage corresponds to casual reading, while the full-attention
decoding stage enables careful thinking. This analogy also suggests that Chain-of-Thought (CoT)
during decoding may be particularly beneficial, as
extended generation (i.e. ""thinking"") could compensate for the insufficient contextual information
gathered during the prefilling stage.


(a) FA Decode (b) Keep First


Figure 1: (a) Attention mask for FA Decode. SWA is
used for prompt tokens (prefill), and full attention is
used for generated tokens (decode). (b) Attention mask
for SWA combined with Keep First _k_ Tokens.



**3.1.1** **Keep First k Tokens**

Xiao et al. demonstrate that models pretrained with
full attention allocate a disproportionate amount
of attention to the initial tokens (""attention sink""),
and removing the visibility of these tokens causes
performance collapse. Their solution, streaming
attention, involves permanently retaining the attention to these ""sink"" tokens while using SWA, which
successfully maintains the stability of the attention
distribution and the model’s output. Our method
is basically the same: as shown in Figure 1b, any
subsequent token can attend to its local window
**and** the initial _k_ tokens.

Notably, however, our method extends beyond
its original version. Streaming attention operates
only at the KV cache level; specifically, the KV
cache of the sink tokens is retained during decoding, while the prefilling stage is not modified or accelerated at all. In contrast, we directly customize
the Flash-Attention-2 (Dao, 2024) kernel to implement such attention mask, thereby accelerating
prefilling via SWA as well, and eliminating the
need to modify KV cache.


**3.2** **Interleaving Layers**


This method retains full attention on a subset of

layers while applying SWA to the remainder, providing a simple hybrid mechanism to balance the
performance of full attention with the efficiency
of pure SWA. A common strategy involves designating one in every _n_ layers to use full attention
(e.g., layers 0, 2, 4, . . . retain full attention, while all
others use SWA). For example, Gemma-2 (Team,
2024a) uses SWA only for layers [1, 3, 5, . . . ], and
Gemma-3 (Team, 2025a) uses SWA only for layers

[5, 11, 17, ...].
However, for an FA-pretrained model, layers
may exhibit distinct behaviors, suggesting it may
not be optimal to simply assign SWA to layers [1,
3, 5, ...]. Instead, it might be preferable to use
statistical features to select ""lazy"" (mostly focusing
on just recent tokens) layers, as adopted by LightTransfer (Zhang et al., 2024). However, we find
that LightTransfer is not consistently superior in
practice (see Appendix G). Therefore, for simplicity, we still choose the simplest fixed-interval layer
selections in our experiments, such as [0, 2, 4, . . . ]
and [1, 3, 5, ...].


**3.3** **Chain-of-Thought**


Chain-of-Thought (CoT) (Wei et al., 2022) is a
widely used technique for improving model ac


3



```
### Figure - Sliding Window Attention
![Sliding Window Attention](sliding_window_attention.pdf-2-0.png)

**Caption:**  
This figure illustrates the sliding window attention mechanism in a transformer model. The ""Prefill"" region represents the initial input, while the ""Decode"" region shows the subsequent processing.

**Description:**
- The image depicts a grid representing a sequence of data, with a sliding window moving across it.
- The green area indicates the window's current position, highlighting the portion of the sequence being attended to.
- The blue area represents the decoded output based on the attended input.
- The window progressively moves across the sequence, enabling contextual understanding.
```

```
### Figure - Sliding Window Attention
![Sliding Window Attention](sliding_window_attention.pdf-2-1.png)

**Caption:**  
The figure illustrates the sliding window attention mechanism, where attention is retained over a limited window. Attention decreases as the distance from the current position increases.

**Description:**
- The image depicts a grid representing a sequence of tokens.
- The blue squares indicate attention retained across the sliding window.
- The green squares represent attention calculated within the sliding window.
- Attention decreases progressively as the distance 'k' from the current position increases, reflecting a decaying attention mechanism.
- The sliding window size is decreasing as it moves along the sequence.
curacy via reasoning. With the advent of ""thinking"" models, such as DeepSeek-R1 (DeepSeek-AI,
2025a), CoT has evolved from a prompting strategy
to an intrinsic LLM capability. However, whether
CoT has a specific impact in SWA scenarios remains uninvestigated. We explore this by comparing a thinking model with its non-thinking variant,
e.g., Qwen3-4B-Thinking and Qwen3-4B-Instruct,
to verify the effect of CoT on SWA adaptation,
which can produce more notable differences compared to simply adding CoT prompting to the same
model.


**3.4** **Fine-tuning**

---

[Document 2]
Source: sliding_window_attention_annotated_chunks_2k.jsonl
Chunk ID: sliding_window_attention_annotated_chunks_2k_3
Content: **Combinations exhibit strong synergy.** Recipes
that combine multiple methods deliver large gains:


  - **FA Decode + Keep First** _k_ substantially improves over naive SWA (rows 7–9), recovering roughly half to two-thirds of the gap on
the thinking model as _k_ increases. However,
increasing _k_ from 100 to 1000 yields almost
no improvement, indicating that _k_ does not
need to be exceedingly large.


  - **Interleaving Layers + FA Decode** is
markedly stronger (row 13), recovering most
of the gap for the thinking model.


  - **FA Decode + Interleaving Layers + Keep**
**First** _k_ perform best (rows 18). The thinking
model recovers close to 90% of the FA gap
even at 2k window.


**CoT synergizes with FA Decode.** Under recipes
that include FA Decode, the thinking model consistently benefits more than the non-thinking model
(rows 13 and 18), suggesting that CoT synergizes
with FA Decode: preserving global attention at
decoding time enables longer reasoning traces to
capitalize on context processed by SWA, confirming our hypothesis in Section 3.1.


**Sliding window size affects, but is not the deci-**
**sive role.** With FA Decode + Keep First _k_, accuracy improves as the window increases (rows 7,
14, 15), though benefits are modest until 8k. When
added with interleaving FA layers, moving from a
2k to a 4k window is enough to close the remaining
gap on the thinking model (row 22 matches the FA
baseline), indicating that FA Decode and Interleaving are the dominant drivers, and larger windows
mainly smooth residual error.



5


Table 1: Qwen3-4B-Thinking and Qwen3-4B-Instruct results on LongMemEval without SFT


**No.** **window size** **FA layers** **keep first** **FA decode** **Acc think** **Acc non-think**


0 Full [] 0 False **73.0** **62.0**
1 2k [] 0 False 3.2 11.0
2 8k [] 0 False 13.2 19.8


3 2k [] 10 False 16.0 15.6
4 2k [] 0 True 11.8 14.2
5 2k [1, 3, 5, ...] 0 False 13.4 18.4
6 8k [] 0 True 26.2 25.0


7 2k [] 10 True 38.2 20.6
8 2k [] 100 True 50.0 17.8
9 2k [] 1000 True 50.0 20.2
10 2k [0, 2, 4, ...] 10 False 17.0 14.8
11 2k [0, 2, 4, ...] 0 True 32.2 26.0
12 2k [1, 3, 5, ...] 10 False 25.8 36.4
13 2k [1, 3, 5, ...] 0 True 59.2 34.8
14 4k [] 10 True 38.0 24.4
15 8k [] 10 True 49.2 35.2


16 2k [0, 2, 4, ...] 10 True 36.0 17.2
17 2k [1, 3, 5, ...] 10 True 65.0 **53.6**
18 2k [1, 3, 5, ...] 100 True **68.8** 50.6
19 2k [1, 5, 9, ...] 10 True 53.2 31.4
20 2k [1, 9, 17, ...] 10 True 36.4 18.8
21 2k [3, 7, 11, ...] 10 True 54.2 34.6
22 4k [1, 3, 5, ...] 100 True **73.0** **54.2**
23 8k [1, 3, 5, ...] 100 True **71.6** **56.6**



**SWA/FA layer selection has large impacts.** Selecting less layers for FA, e.g. only one fourth or
eighth (row 19, 20), though more efficient, will
significantly decrease the improvement brought by
Interleaving Layers. More importantly, for Qwen34B, configuring _odd-numbered_ layers with full attention is significantly better than _even-numbered_
layers (row 11, 13, 16, 17). However, surprisingly,
this result is reversed for Qwen3-30B-A3B and
Llama3.1-8B-Instruct (row 10 and 11 in Table 4
and 5). This suggests that layer functionalities differ across model families and sizes, necessitating
model-specific layer selection strategies, as discussed in Section 3.2.


Therefore, we answer RQ1: adapting an FA
LLM to SWA is feasible even without any training. But it requires specific combinations of at
least 2 methods, which could be less efficient for
inference.



**5.2** **SWA Adaptation With Fine-tuning**


We next evaluate SWA-aware supervised finetuning, which is expected to provide higher improvement. Table 2 reports LongMemEval_24k
accuracy after SFT under various SWAA configurations. The original full-attention model is also
fine-tuned as a baseline (row 0). Since training is
relatively time-consuming, we only test a representative subset of configurations. Our findings are as
follows:


**Fine-tuning substantially lifts all SWA config-**
**urations.** Comparing all the fine-tuning results
with non-training ones, it is clear that fine-tuning
consistently provides great improvement. However,
simply fine-tuning with naive SWA remains insufficient, only achieving 18.8% and 23.8% accuracy
(row 1).


**FA Decode and Interleaving Layers emerge as**
**dominant components.** After SFT, FA Decode
and Interleaving Layers provide the largest gains.



6


Table 2: Qwen3-4B-Thinking and Qwen3-4B-Instruct results on LongMemEval with SFT


**No.** **window size** **FA layers** **keep first** **FA decode** **Acc think** **Acc non-think**


0 Full [] 0 True **74.6** **63.4**
1 2k [] 0 False 18.8 23.8


2 2k [] 10 False 15.6 /
3 2k [] 0 True 57.9 42.0
4 2k [1, 3, 5, ...] 0 False 63.6 54.6
5 4k [] 0 True 62.6 /


6 2k [] 10 True 56.7 /
7 2k [] 100 True 62.2 42.6
8 2k [1, 3, 5, ...] 0 True **73.2** **58.8**
9 2k [0, 2, 4, ...] 0 True **66.0** /
10 2k [1, 5, 9, ...] 0 True **68.8** 47.0


11 2k [1, 3, 5, ...] 100 True **73.2** **61.4**


Table 3: Recommended SWA adaptation recipes for different needs and scenarios. � means optional.


**Training** ? **Thinking Model?** **Prefer?** **FA Decode** **Interleaving Layers** **Keep First**


No No Any � � �
No Yes Efficiency � � �
No Yes Accuracy � � �


Yes Any Efficiency � � �
Yes Any Accuracy � � �



Using FA Decode alone (row 3) or using Interleaving Layers alone (row 4) both get high accuracy.
And combining both (row 8) further boosts performance to 73 _._ 2 (think) and 58 _._ 8 (non-think), nearly
matching the full-attention SFT baseline in row 0.


**Keep First becomes optional rather than essen-**
**tial.** Before fine-tuning, Keep First is crucial for
stability under SWA. But after SFT, it only provides
minor additional improvement. With FA Decode,
adding _k_ = 100 (row 7) improves over _k_ = 0
(row 3) only 4.5%, and if further combining FA
layers, it almost offers no improvements (row 11
and row 8).


**Effect of sliding window size.** Row 3 and 5
shows that increasing the window from 2k to 4k
with FA Decode improves thinking-model accuracy from 57 _._ 9 to 62 _._ 6. This mirrors the non-SFT
trend that larger windows help, but the dominant
improvements still come from FA Decode and Interleaving Layers.

---

[Document 3]
Source: attention_functional_roles_raw_chunks_2k.jsonl
Chunk ID: attention_functional_roles_raw_chunks_2k_1
Content: Furthermore, we validate the functional importance of these heads by showing that their removal
degrades performance on complex tasks and leads to specific error patterns, while their enhancement improves reasoning capabilities. Our findings provide compelling evidence that these attention
heads play a critical role in multimodal reasoning. This insight not only deepens our understanding of the internal organization of VLMs but also suggests potential avenues for designing more
interpretable and cognitive-inspired multimodal AI systems.


2 COGVISION


In this section, we present our dataset Cognitive Vision (CogVision) that contains cognitive process
in multimodal reasoning. CogVision contains 1,409 main questions and 5,744 subquestions. Each


2


example comprises a main question and answer, its subquestions and subanswers and an annotation
specifying the receptive or cognitive function required for each subquestion.


2.1 COGNITIVE FUNCTIONS


To systematically capture the cognitive processes involved in complex reasoning tasks, we consider eight functions related to complex multimodal reasoning. These functions are inspired by
established frameworks in cognitive science (Anderson, 2014; Diamond, 2013), which highlight the
importance of perception, working memory, and reasoning in human cognition.


The functions related to vision include:


    - **Low-level Visual Reception** : Recognizing basic visual features such as color, shape, size,
position, motion.

    - **High-level Visual Reception** : Integrating visual information to recognize objects, patterns,
and scene structure.

    - **Visual Knowledge Recall** : Applying long-term visual knowledge related to visual concepts and their properties.


The functions related to language include:


    - **Language Information Extraction and Understanding** : locating and understanding relevant information from an external source or prior context.

    - **Language Knowledge Recall** : Accessing domain-specific knowledge without visual input.

    - **Math Reasoning** : Performing counting, arithmetic, comparison, and logic-based operations.

    - **Inference** : deriving implicit information that is not directly stated.

    - **Decision-Making** : Selecting the best outcome among alternatives based on reasoning.


This categorization reflects a natural progression from basic information processing to complex cognitive integration. Both the human brain and VLMs encompass a wide range of functional modules.
Our focus in this work is specifically on reasoning-related cognitive functions. By identifying and
organizing these eight core reasoning functions, we can more clearly examine how VLMs handle
different types of thinking steps, in a way that is both systematic and easy to interpret. Detail descriptions of each function and examples in CogVision can be found in Appendix A.1.


2.2 DATA COLLECTIONS


Based on our categorization of reasoning functions, we sampled 2000 diverse questions from existing commonly used visual reasoning benchmarks, selecting 200 examples each from ClevrMath (Lindstr¨om & Abraham, 2022), MathVision (Wang et al., 2024a), MathVista (Lu et al., 2024b),
MMMU (Yue et al., 2024), ScienceQA (Saikh et al., 2022), OKVQA (Marino et al., 2019) (400
examples), VCR (Zellers et al., 2019), VisuLogic (Xu et al., 2025) and Super-Clevr (Li et al.,
2023b) datasets. These datasets span a range of reasoning problems, including logical, mathematical, and commonsense reasoning. Using the chain-of-thought (CoT) paradigm, we prompted
GPT-4.1 (Achiam et al., 2023) to decompose each main question (with its final answer) into subquestions, each targeting a subanswer and a specific function. The prompt encourages structured,
step-by-step reasoning, ensuring that each subquestion is clear, answerable, and sequentially dependent. This process yields a set of subquestion–answer–function (subQAF) triples for each QA pair:
subQAFs = _{_ ( _qi, ai, fi_ ) _}_ _[k]_ _i_ =1 [, where each contains a subquestion] _[ q][i]_ [, its concise answer] _[ a][i]_ [, and the]
corresponding function label _fi_ . The prompt for generating subquestions are list in Appendix A.6.


2.3 DATA FILTERING AND ANNOTATION


Recent advances enable leveraging large pre-trained models for dataset construction, thanks to their
reasoning capabilities and ability to generate high-quality annotations at scale (Wang et al., 2024b).
While our dataset is automatically generated using a VLM to minimize manual effort, we employ a
rigorous two-stage human verification pipeline to ensure data quality and mitigate hallucination. In
the first stage, three expert annotator independently evaluate whether each subquestion is logically
structured and consistent with natural human reasoning. QA pairs with incoherent or inconsistent


3


decompositions are moved out. In the second stage, annotators verify and, if necessary, relabel the
cognitive function associated with each subquestion to ensure accurate alignment with the intended
mental process. Subanswers are further cross-checked with GPT-o3 (OpenAI, 2024) and adjudicated
by humans in cases of disagreement (details in Appendix A.5). This multi-step verification ensures
that each retained subQAF triplet represents a coherent, interpretable reasoning step grounded in
core cognitive functions. After this refinement, our final dataset comprises 1,409 main QA pairs
and 5,744 validated subQAF triplets. The detailed statistics of the dataset including the number of
triplet in training and testing set and distributions of each function can be found in Appendix A.3.
We analyze in-group subquestion diversity, and the results in the Appendix A.2 show that the eight
functions exhibit broad and overlapping distributions in phrasing patterns and token lengths, indicating no systematic surface-form differences.


3 DETECTION OF COGNITIVE FUNCTIONS


Using the CogVision dataset, we adopt a probing-based framework (Alain & Bengio, 2016; Belinkov, 2022; Tenney et al., 2019) to identify which attention heads in VLMs are associated with
specific functions in reasoning process. Specifically, for each functional annotated subquestion, we
extract head activations (see Subsection 3.1), train classifiers and compute accuracies to identify
contributing heads (see Subsection 3.2). Unlike prior work focusing on a single label, our formulation captures many-to-many relationships between heads and cognitive functions, enabling a more
nuanced analysis of functional specialization and overlap within the model.


3.1 HEAD FEATURE EXTRACTION


Given a large VLM _M_, we generate an answer _a_ _[M]_ _i_ for each subquestion _qi_ derived from a main
question _Qi_ . To support coherent multi-step reasoning, we include preceding subquestions and
their answers as contextual input, emulating the incremental reasoning process observed in human
cognition.


During inference, each input token is first mapped into an embedding and then propagated through
the transformer’s layers. At each layer, attention and feedforward operations update the residual
stream, which is ultimately decoded into token predictions. For each generated token _i_, we extract
attention head outputs _Xi_ = _{x_ _[m]_ _l_ _| l ∈_ 1 _, . . ., Nl, m ∈_ 1 _, . . ., Nh}_ across all layers, where _x_ _[m]_ _l_
denotes the value vector from the _m_ -th head in layer _l_ projected into the residual stream, with _Nh_
the number of heads per layer and _Nl_ the total number of layers.


Let _Nt_ denote the number of tokens in the generated answer _a_ _[M]_ _i_ [. To isolate semantically informative]
content relevant to reasoning, we select the top- _k_ most important tokens, determined by prompting
Qwen3-30B LLM (Yang et al., 2025), yielding an index set _Ik_ with _|Ik|_ = _k_ . For each index
_j ∈Ik_, we extract the corresponding attention head activations _Xj_, and compute the averaged
activation feature for the _m_ -th head in layer _l_ as ¯ _x_ _[m]_ _l_ = _k_ [1] � _j ∈Ikxml_ [. This results in a full set of]
","In SWAA (Sliding Window Attention Adaptation), Chain-of-Thought (CoT) is evaluated as a technique to improve model accuracy via reasoning, particularly in scenarios where Sliding Window Attention is applied. It is hypothesized that CoT during the full-attention decoding stage can compensate for limited contextual information gathered during the SWA-constrained prefilling stage. The study investigates this by comparing ""thinking"" models (with intrinsic CoT capabilities) against ""non-thinking"" variants to observe CoT's effect on SWA adaptation. Findings suggest CoT synergizes with ""Full Attention Decode,"" enabling longer reasoning traces.

In contrast, within the CogVision dataset, CoT is used as a *paradigm for data collection and decomposition*. GPT-4.1 is prompted using the CoT paradigm to decompose complex main questions into a series of subquestions, each with a subanswer and an associated cognitive function. This process aims to systematically capture the cognitive processes involved in multimodal reasoning by structuring problems into step-by-step reasoning components for dataset creation.

Therefore, the key difference is that in SWAA, CoT is a *reasoning strategy or intrinsic model capability being evaluated for its performance impact* under SWA constraints, while in CogVision, CoT is a *methodology used to structure and generate the dataset itself* by breaking down complex questions into subquestions that represent cognitive steps.",117,7285,7403,,3,,2026-01-02T20:16:46.818539
"Compare the ""learning rate"" schedules used for training the Transformer in ""Attention Is All You Need"" and the NAC models in their respective experiments.",Multi-Document Aggregation,"[Document 1]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_7
Content: compact architecture suitable for resource-constrained devices, thereby supporting localized safety.
We utilized three benchmark datasets: (i) PRONOSTIA
(Nectoux et al., 2012), (ii) XJTU-SY (Wang et al., 2018),
and (iii) HUST (Thuan & Hong, 2023). Training is performed on PRONOSTIA, while XJTU-SY and HUST are

used to assess cross-validation. We used the Score met
ric (Nectoux et al., 2012) to assess the performance. We
evaluate generalization using _Bearing 1_ from the first operating condition of each dataset. Figure 3 visualizes the
expected degradation alongside the outputs of NAC. On the
PRONOSTIA dataset, ContiFormer performed the best with



**3.4. Run Time and Memory Experiments**


We evaluate computational requirements on fixed-length
sequences of 1024 steps, 64-dimensional features, 4 heads,
and a Batch size of 1. Each model is run for ten for
ward passes on Google Colab T4-GPU, and we report the
mean runtime with standard deviation, throughput, and peak
memory usage. NAC occupies an intermediate position
in runtime relative to several CT-RNN models, including
GRU-ODE, CfC, and LTC. In terms of memory consumption, NAC uses significantly less memory than mTAN, with
NAC-2k being the least memory-consuming among the CTAttention models. Reducing NAC sparsity from 90% to 20%
has minimal effect on memory, decreasing usage slightly



7


**Neuronal Attention Circuit (NAC) for Representation Learning**



_Table 2._ Run-Time and Memory Benchmark Results


**Run-Time** **Throughput** **Peak Memory**
**Model**
(s) (seq/s) (MB)


RNN 1 _._ 8392 _[±]_ [0] _[.]_ [1933] 0.544 0.29
CT-RNN 7 _._ 1097 _[±]_ [0] _[.]_ [3048] 0.141 0.67
LSTM 2 _._ 6241 _[±]_ [0] _[.]_ [2906] 0.381 0.42
PhasedLSTM 4 _._ 9812 _[±]_ [0] _[.]_ [272] 0.201 0.80
GRU 3 _._ 216 _[±]_ [0] _[.]_ [2566] 0.311 0.54
GRU-ODE 12 _._ 2498 _[±]_ [0] _[.]_ [0525] 0.082 0.64
mmRNN 7 _._ 5852 _[±]_ [0] _[.]_ [2785] 0.132 0.96
LTC 14 _._ 643 _[±]_ [0] _[.]_ [2445] 0.068 0.99
CfC 6 _._ 0988 _[±]_ [0] _[.]_ [2135] 0.164 0.76


Attention 0 _._ 0016 _[±]_ [0] _[.]_ [0001] 625.00 16.86
MHA 0 _._ 0041 _[±]_ [0] _[.]_ [0001] 243.90 69.05
mTAN 0 _._ 0272 _[±]_ [0] _[.]_ [0054] 36.76 790.16
ODEFormer 0 _._ 0317 _[±]_ [0] _[.]_ [0016] 31.55 67.71
CTA 8 _._ 5275 _[±]_ [0] _[.]_ [2355] 0.117 1.43
ContiFormer 0 _._ 066 _[±]_ [0] _[.]_ [0075] 15.15 67.71


NAC-2k 7 _._ 3071 _[±]_ [0] _[.]_ [1547] 0.137 44.75
NAC-32k 7 _._ 2313 _[±]_ [0] _[.]_ [219] 0.138 549.86
NAC-PW 8 _._ 5649 _[±]_ [0] _[.]_ [0203] 0.117 5042.09
NAC-FC 0 _._ 0195 ~~_[±]_~~ ~~[0]~~ ~~_[.]_~~ ~~[0002]~~ 51.28 29.92
NAC-02s 7 _._ 252 _[±]_ [0] _[.]_ [2018] 0.138 151.54
NAC-09s 7 _._ 222 _[±]_ [0] _[.]_ [176] 0.139 150.85
NAC-Exact/05s/8k 7 _._ 4101 ~~_[±]_~~ ~~[0]~~ ~~_[.]_~~ ~~[1586]~~ 0.135 151.50
NAC-Euler 7 _._ 3367 _[±]_ [0] _[.]_ [1719] 0.136 152.22
NAC-Steady 7 _._ 2942 _[±]_ [0] _[.]_ [1451] 0.137 150.86


from 151.54 MB to 150.85 MB. In constrast, decreasing
the Top- _K_ selection from _PW_ to _k_ = 2 drastically reduces
memory consumption from 5042 MB to 44.75 MB, demonstrating the flexibility of NAC.
**Interpreting the Results:** From the experiments, we observe that increasing the sparsity of the NAC layer improves
the robustness of the system and leads to higher overall accuracy. Similarly, increasing the Top- _K_ interactions enhances
accuracy too; however, the benefits diminish as memory
consumption grows. Using Exact mode, Top- _K_ =8 with 50%
sparsity achieves the best balance between accuracy and
efficiency. Steady mode is the fastest, while Euler mode
handles adaptive temporal dynamics.


**4. Discussions**



This research is part of ongoing work on biologically plausible attention mechanisms and represents a pioneering step,
with limitations to be addressed in future work.

**Architectural improvement:** Currently, NAC uses predetermined wiring (AutoNCP) requiring three inputs: number of units (sensory + interneuron + motor), output motor
neurons, and sparsity, with typically 60% of units assigned
to sensory neurons. To integrate with the attention mechanism while preserving wiring, sensory units for _NN_ sensory
are set as unitssensory = � _d_ model0 _.−_ 6 0 _._ 5 � and backbone units as



are set as unitssensory = � _d_ model0 _.−_ 6 0 _._ 5 � and backbone units as

units _backbone_ = _d_ model + � _d_ 0model _._ 6 �, where _⌈·⌉_ and _⌊·⌋_ denote



units _backbone_ = _d_ model + � _d_ 0model _._ 6 �, where _⌈·⌉_ and _⌊·⌋_ denote

the ceiling and floor functions, respectively. This results
in a larger overall architectural size and increased runtime.



Future work will support user-defined NCPs configurations
or randomized wiring to enable more efficient architectures.
**Learnable sparse Top-** _**K**_ **selection:** Sparse Top- _K_ attention can miss important context, is sensitive to _k_, and may
be harder to optimize. A further limitation is that it still
computes the full _QK_ _[⊤]_ matrix, which can dominate the
cost for very long sequences. Future work includes adaptive
or learnable Top- _K_ selection, improved key scoring, and
hardware-aware optimization to strengthen accuracy and
robustness.


**5. Conclusion**

---

[Document 2]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_13
Content: which depends on ( _ϕ, ωτ_ ) both through the equilibrium _a_ _[∗]_

and the exponential term.
**Derivative with respect to** _ϕ_ **:** We obtain



_Interpretation_ : The gradient with respect to _ωτ_ contains
a transient term proportional to _te_ _[−][ω][τ][ t]_, which dominates
at intermediate times, and a steady-state contribution proportional to _−ϕ/ωτ_ [2][, which persists asymptotically. Thus,]
sensitivity to _ωτ_ is time-dependent, peaking before vanishing exponentially in the transient component.


**C.2. Gradient-Based Training**


Like Neural ODEs (Chen et al., 2018) and CT-RNNs
(Rubanova et al., 2019), NAC produces differentiable computational graphs and can be trained using gradient-based
optimization, such as the adjoint sensitivity method (Cao
et al., 2003) or backpropagation through time (BPTT) (LeCun et al., 1988). In this work, we use BPTT exclusively,
as the adjoint sensitivity method can introduce numerical
errors (Zhuang et al., 2020).


**C.3. Efficiency and Complexity**


Table 3 summarizes the computational complexity of different sequence models. For sequence prediction over length
_n_ with hidden dimension _k_, RNNs scale linearly, _O_ ( _nk_ ),
while Attention and NAC scale quadratically, _O_ ( _n_ [2] _k_ ).
ODE-based models, such as LNNs, incur an additional
multiplicative factor _S_ for the number of solver steps.
For single-time-step prediction, RNNs and LSTMs require
_O_ ( _k_ ), whereas Attention and NAC require _O_ ( _nk_ ) when
recomputing attention over the full sequence.



_∂at_

_∂ϕ_ [= 1] _[ −]_ _ω_ _[e]_ _τ_ _[−][ω][τ][ t]_



_∂at_



(46)
_ωτ_



_Interpretation_ : For large _ωτ_, the gradient with respect to _ϕ_
saturates quickly but shrinks to scale _O_ (1 _/ωτ_ ), potentially
slowing learning of _ϕ_ . Conversely, very small _ωτ_ leads
to large steady-state gradients, which may destabilize optimization.

**Derivative with respect to** _ωτ_ **:** Here, both the equilibrium
and the decay rate depend on _ωτ_, yielding



_∂at_
= _−_ _[ϕ]_
_∂ωτ_ _ωτ_ [2]



�1 _−_ _e_ _[−][ω][τ][ t]_ [�] _−_ ( _a_ 0 _−_ _a_ _[∗]_ ) _t e_ _[−][ω][τ][ t]_ _._ (47)



13


**Neuronal Attention Circuit (NAC) for Representation Learning**



_Table 3._ Sequence and time-step prediction complexity. _n_ is the
sequence length and _k_ is the hidden/model dimension.


**Model** **Sequence** **Time-step**
RNN _O_ ( _nk_ ) _O_ ( _k_ )
Attention _O_ ( _n_ [2] _k_ ) _O_ ( _nk_ )
LNN (ODEsolve) _O_ ( _nk · S_ ) _O_ ( _k · S_ )
NAC-Exact _O_ ( _n_ [2] _k_ ) _O_ ( _nk_ )
NAC-Euler _O_ ( _n_ [2] _k_ ) _O_ ( _nk_ )


**D. Evaluation**


**D.1. Related Works**


The brief description for related works is divided into four
subcategories.
**DT-RNNs:** RNN (Rumelhart et al., 1985) captures sequential dependencies in time-series data by updating a hidden state from the current observation and the previous
state. LSTM (Hochreiter & Schmidhuber, 1997) extends
RNNs with input, output, and forget gates, allowing the
network to maintain and update long-term memory, which
improves modeling of long-term dependencies in time-series
sequences. GRU (Cho et al., 2014) simplifies the LSTM
architecture by combining the forget and input gates into a
single update gate, allowing efficient modeling of long-term
dependencies in time-series sequences.
**CT-RNNs:** CT-RNN (Rubanova et al., 2019) model temporal dynamics using differential equations, enabling hidden
states to evolve continuously over time in response to inputs, which is particularly useful for irregularly sampled
time-series data. PhasedLSTM (Neil et al., 2016) introduces a time gate that updates hidden states according to
a rhythmic schedule, enabling efficient modeling of asynchronous or irregularly sampled time-series. GRU-ODE
(De Brouwer et al., 2019) extends the GRU to continuous
time, evolving hidden states via ODEs to handle sequences
with non-uniform time intervals. mmRNN (Lechner &
Hasani, 2022) combines short-term and long-term memory units to capture both fast-changing and slowly evolving
patterns in sequential data. LTC (Hasani et al., 2021) use
neurons with learnable, input-dependent time constants to
adapt the speed of dynamics and capture complex temporal
patterns in continuous-time data. CfC (Hasani et al., 2022)
approximate LTC dynamics analytically, providing efficient
continuous-time modeling without relying on numerical
ODE solvers.

**DT-Attentions:** Attention (Vaswani et al., 2017) computes
attention weights by measuring similarity between queries
and keys, scaling the results, and applying softmax to weigh
time-step contributions. Multi-Head Attention (Vaswani
et al., 2017) applies multiple parallel scaled dot-product
attention mechanisms, capturing different types of temporal
dependencies simultaneously for complex time-series modeling.



**CT-Attentions:** mTAN (Shukla & Marlin, 2021) learns
continuous-time embeddings and uses time-based attention
to interpolate irregular observations into a fixed-length representation for downstream encoder-decoder modeling. CTA
(Chien & Chen, 2021) generalizes discrete-time attention
to continuous-time by representing hidden states, context
vectors, and attention scores as functions whose dynamics are modeled via neural networks and integrated using
ODE solvers for irregular sequences. ODEFormer(d’Ascoli
et al., 2023) trains a sequence-to-sequence transformer on
synthetic trajectories to directly output a symbolic ODE
system from noisy, irregular time-series data. ContiFormer
(Chen et al., 2023) builds a continuous-time Transformer by
pairing ODE-defined latent trajectories with a time-aware
attention mechanism to model dynamic relationships in irregular time-series data.


**D.2. Ablations Details**

---

[Document 3]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_4
Content: **2.3. NAC as Universal Approximator**


We now establish the universal approximation capability of
NAC by extending the classical Universal Approximation
Theorem (UAT) (Nishijima, 2021) to the proposed mechanism. For brevity, we consider a network with a single
NAC layer processing fixed-dimensional inputs, though the
argument generalizes to sequences.


**Theorem 2** (Universal Approximation by NAC) **.** _Let K ⊂_
R _[n]_ _be a compact set and f_ : _K →_ R _[m]_ _be a continuous_
_function. For any ϵ >_ 0 _, there exists a neural network_
_consisting of a single NAC layer, with sufficiently large_
_model dimension dmodel, number of heads H, sparsity s,_
_and nonlinear activations, such that the network’s output_
_g_ : R _[n]_ _→_ R _[m]_ _satisfies_


sup _∥f_ ( _x_ ) _−_ _g_ ( _x_ ) _∥_ _< ϵ._ (20)
_x∈K_


_The proof is provided in Appendix B.3._


**3. Evaluation**


We evaluate the proposed architecture against a range of
baselines, including (DT & CT) RNN, (DT & CT) attention,
and multiple NAC ablation configurations. Experiments
are conducted across diverse domains, including irregular
time-series modeling, lane keeping of autonomous vehicles,
and Industry 4.0 prognostics. All results are obtained using 5-fold cross-validation, where models are trained using
BPTT (see Appendix C.2) on each fold and evaluated across
all folds. We report the mean ( _µ_ ) and standard deviation ( _σ_ )
to capture variability and quantify uncertainty in the predictions. Table 1 provides results for all experiments, and the
details of the baselines, ablation, environment utilized, the
data curation and preprocessing, and neural network architectures for all experiments are provided in the Appendix
D.3.


**3.1. Irregular Time-series**


We evaluate the proposed architecture on two irregular timeseries datasets: (i) Event-based MNIST; and (ii) Person
Activity Recognition (PAR).



For finite _t_, the exponential factor (1 _−_ _e_ _[−][ω][τ][ t]_ ) regulates the
buildup of attention, giving _ωτ_ a temporal gating role. Normalizing across all keys via _softmax_ yields attention weights
_αt_ = softmax( _at_ ), defining a valid probability distribution
where _ϕ_ amplifies or suppresses content alignments, and _ωτ_
shapes both the speed and saturation of these preferences.
As _t →∞_, the trajectory converges to the steady state


_a_ _[∗]_ _t_ [=] _[ϕ]_ _≈_ _[q][⊤][k]_ _,_ (18)

_ωτ_ ~~_√_~~ _dk_


which is analogous to scaled-dot attention under specific
parameterization when the backbone _NN_ backbone is configured as a linear projection such that _ϕ_ ( **u** ) = _q_ _[⊤]_ _k_ and
_ωτ_ ( _u_ ) = _[√]_ _dk_ (e.g., by setting NCP weights to emulate
bilinear forms and disabling nonlinearities). In general, the
nonlinear backbone allows for more expressive similarities,
with the approximation holding when trained to mimic dot
products.
**Attention output:** Finally, the attention output is computed
by integrating the attention weights with the value matrix:


NAC( _q, k, v_ ) = _αtvtdt_ (19)
� _T_


In practice, the integration is approximated using a Riemannstyle approach, where the weighted elements are computed
by multiplying each _vt_ with its corresponding _αt_ . These are
then summed and multiplied by a fixed pseudo-time step
_δt_, chosen as a scalar (typically between 0.5–1.0) hyperparameter during layer initialization. This yields a continuous
analogue of standard weighted sums, giving finer resolution
of the attention trajectory without altering the underlying
values. Sensitivity to attention output w.r.t _δt_ is visualized
in Appendix D.2.


2.2.1. EXTENSION TO MULTI-HEAD


To scale this mechanism to multi-head attention, we project
the input sequence into _H_ independent subspaces (heads)
of dimension _d_ model _/H_, yielding query, key, and value tensors ( _q_ [(] _[h]_ [)] _, k_ [(] _[h]_ [)] _, v_ [(] _[h]_ [)] ) for _h ∈{_ 1 _, . . ., H}_ . For each head,
pairwise logits are computed according to Eqns. 2,3 or 18,



5


**Neuronal Attention Circuit (NAC) for Representation Learning**


_Figure 2._ Illustration of the architecture of **(a)** Neuronal Attention Circuit mechanism ; **(b)** Multi-Head Extension



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/neuronal_attention_circuits.pdf-5-0.png)

**Event-based MNIST:** Event-based MNIST is the trans
formation of the widely recognized MNIST dataset with
irregular sampling added originally proposed in (Lechner
& Hasani, 2022). The transformation was done in two
steps: (i) flattening each 28×28 image into a time series
of length 784, and (ii) encoding the binary time series into
an event-based format by collapsing consecutive identical
values (e.g., 1,1,1,1 → (1, t=4)). This representation requires models to handle temporal dependencies effectively.
NAC-PW achieved first place with an accuracy of 96.64%,
followed by NAC-Exact/05s/8k at 96.12%. GRU-ODE and
ContiFormer ranked third with 96.04%.

**Person Activity Recognition (PAR):** We employed the
Localized Person Activity dataset from UC Irvine (Vidulin
et al., 2010). The dataset contains data from five participants,
each equipped with inertial measurement sensors sampled
every 211 ms. The goal of this experiment is to predict a
person’s activity from a set of predefined actions, making it
a classification task. All models performed well on this task,
with NAC-PW achieving 89.15% accuracy and taking first
place. NAC-Exact/05s/8k and GRU-ODE ranked second
with 89.01% accuracy, while NAC-02s ranked third with
88.84% mean accuracy.


**3.2. Lane-Keeping of Autonomous Vehicles**


Lane keeping in autonomous vehicles (AVs) is a fundamental problem in robotics and AI. Early works (Tiang et al.,
2018; Park et al., 2021) primarily emphasized accuracy,
often relying on large models. More recent research (Lechner et al., 2020; Razzaq & Hongwei, 2023) has shifted toward designing compact architectures suitable for resourceconstrained devices. The goal of this experiment is to create
a long causal structure between the road’s horizon and the
","The provided context does not contain information about the ""learning rate"" schedules used for training either the Transformer in ""Attention Is All You Need"" or the NAC models. It mentions that NAC models are trained using gradient-based optimization like Backpropagation Through Time (BPTT), but it does not specify the learning rate schedule.",112,2900,3013,,3,,2026-01-02T20:16:49.833968
"How does the ""Universal Approximation Theorem"" proof for NAC compare to the theoretical justifications for self-attention provided in ""Attention Is All You Need""?",Multi-Document Aggregation,"[Document 1]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_4
Content: **2.3. NAC as Universal Approximator**


We now establish the universal approximation capability of
NAC by extending the classical Universal Approximation
Theorem (UAT) (Nishijima, 2021) to the proposed mechanism. For brevity, we consider a network with a single
NAC layer processing fixed-dimensional inputs, though the
argument generalizes to sequences.


**Theorem 2** (Universal Approximation by NAC) **.** _Let K ⊂_
R _[n]_ _be a compact set and f_ : _K →_ R _[m]_ _be a continuous_
_function. For any ϵ >_ 0 _, there exists a neural network_
_consisting of a single NAC layer, with sufficiently large_
_model dimension dmodel, number of heads H, sparsity s,_
_and nonlinear activations, such that the network’s output_
_g_ : R _[n]_ _→_ R _[m]_ _satisfies_


sup _∥f_ ( _x_ ) _−_ _g_ ( _x_ ) _∥_ _< ϵ._ (20)
_x∈K_


_The proof is provided in Appendix B.3._


**3. Evaluation**


We evaluate the proposed architecture against a range of
baselines, including (DT & CT) RNN, (DT & CT) attention,
and multiple NAC ablation configurations. Experiments
are conducted across diverse domains, including irregular
time-series modeling, lane keeping of autonomous vehicles,
and Industry 4.0 prognostics. All results are obtained using 5-fold cross-validation, where models are trained using
BPTT (see Appendix C.2) on each fold and evaluated across
all folds. We report the mean ( _µ_ ) and standard deviation ( _σ_ )
to capture variability and quantify uncertainty in the predictions. Table 1 provides results for all experiments, and the
details of the baselines, ablation, environment utilized, the
data curation and preprocessing, and neural network architectures for all experiments are provided in the Appendix
D.3.


**3.1. Irregular Time-series**


We evaluate the proposed architecture on two irregular timeseries datasets: (i) Event-based MNIST; and (ii) Person
Activity Recognition (PAR).



For finite _t_, the exponential factor (1 _−_ _e_ _[−][ω][τ][ t]_ ) regulates the
buildup of attention, giving _ωτ_ a temporal gating role. Normalizing across all keys via _softmax_ yields attention weights
_αt_ = softmax( _at_ ), defining a valid probability distribution
where _ϕ_ amplifies or suppresses content alignments, and _ωτ_
shapes both the speed and saturation of these preferences.
As _t →∞_, the trajectory converges to the steady state


_a_ _[∗]_ _t_ [=] _[ϕ]_ _≈_ _[q][⊤][k]_ _,_ (18)

_ωτ_ ~~_√_~~ _dk_


which is analogous to scaled-dot attention under specific
parameterization when the backbone _NN_ backbone is configured as a linear projection such that _ϕ_ ( **u** ) = _q_ _[⊤]_ _k_ and
_ωτ_ ( _u_ ) = _[√]_ _dk_ (e.g., by setting NCP weights to emulate
bilinear forms and disabling nonlinearities). In general, the
nonlinear backbone allows for more expressive similarities,
with the approximation holding when trained to mimic dot
products.
**Attention output:** Finally, the attention output is computed
by integrating the attention weights with the value matrix:


NAC( _q, k, v_ ) = _αtvtdt_ (19)
� _T_


In practice, the integration is approximated using a Riemannstyle approach, where the weighted elements are computed
by multiplying each _vt_ with its corresponding _αt_ . These are
then summed and multiplied by a fixed pseudo-time step
_δt_, chosen as a scalar (typically between 0.5–1.0) hyperparameter during layer initialization. This yields a continuous
analogue of standard weighted sums, giving finer resolution
of the attention trajectory without altering the underlying
values. Sensitivity to attention output w.r.t _δt_ is visualized
in Appendix D.2.


2.2.1. EXTENSION TO MULTI-HEAD


To scale this mechanism to multi-head attention, we project
the input sequence into _H_ independent subspaces (heads)
of dimension _d_ model _/H_, yielding query, key, and value tensors ( _q_ [(] _[h]_ [)] _, k_ [(] _[h]_ [)] _, v_ [(] _[h]_ [)] ) for _h ∈{_ 1 _, . . ., H}_ . For each head,
pairwise logits are computed according to Eqns. 2,3 or 18,



5


**Neuronal Attention Circuit (NAC) for Representation Learning**


_Figure 2._ Illustration of the architecture of **(a)** Neuronal Attention Circuit mechanism ; **(b)** Multi-Head Extension



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/neuronal_attention_circuits.pdf-5-0.png)

**Event-based MNIST:** Event-based MNIST is the trans
formation of the widely recognized MNIST dataset with
irregular sampling added originally proposed in (Lechner
& Hasani, 2022). The transformation was done in two
steps: (i) flattening each 28×28 image into a time series
of length 784, and (ii) encoding the binary time series into
an event-based format by collapsing consecutive identical
values (e.g., 1,1,1,1 → (1, t=4)). This representation requires models to handle temporal dependencies effectively.
NAC-PW achieved first place with an accuracy of 96.64%,
followed by NAC-Exact/05s/8k at 96.12%. GRU-ODE and
ContiFormer ranked third with 96.04%.

**Person Activity Recognition (PAR):** We employed the
Localized Person Activity dataset from UC Irvine (Vidulin
et al., 2010). The dataset contains data from five participants,
each equipped with inertial measurement sensors sampled
every 211 ms. The goal of this experiment is to predict a
person’s activity from a set of predefined actions, making it
a classification task. All models performed well on this task,
with NAC-PW achieving 89.15% accuracy and taking first
place. NAC-Exact/05s/8k and GRU-ODE ranked second
with 89.01% accuracy, while NAC-02s ranked third with
88.84% mean accuracy.


**3.2. Lane-Keeping of Autonomous Vehicles**


Lane keeping in autonomous vehicles (AVs) is a fundamental problem in robotics and AI. Early works (Tiang et al.,
2018; Park et al., 2021) primarily emphasized accuracy,
often relying on large models. More recent research (Lechner et al., 2020; Razzaq & Hongwei, 2023) has shifted toward designing compact architectures suitable for resourceconstrained devices. The goal of this experiment is to create
a long causal structure between the road’s horizon and the

---

[Document 2]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_8
Content: the ceiling and floor functions, respectively. This results
in a larger overall architectural size and increased runtime.



Future work will support user-defined NCPs configurations
or randomized wiring to enable more efficient architectures.
**Learnable sparse Top-** _**K**_ **selection:** Sparse Top- _K_ attention can miss important context, is sensitive to _k_, and may
be harder to optimize. A further limitation is that it still
computes the full _QK_ _[⊤]_ matrix, which can dominate the
cost for very long sequences. Future work includes adaptive
or learnable Top- _K_ selection, improved key scoring, and
hardware-aware optimization to strengthen accuracy and
robustness.


**5. Conclusion**


In this paper, we introduce the Neuronal Attention Circuit
(NAC), a biologically inspired attention mechanism that
reformulates attention logits as the solution to a first-order
ODE modulated by nonlinear, interlinked gates derived from
repurposing _C.elegans_ nematode NCPs. NAC bridges discrete attention with continuous-time dynamics, enabling
adaptive temporal processing without the limitations inherent to traditional scaled dot-product attention. Based on the
solution to ODE, we introduce three computational modes:
(i) Euler, based on _explicit Euler_ integration; (ii) Exact, providing closed-form solutions; and (iii) Steady, approximating equilibrium states. In addition, a sparse Top- _K_ pairwise
concatenation scheme is introduced to mitigate the memory intensity. Theoretically, we establish NAC’s log-state
stability, exponential error bounds, and universal approximation, thereby providing rigorous guarantees of convergence
and expressiveness. Empirical evaluations demonstrate that
NAC achieves state-of-the-art performance across diverse
tasks, including irregularly sampled time-series benchmarks,
autonomous vehicle lane-keeping, and industrial prognostics. Moreover, NAC occupies an intermediate position
between CT-RNNs and CT-Attention, offering robust temporal modeling while requiring less runtime than CT-RNNs
and less memory than CT-Attention models.


**Reproducibility Statement**


The code for reproducibility is available at
[https://github.com/itxwaleedrazzaq/](https://github.com/itxwaleedrazzaq/neuronal_attention_circuit)

[neuronal_attention_circuit](https://github.com/itxwaleedrazzaq/neuronal_attention_circuit)


**Impact Statement**


The work addresses the growing field of continuous-time
attention and pioneers a biologically plausible mechanism.
It encourages research into sparse, adaptive networks that
resemble natural wiring. From a societal perspective, it
supports more robust AI in resource-limited settings, but it
also raises ethical concerns when applied to areas such as
surveillance or autonomous systems.



8


**Neuronal Attention Circuit (NAC) for Representation Learning**



**References**


Introduction to self-driving cars. URL
[https://www.udacity.com/course/](https://www.udacity.com/course/intro-to-self-driving-cars--nd113)
[intro-to-self-driving-cars--nd113.](https://www.udacity.com/course/intro-to-self-driving-cars--nd113)


Aguiar-Conraria, L. and Soares, M. J. The continuous
wavelet transform: Moving beyond uni-and bivariate analysis. _Journal of economic surveys_, 28(2):344–375, 2014.


Beltagy, I., Peters, M. E., and Cohan, A. Longformer: The long-document transformer. _arXiv preprint_
_arXiv:2004.05150_, 2020.


Bojarski, M., Del Testa, D., Dworakowski, D., Firner, B.,
Flepp, B., Goyal, P., Jackel, L. D., Monfort, M., Muller,
U., Zhang, J., et al. End to end learning for self-driving
cars. _arXiv preprint arXiv:1604.07316_, 2016.


Brockman, G., Cheung, V., Pettersson, L., Schneider, J.,
Schulman, J., Tang, J., and Zaremba, W. Openai gym.
_arXiv preprint arXiv:1606.01540_, 2016.


Cao, Y., Li, S., Petzold, L., and Serban, R. Adjoint sensitivity analysis for differential-algebraic equations: The
adjoint dae system and its numerical solution. _SIAM_
_journal on scientific computing_, 24(3):1076–1089, 2003.


Chen, R. T., Rubanova, Y., Bettencourt, J., and Duvenaud,
D. K. Neural ordinary differential equations. _Advances_
_in neural information processing systems_, 31, 2018.


Chen, Y., Ren, K., Wang, Y., Fang, Y., Sun, W., and Li, D.
Contiformer: Continuous-time transformer for irregular
time series modeling. _Advances in Neural Information_
_Processing Systems_, 36:47143–47175, 2023.


Chien, J.-T. and Chen, Y.-H. Continuous-time attention for
sequential learning. In _Proceedings of the AAAI confer-_
_ence on artificial intelligence_, volume 35, pp. 7116–7124,
2021.


Cho, K., Van Merrienboer, B., Gulcehre, C., Bahdanau,¨
D., Bougares, F., Schwenk, H., and Bengio, Y. Learning phrase representations using rnn encoder-decoder
for statistical machine translation. _arXiv preprint_
_arXiv:1406.1078_, 2014.


d’Ascoli, S., Becker, S., Mathis, A., Schwaller, P., and
Kilbertus, N. Odeformer: Symbolic regression of dynamical systems with transformers. _arXiv preprint_
_arXiv:2310.05573_, 2023.


De Brouwer, E., Simm, J., Arany, A., and Moreau, Y.
Gru-ode-bayes: Continuous modeling of sporadicallyobserved time series. _Advances in neural information_
_processing systems_, 32, 2019.



Deng, L. The mnist database of handwritten digit images
for machine learning research [best of the web]. _IEEE_
_signal processing magazine_, 29(6):141–142, 2012.


Ding, Y., Jia, M., Miao, Q., and Huang, P. Remaining
useful life estimation using deep metric transfer learning
for kernel regression. _Reliability Engineering & System_
_Safety_, 212:107583, 2021.


Hasani, R., Lechner, M., Amini, A., Rus, D., and Grosu,
R. Liquid time-constant networks. In _Proceedings of the_
_AAAI Conference on Artificial Intelligence_, volume 35,
pp. 7657–7666, 2021.


Hasani, R., Lechner, M., Amini, A., Liebenwein, L., Ray,
A., Tschaikowski, M., Teschl, G., and Rus, D. Closed
form continuous-time neural networks. _Nature Machine_

_Intelligence_, 4(11):992–1003, 2022.


Hochreiter, S. The vanishing gradient problem during learning recurrent neural nets and problem solutions. _Interna-_
_tional Journal of Uncertainty, Fuzziness and Knowledge-_
_Based Systems_, 6(02):107–116, 1998.


Hochreiter, S. and Schmidhuber, J. Long short-term memory.
_Neural computation_, 9(8):1735–1780, 1997.


Hong, H. S. and Thuan, N. Hust bearing: a practical dataset
for ball bearing fault diagnosis. _Mendeley Data_, 3, 2023.


John, F. On integration of parabolic equations by difference methods: I. linear and quasi-linear equations for the
infinite interval. _Communications on Pure and Applied_
_Mathematics_, 5(2):155–211, 1952.

---

[Document 3]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_3
Content: Scores: _S ←_ _Q · K_ _[⊤]_

Effective Top- _K_ : _K_ eff _←_ min( _K, Tk_ )
Indices: _I_ topk _←_ top ~~k~~ ( _S, K_ eff)
Gather: _K_ selected _←_ gather( _K, I_ topk) _∈_ R _[B][×][H][×][T][q]_ _[×][K]_ [eff] _[×][D]_

Tiled: _Q_ tiled _←_ tile( _Q, K_ eff) _∈_ R _[B][×][H][×][T][q]_ _[×][K]_ [eff] _[×][D]_

Concatenate: _U_ topk _←_ [ _Q_ tiled; _K_ selected ] _∈_ R _[B][×][H][×][T][q]_ _[×][K]_ [eff] _[×]_ [2] _[D]_

**return** _U_ topk


**2.2. Designing the Neural Network**


We now outline the design of a neural network layer guided
by the preceding analysis. The process involves five steps:
(i) repurposing NCPs; (ii) input curation; (iii) construction
of the time vector ( _t_ ); (iv) computing attention logits and
weights; and (v) generating the attention output. Figure 2
provides a graphical overview of NAC.
**Repurposing NCPs:** We repurpose the NCPs framework
by converting its fixed, biologically derived wiring (see Figure 1(a)) into a flexible recurrent architecture that allows
configurable input–output mappings. Instead of enforcing
a static connectome, our approach exposes adjacency matrices as modifiable structures defining sparse input and
recurrent connections. This enables selective information

routing across neuron groups while retaining the original circuit topology. Decoupling wiring specifications from model
instantiation allows dynamic connectivity adjustments to
accommodate different input modalities without full retraining. Algorithm 1 summarizes the steps for repurposing the
NCPs wiring mechanism. Key features include group-wise
masking for neuron isolation, adaptive remapping of inputs
and outputs for task-specific adaptation, and tunable sparsity
_s_ to balance expressiveness and efficiency.
In our implementation, the sensory neuron gate ( _NN_ sensory)
projects the _q_, _k_, and _v_ representations (see Figure 1(b)).
This enables sensory neurons to maintain structured, contextaware representations rather than collapsing inputs into fully



connected layers. As a result, the network preserves locality
and modularity, which improves information routing.


_NN_ sensory = NCPCell( _G_ input = [ _Ns_ ] _, G_ output = [ _Ns_ ] _,_

_D_ = [ _Ni, Nc, Nm_ ] _, s_ )
(13)
The inter-to-motor pathways form a backbone network
( _NN_ backbone) with branches that compute _ϕ_ and _ωτ_ (see
Figure 1(c)). Instead of learning _ϕ_ and _ωτ_ independently,
this backbone allows the model to learn shared representations, enabling multiple benefits: (i) separate head layers
enable the system to capture temporal and structural dependencies independently; (ii) accelerates convergence during
training.


_NN_ backbone = NCPCell( _G_ input = [ _Ni_ ] _, G_ output = [ _Nm_ ] _,_

_D_ = [ _Ns_ ] _, s_ )
(14)
The output heads are defined as:


_ϕ_ = _σ_ ( _NN_ backbone( **u** )) (15)

_ωτ_ = softplus( _NN_ backbone( **u** )) + _ε,_ _ε >_ 0 (16)


Here, _ϕ_ serves as a _content–target gate_ head, where the
sigmoid function _σ_ ( _·_ ) determines the target signal strength.
In contrast, _ωτ_ is a strictly positive _time–constant gate_ head
that controls the rate of convergence and the steady-state
amplitude. Conceptually, this parallels recurrent gating: _ϕ_
regulates _what_ content to emphasize, while _ωτ_ governs _how_
_quickly_ and _to what extent_ it is expressed.
**Input Curation:** We experimented with different
strategies for constructing query–key inputs. Initially, we implemented full pairwise concatenation,
where queries _Q ∈_ R _[B][×][H][×][T][q][×][D]_ are combined with
all keys _K_ _∈_ R _[B][×][H][×][T][k][×][D]_ to form a joint tensor
_U ∈_ R _[B][×][H][×][T][q][×][T][k][×]_ [2] _[D]_ . While this preserved complete
feature information and enabled expressive, learnable
similarity functions, it was memory-intensive, making it impractical for longer sequences. To mitigate this, we applied
a sparse Top- _K_ optimization: for each query, we compute
pairwise scores _S_ = _Q · K_ _[⊤]_ _∈_ R _[B][×][H][×][T][q][×][T][k]_, select the
Top- _K_ eff = min( _K, Tk_ ) keys, and construct concatenated
pairs _U_ topk _∈_ R _[B][×][H][×][T][q][×][K]_ [eff] _[×]_ [2] _[D]_ . This approach preserves
the most relevant interactions while substantially reducing
memory requirements in the concatenation and subsequent
backbone processing stages, allowing the method to scale
linearly with the sequence length in those components.
However, the initial computation of _S_ remains quadratic
(see Appendix C.3). Algorithm 2 outlines the steps required
for input curation.
**Time Vector:** NAC builds on continuous-depth models
as (Hasani et al., 2022) that adapt their temporal dynamics to the task. It constructs an internal, normalized
pseudo-time vector _t_ pseudo using a sigmoidal transformation,
_t_ pseudo = _σ_ ( _ta · t_ + _tb_ ), where _ta_ and _tb_ are learnable affine



4


**Neuronal Attention Circuit (NAC) for Representation Learning**



parameters and _σ_ is the sigmoid function. For time-varying
datasets (e.g., irregularly sampled series), each time point
_t_ is derived from the sample’s timestamp, while for tasks
without meaningful timing, _t_ is set to 1. The resulting _t_ pseudo
lies in [0 _,_ 1] and provides a smooth, bounded representation
of time for modulating the network’s dynamics.
**Attention logits and weights:** Starting from Eqn. 3,
consider the trajectory of a query–key pair with initial
condition _a_ 0 = 0:



_at_ = _[ϕ]_

_ωτ_



�1 _−_ _e_ _[−][ω][τ][ t]_ [�] _,_ (17)



followed by the _softmax_ normalization to calculate attention weights. The resulting attention weights _αt_ [(] _[h]_ [)] are then
used to integrate with the value vector _v_ [(] _[h]_ [)], producing headspecific attention outputs. Finally, these outputs are concatenated and linearly projected back into the model dimension.
This formulation ensures that each head learns distinct dynamic compatibilities governed by its own parameterization
of _ϕ_ and _ωτ_, while the aggregation across heads preserves
the expressive capacity of the standard multi-head attention
mechanism.


**2.3. NAC as Universal Approximator**


We now establish the universal approximation capability of
NAC by extending the classical Universal Approximation
Theorem (UAT) (Nishijima, 2021) to the proposed mechanism. For brevity, we consider a network with a single
NAC layer processing fixed-dimensional inputs, though the
argument generalizes to sequences.
","The provided context states that the Neuronal Attention Circuit (NAC) establishes universal approximation capability by extending the classical Universal Approximation Theorem (UAT). Theorem 2 formally states that for any continuous function on a compact set, there exists a single NAC layer network that can approximate it within an arbitrary epsilon, given sufficiently large model dimension, number of heads, sparsity, and nonlinear activations. The proof for this is mentioned to be in Appendix B.3.

However, the provided context does not contain information regarding the theoretical justifications for self-attention as provided in the paper ""Attention Is All You Need,"" nor does it compare NAC's UAT proof to those justifications.",127,4066,4195,,3,,2026-01-02T20:16:54.031016
"If ""Full Attention Decode"" is applied to a ""Qwen3-4B-Thinking"" model, how does this specifically impact the ""performance-efficiency trade-off"" compared to naive SWA?",Multi-Hop Chain Reasoning,"[Document 1]
Source: sliding_window_attention_annotated_chunks_2k.jsonl
Chunk ID: sliding_window_attention_annotated_chunks_2k_4
Content: **Keep First becomes optional rather than essen-**
**tial.** Before fine-tuning, Keep First is crucial for
stability under SWA. But after SFT, it only provides
minor additional improvement. With FA Decode,
adding _k_ = 100 (row 7) improves over _k_ = 0
(row 3) only 4.5%, and if further combining FA
layers, it almost offers no improvements (row 11
and row 8).


**Effect of sliding window size.** Row 3 and 5
shows that increasing the window from 2k to 4k
with FA Decode improves thinking-model accuracy from 57 _._ 9 to 62 _._ 6. This mirrors the non-SFT
trend that larger windows help, but the dominant
improvements still come from FA Decode and Interleaving Layers.

So, we answer RQ2: fine-tuning brings remarkably high performance restoration, provided we ap


ply **FA Decode**, **Interleaving Layers**, or a combination thereof, while **Keep First** becomes optional.
And the improvement brought by SFT under each
configuration varies significantly, meaning a nearoptimal training-free configuration need not remain
optimal after SFT, and vice versa.


**5.3** **Performance–efficiency Trade-offs and**
**Recommended Recipes**


Although integrating more methods can typically
achieve higher accuracy, it introduces more overhead, indicating that the efficiency of each recipe
must also be evaluated. To assess the performanceefficiency trade-off of different SWAA configurations, we evaluate time-to-first-token (TTFT), timeper-output-token (TPOT), total throughput, and average running time per request. Concretely, we
benchmark Qwen3-4B-Thinking on a single H100
GPU using vLLM’s bench_serve utility (Kwon
et al., 2023) with random input data and 100 total
requests. The prompt length and output length are
set to 128k and 512 tokens, respectively, representing a typical long-context QA setting.



7


```
### Figure - Accuracy vs. Time with SFT
![Accuracy graph](sliding_window_attention.pdf-7-0.png)

**Caption:**  
The figure shows the accuracy of a model with and without Sliding Factor Token (SFT) over time. Different configurations of window size, FA layers, and decode settings are compared.

**Description:**
- The x-axis represents time in seconds, ranging from 0.5 to 3.5 seconds.
- The y-axis represents accuracy in percentage.
- Two lines are plotted: a solid line represents the accuracy with SFT, and a dashed line represents the accuracy without SFT.
- Different colored markers indicate different configurations of the model:
  - Blue markers represent ""Full"" window, 0 FA layers, and False decode.
  - Green markers represent ""2k"" window, 0 FA layers, and False decode.
  - Orange markers represent ""2k"" window, 0 FA layers, and True decode.
  - Pink markers represent ""2k"" window, 1/2 FA layers, and False decode.
  - Light blue markers represent ""2k"" window, 1/2 FA layers, and True decode.
- The accuracy with SFT generally increases with time for all configurations.
- The accuracy without SFT is consistently lower than the accuracy with SFT.

```
### Figure - Accuracy vs. Time with and without SFT
![Accuracy graph](sliding_window_attention.pdf-7-1.png)

**Caption:**  
The figure shows the accuracy of a model with and without Sliding Factor Token (SFT) as a function of time window. Different configurations of FA layers and decode settings are also shown.

**Description:**
- The graph plots accuracy (%) on the y-axis against time (s) on the x-axis.
- Data points are color-coded to represent different configurations: w/ SFT (blue) and w/o SFT (black).
- Each color represents a specific combination of window size, FA layers, FA decode, and decode setting.
- The blue line represents the performance with SFT, while the black line represents the performance without SFT.
- The x-axis ranges from 0.5s to 3.5s, indicating the time window used for analysis.
- The y-axis ranges from 20% to 60%, representing the accuracy percentage.

(a) Qwen3-4B-Thinking (b) Qwen3-4B-Instruct


Figure 2: Accuracy and inference time of each configuration of Qwen3-4B on LongMemEval



To visualize the performance-efficiency tradeoff, Figure 2 plots each configuration’s accuracy
on LongMemEval_24k (Wu et al., 2024) against its
average running time, while detailed TTFT, TPOT,
and throughput statistics for each configuration are
provided in Appendix E. We draw a line between
the full-attention point and the naive-SWA point
as a baseline curve: configurations above this line
offer a better accuracy-latency balance intuitively.
For configurations with nearly identical time costs,
we display only the one with the highest accuracy.
Since **Keep First** _k_ has negligible impact on runtime (Appendix E), all plotted configurations fix
_k_ = 10.


From Figure 2, we observe that many configurations in Figure 2 achieve a clearly better
performance-efficiency ratio than baselines. And
for the thinking model, more points lie above the
baseline curve compared to non-thinking, indicating that **CoT** generally has a positive effect on improving the performance-efficiency ratio of SWAA.


Thus, we finally answer RQ3: many SWAA
configurations all reach excellent performanceefficiency trade-off, but there is no single metric to
quantify such trade-off to decide the globally optimal one. We therefore summarize **recommended**

**SWA adaptation recipes** tailored to various deployment scenarios in Table 3. And we must note
that specific parameters should be flexibly set to
meet application-specific requirements, without the
need to follow our experimental parameters (e.g.,



a 2k window, _k_ = 10). For example, you can increase the window size to 4k or _k_ to 128 for higher
accuracy and acceptable additional overhead.


**6** **Conclusion**


In this work, we validate the feasibility of adapting
full-attention pretrained LLMs to Sliding Window
Attention (SWA) for better efficiency, offering a
cost-effective alternative that avoids training sparseattention models from scratch. By systematically
deconstructing the adaptation process, we identify that the catastrophic degradation observed in
naive implementations can be effectively mitigated
through synergistic combinations of auxiliary methods. Our extensive experiments across the Qwen
and Llama families demonstrate that while trade
offs between computational overhead and model
performance are inevitable, optimized configurations can get an excellent performance-efficiency
balance.


**7** **Limitations**


We speculate that the ideal reasoning trajectory of
the model adapted to SWA should be longer than
the original model, to compensate for the information loss caused by SWA. That means, using
the answers generated by the original model as
fine-tuning data may not be the optimal training
method. Rather, RL methods like GRPO (Shao
et al., 2024) might further help the model adapted
to SWA learn a better reasoning trajectory. How


8


ever, we did not experiment with them since they
are too time-consuming and unstable.
We have not yet implemented the KV cache eviction (or overwriting) mechanism when using SWA;
that is, although the speed is improved, memory
usage is not effectively reduced.
Further experiments may be needed to confirm
whether our conclusions generalize to larger model
sizes, such as 70B.


**References**


Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei
Hou, Jie Tang, Yuxiao Dong, and Juanzi Li. 2024a.
[Longalign: A recipe for long context alignment of](https://arxiv.org/abs/2401.18058)
[large language models.](https://arxiv.org/abs/2401.18058)


Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu,
Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao
Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang,
[and Juanzi Li. 2023. Longbench: A bilingual, multi-](https://arxiv.org/abs/2308.14508)
[task benchmark for long context understanding.](https://arxiv.org/abs/2308.14508)

---

[Document 2]
Source: sliding_window_attention_annotated_chunks_2k.jsonl
Chunk ID: sliding_window_attention_annotated_chunks_2k_3
Content: **Combinations exhibit strong synergy.** Recipes
that combine multiple methods deliver large gains:


  - **FA Decode + Keep First** _k_ substantially improves over naive SWA (rows 7–9), recovering roughly half to two-thirds of the gap on
the thinking model as _k_ increases. However,
increasing _k_ from 100 to 1000 yields almost
no improvement, indicating that _k_ does not
need to be exceedingly large.


  - **Interleaving Layers + FA Decode** is
markedly stronger (row 13), recovering most
of the gap for the thinking model.


  - **FA Decode + Interleaving Layers + Keep**
**First** _k_ perform best (rows 18). The thinking
model recovers close to 90% of the FA gap
even at 2k window.


**CoT synergizes with FA Decode.** Under recipes
that include FA Decode, the thinking model consistently benefits more than the non-thinking model
(rows 13 and 18), suggesting that CoT synergizes
with FA Decode: preserving global attention at
decoding time enables longer reasoning traces to
capitalize on context processed by SWA, confirming our hypothesis in Section 3.1.


**Sliding window size affects, but is not the deci-**
**sive role.** With FA Decode + Keep First _k_, accuracy improves as the window increases (rows 7,
14, 15), though benefits are modest until 8k. When
added with interleaving FA layers, moving from a
2k to a 4k window is enough to close the remaining
gap on the thinking model (row 22 matches the FA
baseline), indicating that FA Decode and Interleaving are the dominant drivers, and larger windows
mainly smooth residual error.



5


Table 1: Qwen3-4B-Thinking and Qwen3-4B-Instruct results on LongMemEval without SFT


**No.** **window size** **FA layers** **keep first** **FA decode** **Acc think** **Acc non-think**


0 Full [] 0 False **73.0** **62.0**
1 2k [] 0 False 3.2 11.0
2 8k [] 0 False 13.2 19.8


3 2k [] 10 False 16.0 15.6
4 2k [] 0 True 11.8 14.2
5 2k [1, 3, 5, ...] 0 False 13.4 18.4
6 8k [] 0 True 26.2 25.0


7 2k [] 10 True 38.2 20.6
8 2k [] 100 True 50.0 17.8
9 2k [] 1000 True 50.0 20.2
10 2k [0, 2, 4, ...] 10 False 17.0 14.8
11 2k [0, 2, 4, ...] 0 True 32.2 26.0
12 2k [1, 3, 5, ...] 10 False 25.8 36.4
13 2k [1, 3, 5, ...] 0 True 59.2 34.8
14 4k [] 10 True 38.0 24.4
15 8k [] 10 True 49.2 35.2


16 2k [0, 2, 4, ...] 10 True 36.0 17.2
17 2k [1, 3, 5, ...] 10 True 65.0 **53.6**
18 2k [1, 3, 5, ...] 100 True **68.8** 50.6
19 2k [1, 5, 9, ...] 10 True 53.2 31.4
20 2k [1, 9, 17, ...] 10 True 36.4 18.8
21 2k [3, 7, 11, ...] 10 True 54.2 34.6
22 4k [1, 3, 5, ...] 100 True **73.0** **54.2**
23 8k [1, 3, 5, ...] 100 True **71.6** **56.6**



**SWA/FA layer selection has large impacts.** Selecting less layers for FA, e.g. only one fourth or
eighth (row 19, 20), though more efficient, will
significantly decrease the improvement brought by
Interleaving Layers. More importantly, for Qwen34B, configuring _odd-numbered_ layers with full attention is significantly better than _even-numbered_
layers (row 11, 13, 16, 17). However, surprisingly,
this result is reversed for Qwen3-30B-A3B and
Llama3.1-8B-Instruct (row 10 and 11 in Table 4
and 5). This suggests that layer functionalities differ across model families and sizes, necessitating
model-specific layer selection strategies, as discussed in Section 3.2.


Therefore, we answer RQ1: adapting an FA
LLM to SWA is feasible even without any training. But it requires specific combinations of at
least 2 methods, which could be less efficient for
inference.



**5.2** **SWA Adaptation With Fine-tuning**


We next evaluate SWA-aware supervised finetuning, which is expected to provide higher improvement. Table 2 reports LongMemEval_24k
accuracy after SFT under various SWAA configurations. The original full-attention model is also
fine-tuned as a baseline (row 0). Since training is
relatively time-consuming, we only test a representative subset of configurations. Our findings are as
follows:


**Fine-tuning substantially lifts all SWA config-**
**urations.** Comparing all the fine-tuning results
with non-training ones, it is clear that fine-tuning
consistently provides great improvement. However,
simply fine-tuning with naive SWA remains insufficient, only achieving 18.8% and 23.8% accuracy
(row 1).


**FA Decode and Interleaving Layers emerge as**
**dominant components.** After SFT, FA Decode
and Interleaving Layers provide the largest gains.



6


Table 2: Qwen3-4B-Thinking and Qwen3-4B-Instruct results on LongMemEval with SFT


**No.** **window size** **FA layers** **keep first** **FA decode** **Acc think** **Acc non-think**


0 Full [] 0 True **74.6** **63.4**
1 2k [] 0 False 18.8 23.8


2 2k [] 10 False 15.6 /
3 2k [] 0 True 57.9 42.0
4 2k [1, 3, 5, ...] 0 False 63.6 54.6
5 4k [] 0 True 62.6 /


6 2k [] 10 True 56.7 /
7 2k [] 100 True 62.2 42.6
8 2k [1, 3, 5, ...] 0 True **73.2** **58.8**
9 2k [0, 2, 4, ...] 0 True **66.0** /
10 2k [1, 5, 9, ...] 0 True **68.8** 47.0


11 2k [1, 3, 5, ...] 100 True **73.2** **61.4**


Table 3: Recommended SWA adaptation recipes for different needs and scenarios. � means optional.


**Training** ? **Thinking Model?** **Prefer?** **FA Decode** **Interleaving Layers** **Keep First**


No No Any � � �
No Yes Efficiency � � �
No Yes Accuracy � � �


Yes Any Efficiency � � �
Yes Any Accuracy � � �



Using FA Decode alone (row 3) or using Interleaving Layers alone (row 4) both get high accuracy.
And combining both (row 8) further boosts performance to 73 _._ 2 (think) and 58 _._ 8 (non-think), nearly
matching the full-attention SFT baseline in row 0.


**Keep First becomes optional rather than essen-**
**tial.** Before fine-tuning, Keep First is crucial for
stability under SWA. But after SFT, it only provides
minor additional improvement. With FA Decode,
adding _k_ = 100 (row 7) improves over _k_ = 0
(row 3) only 4.5%, and if further combining FA
layers, it almost offers no improvements (row 11
and row 8).


**Effect of sliding window size.** Row 3 and 5
shows that increasing the window from 2k to 4k
with FA Decode improves thinking-model accuracy from 57 _._ 9 to 62 _._ 6. This mirrors the non-SFT
trend that larger windows help, but the dominant
improvements still come from FA Decode and Interleaving Layers.

---

[Document 3]
Source: sliding_window_attention_annotated_chunks_2k.jsonl
Chunk ID: sliding_window_attention_annotated_chunks_2k_2
Content: **3.4** **Fine-tuning**


This is the most natural way to mitigate traininginference mismatch. Apparently, the model should
be fine-tuned while SWA is applied, so that the
model’s parameters can be trained to better adapt
to SWA. Meanwhile, the training data must be longcontext tasks, where SWA actually works.
However, most available long-context datasets
only contain brief ground-truth answers, lacking the reasoning steps required for fine-tuning
a ""thinking"" model. Since our goal is to _restore_
the model’s original capabilities under SWA rather
than teach it new ones, instead of directly using the
original dataset, we adopt an approach similar to
self-distillation (Yang et al., 2024). Specifically, we
utilize the original full-attention model to generate
new answers for the dataset’s questions, and these
generated answers are then filtered for correctness
using GPT-5-Mini (OpenAI, 2025), to make up our
training dataset. For each question, we sample 4
answers with temperature 1, because we find this
strategy is slightly better than generating only one
answer with temperature 0.


**4** **Experiment Setup**


We organize our experiments around three research
questions:


**RQ1: Is SWA adaptation feasible without any**
**additional training?** We evaluate whether
an FA LLM can be adapted to SWA using
only inference-time modifications, and which
combinations of techniques are necessary.


**RQ2: How much does fine-tuning with SWA im-**
**prove performance?** We study the effect of
SWA-aware fine-tuning on long-context performance and identify which components of
SWAA are still required.



**RQ3: Which SWAA configurations achieves the**
**optimal performance-efficiency trade-offs?**
We evaluate how different SWAA configurations trade off accuracy against inference la
tency.


**4.1** **Models**


Our primary experiments use Qwen3-4B-Thinking
and Qwen3-4B-Instruct (Team, 2025b). The Thinking variant enforces chain-of-thought (CoT) style
reasoning, whereas the Instruct variant usually just
answers briefly. To ensure generality, we additionally evaluate Qwen3-30B-A3B-Thinking, Qwen330B-A3B-Instruct (Team, 2025b), and Llama3.18B-Instruct (Touvron et al., 2023).

All models are served with vLLM in float16

precision using a batch size of 64. We use greedy
decoding (temperature = 0) for all evaluations. In
preliminary experiments, we observed that vLLM
yields slightly lower (about 1% to 5%) scores
than HuggingFace Transformers due to precisionrelated discrepancies.


**4.2** **Evaluation Dataset**


SWA is identical to full attention when the context

length is within the window size. Even if the model
is fine-tuned, we can pre-calculate the prompt
length and simply disable the LoRA adapters for
short prompts to get completely the same response
as the original model. Therefore, our experiments
focus exclusively on long-context benchmarks with
inputs exceeding 16k tokens, as re-evaluating models on standard short-context benchmarks (e.g.,
MMLU (Hendrycks et al., 2021), GPQA (Rein
et al., 2023)) is completely unnecessary.
Since we find other long-context benchmarks are
either too easy or too difficult for 4B-level models (see Appendix B), we ultimately select LongMemEval (Wu et al., 2024), a benchmark consisting of various types of long-context QA tasks with
moderate difficulty, although it is originally designed for agent memory system evaluation. Its
context length is controllable by selecting a specific number of chat sessions to concatenate as the
context from a pool of hundreds of sessions (a session contains the chat history between user and
assistant within a day). To create a moderately difficult and discriminative evaluation, we construct
**LongMemEval_24k** by sampling 10 sessions, resulting in 500 samples ranging from 16k to 32k
with an average context length of 24k.



4


For additional validation of generalizability, as
shown in Appendix D, we also experiment on
LongBench-V2 (Bai et al., 2024b), a more modern and challenging benchmark that requires deep
reasoning across various real-world tasks.


**4.3** **Training Details**


For the fine-tuning dataset, we initially considered LongAlign (Bai et al., 2024a), a widely used
long-context fine-tuning dataset for adapt a regularlength model to long-context tasks. However, since

_∼_
its sample count ( 10,000) is insufficient, we incorporate an additional 6,000 samples from Fusangv1-long (Pan, 2024), a more comprehensive corpus
of over 40,000 long-context samples that includes
LongAlign as a subset.
We perform SWA-aware fine-tuning using LoRA
(Hu et al., 2022). Unless otherwise noted, we use
rank _r_ = 16 and _α_ = 128, and apply LoRA only
to the query, key, and value projection modules.
We adopt this parameter-efficient setting because
full-parameter fine-tuning often leads to overfitting
and degradation of the model’s original capabilities
in our preliminary experiments. We use a learning
rate of 1e-4 with a cosine decay schedule. Models
are fine-tuned for a single epoch on the sampled
long-context dataset since we observe no meaningful gains from additional epochs (see Appendix F).
Once training takes approximately 12 hours on an
8*H20 GPU server for Qwen3-4B and 30 hours for
Qwen3-30B-A3B.


**5** **Experiment Results**


**5.1** **SWA Adaptation Without Fine-tuning**


We first study SWA adaptation without any additional training. Table 1 reports LongMemEval_24k
accuracy for Qwen3-4B-Thinking (""think"") and
Qwen3-4B-Instruct (""non-think"") under different
combinations of SWAA components. In most settings, we use an aggressive 2k window to amplify
the impact of SWA. The configurations are ranked
by the number of methods applied (0, 1, 2, or 3 of
Interleaving Layers, Keep First and FA Decode).
Rows 1 (original model) and 2 (naive SWA) serve
as upper and lower baselines, respectively. In the
column ""FA layers"", the value records which layers use full attention, and [] means all the layers
use SWA, i.e., this method is not enabled. In the
column ""keep first"", the value is _k_ in Keep First
_k_ Tokens. When comparing results, an accuracy
difference of less than 5% is usually considered



statistically insignificant. From the results, we find
that:


**Naive SWA is not viable.** Naively replacing FA
with a 2k sliding window attention (row 1) drops
accuracy significantly to 3 _._ 2 and 11 _._ 0, respectively.
Even with an 8k window (row 2), accuracy only recovers to 13 _._ 2 and 19 _._ 8, far below the FA baseline.


**Single method helps, but cannot close the gap.**
Each method—Keep First, FA Decode, or Interleaving Layers—improves over naive SWA (rows 3–6),
yet each alone recovers only a small fraction of the
FA gap and remains well below the baseline. In
short, no single method is sufficient.


**Combinations exhibit strong synergy.** Recipes
that combine multiple methods deliver large gains:


  - **FA Decode + Keep First** _k_ substantially improves over naive SWA (rows 7–9), recovering roughly half to two-thirds of the gap on
the thinking model as _k_ increases. However,
increasing _k_ from 100 to 1000 yields almost
no improvement, indicating that _k_ does not
need to be exceedingly large.


  - **Interleaving Layers + FA Decode** is
markedly stronger (row 13), recovering most
of the gap for the thinking model.


  - **FA Decode + Interleaving Layers + Keep**
**First** _k_ perform best (rows 18). The thinking
model recovers close to 90% of the FA gap
even at 2k window.
","When ""Full Attention Decode"" (FA Decode) is applied to a ""Qwen3-4B-Thinking"" model, it significantly improves performance (accuracy) compared to naive SWA, leading to a better performance-efficiency trade-off.

Specifically:
*   **Performance (Accuracy):**
    *   Without fine-tuning, FA Decode alone (11.8% accuracy for 2k window) substantially improves over naive SWA (3.2% accuracy for 2k window).
    *   With SWA-aware fine-tuning (SFT), FA Decode alone achieves 57.9% accuracy (2k window), which is a substantial gain compared to naive SWA with SFT (18.8% accuracy).
    *   The ""thinking model"" (Qwen3-4B-Thinking) consistently benefits more from recipes including FA Decode, as CoT synergizes with FA Decode by preserving global attention at decoding time, enabling longer reasoning traces.
*   **Efficiency Trade-off:**
    *   While integrating more methods generally introduces more overhead, configurations that include FA Decode can achieve a ""clearly better performance-efficiency ratio"" than the baseline curve drawn between the full-attention point and the naive-SWA point. This indicates a better accuracy-latency balance.
    *   The context evaluates the trade-off using metrics like time-to-first-token (TTFT), time-per-output-token (TPOT), total throughput, and average running time per request, and visualizes it by plotting accuracy against average running time.

The context does not provide specific quantitative efficiency metrics (like TTFT or TPOT) directly comparing FA Decode alone against naive SWA alone for the Qwen3-4B-Thinking model. However, it strongly implies that the significant accuracy gains from FA Decode, especially with SFT, place it in a much more favorable position on the performance-efficiency curve compared to naive SWA.",131,9744,9876,,3,,2026-01-02T20:17:03.908197
"Trace the flow of information from ""input embeddings"" to ""output probabilities"" in the Transformer architecture, explicitly mentioning the role of ""Add & Norm"" layers.",Multi-Hop Chain Reasoning,"[Document 1]
Source: attention_is_all_you_need_raw_chunks_2k.jsonl
Chunk ID: attention_is_all_you_need_raw_chunks_2k_1
Content: [10], consuming the previously generated symbols as additional input when generating the next.


2


![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_is_all_you_need.pdf-2-0.png)

Figure 1: The Transformer - model architecture.


The Transformer follows this overall architecture using stacked self-attention and point-wise, fully
connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,
respectively.


**3.1** **Encoder and Decoder Stacks**


**Encoder:** The encoder is composed of a stack of _N_ = 6 identical layers. Each layer has two
sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network. We employ a residual connection [11] around each of
the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is
LayerNorm( _x_ + Sublayer( _x_ )), where Sublayer( _x_ ) is the function implemented by the sub-layer
itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding
layers, produce outputs of dimension _d_ model = 512.


**Decoder:** The decoder is also composed of a stack of _N_ = 6 identical layers. In addition to the two
sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head
attention over the output of the encoder stack. Similar to the encoder, we employ residual connections
around each of the sub-layers, followed by layer normalization. We also modify the self-attention
sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This
masking, combined with fact that the output embeddings are offset by one position, ensures that the
predictions for position _i_ can depend only on the known outputs at positions less than _i_ .


**3.2** **Attention**


An attention function can be described as mapping a query and a set of key-value pairs to an output,
where the query, keys, values, and output are all vectors. The output is computed as a weighted sum


3


Scaled Dot-Product Attention Multi-Head Attention


Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several
attention layers running in parallel.


of the values, where the weight assigned to each value is computed by a compatibility function of the
query with the corresponding key.


**3.2.1** **Scaled Dot-Product Attention**


We call our particular attention ""Scaled Dot-Product Attention"" (Figure 2). The input consists of
queries and keys of dimension _dk_, and values of dimension _dv_ . We compute the dot products of the
query with all keys, divide each by _[√]_ _dk_, and apply a softmax function to obtain the weights on the
values.


In practice, we compute the attention function on a set of queries simultaneously, packed together
into a matrix _Q_ . The keys and values are also packed together into matrices _K_ and _V_ . We compute
the matrix of outputs as:


Attention( _Q, K, V_ ) = softmax( _[Q][K]_ _[T]_ ) _V_ (1)

~~_√_~~ _dk_


The two most commonly used attention functions are additive attention [2], and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor
of ~~_√_~~ 1 _dk_ . Additive attention computes the compatibility function using a feed-forward network with
a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is
much faster and more space-efficient in practice, since it can be implemented using highly optimized
matrix multiplication code.


While for small values of _dk_ the two mechanisms perform similarly, additive attention outperforms
dot product attention without scaling for larger values of _dk_ [3]. We suspect that for large values of
_dk_, the dot products grow large in magnitude, pushing the softmax function into regions where it has
extremely small gradients [4] . To counteract this effect, we scale the dot products by ~~_√_~~ 1 _dk_ .


**3.2.2** **Multi-Head Attention**


Instead of performing a single attention function with _d_ model-dimensional keys, values and queries,
we found it beneficial to linearly project the queries, keys and values _h_ times with different, learned
linear projections to _dk_, _dk_ and _dv_ dimensions, respectively. On each of these projected versions of
queries, keys and values we then perform the attention function in parallel, yielding _dv_ -dimensional


4To illustrate why the dot products get large, assume that the components of _q_ and _k_ are independent random
variables with mean 0 and variance 1. Then their dot product, _q · k_ = [�] _[d]_ _i_ =1 _[k]_ _[q][i][k][i]_ [, has mean][ 0][ and variance] _[ d][k]_ [.]


4



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_is_all_you_need.pdf-3-0.png)

![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_is_all_you_need.pdf-3-1.png)
output values. These are concatenated and once again projected, resulting in the final values, as
depicted in Figure 2.


Multi-head attention allows the model to jointly attend to information from different representation
subspaces at different positions. With a single attention head, averaging inhibits this.


MultiHead( _Q, K, V_ ) = Concat(head1 _, ...,_ headh) _W_ _[O]_

where headi = Attention( _QWi_ _[Q][, KW][ K]_ _i_ _[, V W][ V]_ _i_ [)]


Where the projections are parameter matrices _Wi_ _[Q]_ _∈_ R _[d]_ [model] _[×][d][k]_, _Wi_ _[K]_ _∈_ R _[d]_ [model] _[×][d][k]_, _Wi_ _[V]_ _∈_ R _[d]_ [model] _[×][d][v]_
and _W_ _[O]_ _∈_ R _[hd][v][×][d]_ [model] .


In this work we employ _h_ = 8 parallel attention layers, or heads. For each of these we use
_dk_ = _dv_ = _d_ model _/h_ = 64. Due to the reduced dimension of each head, the total computational cost
is similar to that of single-head attention with full dimensionality.


**3.2.3** **Applications of Attention in our Model**


The Transformer uses multi-head attention in three different ways:


    - In ""encoder-decoder attention"" layers, the queries come from the previous decoder layer,
and the memory keys and values come from the output of the encoder. This allows every
position in the decoder to attend over all positions in the input sequence. This mimics the
typical encoder-decoder attention mechanisms in sequence-to-sequence models such as

[38, 2, 9].


    - The encoder contains self-attention layers. In a self-attention layer all of the keys, values
and queries come from the same place, in this case, the output of the previous layer in the
encoder. Each position in the encoder can attend to all positions in the previous layer of the
encoder.


    - Similarly, self-attention layers in the decoder allow each position in the decoder to attend to
all positions in the decoder up to and including that position. We need to prevent leftward
information flow in the decoder to preserve the auto-regressive property. We implement this
inside of scaled dot-product attention by masking out (setting to _−∞_ ) all values in the input
of the softmax which correspond to illegal connections. See Figure 2.

---

[Document 2]
Source: attention_is_all_you_need_raw_chunks_2k.jsonl
Chunk ID: attention_is_all_you_need_raw_chunks_2k_0
Content: Provided proper attribution is provided, Google hereby grants permission to
reproduce the tables and figures in this paper solely for use in journalistic or
scholarly works.

## **Attention Is All You Need**



**Niki Parmar** _[∗]_
Google Research
```
nikip@google.com

```


**Ashish Vaswani** _[∗]_
Google Brain
```
avaswani@google.com

```

**Llion Jones** _[∗]_
Google Research
```
 llion@google.com

```


**Noam Shazeer** _[∗]_
Google Brain
```
noam@google.com

```


**Jakob Uszkoreit** _[∗]_
Google Research
```
usz@google.com

```


**Aidan N. Gomez** _[∗†]_
University of Toronto
```
aidan@cs.toronto.edu

```


**Łukasz Kaiser** _[∗]_
Google Brain
```
lukaszkaiser@google.com

```


**Illia Polosukhin** _[∗‡]_

```
             illia.polosukhin@gmail.com

```

**Abstract**


The dominant sequence transduction models are based on complex recurrent or
convolutional neural networks that include an encoder and a decoder. The best
performing models also connect the encoder and decoder through an attention
mechanism. We propose a new simple network architecture, the Transformer,
based solely on attention mechanisms, dispensing with recurrence and convolutions
entirely. Experiments on two machine translation tasks show these models to
be superior in quality while being more parallelizable and requiring significantly
less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including
ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,
our model establishes a new single-model state-of-the-art BLEU score of 41.8 after
training for 3.5 days on eight GPUs, a small fraction of the training costs of the
best models from the literature. We show that the Transformer generalizes well to
other tasks by applying it successfully to English constituency parsing both with
large and limited training data.


_∗_ Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started
the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and
has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head
attention and the parameter-free position representation and became the other person involved in nearly every
detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and
tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and
efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and
implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating
our research.

_†_ Work performed while at Google Brain.

_‡_ Work performed while at Google Research.


31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.


**1** **Introduction**


Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks
in particular, have been firmly established as state of the art approaches in sequence modeling and
transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous
efforts have since continued to push the boundaries of recurrent language models and encoder-decoder
architectures [38, 24, 15].


Recurrent models typically factor computation along the symbol positions of the input and output
sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden
states _ht_, as a function of the previous hidden state _ht−_ 1 and the input for position _t_ . This inherently
sequential nature precludes parallelization within training examples, which becomes critical at longer
sequence lengths, as memory constraints limit batching across examples. Recent work has achieved
significant improvements in computational efficiency through factorization tricks [21] and conditional
computation [32], while also improving model performance in case of the latter. The fundamental
constraint of sequential computation, however, remains.


Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in
the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms
are used in conjunction with a recurrent network.


In this work we propose the Transformer, a model architecture eschewing recurrence and instead
relying entirely on an attention mechanism to draw global dependencies between input and output.
The Transformer allows for significantly more parallelization and can reach a new state of the art in
translation quality after being trained for as little as twelve hours on eight P100 GPUs.


**2** **Background**


The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU

[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building
block, computing hidden representations in parallel for all input and output positions. In these models,
the number of operations required to relate signals from two arbitrary input or output positions grows
in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes
it more difficult to learn dependencies between distant positions [12]. In the Transformer this is
reduced to a constant number of operations, albeit at the cost of reduced effective resolution due
to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as
described in section 3.2.


Self-attention, sometimes called intra-attention is an attention mechanism relating different positions
of a single sequence in order to compute a representation of the sequence. Self-attention has been
used successfully in a variety of tasks including reading comprehension, abstractive summarization,
textual entailment and learning task-independent sentence representations [4, 27, 28, 22].


End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering and
language modeling tasks [34].


To the best of our knowledge, however, the Transformer is the first transduction model relying
entirely on self-attention to compute representations of its input and output without using sequencealigned RNNs or convolution. In the following sections, we will describe the Transformer, motivate
self-attention and discuss its advantages over models such as [17, 18] and [9].


**3** **Model Architecture**


Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].
Here, the encoder maps an input sequence of symbol representations ( _x_ 1 _, ..., xn_ ) to a sequence
of continuous representations **z** = ( _z_ 1 _, ..., zn_ ). Given **z**, the decoder then generates an output
sequence ( _y_ 1 _, ..., ym_ ) of symbols one element at a time. At each step the model is auto-regressive

[10], consuming the previously generated symbols as additional input when generating the next.


2


![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_is_all_you_need.pdf-2-0.png)

Figure 1: The Transformer - model architecture.


The Transformer follows this overall architecture using stacked self-attention and point-wise, fully
connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,
respectively.


**3.1** **Encoder and Decoder Stacks**

---

[Document 3]
Source: attention_is_all_you_need_raw_chunks_2k.jsonl
Chunk ID: attention_is_all_you_need_raw_chunks_2k_2
Content: [38, 2, 9].


    - The encoder contains self-attention layers. In a self-attention layer all of the keys, values
and queries come from the same place, in this case, the output of the previous layer in the
encoder. Each position in the encoder can attend to all positions in the previous layer of the
encoder.


    - Similarly, self-attention layers in the decoder allow each position in the decoder to attend to
all positions in the decoder up to and including that position. We need to prevent leftward
information flow in the decoder to preserve the auto-regressive property. We implement this
inside of scaled dot-product attention by masking out (setting to _−∞_ ) all values in the input
of the softmax which correspond to illegal connections. See Figure 2.


**3.3** **Position-wise Feed-Forward Networks**


In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully
connected feed-forward network, which is applied to each position separately and identically. This
consists of two linear transformations with a ReLU activation in between.


FFN( _x_ ) = max(0 _, xW_ 1 + _b_ 1) _W_ 2 + _b_ 2 (2)


While the linear transformations are the same across different positions, they use different parameters
from layer to layer. Another way of describing this is as two convolutions with kernel size 1.
The dimensionality of input and output is _d_ model = 512, and the inner-layer has dimensionality
_dff_ = 2048.


**3.4** **Embeddings and Softmax**


Similarly to other sequence transduction models, we use learned embeddings to convert the input
tokens and output tokens to vectors of dimension _d_ model. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In
our model, we share the same weight matrix between the two embedding layers and the pre-softmax
linear transformation, similar to [30]. In the embedding layers, we multiply those weights by _[√]_ _d_ model.


5


Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations
for different layer types. _n_ is the sequence length, _d_ is the representation dimension, _k_ is the kernel
size of convolutions and _r_ the size of the neighborhood in restricted self-attention.


Layer Type Complexity per Layer Sequential Maximum Path Length
Operations
Self-Attention _O_ ( _n_ [2] _· d_ ) _O_ (1) _O_ (1)
Recurrent _O_ ( _n · d_ [2] ) _O_ ( _n_ ) _O_ ( _n_ )
Convolutional _O_ ( _k · n · d_ [2] ) _O_ (1) _O_ ( _logk_ ( _n_ ))
Self-Attention (restricted) _O_ ( _r · n · d_ ) _O_ (1) _O_ ( _n/r_ )


**3.5** **Positional Encoding**


Since our model contains no recurrence and no convolution, in order for the model to make use of the
order of the sequence, we must inject some information about the relative or absolute position of the
tokens in the sequence. To this end, we add ""positional encodings"" to the input embeddings at the
bottoms of the encoder and decoder stacks. The positional encodings have the same dimension _d_ model
as the embeddings, so that the two can be summed. There are many choices of positional encodings,
learned and fixed [9].


In this work, we use sine and cosine functions of different frequencies:


_PE_ ( _pos,_ 2 _i_ ) = _sin_ ( _pos/_ 10000 [2] _[i/d]_ [model] )

_PE_ ( _pos,_ 2 _i_ +1) = _cos_ ( _pos/_ 10000 [2] _[i/d]_ [model] )


where _pos_ is the position and _i_ is the dimension. That is, each dimension of the positional encoding
corresponds to a sinusoid. The wavelengths form a geometric progression from 2 _π_ to 10000 _·_ 2 _π_ . We
chose this function because we hypothesized it would allow the model to easily learn to attend by
relative positions, since for any fixed offset _k_, _PEpos_ + _k_ can be represented as a linear function of
_PEpos_ .


We also experimented with using learned positional embeddings [9] instead, and found that the two
versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version
because it may allow the model to extrapolate to sequence lengths longer than the ones encountered
during training.


**4** **Why Self-Attention**


In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations
( _x_ 1 _, ..., xn_ ) to another sequence of equal length ( _z_ 1 _, ..., zn_ ), with _xi, zi ∈_ R _[d]_, such as a hidden
layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we
consider three desiderata.


One is the total computational complexity per layer. Another is the amount of computation that can
be parallelized, as measured by the minimum number of sequential operations required.


The third is the path length between long-range dependencies in the network. Learning long-range
dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the
ability to learn such dependencies is the length of the paths forward and backward signals have to
traverse in the network. The shorter these paths between any combination of positions in the input
and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare
the maximum path length between any two input and output positions in networks composed of the
different layer types.


As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially
executed operations, whereas a recurrent layer requires _O_ ( _n_ ) sequential operations. In terms of
computational complexity, self-attention layers are faster than recurrent layers when the sequence


6


length _n_ is smaller than the representation dimensionality _d_, which is most often the case with
sentence representations used by state-of-the-art models in machine translations, such as word-piece

[38] and byte-pair [31] representations. To improve computational performance for tasks involving
very long sequences, self-attention could be restricted to considering only a neighborhood of size _r_ in
the input sequence centered around the respective output position. This would increase the maximum
path length to _O_ ( _n/r_ ). We plan to investigate this approach further in future work.


A single convolutional layer with kernel width _k < n_ does not connect all pairs of input and output
positions. Doing so requires a stack of _O_ ( _n/k_ ) convolutional layers in the case of contiguous kernels,
or _O_ ( _logk_ ( _n_ )) in the case of dilated convolutions [18], increasing the length of the longest paths
between any two positions in the network. Convolutional layers are generally more expensive than
recurrent layers, by a factor of _k_ . Separable convolutions [6], however, decrease the complexity
considerably, to _O_ ( _k · n · d_ + _n · d_ [2] ). Even with _k_ = _n_, however, the complexity of a separable
convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,
the approach we take in our model.


As side benefit, self-attention could yield more interpretable models. We inspect attention distributions
from our models and present and discuss examples in the appendix. Not only do individual attention
heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic
and semantic structure of the sentences.


**5** **Training**


This section describes the training regime for our models.


**5.1** **Training Data and Batching**
","The flow of information in the Transformer architecture from ""input embeddings"" to ""output probabilities"" is as follows:

1.  **Input Embeddings and Positional Encoding:**
    *   Input tokens are first converted into learned **input embeddings** of dimension `d_model`.
    *   To these embeddings, **positional encodings** (using sine and cosine functions) are added. This combined representation serves as the initial input to the encoder stack.

2.  **Encoder Stack:**
    *   The encoder consists of a stack of `N` identical layers. Each encoder layer has two sub-layers:
        *   **Multi-Head Self-Attention:** This mechanism allows each position in the encoder to attend to all positions in the previous layer's output.
        *   **Add & Norm:** Following the multi-head self-attention sub-layer, a **residual connection** is applied (adding the sub-layer's input `x` to its output `Sublayer(x)`), which is then followed by **layer normalization**. The output of this step is `LayerNorm(x + Sublayer(x))`.
        *   **Position-wise Feed-Forward Network:** This is a fully connected feed-forward network applied to each position separately and identically.
        *   **Add & Norm:** Again, a **residual connection** is applied around the feed-forward network, followed by **layer normalization**. The output of this step is `LayerNorm(x + Sublayer(x))`.
    *   The output of the final encoder layer forms the ""memory keys and values"" that the decoder will use.

3.  **Decoder Stack:**
    *   The decoder also consists of a stack of `N` identical layers. Each decoder layer has three sub-layers:
        *   **Masked Multi-Head Self-Attention:** This sub-layer allows each position in the decoder to attend to all positions up to and including itself in the decoder's input sequence. It is ""masked"" to prevent attending to subsequent positions, preserving the auto-regressive property.
        *   **Add & Norm:** A **residual connection** is applied around the masked multi-head self-attention sub-layer, followed by **layer normalization**.
        *   **Encoder-Decoder Multi-Head Attention:** This sub-layer performs multi-head attention over the output of the encoder stack. The queries come from the previous decoder layer, while the keys and values come from the encoder's output.
        *   **Add & Norm:** A **residual connection** is applied around this encoder-decoder attention sub-layer, followed by **layer normalization**.
        *   **Position-wise Feed-Forward Network:** Similar to the encoder, this is a fully connected feed-forward network.
        *   **Add & Norm:** A **residual connection** is applied around the feed-forward network, followed by **layer normalization**.
    *   The output of the final decoder layer is a continuous representation.

4.  **Output Probabilities:**
    *   The final output of the decoder stack undergoes a **learned linear transformation**.
    *   This linear transformation is then followed by a **softmax function** to convert the output into predicted next-token probabilities.",115,7811,7927,,3,,2026-01-02T20:17:11.836107
"Explain how ""masking cognitive heads"" in VLMs leads to performance degradation on ""downstream tasks"" like ""OK-VQA"".",Multi-Hop Chain Reasoning,"[Document 1]
Source: attention_functional_roles_raw_chunks_2k.jsonl
Chunk ID: attention_functional_roles_raw_chunks_2k_4
Content: dom activation and mean activation. In the random activation setting, activations are substituted
with those from a randomly selected subquestion belonging to a different function. In the mean
activation setting, activations are replaced with the average activation computed over all subquestions associated with another function (details in Appendix A.10). As shown in Table 2, both types
of activation patching result in substantial performance degradation for cognitive heads, consistent
with the effects observed under masking interventions.


Table 2: Ablation study of different activation-masking methods on Qwen2.5-VL-3B. Random:
random activation. Mean: mean activation. Scalar: masking.







|VisionmainlyCognitiveFunctions LanguagemainlyCognitiveFunctions<br>Method Inter Head<br>Low-Level High -Level Recall Info Recall Math Inference Decision<br>llm acc llm acc llm acc llm acc llm acc llm acc llm acc llm acc|Col2|
|---|---|
|Random<br>cognitive|**0.00 0.00** 20.45 23.48** 62.12 65.15** 35.79 57.41 12.86 24.29 17.07 17.07 14.32 14.32 6.25 9.38|
|Mean<br>cognitive|3.80 3.80 17.39 19.91 66.67 65.15** 35.19 55.56 10.00 21.43** 37.17 34.15<br>3.80<br>3.80<br>6.25 9.38|
|Scalar<br>cognitive|6.45 6.45** 16.67 18.94** 75.76 75.76 62.96 81.48 57.14 62.86<br>**2.43**<br>**2.43**<br>**0.00**<br>**0.00**<br>**3.13 4.69**|


We further examine how vision-related and language-related heads attend to their respective modalities. For the top-30 heads of each function, we compute the average attention weight on visual tokens
across the test set. As shown in Appendix A.9, vision-related heads (e.g., high-level and low-level


7


![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-7-0.png)

Figure 5: The performance of VLMs on target functions after masking out top K cognitive heads on
intervention functions. The scores are based on LLM-Judge.


Table 3: Study on the influence of low-level cognitive heads for high-order function on Qwen2.5VL-3B. The score is measured based on LLM-judge. We only evaluate subquestions that the model
originally answered correctly. This filtering ensures that any observed drop in performance is caused
solely by the intervention. Notably, the model’s own generated outputs are used as inputs for subsequent subquestions.


Vision Recall Language Recall Info. Low-Level High Level Math Decision Inference


✗ ✓ ✓ ✓ ✓ 50 _._ 00 _↓_ 50.00 54 _._ 55 _↓_ 45.45 54 _._ 17 _↓_ 45.83
✓ ✗ ✓ ✓ ✓ 16 _._ 67 _↓_ 83.33 56 _._ 25 _↓_ 43.75 65 _._ 22 _↓_ 34.78
✓ ✓ ✗ ✓ ✓ 22 _._ 22 _↓_ 77.78 57 _._ 89 _↓_ 42.11 51 _._ 61 _↓_ 48.39
✓ ✓ ✓ ✗ ✓ 27 _._ 27 _↓_ 72.73 72 _._ 73 _↓_ 27.27 59 _._ 09 _↓_ 40.91
✓ ✓ ✓ ✓ ✗ 33 _._ 96 _↓_ 66.04 64 _._ 29 _↓_ 35.71 53 _._ 95 _↓_ 46.05


visual reception) predominantly focus on image tokens, capturing spatial and object-level information, whereas language-related heads (e.g., language knowledge recall) concentrate on text tokens.
Interestingly, Qwen and Intern models allocate more attention to text, while Gemma emphasizes
vision, revealing family-specific modality preferences. We also observe heads with cross-modal attention that respond to both visual and textual tokens, likely mediating interactions between visual
perception and linguistic reasoning. These findings suggest that functional specialization in VLMs
is complemented by selective cross-modal integration, enabling coherent multimodal reasoning.


4.3 RELATIONSHIP AMONG COGNITIVE HEADS


While cognitive heads are specialized for distinct functions, understanding their relationships is
crucial for revealing how complex reasoning emerges from their cooperation.


**Heads Across Functions** The neural system is inherently complex, with individual neurons often
participating in multiple functions (Mante et al., 2013). We observe a similar phenomenon in VLMs:
certain functional heads overlap, with a single head participating in multiple cognitive roles (e.g., in
the Qwen2.5-VL-7B model, 18% of cognitive heads across eight functions participate in more than
one function). In our probing-based method, we quantify and rank the accuracy of attention heads for
each cognitive function. A head that ranks highly for one function may also exhibit non-negligible
importance for others, leading to the phenomenon of ”Heads Across Functions”. Notably, even if
a head ranks in the top 10% for multiple cognitive functions, our ranking still reveals a primary
function for which it is most diagnostic. In summary, functional heads serve not only as specialized
units but also as integrative components that bridge multiple reasoning processes.


**Hierarchical Structure** Humans often solve complex tasks through step-by-step reasoning, where
former functions, such as low-level visual reception, support higher-level processes like inference
and decision making. The CogVision dataset reflects this hierarchy: under CoT, early subquestions
focus on information extraction, while later ones require more complex reasoning. Leveraging this


8


structure, we test whether VLMs exhibit similar functional dependencies by masking attention heads
associated with early-stage functions and observing the impact on subsequent reasoning steps. As
shown in Table 3, masking vision or language knowledge recall heads significantly impairs laterstage performance, particularly in decision making. These results suggest that VLMs exhibit an
emergent hierarchical organization, where early cognitive functions support more advanced reasoning. The prompt used for VLMs can be found in Appendix A.7.


4.4 INFLUENCE OF FUNCTIONAL HEADS ON DOWNSTREAM TASKS


In this section, we investigate how functional heads influence downstream tasks through both negative interventions (masking out function heads) and positive interventions (shifting heads toward
specific functions).


**Negative Intervention:** We randomly
sample 200 question for two VQA bench- Table 4: Negative Intervention on Visual Question Anmarks, OK-VQA and Clevr, both reason- swering task (OK-VQA and Clevr-Math). The scores
ing tasks. We perform negative interven- are based on LLM-Judge.
tion by masking high-level visual reception heads on OK-VQA and math reason- Dataset Inter ~~H~~ ead Model

leads to a significant performance drop after **0.00** **59.00** 29.00 **13.00** 14.00 **13.00**
across all models. Further analysis in Ap
|Dataset Inter Head|Model|
|---|---|
|Dataset<br>Inter~~ H~~ead|Qwen3B Qwen7B Intern2B Intern8B gemma2B gemma4B|
|OK-VQA<br>before<br>after|54.00<br>55.00<br>46.00<br>51.00<br>21.00<br>20.00<br>**7.00**<br>55.00<br>**44.00**<br>53.00<br>**18.00**<br>21.00|
|Clevr-Mathbefore<br>after|94.00<br>70.00<br>20.00<br>93.00<br>14.00<br>14.00<br>**0.00**<br>**59.00**<br>29.00<br>**13.00**<br>14.00<br>**13.00**|

---

[Document 2]
Source: attention_functional_roles_raw_chunks_2k.jsonl
Chunk ID: attention_functional_roles_raw_chunks_2k_3
Content: 5


![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-5-0.png)

Figure 2: The existence of cognitive heads in Qwen2.5-VL-7B responsible for eight distinct functions in complex reasoning tasks. The x-axis represents the head index, while the y-axis indicates
the layer index. The values denote head importance scores, capped at a cutoff of 0.60.


Figure 3: Pearson Correlation between different functions across two models.


the ROUGE score (Chin-Yew, 2004) or the semantic similarity score surpasses 0.6. This provides a
comprehensive evaluation of performance degradation.


We gradually mask out the number of cognitive heads and and observe how model behavior changes.
As shown in Fig 4, randomly masking up to 10% of heads has minimal impact on the overall performance of Qwen2.5-VL-3B. In contrast, masking a similar number of cognitive heads leads to a
substantial drop across multiple functions. Notably, when more than 25% of heads are randomly
masked, performance also declines sharply, as this begins to include functionally critical heads.
These results further highlight the sparsity and importance of functional heads.


For each function, we select the top 10% of heads with the highest accuracy as cognitive heads. As
shown in Table 1, masking cognitive heads lead to a substantial decline in performance, whereas
masking an equal number of random heads results in only minor degradation across all VLMs. In
some cases, masking identified cognitive heads reduces accuracy to zero, indicating that the model
cannot perform the corresponding function without them. The t-test analysis (Appendix A.9) shows
that the difference between cognitive masking and random masking is statistically significant, with
_p ≪_ 0 _._ 05 in nearly all cases. To further validate their functional roles, we mask heads associated
with one function (e.g., language knowledge recall) while evaluating performance on a different
function (e.g., vision knowledge recall). As shown in Fig 5, masking the relevant functional heads
yields a significantly larger performance drop than masking unrelated heads, confirming their functional specialization.


In addition to masking, we also conduct activation patching, where the activations of cognitive heads
associated with one function are replaced by those from another function using two strategies: ran

6



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-5-1.png)
![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-6-0.png)

Figure 4: The performance of Qwen2.5-VL-3B after masking out top K cognitive heads vs K random
heads on inference, high-level visual reception, low-level visual reception, and decion-making.


Table 1: Intervention results (mean accuracy over 5 runs %) of cognitive heads vs. random heads
across 8 functions: **Low-level** Visual Reception, **High-level** Visual Reception, Vision Knowledge
**Recall**, Language **Info** rmation Extraction and Understanding, Language Knowledge **Recall**, **Math**
Reasoning, **Inference**, and **Decision** Making. For model names, Intern2B: InternVL3-2B, Intern8B:
InternVL3-8B, gemma2B: gemma-3n-e2b-it, gemma4B: gemma-3n-e4b-it, Qwen3B: Qwen2.5VL-3B-Instruct, Qwen7B: Qwen2.5-VL-7B-Instruct. Lower values indicate more effective intervention outcomes, suggesting that the corresponding heads play a greater role in the cognitive function.







|VisionmainlyFunctions LanguagemainlyFunctions<br>Model Inter Head<br>Low-Level High -Le vel Recall Info Recall Math Inference Decision<br>llm acc llm acc llm acc llm acc llm acc llm acc llm acc llm acc|Col2|
|---|---|
|Intern2B<br>random<br>cognitive|75.61 82.44<br>87.5<br>88.75 89.15<br>86.1<br>57.84 69.19 84.06 84.64 81.29 90.97 75.06 74.12 67.22 71.67<br>**60.24 62.68 75.71 76.61 73.05 64.58** 66.76 68.92** 44.93 46.38 61.29 64.52 71.76 64.71 48.06 52.22**|
|Intern8B<br>random<br>cognitive|92.2<br>93.17 88.47 91.25 87.61 82.25 59.41 69.12 87.59 89.37 79.25 84.15 82.64 85.05 66.27 73.49<br>**68.78 78.05 56.94 65.97 71.69 70.56**<br>**8.82**<br>**19.12 74.68 77.22 20.75**<br>**56.6**<br>**43.96 42.86** 66.27** 66.27**|
|gemma2B<br>random<br>cognitive|58.37 76.33 57.53 67.64 54.57 62.57 55.07 60.82 81.05 82.11 23.12<br>55.0<br>57.44 63.49<br>30.0<br>66.94<br>**48.98**<br>**55.1**<br>**55.06 63.48**<br>**2.86**<br>**8.57**<br>**30.27 38.49**<br>**50.0**<br>**47.37 11.25 36.88 36.98 52.09 19.44 54.17**|
|gemma4B<br>random<br>cognitive|36.0<br>48.73 29.22 38.65 33.52 34.08 25.12 27.56 40.27 47.84 27.91 44.19<br>57.3<br>57.08 18.29 34.57<br>**29.09 41.82 10.88 21.24**<br>**5.63**<br>**14.08**<br>**22.2**<br>**36.34**<br>**9.46**<br>**13.51** 27.91** 53.49 36.18 32.81**<br>**4.29**<br>**15.71**|
|Qwen3B<br>random<br>cognitive|70.97 82.58 82.42 86.06 88.48 86.36 52.22 58.89 85.14 86.57 65.85 89.76 85.32 90.13 63.44 71.88<br>**12.9**<br>**12.9**<br>**12.12 16.67 77.88 77.88** 55.56 62.96** 61.43 68.57**<br>**0.0**<br>**0.0**<br>**1.27**<br>**1.27**<br>**1.56**<br>**4.69**|
|Qwen7B<br>random<br>cognitive|83.2<br>88.8<br>84.57 89.51 79.43 80.29 75.08 79.38 90.13<br>86.4<br>67.84 72.94 80.67 83.33 75.14 79.19<br>**30.0**<br>**38.0**<br>**73.21 73.83 21.43 22.86 15.38 33.85**<br>**84.0**<br>**78.67** 68.63 72.55 85.56 81.11** 25.68 27.03**|


dom activation and mean activation. In the random activation setting, activations are substituted
with those from a randomly selected subquestion belonging to a different function. In the mean
activation setting, activations are replaced with the average activation computed over all subquestions associated with another function (details in Appendix A.10). As shown in Table 2, both types
of activation patching result in substantial performance degradation for cognitive heads, consistent
with the effects observed under masking interventions.

---

[Document 3]
Source: attention_functional_roles_raw_chunks_2k.jsonl
Chunk ID: attention_functional_roles_raw_chunks_2k_5
Content: pendix A.11 shows that masking the **math reasoning** heads leads to errors in arithmetic tasks, while
visual receptive functions remain largely unaffected. This confirms that these cognitive heads are
crucial for specific functions and highlights the robustness and generalizability of our method.


**Positive Intervention:** We calculate the activation directions of different functions using the CogVision dataset. For each function, the activation direction of a head at layer _l_ and index _h_ is computed

as:
dir _[h]_ _l_ [=][ E] _[i][∈D]_ correct � _x_ _[h]_ _l_ [(] _[i]_ [)] � _−_ E _i∈D_ incorrect � _x_ _[h]_ _l_ [(] _[i]_ [)] � (3)

where _x_ _[h]_ _l_ [(] _[i]_ [)][ denotes the activation of head at layer] _[ l]_ [ and index] _[ h]_ [, and] _[ D]_ [correct][ and] _[ D]_ [incorrect][ represent]
the sets of samples answered correctly and incorrectly, respectively. Then we estimate the standard
deviation of activations (Li et al., 2023a) along the cognitive function direction to be _σl_ _[h]_ [, and shift]
original head activation as _x_ _[h]_ _l_ [(] _[i]_ [)] _[ ←]_ _[x][h]_ _l_ [(] _[i]_ [) +] _[ ασ]_ _l_ _[h]_ [dir] _l_ _[h]_ [, where] _[ α]_ [ is a parameter.]


The experimental results in Table 5 show that enhancing the activation of functional heads along
their corresponding functional directions improves performance on the related tasks. For example,
positive intervention on vision knowledge recall heads in InternVL3-8B increased accuracy on the
corresponding CogVision question-answering task from 88.54% to 91.67%. Similarly, enhancing
function-specific heads can also boost performance on downstream tasks. Here, we set _α_ = 0 _._ 1
for all datasets, though tuning this parameter may further improve performance. Case analyses are
provided in Appendix A.11.


5 RELATED WORKS


**Neural Networks and the Brain.** Understanding the relationship between artificial neural networks (ANNs) and the biological brain has been a long-standing goal in both neuroscience and
machine learning. Early studies demonstrated that convolutional neural networks (CNNs) trained
on visual tasks develop hierarchical representations reminiscent of the ventral visual stream in primates (Yamins et al., 2014; Cadieu et al., 2014). Subsequent work extended this line of inquiry
to recurrent and transformer-based architectures, showing that attention mechanisms can emulate
aspects of selective processing observed in cortical circuits (Tsividis et al., 2017). More recently,
large language models (LLMs) have exhibited striking parallels with human brain activity during
language processing. In particular, transformer-based models such as GPT-2 produce internal representations that align with neural responses in language-selective brain regions (Caucheteux et al.,
2022; Schrimpf et al., 2021). Some works (Schulze Buschoff et al., 2025; Li et al., 2024) have
studied how VLMs perform differently from humans from a cognitive perspective. Furthermore, the


9



Table 4: Negative Intervention on Visual Question Answering task (OK-VQA and Clevr-Math). The scores
are based on LLM-Judge.








Table 5: Positive Intervention on CogVision test set and other Visual Question Answering benchmarks (OK-VQA, MathVista and Visulogic). The scores are based on LLM-Judge. For OK-VQA,
we perform positive intervention by masking 10% high-level visual reception head, MathVista for
math reasoning heads, and Visulogic for decision-making heads.

|In Domain Out of Domain<br>Model InterHead<br>Math Vision Recall Lang Recall Info Low-Level Inference High-Level Decision OK-VQA MathVista Visulogic|Col2|Col3|
|---|---|---|
|Qwen3B<br>before<br>after|46.40<br>82.29<br>81.37<br>48.03<br>78.26<br>65.87<br>77.38<br>29.73<br>46.40<br>**85.42**<br>**84.31**<br>44.74<br>78.26<br>**69.84**<br>**77.98**<br>**32.43**|62.50<br>60.00<br>26.50<br>62.00<br>60.00<br>**28.00**|
|Qwen7B<br>before<br>after|52.00<br>84.38<br>86.27<br>43.42<br>77.18<br>69.84<br>82.44<br>36.94<br>**52.80**<br>**85.42**<br>86.27<br>**46.05**<br>**82.61**<br>**74.60**<br>82.44<br>36.94|66.00<br>63.00<br>24.00<br>**67.50**<br>63.00<br>**24.50**|
|Intern2B<br>before<br>after|38.40<br>79.17<br>80.39<br>44.08<br>78.26<br>61.90<br>81.55<br>34.23<br>38.40<br>78.13<br>**84.31**<br>42.76<br>**80.43**<br>**64.29**<br>**82.44**<br>**35.14**|58.50<br>51.50<br>24.00<br>**61.50**<br>**52.00**<br>**26.00**|
|Intern8B<br>before<br>after|52.00<br>88.54<br>82.35<br>45.39<br>88.04<br>73.81<br>86.61<br>42.34<br>**52.80**<br>**91.67**<br>**84.31**<br>**46.71**<br>86.96<br>73.81<br>**87.80**<br>**43.24**|67.00<br>66.00<br>24.00<br>**67.50**<br>66.00<br>**26.00**|
|gemma2B<br>before<br>after|26.40<br>62.50<br>80.39<br>32.24<br>34.78<br>48.41<br>33.93<br>23.42<br>**28.00**<br>**64.58**<br>**84.31**<br>**38.16**<br>34.78<br>**50.00**<br>30.95<br>20.72|29.00<br>24.50<br>26.00<br>**29.50**<br>24.50<br>**29.00**|
|gemma4B<br>before<br>after|32.80<br>62.50<br>83.33<br>32.89<br>27.17<br>52.38<br>33.63<br>18.92<br>**35.20**<br>**65.63**<br>**85.29**<br>**35.53**<br>27.17<br>**53.97**<br>**33.63**<br>**19.82**|31.50<br>24.00<br>27.00<br>**33.00**<br>**26.50**<br>**27.50**|



chain-of-thought (CoT) paradigm has been argued to mirror step-by-step human reasoning, leading to improved problem-solving performance. These findings motivate the design of interpretable,
functionally specialized modules in artificial networks, bridging insights from neuroscience with
advances in multimodal reasoning.
","Masking cognitive heads in Vision-Language Models (VLMs) leads to performance degradation on downstream tasks like OK-VQA because these heads are crucial for specific cognitive functions required by such tasks.

Specifically, for the OK-VQA task, performing a ""negative intervention"" by masking ""high-level visual reception heads"" results in a significant performance drop across all models tested (e.g., Qwen3B, Qwen7B, Intern2B, Intern8B, gemma2B, gemma4B). This indicates that the VLM's ability to process and understand high-level visual information, which is mediated by these cognitive heads, is essential for successfully answering questions in OK-VQA.",122,4438,4562,,3,,2026-01-02T20:17:16.400120
"How does the ""explicit Euler solver"" in NAC facilitate the ""adaptive temporal dynamics"" required for ""irregular time-series classification""?",Multi-Hop Chain Reasoning,"[Document 1]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_13
Content: which depends on ( _ϕ, ωτ_ ) both through the equilibrium _a_ _[∗]_

and the exponential term.
**Derivative with respect to** _ϕ_ **:** We obtain



_Interpretation_ : The gradient with respect to _ωτ_ contains
a transient term proportional to _te_ _[−][ω][τ][ t]_, which dominates
at intermediate times, and a steady-state contribution proportional to _−ϕ/ωτ_ [2][, which persists asymptotically. Thus,]
sensitivity to _ωτ_ is time-dependent, peaking before vanishing exponentially in the transient component.


**C.2. Gradient-Based Training**


Like Neural ODEs (Chen et al., 2018) and CT-RNNs
(Rubanova et al., 2019), NAC produces differentiable computational graphs and can be trained using gradient-based
optimization, such as the adjoint sensitivity method (Cao
et al., 2003) or backpropagation through time (BPTT) (LeCun et al., 1988). In this work, we use BPTT exclusively,
as the adjoint sensitivity method can introduce numerical
errors (Zhuang et al., 2020).


**C.3. Efficiency and Complexity**


Table 3 summarizes the computational complexity of different sequence models. For sequence prediction over length
_n_ with hidden dimension _k_, RNNs scale linearly, _O_ ( _nk_ ),
while Attention and NAC scale quadratically, _O_ ( _n_ [2] _k_ ).
ODE-based models, such as LNNs, incur an additional
multiplicative factor _S_ for the number of solver steps.
For single-time-step prediction, RNNs and LSTMs require
_O_ ( _k_ ), whereas Attention and NAC require _O_ ( _nk_ ) when
recomputing attention over the full sequence.



_∂at_

_∂ϕ_ [= 1] _[ −]_ _ω_ _[e]_ _τ_ _[−][ω][τ][ t]_



_∂at_



(46)
_ωτ_



_Interpretation_ : For large _ωτ_, the gradient with respect to _ϕ_
saturates quickly but shrinks to scale _O_ (1 _/ωτ_ ), potentially
slowing learning of _ϕ_ . Conversely, very small _ωτ_ leads
to large steady-state gradients, which may destabilize optimization.

**Derivative with respect to** _ωτ_ **:** Here, both the equilibrium
and the decay rate depend on _ωτ_, yielding



_∂at_
= _−_ _[ϕ]_
_∂ωτ_ _ωτ_ [2]



�1 _−_ _e_ _[−][ω][τ][ t]_ [�] _−_ ( _a_ 0 _−_ _a_ _[∗]_ ) _t e_ _[−][ω][τ][ t]_ _._ (47)



13


**Neuronal Attention Circuit (NAC) for Representation Learning**



_Table 3._ Sequence and time-step prediction complexity. _n_ is the
sequence length and _k_ is the hidden/model dimension.


**Model** **Sequence** **Time-step**
RNN _O_ ( _nk_ ) _O_ ( _k_ )
Attention _O_ ( _n_ [2] _k_ ) _O_ ( _nk_ )
LNN (ODEsolve) _O_ ( _nk · S_ ) _O_ ( _k · S_ )
NAC-Exact _O_ ( _n_ [2] _k_ ) _O_ ( _nk_ )
NAC-Euler _O_ ( _n_ [2] _k_ ) _O_ ( _nk_ )


**D. Evaluation**


**D.1. Related Works**


The brief description for related works is divided into four
subcategories.
**DT-RNNs:** RNN (Rumelhart et al., 1985) captures sequential dependencies in time-series data by updating a hidden state from the current observation and the previous
state. LSTM (Hochreiter & Schmidhuber, 1997) extends
RNNs with input, output, and forget gates, allowing the
network to maintain and update long-term memory, which
improves modeling of long-term dependencies in time-series
sequences. GRU (Cho et al., 2014) simplifies the LSTM
architecture by combining the forget and input gates into a
single update gate, allowing efficient modeling of long-term
dependencies in time-series sequences.
**CT-RNNs:** CT-RNN (Rubanova et al., 2019) model temporal dynamics using differential equations, enabling hidden
states to evolve continuously over time in response to inputs, which is particularly useful for irregularly sampled
time-series data. PhasedLSTM (Neil et al., 2016) introduces a time gate that updates hidden states according to
a rhythmic schedule, enabling efficient modeling of asynchronous or irregularly sampled time-series. GRU-ODE
(De Brouwer et al., 2019) extends the GRU to continuous
time, evolving hidden states via ODEs to handle sequences
with non-uniform time intervals. mmRNN (Lechner &
Hasani, 2022) combines short-term and long-term memory units to capture both fast-changing and slowly evolving
patterns in sequential data. LTC (Hasani et al., 2021) use
neurons with learnable, input-dependent time constants to
adapt the speed of dynamics and capture complex temporal
patterns in continuous-time data. CfC (Hasani et al., 2022)
approximate LTC dynamics analytically, providing efficient
continuous-time modeling without relying on numerical
ODE solvers.

**DT-Attentions:** Attention (Vaswani et al., 2017) computes
attention weights by measuring similarity between queries
and keys, scaling the results, and applying softmax to weigh
time-step contributions. Multi-Head Attention (Vaswani
et al., 2017) applies multiple parallel scaled dot-product
attention mechanisms, capturing different types of temporal
dependencies simultaneously for complex time-series modeling.



**CT-Attentions:** mTAN (Shukla & Marlin, 2021) learns
continuous-time embeddings and uses time-based attention
to interpolate irregular observations into a fixed-length representation for downstream encoder-decoder modeling. CTA
(Chien & Chen, 2021) generalizes discrete-time attention
to continuous-time by representing hidden states, context
vectors, and attention scores as functions whose dynamics are modeled via neural networks and integrated using
ODE solvers for irregular sequences. ODEFormer(d’Ascoli
et al., 2023) trains a sequence-to-sequence transformer on
synthetic trajectories to directly output a symbolic ODE
system from noisy, irregular time-series data. ContiFormer
(Chen et al., 2023) builds a continuous-time Transformer by
pairing ODE-defined latent trajectories with a time-aware
attention mechanism to model dynamic relationships in irregular time-series data.


**D.2. Ablations Details**

---

[Document 2]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_1
Content: **Motivation behind this formulation:** The proposed
formulation is loosely motivated by the input-dependent
time-constant mechanism of Liquid Neural Networks
(LNNs), a class of CT-RNNs inspired by biological nervous
systems and synaptic transmission. In this framework, the
dynamics of non-spiking neurons are described by a linear
ODE with nonlinear, interlinked gates: _ddt_ **xt** = **[x]** _τ_ **[t]** [+] **[ S][t]** _[,]_

where **xt** denotes the hidden state and **St** _∈_ R _[M]_ represents
a nonlinear contribution defined as _f_ ( **xt** _,_ **u** _, t, θ_ )( _A −_ **xt** ).
Here, _A_ and _θ_ are learnable parameters. Plugging **St**
yields _[d]_ **[x][t]** [=] _[ −]_ � _−_ [1] _[−]_ _[f]_ [(] **[x][t]** _[,]_ **[ u]** _[, t, θ]_ [)] � **xt** + _f_ ( **xt** _,_ **u** _, t, θ_ ) _A_ .



_an_ +1 = _an_ + ∆ _t_ ( _−ωτ_ _an_ + _ϕ_ ) _._ (2)


**Closed-form (Exact) Computation of NAC:** We now devise the analytical solution for Eqn. 1. Let both _ωτ_ and
_ϕ_ be fixed in pseudo-time interval (frozen-coefficient approximation (John, 1952)) with initial condition _a_ 0, then
closed-form solution is:



**xt** = **[x][t]**

_dt_ _τ_




**[x][t]** _−_ [1]

_dt_ [=] _[ −]_ � _τ_



yields _[d]_ _dt_ **[x][t]** [=] _[ −]_ � _−_ _τ_ [1] _[−]_ _[f]_ [(] **[x][t]** _[,]_ **[ u]** _[, t, θ]_ [)] � **xt** + _f_ ( **xt** _,_ **u** _, t, θ_ ) _A_ .

LNNs are known for their strong expressivity, stability, and
performance in irregularly sampled time-series modeling
(Hasani et al., 2021; 2022).
**NAC’s forward-pass update using ODE solver:** The
state of NAC at time _t_ can be computed using a numerical
ODE solver that simulates the dynamics from an initial
state _a_ 0 to _at_ . The solver discretizes the continuous interval

[0 _, T_ ] into steps [ _t_ 0 _, t_ 1 _, t_ 2 _, . . ., tn_ ], with each step updating
the state from _ti_ to _ti_ +1. For our purposes, we use the
_explicit Euler_ solver, which is simple, efficient, and easy to
implement. Although methods such as Runge-Kutta may
offer higher accuracy, their computational overhead makes
them less suitable for large-scale neural simulations that
require numerous updates, especially since the logits are
normalized, and exact precision is not necessary. Let the
step size be ∆ _t_, with discrete times _tn_ = _n_ ∆ _t_ and logit
states _an_ = _a_ ( _tn_ ). Using the _explicit Euler_ method, the
update is



_dat_

_dt_ [=] _[ −]_ ~~�~~ _[f][ω][τ]_ [ ([] **[q]** � [;] **[ k]** ~~�~~ []] _[, θ][ω][τ]_ [ )] ~~�~~
_ωτ_ ( **u** )



_at_ + _fϕ_ ([ **q** ; **k** ] _, θϕ_ ) _,_ (1)
� ~~�~~ � ~~�~~
_ϕ_ ( **u** )



_at_ = _a_ _[∗]_ + ( _a_ 0 _−_ _a_ _[∗]_ ) _e_ _[−][ω][τ][ t]_
���� ~~�~~ ~~��~~ ~~�~~
steady-state transient



(3)



where **u** = [ **q** ; **k** ] denotes the sparse Top- _K_ concatenated
query–key input. _ωτ_ represents a learnable time-constant
gate head, _ϕ_ denotes a nonlinear content-target head. Both
gates are parameterized by a backbone network derived



Here, _a_ _[∗]_ = _ϕ/ωτ_ is the steady-state solution. The full
derivation is provided in Appendix B.1.



2


**Neuronal Attention Circuit (NAC) for Representation Learning**



**2.1. Stability Analysis of NAC**


We now investigate the stability bounds of _NAC_ under both
the ODE-based and the Closed-Form formulations.


2.1.1. STATE STABILITY


We analyze state stability in both single-connection and
multi-connection settings. This analysis establishes the
boundedness of the attention logit state trajectory, ensuring that, under positive decay rates, the dynamics remain
well-behaved without divergence or overshoot.

**Theorem 1** (State Stability) **.** _Let a_ [(] _t_ _[i]_ [)] _denote the state of_
_the i-th attention logit governed by da_ [(] _t_ _[i]_ [)] _[/dt]_ [ =] _[ −][ω][τ]_ _[a]_ [(] _t_ _[i]_ [)] +
_ϕ. Assume that ϕ and ωτ decompose across M incom-_
_ing connections as ϕ_ = [�] _[M]_ _j_ =1 _[f][ϕ]_ [([] **[q]** _[i]_ [;] **[ k]** _[j]_ [])] _[, and][ ω][τ]_ [ =]
_M_
� _j_ =1 _[f][ω]_ _τ_ [([] **[q]** _[i]_ [;] **[ k]** _[j]_ [])] _[,][ with][ f][ω]_ _τ_ _>_ 0 _._ _Define the per-_
_connection equilibrium Ai,j_ = _fϕ_ ([ **q** _i_ ; **k** _j_ ]) _/fωτ_ ([ **q** _i_ ; **k** _j_ ]) _,_
_and let A_ [min] _i_ = min _j Ai,j and A_ [max] _i_ = max _j Ai,j. Then_
_for any finite horizon t ∈_ [0 _, T_ ] _, the state trajectory satisfies_


min(0 _, A_ [min] _i_ ) _≤_ _a_ [(] _t_ _[i]_ [)] _≤_ max(0 _, A_ [max] _i_ ) _,_ (4)


_provided the initial condition ai_ (0) _lies within this range. In_
_the special case of a single connection (M_ = 1 _), the bounds_
_collapse to_


min(0 _, Ai_ ) _≤_ _a_ [(] _t_ _[i]_ [)] _≤_ max(0 _, Ai_ ) _,_ (5)


_where Ai_ = _fϕ/ωτ is exactly the steady-state solution from_
_Eqn. 3. The proof is provided in the Appendix B.2._


2.1.2. CLOSED-FORM ERROR & EXPONENTIAL BOUNDS

---

[Document 3]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_2
Content: _provided the initial condition ai_ (0) _lies within this range. In_
_the special case of a single connection (M_ = 1 _), the bounds_
_collapse to_


min(0 _, Ai_ ) _≤_ _a_ [(] _t_ _[i]_ [)] _≤_ max(0 _, Ai_ ) _,_ (5)


_where Ai_ = _fϕ/ωτ is exactly the steady-state solution from_
_Eqn. 3. The proof is provided in the Appendix B.2._


2.1.2. CLOSED-FORM ERROR & EXPONENTIAL BOUNDS


We now examine the asymptotic stability, error characterization, and exponential boundedness of the closed-form
formulation. We begin by quantifying the deviation of the
trajectory from its steady-state solution. Define the instanta
neous error
_εt_ = _at −_ _a_ _[∗]_ _,_ (6)


which measures the distance of the system state to equilibrium at time _t_ . From Eqn. 3, the error admits the exact
representation


_εt_ = ( _a_ 0 _−_ _a_ _[∗]_ ) _e_ _[−][ω][τ][ t]_ (7)


In particular, the pointwise absolute error is given by


_|εt|_ = _|a_ 0 _−_ _a_ _[∗]_ _| e_ _[−][ω][τ][ t]_ (8)


This reveals that convergence is not merely asymptotic but
follows an exact exponential law, controlled by the rate parameter _ωτ_ . This yields the following finite-time guarantee.

**Corollary 1** (Exponential decay bound) **.** _If ωτ >_ 0 _, then_
_for all t ≥_ 0 _,_


_|at −_ _a_ _[∗]_ _| ≤|a_ 0 _−_ _a_ _[∗]_ _| e_ _[−][ω][τ][ t]_ _._ (9)



_Remark:_ If _ωτ >_ 0 then lim _t→∞_ _e_ _[−][ω][τ][ t]_ = 0, therefore
_at →_ _a_ _[∗]_ . The convergence is exponential with rate _ωτ_ .
If _ωτ <_ 0 then _e_ _[−][ω][τ][ t]_ = _e_ _[|][ω][τ][ |][t]_ diverges so _at_ grows
exponentially away from _a_ _[∗]_ in magnitude (unless initial
offset _a_ 0 _−_ _a_ _[∗]_ = 0, a measure-zero case). If _ωτ_ = 0 the
ODE is ˙ _a_ = _ϕ_ and the solution is linear in _t_ (unless _ϕ_ = 0).
For bounded dynamics that converge to an interpretable
steady-state, it is required that _ωτ >_ 0.


**Corollary 2** (Uniform initialization) **.** _If the initialization is_
_only known to belong to a bounded set, i.e., |a_ 0 _−_ _a_ _[∗]_ _| ≤_ _M_
_for some M >_ 0 _, then the error admits the uniform bound_


_|at −_ _a_ _[∗]_ _| ≤_ _Me_ _[−][ω][τ][ t]_ _._ (10)


_Remark:_ This bound highlights that exponential convergence holds uniformly across all admissible initial
conditions, with the constant _M_ capturing the worst-case
deviation.


**Corollary 3** (Sample complexity to _δ_ -accuracy) **.** _A natural_
_operational question is the time required to achieve a target_
_tolerance δ >_ 0 _. Solving_


_|a_ 0 _−_ _a_ _[∗]_ _|e_ _[−][ω][τ][ t]_ _≤_ _δ,_ (11)


_We obtain the threshold_




[1] ln _[|][a]_ [0] _[ −]_ _[a][∗][|]_

_ωτ_ _δ_



_t ≥_ [1]



_._ (12)
_δ_



_Remark:_ The convergence rate is inversely proportional to
_ωτ_, and the required time scales only logarithmically in
the accuracy level 1 _/δ_ . Intuitively, larger _ωτ_ accelerates
contraction towards equilibrium, yielding faster attainment
of any prescribed tolerance.


_Figure 1._ Illustration of **(a)** NCPs with pre-determined wiring; **(b)**
Sensory gate, where sensory neurons are active, and the remaining
neurons are disabled for the _q_, _k_, and _v_ projections; **(c)** Backbone,
showing inter-motor projections with sensory neurons disabled in
extended heads for computing _ϕ_ and _ωτ_ .



3



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/neuronal_attention_circuits.pdf-2-0.png)
**Neuronal Attention Circuit (NAC) for Representation Learning**



**Algorithm 1** Repurposed NCPCell


**Require:** Wiring _W_ with ( _A_ in _, A_ rec), groups ( _Ns, Ni, Nc, Nm_ ),
activation _α_, input group _G_ input, output group _G_ output, disabled
groups _D_
**Ensure:** Output _yt ∈_ R _[B][×][d]_ [out], state **x** _t ∈_ R _[B][×][d][h]_

Binary mask: _M_ rec _←|A_ rec _|_, _M_ in _←|A_ in _|_
Initialize parameters : _W_ in _, W_ rec _, b, w_ in _, b_ in _, w_ out _, b_ out
Input neurons: _I_ in _←_ _G_ input
Output neurons: _I_ out _←_ _G_ output
Define activation mask: maskact _,i_ = 0 if _i ∈D_ else 1
Input Projections: ˜ _ut ←_ _ut ⊙_ _w_ in + _b_ in
Recurrent computation:Sparse computation: _st ← rt ←u_ ˜ _t_ ( **x** _Wt−_ in1 _⊙_ ( _WM_ recin _⊙_ ) _M_ rec)
Neuron update: **x** _t ←_ _α_ ( _rt_ + _st_ + _b_ ) _⊙_ maskact
Output mapping: _yt ←_ ( **x** _t_ [ _I_ out] _⊙_ _w_ out) + _b_ out
**return** ( _yt,_ **x** _t_ )


**Algorithm 2** Sparse Top- _K_ Pairwise Concatenation


**Require:** Keys _K ∈_ R _[B][×][H][×][T][k][×][D]_, Top- _K_ value _K_
**Ensure:** concatenated tensor _U ∈_ R _[B][×][H][×][T][q]_ _[×][K]_ [eff] _[×]_ [2] _[D]_

Scores: _S ←_ _Q · K_ _[⊤]_

Effective Top- _K_ : _K_ eff _←_ min( _K, Tk_ )
Indices: _I_ topk _←_ top ~~k~~ ( _S, K_ eff)
Gather: _K_ selected _←_ gather( _K, I_ topk) _∈_ R _[B][×][H][×][T][q]_ _[×][K]_ [eff] _[×][D]_
","The explicit Euler solver in NAC facilitates the forward-pass update by discretizing the continuous interval of the attention logit state into steps, updating the state from `ti` to `ti+1` using a step size `∆t`. The update rule is given by:

`an+1 = an + ∆t (−ωτ an + ϕ)`

While the documents state that NAC, as an ODE-based model, is particularly useful for irregularly sampled time-series data because it models temporal dynamics using differential equations, enabling hidden states to evolve continuously over time in response to inputs, they do not explicitly detail how the *explicit Euler solver itself* facilitates ""adaptive temporal dynamics."" The Euler solver is described as a simple, efficient, and easy-to-implement method for simulating these continuous dynamics.",122,7140,7264,,3,,2026-01-02T20:17:23.665888
"Derive the connection between ""sparse attention patterns"" in SWAA and the ""computational cost"" reduction compared to ""full-causal-attention"" models.",Multi-Hop Chain Reasoning,"[Document 1]
Source: sliding_window_attention_annotated_chunks_2k.jsonl
Chunk ID: sliding_window_attention_annotated_chunks_2k_0
Content: ## **Sliding Window Attention Adaptation**

Yijiong Yu [a], Jiale Liu [b], Qingyun Wu [b], Huazheng Wang [a], and Ji Pei [c]


aOregon State University, {yuyiji, huazheng.wang}@oregonstate.edu
bPenn State University, {jiale.liu, qingyun.wu}@psu.edu
cDeepSolution, research@deepsolution.chat



**Abstract**


The self-attention mechanism in Transformer
based Large Language Models (LLMs) scales
quadratically with input length, making longcontext inference expensive. Sliding window attention (SWA) reduces this cost to linear complexity, but naively enabling complete
SWA at inference-time for models pretrained
with full attention (FA) causes severe longcontext performance degradation due to training–inference mismatch. This makes us wonder: _Can FA-pretrained LLMs be well adapted_
_to SWA without pretraining?_ We investigate
this by proposing Sliding Window Attention
Adaptation (SWAA), a set of practical recipes
that combine five methods for better adaptation: (i) applying SWA only during prefilling;
(ii) preserving “sink” tokens; (iii) interleaving
FA/SWA layers; (iv) chain-of-thought (CoT);
and (v) fine-tuning. Our experiments show that
SWA adaptation is feasible while non-trivial:
no single method suffices, yet specific synergistic combinations effectively recover the original
long-context performance. We further analyze
the performance-efficiency trade-offs of different SWAA configurations and provide recommended recipes for diverse scenarios. Our code
[is available at github.](https://github.com/yuyijiong/sliding-window-attention-adaptation)


**1** **Introduction**


Transformer-based Large Language Models
(LLMs) (Vaswani et al., 2017) demonstrate
remarkable capabilities, but their self-attention
scales quadratically with the input sequence length,
making long context processing inefficient. Sliding
Window Attention (SWA), the most straightforward and widely adopted sparse attention
pattern, which restricts each token’s attention to a
fixed-size local window, reduces the computational
complexity to linearity, along with some other
benefits (see Appendix A).
To apply SWA to LLMs, typical solutions involve training a model with SWA from scratch, but



are prohibitively costly and cannot match the performance of state-of-the-art full-causal-attention

models like Qwen3 (Team, 2025b), mainly due
to the inability to reproduce pretraining data.
Training-free methods like streaming attention
(Xiao et al., 2024) can stabilize LLM outputs by retaining “sink tokens” while applying SWA, which
greatly improve efficiency but inevitably suffer
from severe long-context performance degradation
possibly due to the inaccessibility of distant tokens’
information (Xiao, 2025). This motivates a critical,
unexplored question: _Can a full-attention model_
_be adapted to sliding window attention at low cost_
_while maintaining long-context performance?_
We answer Yes to this question by proposing
Sliding Window Attention Adaptation(SWAA), a
set of recipes for adapting FA-pretrained models to
SWA, which requires neither costly pretraining nor
modifications to the standard Transformer architecture. Specifically, it systematically combines five
practical and composable methods:


1. **Full Attention (FA) Decode** : applying SWA
only during the prefilling stage while switching back to full attention for decoding.


2. **Keep First** _k_ **Tokens** : explicitly preserving
attention to the first _k_ “sink” tokens.


3. **Interleaving FA/SWA layers** : mix fullattention and SWA layers (e.g., assigning
SWA to half layers).


4. **Chain-of-Thought (CoT)** : enforcing an explicit ""thinking"" process during decoding.


5. **Fine-tuning with SWA** : lightweight SWAaware supervised fine-tuning on long-context
data.


Among these, FA Decode is a novel method we
introduce. Keep First _k_ Tokens and FA/SWA Interleaving have been proven effective in prior work



1


(Xiao et al., 2024; Team, 2024a; Zhang et al., 2024),
while CoT and fine-tuning are common LLM techniques. However, how these methods should be
combined to be actually effective for SWA adaptation remains unexplored.
Therefore, in our experiments, we evaluate
SWAA on Qwen3 (Team, 2025b) and Llama3.1
(Team, 2024b) across several long-context benchmarks, measuring both performance and efficiency
under a wide range of SWAA recipes. First, we
find that each method makes a distinct contribution,
but no single ingredient suffices to make SWA competitive with full attention. Second, we show that
specific synergistic combinations of methods can
recover a large fraction of the original long-context
performance. Third, we analyze the performanceefficiency trade-offs of different SWAA recipes and
identify some recommended configurations suitable for different deployment scenarios.
Rather than proposing a single globally optimal
configuration, we view SWAA as a flexible toolkit
of practical recipes: practitioners can select SWAA
recipes that match their accuracy and efficiency
constraints, or compose their own SWA adaptation
strategies by combining the available ingredients.
Our key contributions are:


1. We perform the first systematic study on adapting FA-pretrained LLMs to SWA without pretraining, revealing novel insights about how
SWA impacts LLMs and providing a foundation for future research in efficient sparse
attention.


2. We propose SWAA, a set of practical
SWA adaptation recipes that offer a robust
performance-efficiency balance for various
use cases, accelerating LLM inference from
the bottom level.


3. We implement our methods with FlashAttention (Dao, 2024) and vLLM (Kwon
et al., 2023), making it plug-and-play and userfriendly for practical deployment.


**2** **Related Works**


The _O_ ( _N_ [2] ) complexity of self-attention in Transformers (Vaswani et al., 2017) has spurred a wide
field of research about more efficient language
model architectures. Among the two most popular technological routes are sparse attention and
linear attention.



**2.1** **Sparse Attention**


Our work falls in this category. Sliding Window
Attention (SWA) represents the most basic form
of local sparse attention, yet its performance is inherently limited. Therefore, model architectures
such as Longformer (Beltagy et al., 2020), BigBird
(Zaheer et al., 2020), and RATTENTION (Wang
et al., 2025) combine local SWA on most tokens
with special global attention on specific tokens to
create a more powerful, albeit still sparse, pattern.
Popular LLMs like Gemma2 (Team, 2024a) adopt
SWA in half of their layers to balance the efficiency
of SWA and peformance of FA. Sliding Window
Attention Training (SWAT) (Fu et al., 2025b) introduces architectural changes, such as sigmoid
activation and balanced position embeddings, to
stabilize SWA performance. More advanced methods like Deepseek-sparse-attention (Yuan et al.,
2025; DeepSeek-AI, 2025b), although achieving
excellent quality, involve more complicated implementation and optimization due to semantic-aware
attention operations (e.g., selecting the most important tokens based on attention weights). Regardless,
almost all of the above methods require pretraining with a specific sparse pattern, which is costly
and fails to leverage the advantages of existing pretrained models.


LightTransfer (Zhang et al., 2024) is a promising
attempt at adapting existing models to SWA without pretraining, which has the same motivation as
ours. But it may generalize poorly across model
families (see Appendix G).


**2.2** **Linear Attention**


An alternative approach involves reformulating
the attention mechanism entirely to achieve linear, _O_ ( _N_ ), complexity. This includes methods
such as RNN-like linear attention transformers

---

[Document 2]
Source: sliding_window_attention_annotated_chunks_2k.jsonl
Chunk ID: sliding_window_attention_annotated_chunks_2k_1
Content: LightTransfer (Zhang et al., 2024) is a promising
attempt at adapting existing models to SWA without pretraining, which has the same motivation as
ours. But it may generalize poorly across model
families (see Appendix G).


**2.2** **Linear Attention**


An alternative approach involves reformulating
the attention mechanism entirely to achieve linear, _O_ ( _N_ ), complexity. This includes methods
such as RNN-like linear attention transformers

(Katharopoulos et al., 2020; Peng et al., 2023;
Sun et al., 2023) and structured state-space models
(SSMs) like Mamba (Gu and Dao, 2023). Many
works such as Jamba and Nemotron-Flash(Lieber
et al., 2024; Linsong Chu et al., 2024; Team et al.,
2025; Fu et al., 2025a) interleave linear attention
layers with traditional attention layers to create hybrid model structures. While promising, they represent a fundamental architectural departure from the
standard Transformer, and their performance is generally weaker than traditional Transformer-based

LLMs.



2


**3** **Candidate Methods for SWA**

**Adaptation**


As established, a naive application of SWA leads
to severe long-context performance degradation.
Therefore, we investigate five methods that can potentially facilitate SWA adaptation from distinct
perspectives. However, every method except finetuning has some drawbacks to LLM inference efficiency, as discussed in Appendix A. Therefore,
although these methods are not mutually exclusive,
we should not indiscriminately adopt all of them.
Instead, we must evaluate various combinations to
identify the optimal trade-off between performance
and efficiency.


**3.1** **Full Attention Decode**


This method applies SWA **only** to the prefilling
stage. During the decoding (auto-regressive generation) stage, each token still employs full attention,
allowing access to all previous tokens in the context. The resulting attention mask is depicted in
Figure 1a.
This novel approach, first proposed in our study,
is inspired by human reading comprehension: humans typically scan a passage casually (prefilling)
before thinking deeply to formulate an answer (decoding) for a specific problem. We term this strategy ""reading casually, thinking carefully."" In our
design, the SWA-constrained prefilling stage corresponds to casual reading, while the full-attention
decoding stage enables careful thinking. This analogy also suggests that Chain-of-Thought (CoT)
during decoding may be particularly beneficial, as
extended generation (i.e. ""thinking"") could compensate for the insufficient contextual information
gathered during the prefilling stage.


(a) FA Decode (b) Keep First


Figure 1: (a) Attention mask for FA Decode. SWA is
used for prompt tokens (prefill), and full attention is
used for generated tokens (decode). (b) Attention mask
for SWA combined with Keep First _k_ Tokens.



**3.1.1** **Keep First k Tokens**

Xiao et al. demonstrate that models pretrained with
full attention allocate a disproportionate amount
of attention to the initial tokens (""attention sink""),
and removing the visibility of these tokens causes
performance collapse. Their solution, streaming
attention, involves permanently retaining the attention to these ""sink"" tokens while using SWA, which
successfully maintains the stability of the attention
distribution and the model’s output. Our method
is basically the same: as shown in Figure 1b, any
subsequent token can attend to its local window
**and** the initial _k_ tokens.

Notably, however, our method extends beyond
its original version. Streaming attention operates
only at the KV cache level; specifically, the KV
cache of the sink tokens is retained during decoding, while the prefilling stage is not modified or accelerated at all. In contrast, we directly customize
the Flash-Attention-2 (Dao, 2024) kernel to implement such attention mask, thereby accelerating
prefilling via SWA as well, and eliminating the
need to modify KV cache.


**3.2** **Interleaving Layers**


This method retains full attention on a subset of

layers while applying SWA to the remainder, providing a simple hybrid mechanism to balance the
performance of full attention with the efficiency
of pure SWA. A common strategy involves designating one in every _n_ layers to use full attention
(e.g., layers 0, 2, 4, . . . retain full attention, while all
others use SWA). For example, Gemma-2 (Team,
2024a) uses SWA only for layers [1, 3, 5, . . . ], and
Gemma-3 (Team, 2025a) uses SWA only for layers

[5, 11, 17, ...].
However, for an FA-pretrained model, layers
may exhibit distinct behaviors, suggesting it may
not be optimal to simply assign SWA to layers [1,
3, 5, ...]. Instead, it might be preferable to use
statistical features to select ""lazy"" (mostly focusing
on just recent tokens) layers, as adopted by LightTransfer (Zhang et al., 2024). However, we find
that LightTransfer is not consistently superior in
practice (see Appendix G). Therefore, for simplicity, we still choose the simplest fixed-interval layer
selections in our experiments, such as [0, 2, 4, . . . ]
and [1, 3, 5, ...].


**3.3** **Chain-of-Thought**


Chain-of-Thought (CoT) (Wei et al., 2022) is a
widely used technique for improving model ac


3



```
### Figure - Sliding Window Attention
![Sliding Window Attention](sliding_window_attention.pdf-2-0.png)

**Caption:**  
This figure illustrates the sliding window attention mechanism in a transformer model. The ""Prefill"" region represents the initial input, while the ""Decode"" region shows the subsequent processing.

**Description:**
- The image depicts a grid representing a sequence of data, with a sliding window moving across it.
- The green area indicates the window's current position, highlighting the portion of the sequence being attended to.
- The blue area represents the decoded output based on the attended input.
- The window progressively moves across the sequence, enabling contextual understanding.
```

```
### Figure - Sliding Window Attention
![Sliding Window Attention](sliding_window_attention.pdf-2-1.png)

**Caption:**  
The figure illustrates the sliding window attention mechanism, where attention is retained over a limited window. Attention decreases as the distance from the current position increases.

**Description:**
- The image depicts a grid representing a sequence of tokens.
- The blue squares indicate attention retained across the sliding window.
- The green squares represent attention calculated within the sliding window.
- Attention decreases progressively as the distance 'k' from the current position increases, reflecting a decaying attention mechanism.
- The sliding window size is decreasing as it moves along the sequence.
curacy via reasoning. With the advent of ""thinking"" models, such as DeepSeek-R1 (DeepSeek-AI,
2025a), CoT has evolved from a prompting strategy
to an intrinsic LLM capability. However, whether
CoT has a specific impact in SWA scenarios remains uninvestigated. We explore this by comparing a thinking model with its non-thinking variant,
e.g., Qwen3-4B-Thinking and Qwen3-4B-Instruct,
to verify the effect of CoT on SWA adaptation,
which can produce more notable differences compared to simply adding CoT prompting to the same
model.


**3.4** **Fine-tuning**

---

[Document 3]
Source: sliding_window_attention_annotated_chunks_2k.jsonl
Chunk ID: sliding_window_attention_annotated_chunks_2k_3
Content: **Combinations exhibit strong synergy.** Recipes
that combine multiple methods deliver large gains:


  - **FA Decode + Keep First** _k_ substantially improves over naive SWA (rows 7–9), recovering roughly half to two-thirds of the gap on
the thinking model as _k_ increases. However,
increasing _k_ from 100 to 1000 yields almost
no improvement, indicating that _k_ does not
need to be exceedingly large.


  - **Interleaving Layers + FA Decode** is
markedly stronger (row 13), recovering most
of the gap for the thinking model.


  - **FA Decode + Interleaving Layers + Keep**
**First** _k_ perform best (rows 18). The thinking
model recovers close to 90% of the FA gap
even at 2k window.


**CoT synergizes with FA Decode.** Under recipes
that include FA Decode, the thinking model consistently benefits more than the non-thinking model
(rows 13 and 18), suggesting that CoT synergizes
with FA Decode: preserving global attention at
decoding time enables longer reasoning traces to
capitalize on context processed by SWA, confirming our hypothesis in Section 3.1.


**Sliding window size affects, but is not the deci-**
**sive role.** With FA Decode + Keep First _k_, accuracy improves as the window increases (rows 7,
14, 15), though benefits are modest until 8k. When
added with interleaving FA layers, moving from a
2k to a 4k window is enough to close the remaining
gap on the thinking model (row 22 matches the FA
baseline), indicating that FA Decode and Interleaving are the dominant drivers, and larger windows
mainly smooth residual error.



5


Table 1: Qwen3-4B-Thinking and Qwen3-4B-Instruct results on LongMemEval without SFT


**No.** **window size** **FA layers** **keep first** **FA decode** **Acc think** **Acc non-think**


0 Full [] 0 False **73.0** **62.0**
1 2k [] 0 False 3.2 11.0
2 8k [] 0 False 13.2 19.8


3 2k [] 10 False 16.0 15.6
4 2k [] 0 True 11.8 14.2
5 2k [1, 3, 5, ...] 0 False 13.4 18.4
6 8k [] 0 True 26.2 25.0


7 2k [] 10 True 38.2 20.6
8 2k [] 100 True 50.0 17.8
9 2k [] 1000 True 50.0 20.2
10 2k [0, 2, 4, ...] 10 False 17.0 14.8
11 2k [0, 2, 4, ...] 0 True 32.2 26.0
12 2k [1, 3, 5, ...] 10 False 25.8 36.4
13 2k [1, 3, 5, ...] 0 True 59.2 34.8
14 4k [] 10 True 38.0 24.4
15 8k [] 10 True 49.2 35.2


16 2k [0, 2, 4, ...] 10 True 36.0 17.2
17 2k [1, 3, 5, ...] 10 True 65.0 **53.6**
18 2k [1, 3, 5, ...] 100 True **68.8** 50.6
19 2k [1, 5, 9, ...] 10 True 53.2 31.4
20 2k [1, 9, 17, ...] 10 True 36.4 18.8
21 2k [3, 7, 11, ...] 10 True 54.2 34.6
22 4k [1, 3, 5, ...] 100 True **73.0** **54.2**
23 8k [1, 3, 5, ...] 100 True **71.6** **56.6**



**SWA/FA layer selection has large impacts.** Selecting less layers for FA, e.g. only one fourth or
eighth (row 19, 20), though more efficient, will
significantly decrease the improvement brought by
Interleaving Layers. More importantly, for Qwen34B, configuring _odd-numbered_ layers with full attention is significantly better than _even-numbered_
layers (row 11, 13, 16, 17). However, surprisingly,
this result is reversed for Qwen3-30B-A3B and
Llama3.1-8B-Instruct (row 10 and 11 in Table 4
and 5). This suggests that layer functionalities differ across model families and sizes, necessitating
model-specific layer selection strategies, as discussed in Section 3.2.


Therefore, we answer RQ1: adapting an FA
LLM to SWA is feasible even without any training. But it requires specific combinations of at
least 2 methods, which could be less efficient for
inference.



**5.2** **SWA Adaptation With Fine-tuning**


We next evaluate SWA-aware supervised finetuning, which is expected to provide higher improvement. Table 2 reports LongMemEval_24k
accuracy after SFT under various SWAA configurations. The original full-attention model is also
fine-tuned as a baseline (row 0). Since training is
relatively time-consuming, we only test a representative subset of configurations. Our findings are as
follows:


**Fine-tuning substantially lifts all SWA config-**
**urations.** Comparing all the fine-tuning results
with non-training ones, it is clear that fine-tuning
consistently provides great improvement. However,
simply fine-tuning with naive SWA remains insufficient, only achieving 18.8% and 23.8% accuracy
(row 1).


**FA Decode and Interleaving Layers emerge as**
**dominant components.** After SFT, FA Decode
and Interleaving Layers provide the largest gains.



6


Table 2: Qwen3-4B-Thinking and Qwen3-4B-Instruct results on LongMemEval with SFT


**No.** **window size** **FA layers** **keep first** **FA decode** **Acc think** **Acc non-think**


0 Full [] 0 True **74.6** **63.4**
1 2k [] 0 False 18.8 23.8


2 2k [] 10 False 15.6 /
3 2k [] 0 True 57.9 42.0
4 2k [1, 3, 5, ...] 0 False 63.6 54.6
5 4k [] 0 True 62.6 /


6 2k [] 10 True 56.7 /
7 2k [] 100 True 62.2 42.6
8 2k [1, 3, 5, ...] 0 True **73.2** **58.8**
9 2k [0, 2, 4, ...] 0 True **66.0** /
10 2k [1, 5, 9, ...] 0 True **68.8** 47.0


11 2k [1, 3, 5, ...] 100 True **73.2** **61.4**


Table 3: Recommended SWA adaptation recipes for different needs and scenarios. � means optional.


**Training** ? **Thinking Model?** **Prefer?** **FA Decode** **Interleaving Layers** **Keep First**


No No Any � � �
No Yes Efficiency � � �
No Yes Accuracy � � �


Yes Any Efficiency � � �
Yes Any Accuracy � � �



Using FA Decode alone (row 3) or using Interleaving Layers alone (row 4) both get high accuracy.
And combining both (row 8) further boosts performance to 73 _._ 2 (think) and 58 _._ 8 (non-think), nearly
matching the full-attention SFT baseline in row 0.


**Keep First becomes optional rather than essen-**
**tial.** Before fine-tuning, Keep First is crucial for
stability under SWA. But after SFT, it only provides
minor additional improvement. With FA Decode,
adding _k_ = 100 (row 7) improves over _k_ = 0
(row 3) only 4.5%, and if further combining FA
layers, it almost offers no improvements (row 11
and row 8).


**Effect of sliding window size.** Row 3 and 5
shows that increasing the window from 2k to 4k
with FA Decode improves thinking-model accuracy from 57 _._ 9 to 62 _._ 6. This mirrors the non-SFT
trend that larger windows help, but the dominant
improvements still come from FA Decode and Interleaving Layers.
","Sliding Window Attention (SWA), which is a widely adopted sparse attention pattern used in SWAA, restricts each token's attention to a fixed-size local window. This mechanism reduces the computational complexity from quadratic, $O(N^2)$, for full-causal-attention (or full attention) models to linear, $O(N)$, with respect to the input sequence length ($N$). This reduction in complexity directly translates to a computational cost reduction, making long-context inference less expensive.",137,2418,2556,,3,,2026-01-02T20:17:26.224435
"Explain the reasoning chain that leads from ""human brain anatomy"" inspiration to the definition of ""eight cognitive functions"" in the CogVision framework.",Multi-Hop Chain Reasoning,"[Document 1]
Source: attention_functional_roles_raw_chunks_2k.jsonl
Chunk ID: attention_functional_roles_raw_chunks_2k_1
Content: Furthermore, we validate the functional importance of these heads by showing that their removal
degrades performance on complex tasks and leads to specific error patterns, while their enhancement improves reasoning capabilities. Our findings provide compelling evidence that these attention
heads play a critical role in multimodal reasoning. This insight not only deepens our understanding of the internal organization of VLMs but also suggests potential avenues for designing more
interpretable and cognitive-inspired multimodal AI systems.


2 COGVISION


In this section, we present our dataset Cognitive Vision (CogVision) that contains cognitive process
in multimodal reasoning. CogVision contains 1,409 main questions and 5,744 subquestions. Each


2


example comprises a main question and answer, its subquestions and subanswers and an annotation
specifying the receptive or cognitive function required for each subquestion.


2.1 COGNITIVE FUNCTIONS


To systematically capture the cognitive processes involved in complex reasoning tasks, we consider eight functions related to complex multimodal reasoning. These functions are inspired by
established frameworks in cognitive science (Anderson, 2014; Diamond, 2013), which highlight the
importance of perception, working memory, and reasoning in human cognition.


The functions related to vision include:


    - **Low-level Visual Reception** : Recognizing basic visual features such as color, shape, size,
position, motion.

    - **High-level Visual Reception** : Integrating visual information to recognize objects, patterns,
and scene structure.

    - **Visual Knowledge Recall** : Applying long-term visual knowledge related to visual concepts and their properties.


The functions related to language include:


    - **Language Information Extraction and Understanding** : locating and understanding relevant information from an external source or prior context.

    - **Language Knowledge Recall** : Accessing domain-specific knowledge without visual input.

    - **Math Reasoning** : Performing counting, arithmetic, comparison, and logic-based operations.

    - **Inference** : deriving implicit information that is not directly stated.

    - **Decision-Making** : Selecting the best outcome among alternatives based on reasoning.


This categorization reflects a natural progression from basic information processing to complex cognitive integration. Both the human brain and VLMs encompass a wide range of functional modules.
Our focus in this work is specifically on reasoning-related cognitive functions. By identifying and
organizing these eight core reasoning functions, we can more clearly examine how VLMs handle
different types of thinking steps, in a way that is both systematic and easy to interpret. Detail descriptions of each function and examples in CogVision can be found in Appendix A.1.


2.2 DATA COLLECTIONS


Based on our categorization of reasoning functions, we sampled 2000 diverse questions from existing commonly used visual reasoning benchmarks, selecting 200 examples each from ClevrMath (Lindstr¨om & Abraham, 2022), MathVision (Wang et al., 2024a), MathVista (Lu et al., 2024b),
MMMU (Yue et al., 2024), ScienceQA (Saikh et al., 2022), OKVQA (Marino et al., 2019) (400
examples), VCR (Zellers et al., 2019), VisuLogic (Xu et al., 2025) and Super-Clevr (Li et al.,
2023b) datasets. These datasets span a range of reasoning problems, including logical, mathematical, and commonsense reasoning. Using the chain-of-thought (CoT) paradigm, we prompted
GPT-4.1 (Achiam et al., 2023) to decompose each main question (with its final answer) into subquestions, each targeting a subanswer and a specific function. The prompt encourages structured,
step-by-step reasoning, ensuring that each subquestion is clear, answerable, and sequentially dependent. This process yields a set of subquestion–answer–function (subQAF) triples for each QA pair:
subQAFs = _{_ ( _qi, ai, fi_ ) _}_ _[k]_ _i_ =1 [, where each contains a subquestion] _[ q][i]_ [, its concise answer] _[ a][i]_ [, and the]
corresponding function label _fi_ . The prompt for generating subquestions are list in Appendix A.6.


2.3 DATA FILTERING AND ANNOTATION


Recent advances enable leveraging large pre-trained models for dataset construction, thanks to their
reasoning capabilities and ability to generate high-quality annotations at scale (Wang et al., 2024b).
While our dataset is automatically generated using a VLM to minimize manual effort, we employ a
rigorous two-stage human verification pipeline to ensure data quality and mitigate hallucination. In
the first stage, three expert annotator independently evaluate whether each subquestion is logically
structured and consistent with natural human reasoning. QA pairs with incoherent or inconsistent


3


decompositions are moved out. In the second stage, annotators verify and, if necessary, relabel the
cognitive function associated with each subquestion to ensure accurate alignment with the intended
mental process. Subanswers are further cross-checked with GPT-o3 (OpenAI, 2024) and adjudicated
by humans in cases of disagreement (details in Appendix A.5). This multi-step verification ensures
that each retained subQAF triplet represents a coherent, interpretable reasoning step grounded in
core cognitive functions. After this refinement, our final dataset comprises 1,409 main QA pairs
and 5,744 validated subQAF triplets. The detailed statistics of the dataset including the number of
triplet in training and testing set and distributions of each function can be found in Appendix A.3.
We analyze in-group subquestion diversity, and the results in the Appendix A.2 show that the eight
functions exhibit broad and overlapping distributions in phrasing patterns and token lengths, indicating no systematic surface-form differences.


3 DETECTION OF COGNITIVE FUNCTIONS


Using the CogVision dataset, we adopt a probing-based framework (Alain & Bengio, 2016; Belinkov, 2022; Tenney et al., 2019) to identify which attention heads in VLMs are associated with
specific functions in reasoning process. Specifically, for each functional annotated subquestion, we
extract head activations (see Subsection 3.1), train classifiers and compute accuracies to identify
contributing heads (see Subsection 3.2). Unlike prior work focusing on a single label, our formulation captures many-to-many relationships between heads and cognitive functions, enabling a more
nuanced analysis of functional specialization and overlap within the model.


3.1 HEAD FEATURE EXTRACTION


Given a large VLM _M_, we generate an answer _a_ _[M]_ _i_ for each subquestion _qi_ derived from a main
question _Qi_ . To support coherent multi-step reasoning, we include preceding subquestions and
their answers as contextual input, emulating the incremental reasoning process observed in human
cognition.


During inference, each input token is first mapped into an embedding and then propagated through
the transformer’s layers. At each layer, attention and feedforward operations update the residual
stream, which is ultimately decoded into token predictions. For each generated token _i_, we extract
attention head outputs _Xi_ = _{x_ _[m]_ _l_ _| l ∈_ 1 _, . . ., Nl, m ∈_ 1 _, . . ., Nh}_ across all layers, where _x_ _[m]_ _l_
denotes the value vector from the _m_ -th head in layer _l_ projected into the residual stream, with _Nh_
the number of heads per layer and _Nl_ the total number of layers.


Let _Nt_ denote the number of tokens in the generated answer _a_ _[M]_ _i_ [. To isolate semantically informative]
content relevant to reasoning, we select the top- _k_ most important tokens, determined by prompting
Qwen3-30B LLM (Yang et al., 2025), yielding an index set _Ik_ with _|Ik|_ = _k_ . For each index
_j ∈Ik_, we extract the corresponding attention head activations _Xj_, and compute the averaged
activation feature for the _m_ -th head in layer _l_ as ¯ _x_ _[m]_ _l_ = _k_ [1] � _j ∈Ikxml_ [. This results in a full set of]

---

[Document 2]
Source: attention_functional_roles_raw_chunks_2k.jsonl
Chunk ID: attention_functional_roles_raw_chunks_2k_5
Content: pendix A.11 shows that masking the **math reasoning** heads leads to errors in arithmetic tasks, while
visual receptive functions remain largely unaffected. This confirms that these cognitive heads are
crucial for specific functions and highlights the robustness and generalizability of our method.


**Positive Intervention:** We calculate the activation directions of different functions using the CogVision dataset. For each function, the activation direction of a head at layer _l_ and index _h_ is computed

as:
dir _[h]_ _l_ [=][ E] _[i][∈D]_ correct � _x_ _[h]_ _l_ [(] _[i]_ [)] � _−_ E _i∈D_ incorrect � _x_ _[h]_ _l_ [(] _[i]_ [)] � (3)

where _x_ _[h]_ _l_ [(] _[i]_ [)][ denotes the activation of head at layer] _[ l]_ [ and index] _[ h]_ [, and] _[ D]_ [correct][ and] _[ D]_ [incorrect][ represent]
the sets of samples answered correctly and incorrectly, respectively. Then we estimate the standard
deviation of activations (Li et al., 2023a) along the cognitive function direction to be _σl_ _[h]_ [, and shift]
original head activation as _x_ _[h]_ _l_ [(] _[i]_ [)] _[ ←]_ _[x][h]_ _l_ [(] _[i]_ [) +] _[ ασ]_ _l_ _[h]_ [dir] _l_ _[h]_ [, where] _[ α]_ [ is a parameter.]


The experimental results in Table 5 show that enhancing the activation of functional heads along
their corresponding functional directions improves performance on the related tasks. For example,
positive intervention on vision knowledge recall heads in InternVL3-8B increased accuracy on the
corresponding CogVision question-answering task from 88.54% to 91.67%. Similarly, enhancing
function-specific heads can also boost performance on downstream tasks. Here, we set _α_ = 0 _._ 1
for all datasets, though tuning this parameter may further improve performance. Case analyses are
provided in Appendix A.11.


5 RELATED WORKS


**Neural Networks and the Brain.** Understanding the relationship between artificial neural networks (ANNs) and the biological brain has been a long-standing goal in both neuroscience and
machine learning. Early studies demonstrated that convolutional neural networks (CNNs) trained
on visual tasks develop hierarchical representations reminiscent of the ventral visual stream in primates (Yamins et al., 2014; Cadieu et al., 2014). Subsequent work extended this line of inquiry
to recurrent and transformer-based architectures, showing that attention mechanisms can emulate
aspects of selective processing observed in cortical circuits (Tsividis et al., 2017). More recently,
large language models (LLMs) have exhibited striking parallels with human brain activity during
language processing. In particular, transformer-based models such as GPT-2 produce internal representations that align with neural responses in language-selective brain regions (Caucheteux et al.,
2022; Schrimpf et al., 2021). Some works (Schulze Buschoff et al., 2025; Li et al., 2024) have
studied how VLMs perform differently from humans from a cognitive perspective. Furthermore, the


9



Table 4: Negative Intervention on Visual Question Answering task (OK-VQA and Clevr-Math). The scores
are based on LLM-Judge.








Table 5: Positive Intervention on CogVision test set and other Visual Question Answering benchmarks (OK-VQA, MathVista and Visulogic). The scores are based on LLM-Judge. For OK-VQA,
we perform positive intervention by masking 10% high-level visual reception head, MathVista for
math reasoning heads, and Visulogic for decision-making heads.

|In Domain Out of Domain<br>Model InterHead<br>Math Vision Recall Lang Recall Info Low-Level Inference High-Level Decision OK-VQA MathVista Visulogic|Col2|Col3|
|---|---|---|
|Qwen3B<br>before<br>after|46.40<br>82.29<br>81.37<br>48.03<br>78.26<br>65.87<br>77.38<br>29.73<br>46.40<br>**85.42**<br>**84.31**<br>44.74<br>78.26<br>**69.84**<br>**77.98**<br>**32.43**|62.50<br>60.00<br>26.50<br>62.00<br>60.00<br>**28.00**|
|Qwen7B<br>before<br>after|52.00<br>84.38<br>86.27<br>43.42<br>77.18<br>69.84<br>82.44<br>36.94<br>**52.80**<br>**85.42**<br>86.27<br>**46.05**<br>**82.61**<br>**74.60**<br>82.44<br>36.94|66.00<br>63.00<br>24.00<br>**67.50**<br>63.00<br>**24.50**|
|Intern2B<br>before<br>after|38.40<br>79.17<br>80.39<br>44.08<br>78.26<br>61.90<br>81.55<br>34.23<br>38.40<br>78.13<br>**84.31**<br>42.76<br>**80.43**<br>**64.29**<br>**82.44**<br>**35.14**|58.50<br>51.50<br>24.00<br>**61.50**<br>**52.00**<br>**26.00**|
|Intern8B<br>before<br>after|52.00<br>88.54<br>82.35<br>45.39<br>88.04<br>73.81<br>86.61<br>42.34<br>**52.80**<br>**91.67**<br>**84.31**<br>**46.71**<br>86.96<br>73.81<br>**87.80**<br>**43.24**|67.00<br>66.00<br>24.00<br>**67.50**<br>66.00<br>**26.00**|
|gemma2B<br>before<br>after|26.40<br>62.50<br>80.39<br>32.24<br>34.78<br>48.41<br>33.93<br>23.42<br>**28.00**<br>**64.58**<br>**84.31**<br>**38.16**<br>34.78<br>**50.00**<br>30.95<br>20.72|29.00<br>24.50<br>26.00<br>**29.50**<br>24.50<br>**29.00**|
|gemma4B<br>before<br>after|32.80<br>62.50<br>83.33<br>32.89<br>27.17<br>52.38<br>33.63<br>18.92<br>**35.20**<br>**65.63**<br>**85.29**<br>**35.53**<br>27.17<br>**53.97**<br>**33.63**<br>**19.82**|31.50<br>24.00<br>27.00<br>**33.00**<br>**26.50**<br>**27.50**|



chain-of-thought (CoT) paradigm has been argued to mirror step-by-step human reasoning, leading to improved problem-solving performance. These findings motivate the design of interpretable,
functionally specialized modules in artificial networks, bridging insights from neuroscience with
advances in multimodal reasoning.

---

[Document 3]
Source: attention_functional_roles_raw_chunks_2k.jsonl
Chunk ID: attention_functional_roles_raw_chunks_2k_6
Content: chain-of-thought (CoT) paradigm has been argued to mirror step-by-step human reasoning, leading to improved problem-solving performance. These findings motivate the design of interpretable,
functionally specialized modules in artificial networks, bridging insights from neuroscience with
advances in multimodal reasoning.


**Attention Heads in Vision–Language Models.** A growing body of interpretability research has
revealed that attention heads in LLMs exhibit functional specialization, such as pattern induction,
truthfulness, information retrieval, and safety alignment (Olsson et al., 2022; Li et al., 2023a; Wu
et al.; Zhou et al., 2024; Zheng et al.). Ma et al. further investigates the diverse cognitive roles that
attention heads play in supporting LLM reasoning.


In the multimodal domain, recent works (Li et al., 2020) have begun to explore the internal mechanisms of Vision–Language Models (VLMs). Studies have shown that certain sparse attention heads
play distinct roles in visual grounding, enabling alignment between textual tokens and image regions without additional fine-tuning (Kang et al., 2025; Bi et al., 2025). Similarly, probing studies
on multimodal pre-trained models (e.g., ViLBERT, LXMERT, UNITER) demonstrate that subsets
of attention heads encode cross-modal interactions and semantic alignment between vision and language (Cao et al., 2020). These works highlight the existence of specialized heads in VLMs but
largely focus on perception-oriented tasks such as grounding or alignment. In contrast, we investigate functionally specialized heads under more complex reasoning settings by aligning attention
head behavior with human cognitive functions.


6 CONCLUSION


We propose an interpretability framework that links attention heads in VLMs to human perceptual
and cognitive functions involved in multimodal reasoning. To enable this, we introduce CogVision, a cognitively grounded dataset that decomposes complex multimodal questions into functional
reasoning steps, and apply probing-based analyses to identify specialized heads supporting these
functions. Our study across diverse VLM families reveals that functional heads are sparse, universal, and intrinsic properties of the models, while varying in number, distribution, and hierarchical
organization. Moreover, we find that certain heads exhibit cross-modal interactions. Intervention
experiments further reveal their causal importance. Our insights into the functional organization
of attention mechanisms provide a foundation for developing more interpretable, robust, and cognitively inspired vision-language models. While our work provides a first step toward exploring
potential similarities between the cognitive processes of VLMs and those of the human brain, we do
not claim complete alignment, nor do we equate observations and analyses of attention heads with
the full scope of human reasoning.


10


**Limitations** While our study provides an initial framework for analyzing attention heads in VLMs,
several limitations remain. We focus on eight predefined cognitive functions, which may not cover
the full spectrum of LLM capabilities; future work could expand this taxonomy to include finergrained or emergent functions. Additionally, we concentrate on attention heads, leaving other components such as MLPs unexplored. Further exploring advanced probing methods and extending the
analysis to other model components, could provide further understandings.


REFERENCES


Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical
report. _arXiv preprint arXiv:2303.08774_, 2023.


Guillaume Alain and Yoshua Bengio. Understanding intermediate layers using linear classifier
probes. _arXiv preprint arXiv:1610.01644_, 2016.


John R Anderson. _Rules of the mind_ . Psychology Press, 2014.


Lawrence W Barsalou. _Cognitive psychology: An overview for cognitive scientists_ . Psychology
Press, 2014.


Yonatan Belinkov. Probing classifiers: Promises, shortcomings, and advances. _Computational_
_Linguistics_, 48(1):207–219, 2022.


Jing Bi, Junjia Guo, Yunlong Tang, Lianggong Bruce Wen, Zhang Liu, Bingjie Wang, and Chenliang
Xu. Unveiling visual perception in language models: An attention head analysis approach. In
_Proceedings of the Computer Vision and Pattern Recognition Conference_, pp. 4135–4144, 2025.


Charles F Cadieu, Ha Hong, Daniel LK Yamins, Nicolas Pinto, Diego Ardila, Ethan A Solomon,
Najib J Majaj, and James J DiCarlo. Deep neural networks rival the representation of primate it
cortex for core visual object recognition. _PLoS computational biology_, 10(12):e1003963, 2014.


Jize Cao, Zhe Gan, Yu Cheng, Licheng Yu, Yen-Chun Chen, and Jingjing Liu. Behind the scene:
Revealing the secrets of pre-trained vision-and-language models. In _European Conference on_
_Computer Vision_, pp. 565–580. Springer, 2020.


Charlotte Caucheteux, Alexandre Gramfort, and Jean-R´emi King. Deep language algorithms predict
semantic comprehension from brain activity. _Scientific reports_, 12(1):16327, 2022.


Lin Chin-Yew. Rouge: A package for automatic evaluation of summaries. In _Proceedings of the_
_Workshop on Text Summarization Branches Out, 2004_, 2004.


Adele Diamond. Executive functions. _Annual review of psychology_, 64(1):135–168, 2013.


Edward M Hubbard, Manuela Piazza, Philippe Pinel, and Stanislas Dehaene. Interactions between
number and space in parietal cortex. _Nature reviews neuroscience_, 6(6):435–448, 2005.


Seil Kang, Jinyeong Kim, Junhyeok Kim, and Seong Jae Hwang. Your large vision-language model
only needs a few attention heads for visual grounding. In _Proceedings of the Computer Vision_
_and Pattern Recognition Conference_, pp. 9339–9350, 2025.


Kenneth Li, Oam Patel, Fernanda Vi´egas, Hanspeter Pfister, and Martin Wattenberg. Inference-time
intervention: Eliciting truthful answers from a language model. _Advances in Neural Information_
_Processing Systems_, 36:41451–41530, 2023a.


Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. What does bert with
vision look at? In _Proceedings of the 58th annual meeting of the association for computational_
_linguistics_, pp. 5265–5275, 2020.


Yijiang Li, Qingying Gao, Tianwei Zhao, Bingyang Wang, Haoran Sun, Haiyun Lyu, Robert D
Hawkins, Nuno Vasconcelos, Tal Golan, Dezhi Luo, et al. Core knowledge deficits in multimodal language models. _arXiv preprint arXiv:2410.10855_, 2024.


11


Zhuowan Li, Xingrui Wang, Elias Stengel-Eskin, Adam Kortylewski, Wufei Ma, Benjamin
Van Durme, and Alan L Yuille. Super-clevr: A virtual benchmark to diagnose domain robustness in visual reasoning. In _Proceedings of the IEEE/CVF conference on computer vision and_
_pattern recognition_, pp. 14963–14973, 2023b.


Adam Dahlgren Lindstr¨om and Savitha Sam Abraham. Clevr-math: A dataset for compositional
language, visual and mathematical reasoning. _arXiv preprint arXiv:2208.05358_, 2022.


Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. _Advances_
_in neural information processing systems_, 36:34892–34916, 2023.


Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren,
Zhuoshu Li, Hao Yang, et al. Deepseek-vl: towards real-world vision-language understanding.
_arXiv preprint arXiv:2403.05525_, 2024a.
","The CogVision framework's ""eight cognitive functions"" are inspired by ""established frameworks in cognitive science (Anderson, 2014; Diamond, 2013), which highlight the importance of perception, working memory, and reasoning in human cognition.""

The provided context does not explicitly state that ""human brain anatomy"" was the direct inspiration for the *definition* of these eight cognitive functions. Instead, it refers to broader ""cognitive science frameworks"" and ""human cognition."" While the document discusses the long-standing goal of understanding the relationship between artificial neural networks and the biological brain, and how attention mechanisms can emulate aspects of selective processing observed in cortical circuits, this is presented as a general motivation for designing cognitive-inspired AI systems, rather than the specific origin of the eight functions' definitions.",126,4902,5028,,3,,2026-01-02T20:17:31.253711
"How does ""repurposing C. elegans NCPs"" lead to the specific ""ODE-based"" formulation of attention logits in NAC?",Multi-Hop Chain Reasoning,"[Document 1]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_0
Content: ## **Neuronal Attention Circuit (NAC) for Representation Learning**

**Waleed Razzaq** [1] **Izis Kankaraway** [1] **Yun-Bo Zhao** [1 2]



**Abstract**

Attention improves representation learning over
RNNs, but its discrete nature limits continuoustime (CT) modeling. We introduce Neuronal Attention Circuit (NAC), a novel, biologically plausible CT-Attention mechanism that reformulates

attention logits computation as the solution to a
linear first-order ODE with nonlinear interlinked
gates derived from repurposing _C. elegans_ Neuronal Circuit Policies (NCPs) wiring mechanism.
NAC replaces dense projections with sparse sensory gates for key-query projections and a sparse
backbone network with two heads for computing
_content-target_ and _learnable time-constant_ gates,
enabling efficient adaptive dynamics. NAC supports three attention logit computation modes: (i)
explicit Euler integration, (ii) exact closed-form
solution, and (iii) steady-state approximation. To
improve memory intensity, we implemented a
sparse Top- _K_ pairwise concatenation scheme that
selectively curates key-query interactions. We
provide rigorous theoretical guarantees, including
state stability, bounded approximation errors, and
universal approximation. Empirically, we implemented NAC in diverse domains, including irregular time-series classification, lane-keeping for
autonomous vehicles, and industrial prognostics.
We observed that NAC matches or outperforms
competing baselines in accuracy and occupies an
intermediate position in runtime and memory efficiency compared with several CT baselines.


**1. Introduction**


Learning representations of sequential data in temporal or
spatio-temporal domains is essential for capturing patterns
and enabling accurate forecasting. Discrete-time Recurrent neural networks (DT-RNNs) such as RNN (Rumelhart et al., 1985; Jordan, 1997), Long-short term memory


1Department of Automation, University of Science & Technology of China, Hefei, China [2] Institute of Artificial Intelligence,
Hefei Comprehensive National Science Center. Correspondence to:
Yun-Bo Zhao _<_ ybzhao@ustc.edu.cn _>_, Waleed Razzaq _<_ waleedrazzaq@mail.ustc.edu.cn _>_ .


_Preprint. December 12, 2025._



(LSTM) (Hochreiter & Schmidhuber, 1997), and Gated Recurrent Unit (GRU) (Cho et al., 2014) model sequential dependencies by iteratively updating hidden states to represent
or predict future elements in a sequence. While effective
for regularly sampled sequences, DT-RNNs face challenges
with irregularly sampled data because they assume uniform
time intervals. In addition, vanishing gradients can make
it difficult to capture long-term dependencies (Hochreiter,
1998).
Continuous-time RNNs (CT-RNNs) (Rubanova et al., 2019)
model hidden states as ordinary differential equations
(ODEs), allowing them to process inputs that arrive at arbitrary or irregular time intervals. Mixed-memory RNNs
(mmRNNs) (Lechner & Hasani, 2022) build on this idea
by separating memory compartments from time-continuous
states, helping maintain stable error propagation while capturing continuous-time dynamics. Liquid neural networks
(LNNs) (Hasani et al., 2021; 2022) take a biologically inspired approach by assigning variable time-constants to hidden states, improving adaptability and robustness, though
vanishing gradients can still pose challenges during training.
The attention mechanisms (Vaswani et al., 2017) mitigate
this limitation by treating all time steps equally and allowing models to focus on the most relevant observations. It
computes the similarity between queries ( _q_ ) and keys ( _k_ ),
scaling by the key dimension to keep gradients stable. MultiHead Attention (MHA) (Vaswani et al., 2017) extends this
by allowing the model to attend to different representation
subspaces in parallel. Variants like Sparse Attention (Tay
et al., 2020; Roy et al., 2021), BigBird (Zaheer et al., 2020),
and Longformer (Beltagy et al., 2020) modify the attention
pattern to reduce computational cost, particularly for long sequences, by attending only to selected positions rather than
all pairs. Even with these improvements, attention-based
methods still rely on discrete scaled dot-product operations,
limiting their ability to model continuous trajectories often
captured by CT counterparts.
Recent work has explored bridging this gap through NeuralODE (Chen et al., 2018) formulation. mTAN (Shukla &
Marlin, 2021) learns CT embeddings and uses time-based
attention to interpolate irregular observations into a fixedlength representation for downstream encoder-decoder modeling. ODEFormer (d’Ascoli et al., 2023) trains a sequenceto-sequence transformer on synthetic trajectories to directly
infer symbolic ODE systems from noisy, irregular data,
though it struggles with chaotic systems and generalization



1


**Neuronal Attention Circuit (NAC) for Representation Learning**



beyond observed conditions. Continuous-time Attention
(CTA) (Chien & Chen, 2021) embeds a continuous-time
attention mechanism within a Neural ODE, allowing attention weights and hidden states to evolve jointly over time.
Still, it remains computationally intensive and sensitive to
the accuracy of the ODE solver. ContiFormer (Chen et al.,
2023) builds a CT-transformer by pairing ODE-defined latent trajectories with a time-aware attention mechanism to
model dynamic relationships in data.
Despite these innovations, a persistent and underexplored
gap remains in developing a biologically plausible attention
mechanism that seamlessly integrates CT dynamics with the
abstraction of the brain’s connectome to handle irregular sequences without prohibitive computational costs. Building
on this, we propose a novel attention mechanism called the
_Neuronal Attention Circuit_ (NAC), in which attention logits
are computed as the solution to a first-order ODE modulated
by nonlinear, interlinked gates derived from repurposing
Neuronal Circuit Policies (NCPs) from the nervous system
of _C. elegans_ nematode (refer to Appendix A.2 for more
information). Unlike standard attention, which projects keyquery pairs through a dense layer, NAC employs a sensory
gate to transform input features and a backbone to model
nonlinear interactions, with multiple heads producing outputs structured for attention logits computation. Based on
the solutions to ODE, we define three computation modes:
(i) Exact, using the closed-form ODE solution; (ii) Euler,
approximating the solution via _explicit Euler_ integration;
and (iii) Steady, using only the steady-state solution, analogous to standard attention scores. To reduce computational
complexity, we implemented a sparse Top- _K_ pairwise concatenation algorithm that selectively curates key-query inputs. We evaluate NAC across multiple domains, including
irregularly sampled time series, autonomous vehicle lanekeeping, and Industry 4.0, comparing it to state-of-the-art
baselines. NAC consistently matches or outperforms these
models, while runtime and peak memory benchmarks place
it between CT-RNNs in terms of speed and CT-Attentions
in terms of memory requirements.


**2. Neuronal Attention Circuit (NAC)**


We propose a simple alternative formulation of the attention
logits _a_ (refer to Appendix A.1 for more information), interpreting them as the solution to a first-order linear ODE
modulated by nonlinear, interlinked gates:



from repurposing NCPs. We refer to this formulation as
the Neuronal Attention Circuit (NAC). It enables the logits
_at_ to evolve dynamically with input-dependent, variable
time constants, mirroring the adaptive temporal dynamics
found in _C. elegans_ nervous systems while improving
computational efficiency and expressiveness. Moreover, it
introduces continuous depth into the attention mechanism,
bridging discrete-layer computation with dynamic temporal
evolution.

**Motivation behind this formulation:** The proposed
formulation is loosely motivated by the input-dependent
time-constant mechanism of Liquid Neural Networks
(LNNs), a class of CT-RNNs inspired by biological nervous
systems and synaptic transmission. In this framework, the
dynamics of non-spiking neurons are described by a linear
ODE with nonlinear, interlinked gates: _ddt_ **xt** = **[x]** _τ_ **[t]** [+] **[ S][t]** _[,]_

---

[Document 2]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_10
Content: Zhuang, J., Jia, M., Ding, Y., and Ding, P. Temporal
convolution-based transferable cross-domain adaptation
approach for remaining useful life estimation under variable failure behaviors. _Reliability Engineering & System_
_Safety_, 216:107946, 2021.



10


**Neuronal Attention Circuit (NAC) for Representation Learning**



**Appendix**


**A. Preliminaries**


**A.1. Attention Mechanism**


Attention mechanisms have become a cornerstone in

modern neural architectures, enabling models to dynamically focus on relevant parts of the input. The concept
was first introduced in the context of neural machine
translation, where it allowed the decoder to weight encoder
outputs according to their importance for generating each
target token. Formally, given a query vector _q ∈_ R _[d]_, key
vectors _K_ = [ _k_ 1 _, k_ 2 _, . . ., kn_ ] _∈_ R _[n][×][d]_, and value vectors
_V_ = [ _v_ 1 _, v_ 2 _, . . ., vn_ ] _∈_ R _[n][×][d]_, the attention mechanism can
be expressed in two steps:


1. Compute the scaled dot attention logits:


_ai_ = _[q][T][ k][i]_ (21)
~~_√_~~ _d_


2. Normalize the logits to get attention weights and compute the output:


_e_ _[a][i]_
_αi_ = softmax( _ai_ ) = ~~_n_~~ (22)
~~�~~ _j_ =1 _[e][a][j]_



exhibit a resting potential at _−_ 70 mV and an activation potential near _−_ 20 mV. Similarly, each _Nm_ is composed of
two subneurons, _Mp_ and _Mn_, and is driven by a controllable variable _y_, which also maps to a biologically plausible
range [ _−_ 70 mV _, −_ 20 mV]. The connections in the NCP
architecture are designed to reflect the biological sparsity
and abstraction of neural circuits. Specifically, connections
from _Ns_ to _Ni_ are feedforward, while those between _Nc_
and _Nm_ are highly recurrent (Lechner et al., 2018). Figure
1(a) illustrates the connectome of NCPs.


**B. Proofs**


In this section, we provide all the proofs.


**B.1. Deriving Closed-form (Exact) Solution**


Although _ϕ_ and _ωτ_ are nonlinear functions of the input
**u** = [ **q** ; **k** ], we derive closed-form solution by treating them
as locally constant over the pseudo-time integration interval
for each query–key pair based on frozen-coefficient approximation (John, 1952). This is accurate whenever the interval
is short or when input variations are slow compared with
the relaxation rate _ωτ_ . Under approximation assumption,
rewrite Eqn. 1 as


_dadtt_ [+] _[ ω][τ]_ _[a][t]_ [ =] _[ ϕ.]_ (24)


This is now a linear first-order ODE. The integrating factor
is



Attention( _q, k, v_ ) =



_n_
� _αivi_ (23)


_i_ =1



Here, _ai_ is the raw attention logit between the query and
each key, and the scaling factor _√d_ prevents large dot prod
ucts from destabilizing the softmax (Vaswani et al., 2017).


**A.2. Neuronal Circuit Policies (NCPs)**


NCPs represent a biologically inspired framework for developing interpretable neural control agents by adapting the
tap-withdrawal circuit found in the nematode _C. elegans_
(Lechner et al., 2018). Unlike traditional spiking neural
networks, the majority of neurons in this circuit exhibit
electronic dynamics, characterized by the passive flow of
electrical charges, resulting in graded potentials. NCPs
are structured as a four-layer hierarchical architecture comprising sensory neurons ( _Ns_ ), interneurons ( _Ni_ ), command
neurons ( _Nc_ ), and motor neurons ( _Nm_ ). The _Ns_ perceive
and respond to external stimulus inputs and are responsible for the initial signal transduction. Each _Ns_ consists
of subneurons _Sp_ and _Sn_ and a system variable _x_ . The
activation of _Sp_ and _Sn_ depends upon the sign of _x_ : _Sp_
becomes activated for _x >_ 0, whereas _Sn_ becomes activated for _x <_ 0. The variable _x_ is mapped to the membrane
potential range of [ _−_ 70 mV _, −_ 20 mV], which is consistent
with the biophysical behavior of nerve cells, which typically



� _ωτ dt_

_µ_ = _e_ � �


Multiply both sides by _µ_ ( _t_ ):



Substitute back:


_e_ _[ω][τ][ t]_ _at −_ _a_ 0 = _ϕ ·_ _[e][ω][τ][ t][ −]_ [1] _._ (30)

_ωτ_



= _e_ _[ω][τ][ t]_ _._ (25)




_[da][t]_
_e_ _[ω][τ][ t]_ _dt_ [+] _[ ω][τ]_ _[e][ω][τ][ t][a][t]_ [ =] _[ ϕe][ω][τ][ t][.]_ (26)


Recognize the left-hand side as the derivative of _e_ _[ω][τ][ t]_ _at_ :


_d_
� _e_ _[ω][τ][ t]_ _at_ � = _ϕe_ _[ω][τ][ t]_ _._ (27)
_dt_


Integrate from 0 to _t_ :


_t_
_e_ _[ω][τ][ t]_ _at −_ _e_ [0] _a_ 0 = _ϕ_ _e_ _[ω][τ][ s]_ _ds._ (28)
�0


Compute the integral (since _ωτ ̸_ = 0):



�0 _t_



_e_ _[ω][τ][ s]_ _ds_ = [1]
0 _ωτ_



_ωτ_



� _e_ _[ω][τ][ t]_ _−_ 1� _._ (29)



11


Rearrange:



**Neuronal Attention Circuit (NAC) for Representation Learning**


CASE 2: MULTIPLE CONNECTIONS ( _M >_ 1).



_e_ _[ω][τ][ t]_ _at_ = _a_ 0 + _[ϕ]_

_ωτ_


Divide both sides by _e_ _[ω][τ][ t]_ :


_at_ = _a_ 0 _e_ _[−][ω][τ][ t]_ + _[ϕ]_

_ωτ_



� _e_ _[ω][τ][ t]_ _−_ 1� _._ (31)


�1 _−_ _e_ _[−][ω][τ][ t]_ [�] _._ (32)



The ODE is



with per-connection equilibria _Aj_ = _ϕj/fj_ . The effective
equilibrium is



_da_

_dt_ [=] _[ −]_ � � _[M]_



_da_



� _fj_ � _a_ +

_j_ =1



_M_
� _fjAj,_ (37)

_j_ =1



Set _a_ _[∗]_ := _[ϕ]_ . Then _at_ = _a_ _[∗]_ + ( _a_ 0 _−_ _a_ _[∗]_ ) _e_ _[−][ω][τ][ t]_, proved.

_ωτ_


**B.2. Proof of Theorem 1**

---

[Document 3]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_14
Content: **D.2. Ablations Details**


The brief descriptions of variants and ablation are also divided into four subcategories:
**Top-** _**K**_ **Ablations:** _NAC-2k_ uses Top- _K_ =2 to compute the
logits and _NAC-32k_ uses Top- _K_ =32. All variants use the
exact computation mode with 50% sparsity.
**Sparsity Ablations:** _NAC-02s_ uses 20% sparsity to compute the logits and _NAC-09s_ uses 90%. _NAC-PW_ employs
full pairwise (non-sparse) concatenation for input curation.
_NAC-FC_ replaces the sparse NCP gating mechanism with
a simple fully connected layer. All variants use the exact
computation mode with Top- _K_ =8.
**Modes variants:** _NAC-Euler_ computes attention logits using the explicit Euler integration method. _NAC-Steady_ derives attention logits from the steady-state solution of the
exact formulation. _NAC-Exact/05s/8k_ computes attention
logits using the closed-form exact solution. It also overlaps
with other ablations, so we combined it into a single one.
All modes use Top- _K_ =8, 50% sparsity and _δt_ =1.0. The
sensitivity of NAC to _δt_ is visualized in Figure 4


_Figure 4._ Effect of _δt_ on output of NAC.



14



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/neuronal_attention_circuits.pdf-13-0.png)
**Neuronal Attention Circuit (NAC) for Representation Learning**



**D.3. Experimental Details**


D.3.1. EVENT-BASED MNIST


**Dataset Explanation and Curation:** The MNIST dataset,
introduced by (Deng, 2012), is a widely used benchmark for
computer vision and image classification tasks. It consists
of 70,000 grayscale images of handwritten digits (0–9), each
of size 28 _×_ 28 pixels, split into 60,000 training and 10,000
testing samples.
**Preprocessing:** We follow the preprocessing pipeline described in (Lechner & Hasani, 2022), which proceeds as
follows. First, a threshold is applied to convert the 8-bit
pixel values into binary values, with 128 as the threshold
on a scale from 0 (minimum intensity) to 255 (maximum
intensity). Second, each 28 _×_ 28 image is reshaped into a
one-dimensional time series of length 784. Third, the binary
time series is encoded in an event-based format, eliminating
consecutive occurrences of the same value; for example, the
sequence [1 _,_ 1 _,_ 1 _,_ 1] is transformed into (1 _, t_ = 4). This encoding introduces a temporal dimension and compresses the
sequences from 784 to an average of 53 time steps. Finally,
to facilitate efficient batching and training, each sequence
is padded to a fixed length of 256, and the time dimension
is normalized such that each symbol corresponds to one
unit of time. The resulting dataset defines a per-sequence
classification problem on irregularly sampled time series.
**Neural Network Architecture:** We develop an end-to-end
hybrid neural network by combining compact convolutional
layers with NAC or counterparts baselines for fair comparison. Detailed hyperparameters and architectural specifications are provided in Table 4.


D.3.2. PERSON ACTIVITY RECOGNITION (PAR)


**Dataset Explanation and Curation:** We used the
Localized Person Activity Recognition dataset provided by
UC Irvine (Vidulin et al., 2010). The dataset comprises
25 recordings of human participants performing different
physical activities. The eleven possible activities are
“walking,” “falling,” “lying down,” “lying,” “sitting down,”
“sitting,” “standing up from lying,” “on all fours,” “sitting
on the ground,” “standing up from sitting,” and “standing
up from sitting on the ground.” The objective of this
experiment is to recognize the participant’s activity from
inertial sensors, formulating the task as a per-time-step
classification problem. The input data consist of sensor
readings from four inertial measurement units placed on
participants’ arms and feet. While the sensors are sampled
at a fixed interval of 211 ms, recordings exhibit different
phase shifts and are thus treated as irregularly sampled time
series.

**Preprocessing:** We first separated each participant’s
recordings based on sequence identity and calculated
elapsed time in seconds using the sampling period. To



mitigate class imbalance, we removed excess samples from
overrepresented classes to match the size of the smallest
class. Subsequently, the data were normalized using a
standard scaler. Finally, the dataset was split into a 90:10
ratio for training and testing.
**Neural Network Architecture:** Following the approach in
Section D.3.1, we developed an end-to-end hybrid neural
network combining convolutional heads with NAC or other
baselines. Hyperparameter details are summarized in Table
4.


D.3.3. AUTONOMOUS VEHICLE


**Dataset Explanation and Curation:** We followed the data
collection methodology described in (Razzaq & Hongwei,
2023). For OpenAI-CarRacing, a PPO-trained agent (5M
timesteps) was used to record 20 episodes, yielding approximately 48,174 RGB images of size 92 _×_ 92 _×_ 3 with
corresponding action labels across five discrete actions (noact, move left, forward, move right, stop). The dataset was
split with 10% reserved for testing and the remaining 90%
for training. For the Udacity simulator, we manually controlled the vehicle for 50 minutes, producing 15647 RGB
images of size 320 _×_ 160 _×_ 3, captured from three camera
streams (left, center, right) along with their corresponding
continuous steering values. This dataset was split into 20%
testing and 80% training.
**Preprocessing:** No preprocessing was applied to the
OpenAI-CarRacing dataset. For the Udacity simulator, we
followed the preprocessing steps in (Shibuya, 2017). Each
image was first cropped to remove irrelevant regions and
resized to 66 _×_ 120 _×_ 3. Images were then converted from
RGB to YUV color space to match the network input. To improve robustness, data augmentation techniques, including
random flips, translations, shadow overlays, and brightness
variations, were applied to simulate lateral shifts and diverse
lighting conditions.
**Neural Network Architecture:** For OpenAI-CarRacing,
we modified the neural network architecture proposed in
(Razzaq & Hongwei, 2023), which combines compact CNN
layers for spatial feature extraction with LNNs to capture
temporal dynamics. In our implementation, the LNN layers
were replaced with NAC and its comparable alternatives
for fair evaluation. Full hyperparameter configurations are
provided in Table 4. For the Udacity simulator, we modified
the network proposed in (Bojarski et al., 2016) by replacing
three latent MLP layers with NAC and its counterparts. Full
hyperparameters for this configuration are summarized in
Table 4.

**Saliency Maps:** A saliency map visualizes the regions of
the input that a model attends to when making decisions.
Figure 5 shows the saliency maps for the OpenAI CarRacing environment. We observe that only NAC (Steady, Euler,



15


**Neuronal Attention Circuit (NAC) for Representation Learning**


_Figure 5._ Saliency maps for OpenAI CarRacing


_Figure 6._ Saliency maps for Udacity Simulator



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/neuronal_attention_circuits.pdf-15-0.png)

![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/neuronal_attention_circuits.pdf-15-1.png)
","The ""repurposing C. elegans NCPs"" leads to the specific ODE-based formulation of attention logits in NAC by providing the ""nonlinear interlinked gates"" that modulate the first-order ODE.

Specifically:
*   NAC reformulates attention logits computation as the solution to a linear first-order ODE.
*   The nonlinear, interlinked gates within this ODE are *derived from* repurposing the wiring mechanism of _C. elegans_ Neuronal Circuit Policies (NCPs).
*   This approach is motivated by the adaptive temporal dynamics found in _C. elegans_ nervous systems, allowing the attention logits to evolve dynamically with input-dependent, variable time constants.
*   The NCPs themselves are a biologically inspired framework, characterized by electronic dynamics and graded potentials, with a structured and sparse architecture.",128,6552,6681,,3,,2026-01-02T20:17:37.936421
"Connect the ""vanishing gradient problem"" in ""DT-RNNs"" to the motivation for developing ""Continuous-time RNNs"" and subsequently ""NAC"".",Multi-Hop Chain Reasoning,"[Document 1]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_13
Content: which depends on ( _ϕ, ωτ_ ) both through the equilibrium _a_ _[∗]_

and the exponential term.
**Derivative with respect to** _ϕ_ **:** We obtain



_Interpretation_ : The gradient with respect to _ωτ_ contains
a transient term proportional to _te_ _[−][ω][τ][ t]_, which dominates
at intermediate times, and a steady-state contribution proportional to _−ϕ/ωτ_ [2][, which persists asymptotically. Thus,]
sensitivity to _ωτ_ is time-dependent, peaking before vanishing exponentially in the transient component.


**C.2. Gradient-Based Training**


Like Neural ODEs (Chen et al., 2018) and CT-RNNs
(Rubanova et al., 2019), NAC produces differentiable computational graphs and can be trained using gradient-based
optimization, such as the adjoint sensitivity method (Cao
et al., 2003) or backpropagation through time (BPTT) (LeCun et al., 1988). In this work, we use BPTT exclusively,
as the adjoint sensitivity method can introduce numerical
errors (Zhuang et al., 2020).


**C.3. Efficiency and Complexity**


Table 3 summarizes the computational complexity of different sequence models. For sequence prediction over length
_n_ with hidden dimension _k_, RNNs scale linearly, _O_ ( _nk_ ),
while Attention and NAC scale quadratically, _O_ ( _n_ [2] _k_ ).
ODE-based models, such as LNNs, incur an additional
multiplicative factor _S_ for the number of solver steps.
For single-time-step prediction, RNNs and LSTMs require
_O_ ( _k_ ), whereas Attention and NAC require _O_ ( _nk_ ) when
recomputing attention over the full sequence.



_∂at_

_∂ϕ_ [= 1] _[ −]_ _ω_ _[e]_ _τ_ _[−][ω][τ][ t]_



_∂at_



(46)
_ωτ_



_Interpretation_ : For large _ωτ_, the gradient with respect to _ϕ_
saturates quickly but shrinks to scale _O_ (1 _/ωτ_ ), potentially
slowing learning of _ϕ_ . Conversely, very small _ωτ_ leads
to large steady-state gradients, which may destabilize optimization.

**Derivative with respect to** _ωτ_ **:** Here, both the equilibrium
and the decay rate depend on _ωτ_, yielding



_∂at_
= _−_ _[ϕ]_
_∂ωτ_ _ωτ_ [2]



�1 _−_ _e_ _[−][ω][τ][ t]_ [�] _−_ ( _a_ 0 _−_ _a_ _[∗]_ ) _t e_ _[−][ω][τ][ t]_ _._ (47)



13


**Neuronal Attention Circuit (NAC) for Representation Learning**



_Table 3._ Sequence and time-step prediction complexity. _n_ is the
sequence length and _k_ is the hidden/model dimension.


**Model** **Sequence** **Time-step**
RNN _O_ ( _nk_ ) _O_ ( _k_ )
Attention _O_ ( _n_ [2] _k_ ) _O_ ( _nk_ )
LNN (ODEsolve) _O_ ( _nk · S_ ) _O_ ( _k · S_ )
NAC-Exact _O_ ( _n_ [2] _k_ ) _O_ ( _nk_ )
NAC-Euler _O_ ( _n_ [2] _k_ ) _O_ ( _nk_ )


**D. Evaluation**


**D.1. Related Works**


The brief description for related works is divided into four
subcategories.
**DT-RNNs:** RNN (Rumelhart et al., 1985) captures sequential dependencies in time-series data by updating a hidden state from the current observation and the previous
state. LSTM (Hochreiter & Schmidhuber, 1997) extends
RNNs with input, output, and forget gates, allowing the
network to maintain and update long-term memory, which
improves modeling of long-term dependencies in time-series
sequences. GRU (Cho et al., 2014) simplifies the LSTM
architecture by combining the forget and input gates into a
single update gate, allowing efficient modeling of long-term
dependencies in time-series sequences.
**CT-RNNs:** CT-RNN (Rubanova et al., 2019) model temporal dynamics using differential equations, enabling hidden
states to evolve continuously over time in response to inputs, which is particularly useful for irregularly sampled
time-series data. PhasedLSTM (Neil et al., 2016) introduces a time gate that updates hidden states according to
a rhythmic schedule, enabling efficient modeling of asynchronous or irregularly sampled time-series. GRU-ODE
(De Brouwer et al., 2019) extends the GRU to continuous
time, evolving hidden states via ODEs to handle sequences
with non-uniform time intervals. mmRNN (Lechner &
Hasani, 2022) combines short-term and long-term memory units to capture both fast-changing and slowly evolving
patterns in sequential data. LTC (Hasani et al., 2021) use
neurons with learnable, input-dependent time constants to
adapt the speed of dynamics and capture complex temporal
patterns in continuous-time data. CfC (Hasani et al., 2022)
approximate LTC dynamics analytically, providing efficient
continuous-time modeling without relying on numerical
ODE solvers.

**DT-Attentions:** Attention (Vaswani et al., 2017) computes
attention weights by measuring similarity between queries
and keys, scaling the results, and applying softmax to weigh
time-step contributions. Multi-Head Attention (Vaswani
et al., 2017) applies multiple parallel scaled dot-product
attention mechanisms, capturing different types of temporal
dependencies simultaneously for complex time-series modeling.



**CT-Attentions:** mTAN (Shukla & Marlin, 2021) learns
continuous-time embeddings and uses time-based attention
to interpolate irregular observations into a fixed-length representation for downstream encoder-decoder modeling. CTA
(Chien & Chen, 2021) generalizes discrete-time attention
to continuous-time by representing hidden states, context
vectors, and attention scores as functions whose dynamics are modeled via neural networks and integrated using
ODE solvers for irregular sequences. ODEFormer(d’Ascoli
et al., 2023) trains a sequence-to-sequence transformer on
synthetic trajectories to directly output a symbolic ODE
system from noisy, irregular time-series data. ContiFormer
(Chen et al., 2023) builds a continuous-time Transformer by
pairing ODE-defined latent trajectories with a time-aware
attention mechanism to model dynamic relationships in irregular time-series data.


**D.2. Ablations Details**

---

[Document 2]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_1
Content: **Motivation behind this formulation:** The proposed
formulation is loosely motivated by the input-dependent
time-constant mechanism of Liquid Neural Networks
(LNNs), a class of CT-RNNs inspired by biological nervous
systems and synaptic transmission. In this framework, the
dynamics of non-spiking neurons are described by a linear
ODE with nonlinear, interlinked gates: _ddt_ **xt** = **[x]** _τ_ **[t]** [+] **[ S][t]** _[,]_

where **xt** denotes the hidden state and **St** _∈_ R _[M]_ represents
a nonlinear contribution defined as _f_ ( **xt** _,_ **u** _, t, θ_ )( _A −_ **xt** ).
Here, _A_ and _θ_ are learnable parameters. Plugging **St**
yields _[d]_ **[x][t]** [=] _[ −]_ � _−_ [1] _[−]_ _[f]_ [(] **[x][t]** _[,]_ **[ u]** _[, t, θ]_ [)] � **xt** + _f_ ( **xt** _,_ **u** _, t, θ_ ) _A_ .



_an_ +1 = _an_ + ∆ _t_ ( _−ωτ_ _an_ + _ϕ_ ) _._ (2)


**Closed-form (Exact) Computation of NAC:** We now devise the analytical solution for Eqn. 1. Let both _ωτ_ and
_ϕ_ be fixed in pseudo-time interval (frozen-coefficient approximation (John, 1952)) with initial condition _a_ 0, then
closed-form solution is:



**xt** = **[x][t]**

_dt_ _τ_




**[x][t]** _−_ [1]

_dt_ [=] _[ −]_ � _τ_



yields _[d]_ _dt_ **[x][t]** [=] _[ −]_ � _−_ _τ_ [1] _[−]_ _[f]_ [(] **[x][t]** _[,]_ **[ u]** _[, t, θ]_ [)] � **xt** + _f_ ( **xt** _,_ **u** _, t, θ_ ) _A_ .

LNNs are known for their strong expressivity, stability, and
performance in irregularly sampled time-series modeling
(Hasani et al., 2021; 2022).
**NAC’s forward-pass update using ODE solver:** The
state of NAC at time _t_ can be computed using a numerical
ODE solver that simulates the dynamics from an initial
state _a_ 0 to _at_ . The solver discretizes the continuous interval

[0 _, T_ ] into steps [ _t_ 0 _, t_ 1 _, t_ 2 _, . . ., tn_ ], with each step updating
the state from _ti_ to _ti_ +1. For our purposes, we use the
_explicit Euler_ solver, which is simple, efficient, and easy to
implement. Although methods such as Runge-Kutta may
offer higher accuracy, their computational overhead makes
them less suitable for large-scale neural simulations that
require numerous updates, especially since the logits are
normalized, and exact precision is not necessary. Let the
step size be ∆ _t_, with discrete times _tn_ = _n_ ∆ _t_ and logit
states _an_ = _a_ ( _tn_ ). Using the _explicit Euler_ method, the
update is



_dat_

_dt_ [=] _[ −]_ ~~�~~ _[f][ω][τ]_ [ ([] **[q]** � [;] **[ k]** ~~�~~ []] _[, θ][ω][τ]_ [ )] ~~�~~
_ωτ_ ( **u** )



_at_ + _fϕ_ ([ **q** ; **k** ] _, θϕ_ ) _,_ (1)
� ~~�~~ � ~~�~~
_ϕ_ ( **u** )



_at_ = _a_ _[∗]_ + ( _a_ 0 _−_ _a_ _[∗]_ ) _e_ _[−][ω][τ][ t]_
���� ~~�~~ ~~��~~ ~~�~~
steady-state transient



(3)



where **u** = [ **q** ; **k** ] denotes the sparse Top- _K_ concatenated
query–key input. _ωτ_ represents a learnable time-constant
gate head, _ϕ_ denotes a nonlinear content-target head. Both
gates are parameterized by a backbone network derived



Here, _a_ _[∗]_ = _ϕ/ωτ_ is the steady-state solution. The full
derivation is provided in Appendix B.1.



2


**Neuronal Attention Circuit (NAC) for Representation Learning**



**2.1. Stability Analysis of NAC**


We now investigate the stability bounds of _NAC_ under both
the ODE-based and the Closed-Form formulations.


2.1.1. STATE STABILITY


We analyze state stability in both single-connection and
multi-connection settings. This analysis establishes the
boundedness of the attention logit state trajectory, ensuring that, under positive decay rates, the dynamics remain
well-behaved without divergence or overshoot.

**Theorem 1** (State Stability) **.** _Let a_ [(] _t_ _[i]_ [)] _denote the state of_
_the i-th attention logit governed by da_ [(] _t_ _[i]_ [)] _[/dt]_ [ =] _[ −][ω][τ]_ _[a]_ [(] _t_ _[i]_ [)] +
_ϕ. Assume that ϕ and ωτ decompose across M incom-_
_ing connections as ϕ_ = [�] _[M]_ _j_ =1 _[f][ϕ]_ [([] **[q]** _[i]_ [;] **[ k]** _[j]_ [])] _[, and][ ω][τ]_ [ =]
_M_
� _j_ =1 _[f][ω]_ _τ_ [([] **[q]** _[i]_ [;] **[ k]** _[j]_ [])] _[,][ with][ f][ω]_ _τ_ _>_ 0 _._ _Define the per-_
_connection equilibrium Ai,j_ = _fϕ_ ([ **q** _i_ ; **k** _j_ ]) _/fωτ_ ([ **q** _i_ ; **k** _j_ ]) _,_
_and let A_ [min] _i_ = min _j Ai,j and A_ [max] _i_ = max _j Ai,j. Then_
_for any finite horizon t ∈_ [0 _, T_ ] _, the state trajectory satisfies_


min(0 _, A_ [min] _i_ ) _≤_ _a_ [(] _t_ _[i]_ [)] _≤_ max(0 _, A_ [max] _i_ ) _,_ (4)


_provided the initial condition ai_ (0) _lies within this range. In_
_the special case of a single connection (M_ = 1 _), the bounds_
_collapse to_


min(0 _, Ai_ ) _≤_ _a_ [(] _t_ _[i]_ [)] _≤_ max(0 _, Ai_ ) _,_ (5)


_where Ai_ = _fϕ/ωτ is exactly the steady-state solution from_
_Eqn. 3. The proof is provided in the Appendix B.2._


2.1.2. CLOSED-FORM ERROR & EXPONENTIAL BOUNDS

---

[Document 3]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_8
Content: the ceiling and floor functions, respectively. This results
in a larger overall architectural size and increased runtime.



Future work will support user-defined NCPs configurations
or randomized wiring to enable more efficient architectures.
**Learnable sparse Top-** _**K**_ **selection:** Sparse Top- _K_ attention can miss important context, is sensitive to _k_, and may
be harder to optimize. A further limitation is that it still
computes the full _QK_ _[⊤]_ matrix, which can dominate the
cost for very long sequences. Future work includes adaptive
or learnable Top- _K_ selection, improved key scoring, and
hardware-aware optimization to strengthen accuracy and
robustness.


**5. Conclusion**


In this paper, we introduce the Neuronal Attention Circuit
(NAC), a biologically inspired attention mechanism that
reformulates attention logits as the solution to a first-order
ODE modulated by nonlinear, interlinked gates derived from
repurposing _C.elegans_ nematode NCPs. NAC bridges discrete attention with continuous-time dynamics, enabling
adaptive temporal processing without the limitations inherent to traditional scaled dot-product attention. Based on the
solution to ODE, we introduce three computational modes:
(i) Euler, based on _explicit Euler_ integration; (ii) Exact, providing closed-form solutions; and (iii) Steady, approximating equilibrium states. In addition, a sparse Top- _K_ pairwise
concatenation scheme is introduced to mitigate the memory intensity. Theoretically, we establish NAC’s log-state
stability, exponential error bounds, and universal approximation, thereby providing rigorous guarantees of convergence
and expressiveness. Empirical evaluations demonstrate that
NAC achieves state-of-the-art performance across diverse
tasks, including irregularly sampled time-series benchmarks,
autonomous vehicle lane-keeping, and industrial prognostics. Moreover, NAC occupies an intermediate position
between CT-RNNs and CT-Attention, offering robust temporal modeling while requiring less runtime than CT-RNNs
and less memory than CT-Attention models.


**Reproducibility Statement**


The code for reproducibility is available at
[https://github.com/itxwaleedrazzaq/](https://github.com/itxwaleedrazzaq/neuronal_attention_circuit)

[neuronal_attention_circuit](https://github.com/itxwaleedrazzaq/neuronal_attention_circuit)


**Impact Statement**


The work addresses the growing field of continuous-time
attention and pioneers a biologically plausible mechanism.
It encourages research into sparse, adaptive networks that
resemble natural wiring. From a societal perspective, it
supports more robust AI in resource-limited settings, but it
also raises ethical concerns when applied to areas such as
surveillance or autonomous systems.



8


**Neuronal Attention Circuit (NAC) for Representation Learning**



**References**


Introduction to self-driving cars. URL
[https://www.udacity.com/course/](https://www.udacity.com/course/intro-to-self-driving-cars--nd113)
[intro-to-self-driving-cars--nd113.](https://www.udacity.com/course/intro-to-self-driving-cars--nd113)


Aguiar-Conraria, L. and Soares, M. J. The continuous
wavelet transform: Moving beyond uni-and bivariate analysis. _Journal of economic surveys_, 28(2):344–375, 2014.


Beltagy, I., Peters, M. E., and Cohan, A. Longformer: The long-document transformer. _arXiv preprint_
_arXiv:2004.05150_, 2020.


Bojarski, M., Del Testa, D., Dworakowski, D., Firner, B.,
Flepp, B., Goyal, P., Jackel, L. D., Monfort, M., Muller,
U., Zhang, J., et al. End to end learning for self-driving
cars. _arXiv preprint arXiv:1604.07316_, 2016.


Brockman, G., Cheung, V., Pettersson, L., Schneider, J.,
Schulman, J., Tang, J., and Zaremba, W. Openai gym.
_arXiv preprint arXiv:1606.01540_, 2016.


Cao, Y., Li, S., Petzold, L., and Serban, R. Adjoint sensitivity analysis for differential-algebraic equations: The
adjoint dae system and its numerical solution. _SIAM_
_journal on scientific computing_, 24(3):1076–1089, 2003.


Chen, R. T., Rubanova, Y., Bettencourt, J., and Duvenaud,
D. K. Neural ordinary differential equations. _Advances_
_in neural information processing systems_, 31, 2018.


Chen, Y., Ren, K., Wang, Y., Fang, Y., Sun, W., and Li, D.
Contiformer: Continuous-time transformer for irregular
time series modeling. _Advances in Neural Information_
_Processing Systems_, 36:47143–47175, 2023.


Chien, J.-T. and Chen, Y.-H. Continuous-time attention for
sequential learning. In _Proceedings of the AAAI confer-_
_ence on artificial intelligence_, volume 35, pp. 7116–7124,
2021.


Cho, K., Van Merrienboer, B., Gulcehre, C., Bahdanau,¨
D., Bougares, F., Schwenk, H., and Bengio, Y. Learning phrase representations using rnn encoder-decoder
for statistical machine translation. _arXiv preprint_
_arXiv:1406.1078_, 2014.


d’Ascoli, S., Becker, S., Mathis, A., Schwaller, P., and
Kilbertus, N. Odeformer: Symbolic regression of dynamical systems with transformers. _arXiv preprint_
_arXiv:2310.05573_, 2023.


De Brouwer, E., Simm, J., Arany, A., and Moreau, Y.
Gru-ode-bayes: Continuous modeling of sporadicallyobserved time series. _Advances in neural information_
_processing systems_, 32, 2019.



Deng, L. The mnist database of handwritten digit images
for machine learning research [best of the web]. _IEEE_
_signal processing magazine_, 29(6):141–142, 2012.


Ding, Y., Jia, M., Miao, Q., and Huang, P. Remaining
useful life estimation using deep metric transfer learning
for kernel regression. _Reliability Engineering & System_
_Safety_, 212:107583, 2021.


Hasani, R., Lechner, M., Amini, A., Rus, D., and Grosu,
R. Liquid time-constant networks. In _Proceedings of the_
_AAAI Conference on Artificial Intelligence_, volume 35,
pp. 7657–7666, 2021.


Hasani, R., Lechner, M., Amini, A., Liebenwein, L., Ray,
A., Tschaikowski, M., Teschl, G., and Rus, D. Closed
form continuous-time neural networks. _Nature Machine_

_Intelligence_, 4(11):992–1003, 2022.


Hochreiter, S. The vanishing gradient problem during learning recurrent neural nets and problem solutions. _Interna-_
_tional Journal of Uncertainty, Fuzziness and Knowledge-_
_Based Systems_, 6(02):107–116, 1998.


Hochreiter, S. and Schmidhuber, J. Long short-term memory.
_Neural computation_, 9(8):1735–1780, 1997.


Hong, H. S. and Thuan, N. Hust bearing: a practical dataset
for ball bearing fault diagnosis. _Mendeley Data_, 3, 2023.


John, F. On integration of parabolic equations by difference methods: I. linear and quasi-linear equations for the
infinite interval. _Communications on Pure and Applied_
_Mathematics_, 5(2):155–211, 1952.
","Based on the provided context:

1.  **Vanishing Gradient Problem in DT-RNNs:** The vanishing gradient problem is identified as an issue during learning in ""recurrent neural nets"" (RNNs) [Document 3]. DT-RNNs (Discrete-Time RNNs) are described as models like RNNs, LSTMs, and GRUs that capture sequential dependencies by updating a hidden state from the current observation and previous state [Document 1]. LSTMs and GRUs were developed to improve modeling of long-term dependencies in time-series sequences, which implicitly addresses issues like vanishing gradients in traditional RNNs [Document 1].

2.  **Motivation for CT-RNNs:** CT-RNNs (Continuous-Time RNNs) are motivated by the need to model temporal dynamics using differential equations, allowing hidden states to evolve continuously over time. This is particularly useful for irregularly sampled time-series data [Document 1]. The context does not explicitly state that CT-RNNs were developed to solve the vanishing gradient problem inherent in DT-RNNs.

3.  **Motivation for NAC:** The Neuronal Attention Circuit (NAC) is described as a biologically inspired attention mechanism that reformulates attention logits as the solution to a first-order ODE, bridging discrete attention with continuous-time dynamics. Its formulation is ""loosely motivated by the input-dependent time-constant mechanism of Liquid Neural Networks (LNNs), a class of CT-RNNs"" [Document 2, 3]. NAC aims to enable adaptive temporal processing without the limitations inherent to traditional scaled dot-product attention and offers robust temporal modeling, occupying an intermediate position between CT-RNNs and CT-Attention [Document 3]. The context does not explicitly link the development of NAC to solving the vanishing gradient problem from DT-RNNs.

In summary, while the vanishing gradient problem is associated with DT-RNNs, the provided context does not explicitly state that the development of CT-RNNs or NAC was motivated by this specific problem. Their motivations are primarily related to handling continuous-time dynamics, irregularly sampled data, and bridging different modeling paradigms.",138,6934,7073,,3,,2026-01-02T20:17:45.010868
"How does the ""training-inference mismatch"" in SWA necessitate the specific combination of ""fine-tuning"" and ""Interleaving Layers"" in SWAA?",Multi-Hop Chain Reasoning,"[Document 1]
Source: sliding_window_attention_annotated_chunks_2k.jsonl
Chunk ID: sliding_window_attention_annotated_chunks_2k_2
Content: **3.4** **Fine-tuning**


This is the most natural way to mitigate traininginference mismatch. Apparently, the model should
be fine-tuned while SWA is applied, so that the
model’s parameters can be trained to better adapt
to SWA. Meanwhile, the training data must be longcontext tasks, where SWA actually works.
However, most available long-context datasets
only contain brief ground-truth answers, lacking the reasoning steps required for fine-tuning
a ""thinking"" model. Since our goal is to _restore_
the model’s original capabilities under SWA rather
than teach it new ones, instead of directly using the
original dataset, we adopt an approach similar to
self-distillation (Yang et al., 2024). Specifically, we
utilize the original full-attention model to generate
new answers for the dataset’s questions, and these
generated answers are then filtered for correctness
using GPT-5-Mini (OpenAI, 2025), to make up our
training dataset. For each question, we sample 4
answers with temperature 1, because we find this
strategy is slightly better than generating only one
answer with temperature 0.


**4** **Experiment Setup**


We organize our experiments around three research
questions:


**RQ1: Is SWA adaptation feasible without any**
**additional training?** We evaluate whether
an FA LLM can be adapted to SWA using
only inference-time modifications, and which
combinations of techniques are necessary.


**RQ2: How much does fine-tuning with SWA im-**
**prove performance?** We study the effect of
SWA-aware fine-tuning on long-context performance and identify which components of
SWAA are still required.



**RQ3: Which SWAA configurations achieves the**
**optimal performance-efficiency trade-offs?**
We evaluate how different SWAA configurations trade off accuracy against inference la
tency.


**4.1** **Models**


Our primary experiments use Qwen3-4B-Thinking
and Qwen3-4B-Instruct (Team, 2025b). The Thinking variant enforces chain-of-thought (CoT) style
reasoning, whereas the Instruct variant usually just
answers briefly. To ensure generality, we additionally evaluate Qwen3-30B-A3B-Thinking, Qwen330B-A3B-Instruct (Team, 2025b), and Llama3.18B-Instruct (Touvron et al., 2023).

All models are served with vLLM in float16

precision using a batch size of 64. We use greedy
decoding (temperature = 0) for all evaluations. In
preliminary experiments, we observed that vLLM
yields slightly lower (about 1% to 5%) scores
than HuggingFace Transformers due to precisionrelated discrepancies.


**4.2** **Evaluation Dataset**


SWA is identical to full attention when the context

length is within the window size. Even if the model
is fine-tuned, we can pre-calculate the prompt
length and simply disable the LoRA adapters for
short prompts to get completely the same response
as the original model. Therefore, our experiments
focus exclusively on long-context benchmarks with
inputs exceeding 16k tokens, as re-evaluating models on standard short-context benchmarks (e.g.,
MMLU (Hendrycks et al., 2021), GPQA (Rein
et al., 2023)) is completely unnecessary.
Since we find other long-context benchmarks are
either too easy or too difficult for 4B-level models (see Appendix B), we ultimately select LongMemEval (Wu et al., 2024), a benchmark consisting of various types of long-context QA tasks with
moderate difficulty, although it is originally designed for agent memory system evaluation. Its
context length is controllable by selecting a specific number of chat sessions to concatenate as the
context from a pool of hundreds of sessions (a session contains the chat history between user and
assistant within a day). To create a moderately difficult and discriminative evaluation, we construct
**LongMemEval_24k** by sampling 10 sessions, resulting in 500 samples ranging from 16k to 32k
with an average context length of 24k.



4


For additional validation of generalizability, as
shown in Appendix D, we also experiment on
LongBench-V2 (Bai et al., 2024b), a more modern and challenging benchmark that requires deep
reasoning across various real-world tasks.


**4.3** **Training Details**


For the fine-tuning dataset, we initially considered LongAlign (Bai et al., 2024a), a widely used
long-context fine-tuning dataset for adapt a regularlength model to long-context tasks. However, since

_∼_
its sample count ( 10,000) is insufficient, we incorporate an additional 6,000 samples from Fusangv1-long (Pan, 2024), a more comprehensive corpus
of over 40,000 long-context samples that includes
LongAlign as a subset.
We perform SWA-aware fine-tuning using LoRA
(Hu et al., 2022). Unless otherwise noted, we use
rank _r_ = 16 and _α_ = 128, and apply LoRA only
to the query, key, and value projection modules.
We adopt this parameter-efficient setting because
full-parameter fine-tuning often leads to overfitting
and degradation of the model’s original capabilities
in our preliminary experiments. We use a learning
rate of 1e-4 with a cosine decay schedule. Models
are fine-tuned for a single epoch on the sampled
long-context dataset since we observe no meaningful gains from additional epochs (see Appendix F).
Once training takes approximately 12 hours on an
8*H20 GPU server for Qwen3-4B and 30 hours for
Qwen3-30B-A3B.


**5** **Experiment Results**


**5.1** **SWA Adaptation Without Fine-tuning**


We first study SWA adaptation without any additional training. Table 1 reports LongMemEval_24k
accuracy for Qwen3-4B-Thinking (""think"") and
Qwen3-4B-Instruct (""non-think"") under different
combinations of SWAA components. In most settings, we use an aggressive 2k window to amplify
the impact of SWA. The configurations are ranked
by the number of methods applied (0, 1, 2, or 3 of
Interleaving Layers, Keep First and FA Decode).
Rows 1 (original model) and 2 (naive SWA) serve
as upper and lower baselines, respectively. In the
column ""FA layers"", the value records which layers use full attention, and [] means all the layers
use SWA, i.e., this method is not enabled. In the
column ""keep first"", the value is _k_ in Keep First
_k_ Tokens. When comparing results, an accuracy
difference of less than 5% is usually considered



statistically insignificant. From the results, we find
that:


**Naive SWA is not viable.** Naively replacing FA
with a 2k sliding window attention (row 1) drops
accuracy significantly to 3 _._ 2 and 11 _._ 0, respectively.
Even with an 8k window (row 2), accuracy only recovers to 13 _._ 2 and 19 _._ 8, far below the FA baseline.


**Single method helps, but cannot close the gap.**
Each method—Keep First, FA Decode, or Interleaving Layers—improves over naive SWA (rows 3–6),
yet each alone recovers only a small fraction of the
FA gap and remains well below the baseline. In
short, no single method is sufficient.


**Combinations exhibit strong synergy.** Recipes
that combine multiple methods deliver large gains:


  - **FA Decode + Keep First** _k_ substantially improves over naive SWA (rows 7–9), recovering roughly half to two-thirds of the gap on
the thinking model as _k_ increases. However,
increasing _k_ from 100 to 1000 yields almost
no improvement, indicating that _k_ does not
need to be exceedingly large.


  - **Interleaving Layers + FA Decode** is
markedly stronger (row 13), recovering most
of the gap for the thinking model.


  - **FA Decode + Interleaving Layers + Keep**
**First** _k_ perform best (rows 18). The thinking
model recovers close to 90% of the FA gap
even at 2k window.

---

[Document 2]
Source: sliding_window_attention_annotated_chunks_2k.jsonl
Chunk ID: sliding_window_attention_annotated_chunks_2k_3
Content: **Combinations exhibit strong synergy.** Recipes
that combine multiple methods deliver large gains:


  - **FA Decode + Keep First** _k_ substantially improves over naive SWA (rows 7–9), recovering roughly half to two-thirds of the gap on
the thinking model as _k_ increases. However,
increasing _k_ from 100 to 1000 yields almost
no improvement, indicating that _k_ does not
need to be exceedingly large.


  - **Interleaving Layers + FA Decode** is
markedly stronger (row 13), recovering most
of the gap for the thinking model.


  - **FA Decode + Interleaving Layers + Keep**
**First** _k_ perform best (rows 18). The thinking
model recovers close to 90% of the FA gap
even at 2k window.


**CoT synergizes with FA Decode.** Under recipes
that include FA Decode, the thinking model consistently benefits more than the non-thinking model
(rows 13 and 18), suggesting that CoT synergizes
with FA Decode: preserving global attention at
decoding time enables longer reasoning traces to
capitalize on context processed by SWA, confirming our hypothesis in Section 3.1.


**Sliding window size affects, but is not the deci-**
**sive role.** With FA Decode + Keep First _k_, accuracy improves as the window increases (rows 7,
14, 15), though benefits are modest until 8k. When
added with interleaving FA layers, moving from a
2k to a 4k window is enough to close the remaining
gap on the thinking model (row 22 matches the FA
baseline), indicating that FA Decode and Interleaving are the dominant drivers, and larger windows
mainly smooth residual error.



5


Table 1: Qwen3-4B-Thinking and Qwen3-4B-Instruct results on LongMemEval without SFT


**No.** **window size** **FA layers** **keep first** **FA decode** **Acc think** **Acc non-think**


0 Full [] 0 False **73.0** **62.0**
1 2k [] 0 False 3.2 11.0
2 8k [] 0 False 13.2 19.8


3 2k [] 10 False 16.0 15.6
4 2k [] 0 True 11.8 14.2
5 2k [1, 3, 5, ...] 0 False 13.4 18.4
6 8k [] 0 True 26.2 25.0


7 2k [] 10 True 38.2 20.6
8 2k [] 100 True 50.0 17.8
9 2k [] 1000 True 50.0 20.2
10 2k [0, 2, 4, ...] 10 False 17.0 14.8
11 2k [0, 2, 4, ...] 0 True 32.2 26.0
12 2k [1, 3, 5, ...] 10 False 25.8 36.4
13 2k [1, 3, 5, ...] 0 True 59.2 34.8
14 4k [] 10 True 38.0 24.4
15 8k [] 10 True 49.2 35.2


16 2k [0, 2, 4, ...] 10 True 36.0 17.2
17 2k [1, 3, 5, ...] 10 True 65.0 **53.6**
18 2k [1, 3, 5, ...] 100 True **68.8** 50.6
19 2k [1, 5, 9, ...] 10 True 53.2 31.4
20 2k [1, 9, 17, ...] 10 True 36.4 18.8
21 2k [3, 7, 11, ...] 10 True 54.2 34.6
22 4k [1, 3, 5, ...] 100 True **73.0** **54.2**
23 8k [1, 3, 5, ...] 100 True **71.6** **56.6**



**SWA/FA layer selection has large impacts.** Selecting less layers for FA, e.g. only one fourth or
eighth (row 19, 20), though more efficient, will
significantly decrease the improvement brought by
Interleaving Layers. More importantly, for Qwen34B, configuring _odd-numbered_ layers with full attention is significantly better than _even-numbered_
layers (row 11, 13, 16, 17). However, surprisingly,
this result is reversed for Qwen3-30B-A3B and
Llama3.1-8B-Instruct (row 10 and 11 in Table 4
and 5). This suggests that layer functionalities differ across model families and sizes, necessitating
model-specific layer selection strategies, as discussed in Section 3.2.


Therefore, we answer RQ1: adapting an FA
LLM to SWA is feasible even without any training. But it requires specific combinations of at
least 2 methods, which could be less efficient for
inference.



**5.2** **SWA Adaptation With Fine-tuning**


We next evaluate SWA-aware supervised finetuning, which is expected to provide higher improvement. Table 2 reports LongMemEval_24k
accuracy after SFT under various SWAA configurations. The original full-attention model is also
fine-tuned as a baseline (row 0). Since training is
relatively time-consuming, we only test a representative subset of configurations. Our findings are as
follows:


**Fine-tuning substantially lifts all SWA config-**
**urations.** Comparing all the fine-tuning results
with non-training ones, it is clear that fine-tuning
consistently provides great improvement. However,
simply fine-tuning with naive SWA remains insufficient, only achieving 18.8% and 23.8% accuracy
(row 1).


**FA Decode and Interleaving Layers emerge as**
**dominant components.** After SFT, FA Decode
and Interleaving Layers provide the largest gains.



6


Table 2: Qwen3-4B-Thinking and Qwen3-4B-Instruct results on LongMemEval with SFT


**No.** **window size** **FA layers** **keep first** **FA decode** **Acc think** **Acc non-think**


0 Full [] 0 True **74.6** **63.4**
1 2k [] 0 False 18.8 23.8


2 2k [] 10 False 15.6 /
3 2k [] 0 True 57.9 42.0
4 2k [1, 3, 5, ...] 0 False 63.6 54.6
5 4k [] 0 True 62.6 /


6 2k [] 10 True 56.7 /
7 2k [] 100 True 62.2 42.6
8 2k [1, 3, 5, ...] 0 True **73.2** **58.8**
9 2k [0, 2, 4, ...] 0 True **66.0** /
10 2k [1, 5, 9, ...] 0 True **68.8** 47.0


11 2k [1, 3, 5, ...] 100 True **73.2** **61.4**


Table 3: Recommended SWA adaptation recipes for different needs and scenarios. � means optional.


**Training** ? **Thinking Model?** **Prefer?** **FA Decode** **Interleaving Layers** **Keep First**


No No Any � � �
No Yes Efficiency � � �
No Yes Accuracy � � �


Yes Any Efficiency � � �
Yes Any Accuracy � � �



Using FA Decode alone (row 3) or using Interleaving Layers alone (row 4) both get high accuracy.
And combining both (row 8) further boosts performance to 73 _._ 2 (think) and 58 _._ 8 (non-think), nearly
matching the full-attention SFT baseline in row 0.


**Keep First becomes optional rather than essen-**
**tial.** Before fine-tuning, Keep First is crucial for
stability under SWA. But after SFT, it only provides
minor additional improvement. With FA Decode,
adding _k_ = 100 (row 7) improves over _k_ = 0
(row 3) only 4.5%, and if further combining FA
layers, it almost offers no improvements (row 11
and row 8).


**Effect of sliding window size.** Row 3 and 5
shows that increasing the window from 2k to 4k
with FA Decode improves thinking-model accuracy from 57 _._ 9 to 62 _._ 6. This mirrors the non-SFT
trend that larger windows help, but the dominant
improvements still come from FA Decode and Interleaving Layers.

---

[Document 3]
Source: sliding_window_attention_annotated_chunks_2k.jsonl
Chunk ID: sliding_window_attention_annotated_chunks_2k_1
Content: LightTransfer (Zhang et al., 2024) is a promising
attempt at adapting existing models to SWA without pretraining, which has the same motivation as
ours. But it may generalize poorly across model
families (see Appendix G).


**2.2** **Linear Attention**


An alternative approach involves reformulating
the attention mechanism entirely to achieve linear, _O_ ( _N_ ), complexity. This includes methods
such as RNN-like linear attention transformers

(Katharopoulos et al., 2020; Peng et al., 2023;
Sun et al., 2023) and structured state-space models
(SSMs) like Mamba (Gu and Dao, 2023). Many
works such as Jamba and Nemotron-Flash(Lieber
et al., 2024; Linsong Chu et al., 2024; Team et al.,
2025; Fu et al., 2025a) interleave linear attention
layers with traditional attention layers to create hybrid model structures. While promising, they represent a fundamental architectural departure from the
standard Transformer, and their performance is generally weaker than traditional Transformer-based

LLMs.



2


**3** **Candidate Methods for SWA**

**Adaptation**


As established, a naive application of SWA leads
to severe long-context performance degradation.
Therefore, we investigate five methods that can potentially facilitate SWA adaptation from distinct
perspectives. However, every method except finetuning has some drawbacks to LLM inference efficiency, as discussed in Appendix A. Therefore,
although these methods are not mutually exclusive,
we should not indiscriminately adopt all of them.
Instead, we must evaluate various combinations to
identify the optimal trade-off between performance
and efficiency.


**3.1** **Full Attention Decode**


This method applies SWA **only** to the prefilling
stage. During the decoding (auto-regressive generation) stage, each token still employs full attention,
allowing access to all previous tokens in the context. The resulting attention mask is depicted in
Figure 1a.
This novel approach, first proposed in our study,
is inspired by human reading comprehension: humans typically scan a passage casually (prefilling)
before thinking deeply to formulate an answer (decoding) for a specific problem. We term this strategy ""reading casually, thinking carefully."" In our
design, the SWA-constrained prefilling stage corresponds to casual reading, while the full-attention
decoding stage enables careful thinking. This analogy also suggests that Chain-of-Thought (CoT)
during decoding may be particularly beneficial, as
extended generation (i.e. ""thinking"") could compensate for the insufficient contextual information
gathered during the prefilling stage.


(a) FA Decode (b) Keep First


Figure 1: (a) Attention mask for FA Decode. SWA is
used for prompt tokens (prefill), and full attention is
used for generated tokens (decode). (b) Attention mask
for SWA combined with Keep First _k_ Tokens.



**3.1.1** **Keep First k Tokens**

Xiao et al. demonstrate that models pretrained with
full attention allocate a disproportionate amount
of attention to the initial tokens (""attention sink""),
and removing the visibility of these tokens causes
performance collapse. Their solution, streaming
attention, involves permanently retaining the attention to these ""sink"" tokens while using SWA, which
successfully maintains the stability of the attention
distribution and the model’s output. Our method
is basically the same: as shown in Figure 1b, any
subsequent token can attend to its local window
**and** the initial _k_ tokens.

Notably, however, our method extends beyond
its original version. Streaming attention operates
only at the KV cache level; specifically, the KV
cache of the sink tokens is retained during decoding, while the prefilling stage is not modified or accelerated at all. In contrast, we directly customize
the Flash-Attention-2 (Dao, 2024) kernel to implement such attention mask, thereby accelerating
prefilling via SWA as well, and eliminating the
need to modify KV cache.


**3.2** **Interleaving Layers**


This method retains full attention on a subset of

layers while applying SWA to the remainder, providing a simple hybrid mechanism to balance the
performance of full attention with the efficiency
of pure SWA. A common strategy involves designating one in every _n_ layers to use full attention
(e.g., layers 0, 2, 4, . . . retain full attention, while all
others use SWA). For example, Gemma-2 (Team,
2024a) uses SWA only for layers [1, 3, 5, . . . ], and
Gemma-3 (Team, 2025a) uses SWA only for layers

[5, 11, 17, ...].
However, for an FA-pretrained model, layers
may exhibit distinct behaviors, suggesting it may
not be optimal to simply assign SWA to layers [1,
3, 5, ...]. Instead, it might be preferable to use
statistical features to select ""lazy"" (mostly focusing
on just recent tokens) layers, as adopted by LightTransfer (Zhang et al., 2024). However, we find
that LightTransfer is not consistently superior in
practice (see Appendix G). Therefore, for simplicity, we still choose the simplest fixed-interval layer
selections in our experiments, such as [0, 2, 4, . . . ]
and [1, 3, 5, ...].


**3.3** **Chain-of-Thought**


Chain-of-Thought (CoT) (Wei et al., 2022) is a
widely used technique for improving model ac


3



```
### Figure - Sliding Window Attention
![Sliding Window Attention](sliding_window_attention.pdf-2-0.png)

**Caption:**  
This figure illustrates the sliding window attention mechanism in a transformer model. The ""Prefill"" region represents the initial input, while the ""Decode"" region shows the subsequent processing.

**Description:**
- The image depicts a grid representing a sequence of data, with a sliding window moving across it.
- The green area indicates the window's current position, highlighting the portion of the sequence being attended to.
- The blue area represents the decoded output based on the attended input.
- The window progressively moves across the sequence, enabling contextual understanding.
```

```
### Figure - Sliding Window Attention
![Sliding Window Attention](sliding_window_attention.pdf-2-1.png)

**Caption:**  
The figure illustrates the sliding window attention mechanism, where attention is retained over a limited window. Attention decreases as the distance from the current position increases.

**Description:**
- The image depicts a grid representing a sequence of tokens.
- The blue squares indicate attention retained across the sliding window.
- The green squares represent attention calculated within the sliding window.
- Attention decreases progressively as the distance 'k' from the current position increases, reflecting a decaying attention mechanism.
- The sliding window size is decreasing as it moves along the sequence.
curacy via reasoning. With the advent of ""thinking"" models, such as DeepSeek-R1 (DeepSeek-AI,
2025a), CoT has evolved from a prompting strategy
to an intrinsic LLM capability. However, whether
CoT has a specific impact in SWA scenarios remains uninvestigated. We explore this by comparing a thinking model with its non-thinking variant,
e.g., Qwen3-4B-Thinking and Qwen3-4B-Instruct,
to verify the effect of CoT on SWA adaptation,
which can produce more notable differences compared to simply adding CoT prompting to the same
model.


**3.4** **Fine-tuning**
","The ""training-inference mismatch"" in SWA leads to severe long-context performance degradation. Fine-tuning is identified as the ""most natural way to mitigate"" this mismatch, as it trains the model's parameters to better adapt to SWA.

However, simply fine-tuning with naive SWA ""remains insufficient,"" only achieving low accuracy (18.8% and 23.8%). This indicates that fine-tuning alone cannot fully overcome the performance issues caused by the mismatch.

It is the combination of fine-tuning with ""FA Decode and Interleaving Layers"" that provides the ""largest gains"" and ""further boosts performance... nearly matching the full-attention SFT baseline."" Interleaving Layers specifically helps by retaining full attention on a subset of layers while applying SWA to the remainder, creating a hybrid mechanism to balance performance and efficiency. Therefore, the combination is necessitated because while fine-tuning adapts the model to SWA, Interleaving Layers (along with FA Decode) are crucial to fully recover and optimize performance that fine-tuning alone cannot achieve, effectively addressing the remaining impact of the training-inference mismatch.",142,7241,7384,,3,,2026-01-02T20:17:52.395599
"Trace the impact of ""positive intervention"" on ""functional heads"" to the resulting improvement in ""visual reasoning tasks"" accuracy.",Multi-Hop Chain Reasoning,"[Document 1]
Source: attention_functional_roles_raw_chunks_2k.jsonl
Chunk ID: attention_functional_roles_raw_chunks_2k_5
Content: pendix A.11 shows that masking the **math reasoning** heads leads to errors in arithmetic tasks, while
visual receptive functions remain largely unaffected. This confirms that these cognitive heads are
crucial for specific functions and highlights the robustness and generalizability of our method.


**Positive Intervention:** We calculate the activation directions of different functions using the CogVision dataset. For each function, the activation direction of a head at layer _l_ and index _h_ is computed

as:
dir _[h]_ _l_ [=][ E] _[i][∈D]_ correct � _x_ _[h]_ _l_ [(] _[i]_ [)] � _−_ E _i∈D_ incorrect � _x_ _[h]_ _l_ [(] _[i]_ [)] � (3)

where _x_ _[h]_ _l_ [(] _[i]_ [)][ denotes the activation of head at layer] _[ l]_ [ and index] _[ h]_ [, and] _[ D]_ [correct][ and] _[ D]_ [incorrect][ represent]
the sets of samples answered correctly and incorrectly, respectively. Then we estimate the standard
deviation of activations (Li et al., 2023a) along the cognitive function direction to be _σl_ _[h]_ [, and shift]
original head activation as _x_ _[h]_ _l_ [(] _[i]_ [)] _[ ←]_ _[x][h]_ _l_ [(] _[i]_ [) +] _[ ασ]_ _l_ _[h]_ [dir] _l_ _[h]_ [, where] _[ α]_ [ is a parameter.]


The experimental results in Table 5 show that enhancing the activation of functional heads along
their corresponding functional directions improves performance on the related tasks. For example,
positive intervention on vision knowledge recall heads in InternVL3-8B increased accuracy on the
corresponding CogVision question-answering task from 88.54% to 91.67%. Similarly, enhancing
function-specific heads can also boost performance on downstream tasks. Here, we set _α_ = 0 _._ 1
for all datasets, though tuning this parameter may further improve performance. Case analyses are
provided in Appendix A.11.


5 RELATED WORKS


**Neural Networks and the Brain.** Understanding the relationship between artificial neural networks (ANNs) and the biological brain has been a long-standing goal in both neuroscience and
machine learning. Early studies demonstrated that convolutional neural networks (CNNs) trained
on visual tasks develop hierarchical representations reminiscent of the ventral visual stream in primates (Yamins et al., 2014; Cadieu et al., 2014). Subsequent work extended this line of inquiry
to recurrent and transformer-based architectures, showing that attention mechanisms can emulate
aspects of selective processing observed in cortical circuits (Tsividis et al., 2017). More recently,
large language models (LLMs) have exhibited striking parallels with human brain activity during
language processing. In particular, transformer-based models such as GPT-2 produce internal representations that align with neural responses in language-selective brain regions (Caucheteux et al.,
2022; Schrimpf et al., 2021). Some works (Schulze Buschoff et al., 2025; Li et al., 2024) have
studied how VLMs perform differently from humans from a cognitive perspective. Furthermore, the


9



Table 4: Negative Intervention on Visual Question Answering task (OK-VQA and Clevr-Math). The scores
are based on LLM-Judge.








Table 5: Positive Intervention on CogVision test set and other Visual Question Answering benchmarks (OK-VQA, MathVista and Visulogic). The scores are based on LLM-Judge. For OK-VQA,
we perform positive intervention by masking 10% high-level visual reception head, MathVista for
math reasoning heads, and Visulogic for decision-making heads.

|In Domain Out of Domain<br>Model InterHead<br>Math Vision Recall Lang Recall Info Low-Level Inference High-Level Decision OK-VQA MathVista Visulogic|Col2|Col3|
|---|---|---|
|Qwen3B<br>before<br>after|46.40<br>82.29<br>81.37<br>48.03<br>78.26<br>65.87<br>77.38<br>29.73<br>46.40<br>**85.42**<br>**84.31**<br>44.74<br>78.26<br>**69.84**<br>**77.98**<br>**32.43**|62.50<br>60.00<br>26.50<br>62.00<br>60.00<br>**28.00**|
|Qwen7B<br>before<br>after|52.00<br>84.38<br>86.27<br>43.42<br>77.18<br>69.84<br>82.44<br>36.94<br>**52.80**<br>**85.42**<br>86.27<br>**46.05**<br>**82.61**<br>**74.60**<br>82.44<br>36.94|66.00<br>63.00<br>24.00<br>**67.50**<br>63.00<br>**24.50**|
|Intern2B<br>before<br>after|38.40<br>79.17<br>80.39<br>44.08<br>78.26<br>61.90<br>81.55<br>34.23<br>38.40<br>78.13<br>**84.31**<br>42.76<br>**80.43**<br>**64.29**<br>**82.44**<br>**35.14**|58.50<br>51.50<br>24.00<br>**61.50**<br>**52.00**<br>**26.00**|
|Intern8B<br>before<br>after|52.00<br>88.54<br>82.35<br>45.39<br>88.04<br>73.81<br>86.61<br>42.34<br>**52.80**<br>**91.67**<br>**84.31**<br>**46.71**<br>86.96<br>73.81<br>**87.80**<br>**43.24**|67.00<br>66.00<br>24.00<br>**67.50**<br>66.00<br>**26.00**|
|gemma2B<br>before<br>after|26.40<br>62.50<br>80.39<br>32.24<br>34.78<br>48.41<br>33.93<br>23.42<br>**28.00**<br>**64.58**<br>**84.31**<br>**38.16**<br>34.78<br>**50.00**<br>30.95<br>20.72|29.00<br>24.50<br>26.00<br>**29.50**<br>24.50<br>**29.00**|
|gemma4B<br>before<br>after|32.80<br>62.50<br>83.33<br>32.89<br>27.17<br>52.38<br>33.63<br>18.92<br>**35.20**<br>**65.63**<br>**85.29**<br>**35.53**<br>27.17<br>**53.97**<br>**33.63**<br>**19.82**|31.50<br>24.00<br>27.00<br>**33.00**<br>**26.50**<br>**27.50**|



chain-of-thought (CoT) paradigm has been argued to mirror step-by-step human reasoning, leading to improved problem-solving performance. These findings motivate the design of interpretable,
functionally specialized modules in artificial networks, bridging insights from neuroscience with
advances in multimodal reasoning.

---

[Document 2]
Source: attention_functional_roles_raw_chunks_2k.jsonl
Chunk ID: attention_functional_roles_raw_chunks_2k_1
Content: Furthermore, we validate the functional importance of these heads by showing that their removal
degrades performance on complex tasks and leads to specific error patterns, while their enhancement improves reasoning capabilities. Our findings provide compelling evidence that these attention
heads play a critical role in multimodal reasoning. This insight not only deepens our understanding of the internal organization of VLMs but also suggests potential avenues for designing more
interpretable and cognitive-inspired multimodal AI systems.


2 COGVISION


In this section, we present our dataset Cognitive Vision (CogVision) that contains cognitive process
in multimodal reasoning. CogVision contains 1,409 main questions and 5,744 subquestions. Each


2


example comprises a main question and answer, its subquestions and subanswers and an annotation
specifying the receptive or cognitive function required for each subquestion.


2.1 COGNITIVE FUNCTIONS


To systematically capture the cognitive processes involved in complex reasoning tasks, we consider eight functions related to complex multimodal reasoning. These functions are inspired by
established frameworks in cognitive science (Anderson, 2014; Diamond, 2013), which highlight the
importance of perception, working memory, and reasoning in human cognition.


The functions related to vision include:


    - **Low-level Visual Reception** : Recognizing basic visual features such as color, shape, size,
position, motion.

    - **High-level Visual Reception** : Integrating visual information to recognize objects, patterns,
and scene structure.

    - **Visual Knowledge Recall** : Applying long-term visual knowledge related to visual concepts and their properties.


The functions related to language include:


    - **Language Information Extraction and Understanding** : locating and understanding relevant information from an external source or prior context.

    - **Language Knowledge Recall** : Accessing domain-specific knowledge without visual input.

    - **Math Reasoning** : Performing counting, arithmetic, comparison, and logic-based operations.

    - **Inference** : deriving implicit information that is not directly stated.

    - **Decision-Making** : Selecting the best outcome among alternatives based on reasoning.


This categorization reflects a natural progression from basic information processing to complex cognitive integration. Both the human brain and VLMs encompass a wide range of functional modules.
Our focus in this work is specifically on reasoning-related cognitive functions. By identifying and
organizing these eight core reasoning functions, we can more clearly examine how VLMs handle
different types of thinking steps, in a way that is both systematic and easy to interpret. Detail descriptions of each function and examples in CogVision can be found in Appendix A.1.


2.2 DATA COLLECTIONS


Based on our categorization of reasoning functions, we sampled 2000 diverse questions from existing commonly used visual reasoning benchmarks, selecting 200 examples each from ClevrMath (Lindstr¨om & Abraham, 2022), MathVision (Wang et al., 2024a), MathVista (Lu et al., 2024b),
MMMU (Yue et al., 2024), ScienceQA (Saikh et al., 2022), OKVQA (Marino et al., 2019) (400
examples), VCR (Zellers et al., 2019), VisuLogic (Xu et al., 2025) and Super-Clevr (Li et al.,
2023b) datasets. These datasets span a range of reasoning problems, including logical, mathematical, and commonsense reasoning. Using the chain-of-thought (CoT) paradigm, we prompted
GPT-4.1 (Achiam et al., 2023) to decompose each main question (with its final answer) into subquestions, each targeting a subanswer and a specific function. The prompt encourages structured,
step-by-step reasoning, ensuring that each subquestion is clear, answerable, and sequentially dependent. This process yields a set of subquestion–answer–function (subQAF) triples for each QA pair:
subQAFs = _{_ ( _qi, ai, fi_ ) _}_ _[k]_ _i_ =1 [, where each contains a subquestion] _[ q][i]_ [, its concise answer] _[ a][i]_ [, and the]
corresponding function label _fi_ . The prompt for generating subquestions are list in Appendix A.6.


2.3 DATA FILTERING AND ANNOTATION


Recent advances enable leveraging large pre-trained models for dataset construction, thanks to their
reasoning capabilities and ability to generate high-quality annotations at scale (Wang et al., 2024b).
While our dataset is automatically generated using a VLM to minimize manual effort, we employ a
rigorous two-stage human verification pipeline to ensure data quality and mitigate hallucination. In
the first stage, three expert annotator independently evaluate whether each subquestion is logically
structured and consistent with natural human reasoning. QA pairs with incoherent or inconsistent


3


decompositions are moved out. In the second stage, annotators verify and, if necessary, relabel the
cognitive function associated with each subquestion to ensure accurate alignment with the intended
mental process. Subanswers are further cross-checked with GPT-o3 (OpenAI, 2024) and adjudicated
by humans in cases of disagreement (details in Appendix A.5). This multi-step verification ensures
that each retained subQAF triplet represents a coherent, interpretable reasoning step grounded in
core cognitive functions. After this refinement, our final dataset comprises 1,409 main QA pairs
and 5,744 validated subQAF triplets. The detailed statistics of the dataset including the number of
triplet in training and testing set and distributions of each function can be found in Appendix A.3.
We analyze in-group subquestion diversity, and the results in the Appendix A.2 show that the eight
functions exhibit broad and overlapping distributions in phrasing patterns and token lengths, indicating no systematic surface-form differences.


3 DETECTION OF COGNITIVE FUNCTIONS


Using the CogVision dataset, we adopt a probing-based framework (Alain & Bengio, 2016; Belinkov, 2022; Tenney et al., 2019) to identify which attention heads in VLMs are associated with
specific functions in reasoning process. Specifically, for each functional annotated subquestion, we
extract head activations (see Subsection 3.1), train classifiers and compute accuracies to identify
contributing heads (see Subsection 3.2). Unlike prior work focusing on a single label, our formulation captures many-to-many relationships between heads and cognitive functions, enabling a more
nuanced analysis of functional specialization and overlap within the model.


3.1 HEAD FEATURE EXTRACTION


Given a large VLM _M_, we generate an answer _a_ _[M]_ _i_ for each subquestion _qi_ derived from a main
question _Qi_ . To support coherent multi-step reasoning, we include preceding subquestions and
their answers as contextual input, emulating the incremental reasoning process observed in human
cognition.


During inference, each input token is first mapped into an embedding and then propagated through
the transformer’s layers. At each layer, attention and feedforward operations update the residual
stream, which is ultimately decoded into token predictions. For each generated token _i_, we extract
attention head outputs _Xi_ = _{x_ _[m]_ _l_ _| l ∈_ 1 _, . . ., Nl, m ∈_ 1 _, . . ., Nh}_ across all layers, where _x_ _[m]_ _l_
denotes the value vector from the _m_ -th head in layer _l_ projected into the residual stream, with _Nh_
the number of heads per layer and _Nl_ the total number of layers.


Let _Nt_ denote the number of tokens in the generated answer _a_ _[M]_ _i_ [. To isolate semantically informative]
content relevant to reasoning, we select the top- _k_ most important tokens, determined by prompting
Qwen3-30B LLM (Yang et al., 2025), yielding an index set _Ik_ with _|Ik|_ = _k_ . For each index
_j ∈Ik_, we extract the corresponding attention head activations _Xj_, and compute the averaged
activation feature for the _m_ -th head in layer _l_ as ¯ _x_ _[m]_ _l_ = _k_ [1] � _j ∈Ikxml_ [. This results in a full set of]

---

[Document 3]
Source: attention_functional_roles_raw_chunks_2k.jsonl
Chunk ID: attention_functional_roles_raw_chunks_2k_4
Content: dom activation and mean activation. In the random activation setting, activations are substituted
with those from a randomly selected subquestion belonging to a different function. In the mean
activation setting, activations are replaced with the average activation computed over all subquestions associated with another function (details in Appendix A.10). As shown in Table 2, both types
of activation patching result in substantial performance degradation for cognitive heads, consistent
with the effects observed under masking interventions.


Table 2: Ablation study of different activation-masking methods on Qwen2.5-VL-3B. Random:
random activation. Mean: mean activation. Scalar: masking.







|VisionmainlyCognitiveFunctions LanguagemainlyCognitiveFunctions<br>Method Inter Head<br>Low-Level High -Level Recall Info Recall Math Inference Decision<br>llm acc llm acc llm acc llm acc llm acc llm acc llm acc llm acc|Col2|
|---|---|
|Random<br>cognitive|**0.00 0.00** 20.45 23.48** 62.12 65.15** 35.79 57.41 12.86 24.29 17.07 17.07 14.32 14.32 6.25 9.38|
|Mean<br>cognitive|3.80 3.80 17.39 19.91 66.67 65.15** 35.19 55.56 10.00 21.43** 37.17 34.15<br>3.80<br>3.80<br>6.25 9.38|
|Scalar<br>cognitive|6.45 6.45** 16.67 18.94** 75.76 75.76 62.96 81.48 57.14 62.86<br>**2.43**<br>**2.43**<br>**0.00**<br>**0.00**<br>**3.13 4.69**|


We further examine how vision-related and language-related heads attend to their respective modalities. For the top-30 heads of each function, we compute the average attention weight on visual tokens
across the test set. As shown in Appendix A.9, vision-related heads (e.g., high-level and low-level


7


![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-7-0.png)

Figure 5: The performance of VLMs on target functions after masking out top K cognitive heads on
intervention functions. The scores are based on LLM-Judge.


Table 3: Study on the influence of low-level cognitive heads for high-order function on Qwen2.5VL-3B. The score is measured based on LLM-judge. We only evaluate subquestions that the model
originally answered correctly. This filtering ensures that any observed drop in performance is caused
solely by the intervention. Notably, the model’s own generated outputs are used as inputs for subsequent subquestions.


Vision Recall Language Recall Info. Low-Level High Level Math Decision Inference


✗ ✓ ✓ ✓ ✓ 50 _._ 00 _↓_ 50.00 54 _._ 55 _↓_ 45.45 54 _._ 17 _↓_ 45.83
✓ ✗ ✓ ✓ ✓ 16 _._ 67 _↓_ 83.33 56 _._ 25 _↓_ 43.75 65 _._ 22 _↓_ 34.78
✓ ✓ ✗ ✓ ✓ 22 _._ 22 _↓_ 77.78 57 _._ 89 _↓_ 42.11 51 _._ 61 _↓_ 48.39
✓ ✓ ✓ ✗ ✓ 27 _._ 27 _↓_ 72.73 72 _._ 73 _↓_ 27.27 59 _._ 09 _↓_ 40.91
✓ ✓ ✓ ✓ ✗ 33 _._ 96 _↓_ 66.04 64 _._ 29 _↓_ 35.71 53 _._ 95 _↓_ 46.05


visual reception) predominantly focus on image tokens, capturing spatial and object-level information, whereas language-related heads (e.g., language knowledge recall) concentrate on text tokens.
Interestingly, Qwen and Intern models allocate more attention to text, while Gemma emphasizes
vision, revealing family-specific modality preferences. We also observe heads with cross-modal attention that respond to both visual and textual tokens, likely mediating interactions between visual
perception and linguistic reasoning. These findings suggest that functional specialization in VLMs
is complemented by selective cross-modal integration, enabling coherent multimodal reasoning.


4.3 RELATIONSHIP AMONG COGNITIVE HEADS


While cognitive heads are specialized for distinct functions, understanding their relationships is
crucial for revealing how complex reasoning emerges from their cooperation.


**Heads Across Functions** The neural system is inherently complex, with individual neurons often
participating in multiple functions (Mante et al., 2013). We observe a similar phenomenon in VLMs:
certain functional heads overlap, with a single head participating in multiple cognitive roles (e.g., in
the Qwen2.5-VL-7B model, 18% of cognitive heads across eight functions participate in more than
one function). In our probing-based method, we quantify and rank the accuracy of attention heads for
each cognitive function. A head that ranks highly for one function may also exhibit non-negligible
importance for others, leading to the phenomenon of ”Heads Across Functions”. Notably, even if
a head ranks in the top 10% for multiple cognitive functions, our ranking still reveals a primary
function for which it is most diagnostic. In summary, functional heads serve not only as specialized
units but also as integrative components that bridge multiple reasoning processes.


**Hierarchical Structure** Humans often solve complex tasks through step-by-step reasoning, where
former functions, such as low-level visual reception, support higher-level processes like inference
and decision making. The CogVision dataset reflects this hierarchy: under CoT, early subquestions
focus on information extraction, while later ones require more complex reasoning. Leveraging this


8


structure, we test whether VLMs exhibit similar functional dependencies by masking attention heads
associated with early-stage functions and observing the impact on subsequent reasoning steps. As
shown in Table 3, masking vision or language knowledge recall heads significantly impairs laterstage performance, particularly in decision making. These results suggest that VLMs exhibit an
emergent hierarchical organization, where early cognitive functions support more advanced reasoning. The prompt used for VLMs can be found in Appendix A.7.


4.4 INFLUENCE OF FUNCTIONAL HEADS ON DOWNSTREAM TASKS


In this section, we investigate how functional heads influence downstream tasks through both negative interventions (masking out function heads) and positive interventions (shifting heads toward
specific functions).


**Negative Intervention:** We randomly
sample 200 question for two VQA bench- Table 4: Negative Intervention on Visual Question Anmarks, OK-VQA and Clevr, both reason- swering task (OK-VQA and Clevr-Math). The scores
ing tasks. We perform negative interven- are based on LLM-Judge.
tion by masking high-level visual reception heads on OK-VQA and math reason- Dataset Inter ~~H~~ ead Model

leads to a significant performance drop after **0.00** **59.00** 29.00 **13.00** 14.00 **13.00**
across all models. Further analysis in Ap
|Dataset Inter Head|Model|
|---|---|
|Dataset<br>Inter~~ H~~ead|Qwen3B Qwen7B Intern2B Intern8B gemma2B gemma4B|
|OK-VQA<br>before<br>after|54.00<br>55.00<br>46.00<br>51.00<br>21.00<br>20.00<br>**7.00**<br>55.00<br>**44.00**<br>53.00<br>**18.00**<br>21.00|
|Clevr-Mathbefore<br>after|94.00<br>70.00<br>20.00<br>93.00<br>14.00<br>14.00<br>**0.00**<br>**59.00**<br>29.00<br>**13.00**<br>14.00<br>**13.00**|
","Positive intervention involves calculating the activation directions of different functions for specific attention heads and then enhancing the original head activation by shifting it along these functional directions (x ← x + α * σ * dir). This process aims to boost the activity of ""functional heads,"" which are attention heads identified as crucial for specific cognitive functions like vision knowledge recall, math reasoning, high-level visual reception, and decision-making.

The impact of this positive intervention on functional heads leads to an improvement in visual reasoning tasks accuracy, as demonstrated by experimental results:

*   **General Improvement:** Enhancing the activation of functional heads along their corresponding functional directions improves performance on related tasks.
*   **Specific Example (CogVision):** Positive intervention on vision knowledge recall heads in the InternVL3-8B model increased accuracy on the corresponding CogVision question-answering task from 88.54% to 91.67%.
*   **Downstream Tasks (Table 5):**
    *   **CogVision Test Set:** Across various models (Qwen3B, Qwen7B, Intern2B, Intern8B, gemma2B, gemma4B), positive intervention on different functional heads (e.g., Math, Vision Recall, High-Level) generally shows an increase in accuracy. For instance, Qwen3B's Math accuracy improved from 46.40 to 85.42, and Intern8B's Vision Recall accuracy improved from 88.54 to 91.67.
    *   **OK-VQA:** Most models showed an increase in accuracy after positive intervention (e.g., Qwen7B from 66.00 to 67.50, Intern2B from 58.50 to 61.50).
    *   **MathVista:** Some models showed improvement (e.g., Intern2B from 51.50 to 52.00, gemma4B from 24.00 to 26.50).
    *   **Visulogic:** All models demonstrated improved accuracy (e.g., Qwen3B from 26.50 to 28.00, Intern8B from 24.00 to 26.00).

In summary, by specifically enhancing the activations of attention heads responsible for particular cognitive functions, positive intervention effectively boosts the accuracy of models on various visual reasoning tasks.",117,9098,9216,,3,,2026-01-02T20:18:01.612890
What are the three attention logit computation modes supported by the Neuronal Attention Circuit (NAC)?,Single-Document Fact Lookup,"[Document 1]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_14
Content: **D.2. Ablations Details**


The brief descriptions of variants and ablation are also divided into four subcategories:
**Top-** _**K**_ **Ablations:** _NAC-2k_ uses Top- _K_ =2 to compute the
logits and _NAC-32k_ uses Top- _K_ =32. All variants use the
exact computation mode with 50% sparsity.
**Sparsity Ablations:** _NAC-02s_ uses 20% sparsity to compute the logits and _NAC-09s_ uses 90%. _NAC-PW_ employs
full pairwise (non-sparse) concatenation for input curation.
_NAC-FC_ replaces the sparse NCP gating mechanism with
a simple fully connected layer. All variants use the exact
computation mode with Top- _K_ =8.
**Modes variants:** _NAC-Euler_ computes attention logits using the explicit Euler integration method. _NAC-Steady_ derives attention logits from the steady-state solution of the
exact formulation. _NAC-Exact/05s/8k_ computes attention
logits using the closed-form exact solution. It also overlaps
with other ablations, so we combined it into a single one.
All modes use Top- _K_ =8, 50% sparsity and _δt_ =1.0. The
sensitivity of NAC to _δt_ is visualized in Figure 4


_Figure 4._ Effect of _δt_ on output of NAC.



14



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/neuronal_attention_circuits.pdf-13-0.png)
**Neuronal Attention Circuit (NAC) for Representation Learning**



**D.3. Experimental Details**


D.3.1. EVENT-BASED MNIST


**Dataset Explanation and Curation:** The MNIST dataset,
introduced by (Deng, 2012), is a widely used benchmark for
computer vision and image classification tasks. It consists
of 70,000 grayscale images of handwritten digits (0–9), each
of size 28 _×_ 28 pixels, split into 60,000 training and 10,000
testing samples.
**Preprocessing:** We follow the preprocessing pipeline described in (Lechner & Hasani, 2022), which proceeds as
follows. First, a threshold is applied to convert the 8-bit
pixel values into binary values, with 128 as the threshold
on a scale from 0 (minimum intensity) to 255 (maximum
intensity). Second, each 28 _×_ 28 image is reshaped into a
one-dimensional time series of length 784. Third, the binary
time series is encoded in an event-based format, eliminating
consecutive occurrences of the same value; for example, the
sequence [1 _,_ 1 _,_ 1 _,_ 1] is transformed into (1 _, t_ = 4). This encoding introduces a temporal dimension and compresses the
sequences from 784 to an average of 53 time steps. Finally,
to facilitate efficient batching and training, each sequence
is padded to a fixed length of 256, and the time dimension
is normalized such that each symbol corresponds to one
unit of time. The resulting dataset defines a per-sequence
classification problem on irregularly sampled time series.
**Neural Network Architecture:** We develop an end-to-end
hybrid neural network by combining compact convolutional
layers with NAC or counterparts baselines for fair comparison. Detailed hyperparameters and architectural specifications are provided in Table 4.


D.3.2. PERSON ACTIVITY RECOGNITION (PAR)


**Dataset Explanation and Curation:** We used the
Localized Person Activity Recognition dataset provided by
UC Irvine (Vidulin et al., 2010). The dataset comprises
25 recordings of human participants performing different
physical activities. The eleven possible activities are
“walking,” “falling,” “lying down,” “lying,” “sitting down,”
“sitting,” “standing up from lying,” “on all fours,” “sitting
on the ground,” “standing up from sitting,” and “standing
up from sitting on the ground.” The objective of this
experiment is to recognize the participant’s activity from
inertial sensors, formulating the task as a per-time-step
classification problem. The input data consist of sensor
readings from four inertial measurement units placed on
participants’ arms and feet. While the sensors are sampled
at a fixed interval of 211 ms, recordings exhibit different
phase shifts and are thus treated as irregularly sampled time
series.

**Preprocessing:** We first separated each participant’s
recordings based on sequence identity and calculated
elapsed time in seconds using the sampling period. To



mitigate class imbalance, we removed excess samples from
overrepresented classes to match the size of the smallest
class. Subsequently, the data were normalized using a
standard scaler. Finally, the dataset was split into a 90:10
ratio for training and testing.
**Neural Network Architecture:** Following the approach in
Section D.3.1, we developed an end-to-end hybrid neural
network combining convolutional heads with NAC or other
baselines. Hyperparameter details are summarized in Table
4.


D.3.3. AUTONOMOUS VEHICLE


**Dataset Explanation and Curation:** We followed the data
collection methodology described in (Razzaq & Hongwei,
2023). For OpenAI-CarRacing, a PPO-trained agent (5M
timesteps) was used to record 20 episodes, yielding approximately 48,174 RGB images of size 92 _×_ 92 _×_ 3 with
corresponding action labels across five discrete actions (noact, move left, forward, move right, stop). The dataset was
split with 10% reserved for testing and the remaining 90%
for training. For the Udacity simulator, we manually controlled the vehicle for 50 minutes, producing 15647 RGB
images of size 320 _×_ 160 _×_ 3, captured from three camera
streams (left, center, right) along with their corresponding
continuous steering values. This dataset was split into 20%
testing and 80% training.
**Preprocessing:** No preprocessing was applied to the
OpenAI-CarRacing dataset. For the Udacity simulator, we
followed the preprocessing steps in (Shibuya, 2017). Each
image was first cropped to remove irrelevant regions and
resized to 66 _×_ 120 _×_ 3. Images were then converted from
RGB to YUV color space to match the network input. To improve robustness, data augmentation techniques, including
random flips, translations, shadow overlays, and brightness
variations, were applied to simulate lateral shifts and diverse
lighting conditions.
**Neural Network Architecture:** For OpenAI-CarRacing,
we modified the neural network architecture proposed in
(Razzaq & Hongwei, 2023), which combines compact CNN
layers for spatial feature extraction with LNNs to capture
temporal dynamics. In our implementation, the LNN layers
were replaced with NAC and its comparable alternatives
for fair evaluation. Full hyperparameter configurations are
provided in Table 4. For the Udacity simulator, we modified
the network proposed in (Bojarski et al., 2016) by replacing
three latent MLP layers with NAC and its counterparts. Full
hyperparameters for this configuration are summarized in
Table 4.

**Saliency Maps:** A saliency map visualizes the regions of
the input that a model attends to when making decisions.
Figure 5 shows the saliency maps for the OpenAI CarRacing environment. We observe that only NAC (Steady, Euler,



15


**Neuronal Attention Circuit (NAC) for Representation Learning**


_Figure 5._ Saliency maps for OpenAI CarRacing


_Figure 6._ Saliency maps for Udacity Simulator



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/neuronal_attention_circuits.pdf-15-0.png)

![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/neuronal_attention_circuits.pdf-15-1.png)

---

[Document 2]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_0
Content: ## **Neuronal Attention Circuit (NAC) for Representation Learning**

**Waleed Razzaq** [1] **Izis Kankaraway** [1] **Yun-Bo Zhao** [1 2]



**Abstract**

Attention improves representation learning over
RNNs, but its discrete nature limits continuoustime (CT) modeling. We introduce Neuronal Attention Circuit (NAC), a novel, biologically plausible CT-Attention mechanism that reformulates

attention logits computation as the solution to a
linear first-order ODE with nonlinear interlinked
gates derived from repurposing _C. elegans_ Neuronal Circuit Policies (NCPs) wiring mechanism.
NAC replaces dense projections with sparse sensory gates for key-query projections and a sparse
backbone network with two heads for computing
_content-target_ and _learnable time-constant_ gates,
enabling efficient adaptive dynamics. NAC supports three attention logit computation modes: (i)
explicit Euler integration, (ii) exact closed-form
solution, and (iii) steady-state approximation. To
improve memory intensity, we implemented a
sparse Top- _K_ pairwise concatenation scheme that
selectively curates key-query interactions. We
provide rigorous theoretical guarantees, including
state stability, bounded approximation errors, and
universal approximation. Empirically, we implemented NAC in diverse domains, including irregular time-series classification, lane-keeping for
autonomous vehicles, and industrial prognostics.
We observed that NAC matches or outperforms
competing baselines in accuracy and occupies an
intermediate position in runtime and memory efficiency compared with several CT baselines.


**1. Introduction**


Learning representations of sequential data in temporal or
spatio-temporal domains is essential for capturing patterns
and enabling accurate forecasting. Discrete-time Recurrent neural networks (DT-RNNs) such as RNN (Rumelhart et al., 1985; Jordan, 1997), Long-short term memory


1Department of Automation, University of Science & Technology of China, Hefei, China [2] Institute of Artificial Intelligence,
Hefei Comprehensive National Science Center. Correspondence to:
Yun-Bo Zhao _<_ ybzhao@ustc.edu.cn _>_, Waleed Razzaq _<_ waleedrazzaq@mail.ustc.edu.cn _>_ .


_Preprint. December 12, 2025._



(LSTM) (Hochreiter & Schmidhuber, 1997), and Gated Recurrent Unit (GRU) (Cho et al., 2014) model sequential dependencies by iteratively updating hidden states to represent
or predict future elements in a sequence. While effective
for regularly sampled sequences, DT-RNNs face challenges
with irregularly sampled data because they assume uniform
time intervals. In addition, vanishing gradients can make
it difficult to capture long-term dependencies (Hochreiter,
1998).
Continuous-time RNNs (CT-RNNs) (Rubanova et al., 2019)
model hidden states as ordinary differential equations
(ODEs), allowing them to process inputs that arrive at arbitrary or irregular time intervals. Mixed-memory RNNs
(mmRNNs) (Lechner & Hasani, 2022) build on this idea
by separating memory compartments from time-continuous
states, helping maintain stable error propagation while capturing continuous-time dynamics. Liquid neural networks
(LNNs) (Hasani et al., 2021; 2022) take a biologically inspired approach by assigning variable time-constants to hidden states, improving adaptability and robustness, though
vanishing gradients can still pose challenges during training.
The attention mechanisms (Vaswani et al., 2017) mitigate
this limitation by treating all time steps equally and allowing models to focus on the most relevant observations. It
computes the similarity between queries ( _q_ ) and keys ( _k_ ),
scaling by the key dimension to keep gradients stable. MultiHead Attention (MHA) (Vaswani et al., 2017) extends this
by allowing the model to attend to different representation
subspaces in parallel. Variants like Sparse Attention (Tay
et al., 2020; Roy et al., 2021), BigBird (Zaheer et al., 2020),
and Longformer (Beltagy et al., 2020) modify the attention
pattern to reduce computational cost, particularly for long sequences, by attending only to selected positions rather than
all pairs. Even with these improvements, attention-based
methods still rely on discrete scaled dot-product operations,
limiting their ability to model continuous trajectories often
captured by CT counterparts.
Recent work has explored bridging this gap through NeuralODE (Chen et al., 2018) formulation. mTAN (Shukla &
Marlin, 2021) learns CT embeddings and uses time-based
attention to interpolate irregular observations into a fixedlength representation for downstream encoder-decoder modeling. ODEFormer (d’Ascoli et al., 2023) trains a sequenceto-sequence transformer on synthetic trajectories to directly
infer symbolic ODE systems from noisy, irregular data,
though it struggles with chaotic systems and generalization



1


**Neuronal Attention Circuit (NAC) for Representation Learning**



beyond observed conditions. Continuous-time Attention
(CTA) (Chien & Chen, 2021) embeds a continuous-time
attention mechanism within a Neural ODE, allowing attention weights and hidden states to evolve jointly over time.
Still, it remains computationally intensive and sensitive to
the accuracy of the ODE solver. ContiFormer (Chen et al.,
2023) builds a CT-transformer by pairing ODE-defined latent trajectories with a time-aware attention mechanism to
model dynamic relationships in data.
Despite these innovations, a persistent and underexplored
gap remains in developing a biologically plausible attention
mechanism that seamlessly integrates CT dynamics with the
abstraction of the brain’s connectome to handle irregular sequences without prohibitive computational costs. Building
on this, we propose a novel attention mechanism called the
_Neuronal Attention Circuit_ (NAC), in which attention logits
are computed as the solution to a first-order ODE modulated
by nonlinear, interlinked gates derived from repurposing
Neuronal Circuit Policies (NCPs) from the nervous system
of _C. elegans_ nematode (refer to Appendix A.2 for more
information). Unlike standard attention, which projects keyquery pairs through a dense layer, NAC employs a sensory
gate to transform input features and a backbone to model
nonlinear interactions, with multiple heads producing outputs structured for attention logits computation. Based on
the solutions to ODE, we define three computation modes:
(i) Exact, using the closed-form ODE solution; (ii) Euler,
approximating the solution via _explicit Euler_ integration;
and (iii) Steady, using only the steady-state solution, analogous to standard attention scores. To reduce computational
complexity, we implemented a sparse Top- _K_ pairwise concatenation algorithm that selectively curates key-query inputs. We evaluate NAC across multiple domains, including
irregularly sampled time series, autonomous vehicle lanekeeping, and Industry 4.0, comparing it to state-of-the-art
baselines. NAC consistently matches or outperforms these
models, while runtime and peak memory benchmarks place
it between CT-RNNs in terms of speed and CT-Attentions
in terms of memory requirements.


**2. Neuronal Attention Circuit (NAC)**


We propose a simple alternative formulation of the attention
logits _a_ (refer to Appendix A.1 for more information), interpreting them as the solution to a first-order linear ODE
modulated by nonlinear, interlinked gates:



from repurposing NCPs. We refer to this formulation as
the Neuronal Attention Circuit (NAC). It enables the logits
_at_ to evolve dynamically with input-dependent, variable
time constants, mirroring the adaptive temporal dynamics
found in _C. elegans_ nervous systems while improving
computational efficiency and expressiveness. Moreover, it
introduces continuous depth into the attention mechanism,
bridging discrete-layer computation with dynamic temporal
evolution.

**Motivation behind this formulation:** The proposed
formulation is loosely motivated by the input-dependent
time-constant mechanism of Liquid Neural Networks
(LNNs), a class of CT-RNNs inspired by biological nervous
systems and synaptic transmission. In this framework, the
dynamics of non-spiking neurons are described by a linear
ODE with nonlinear, interlinked gates: _ddt_ **xt** = **[x]** _τ_ **[t]** [+] **[ S][t]** _[,]_

---

[Document 3]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_7
Content: compact architecture suitable for resource-constrained devices, thereby supporting localized safety.
We utilized three benchmark datasets: (i) PRONOSTIA
(Nectoux et al., 2012), (ii) XJTU-SY (Wang et al., 2018),
and (iii) HUST (Thuan & Hong, 2023). Training is performed on PRONOSTIA, while XJTU-SY and HUST are

used to assess cross-validation. We used the Score met
ric (Nectoux et al., 2012) to assess the performance. We
evaluate generalization using _Bearing 1_ from the first operating condition of each dataset. Figure 3 visualizes the
expected degradation alongside the outputs of NAC. On the
PRONOSTIA dataset, ContiFormer performed the best with



**3.4. Run Time and Memory Experiments**


We evaluate computational requirements on fixed-length
sequences of 1024 steps, 64-dimensional features, 4 heads,
and a Batch size of 1. Each model is run for ten for
ward passes on Google Colab T4-GPU, and we report the
mean runtime with standard deviation, throughput, and peak
memory usage. NAC occupies an intermediate position
in runtime relative to several CT-RNN models, including
GRU-ODE, CfC, and LTC. In terms of memory consumption, NAC uses significantly less memory than mTAN, with
NAC-2k being the least memory-consuming among the CTAttention models. Reducing NAC sparsity from 90% to 20%
has minimal effect on memory, decreasing usage slightly



7


**Neuronal Attention Circuit (NAC) for Representation Learning**



_Table 2._ Run-Time and Memory Benchmark Results


**Run-Time** **Throughput** **Peak Memory**
**Model**
(s) (seq/s) (MB)


RNN 1 _._ 8392 _[±]_ [0] _[.]_ [1933] 0.544 0.29
CT-RNN 7 _._ 1097 _[±]_ [0] _[.]_ [3048] 0.141 0.67
LSTM 2 _._ 6241 _[±]_ [0] _[.]_ [2906] 0.381 0.42
PhasedLSTM 4 _._ 9812 _[±]_ [0] _[.]_ [272] 0.201 0.80
GRU 3 _._ 216 _[±]_ [0] _[.]_ [2566] 0.311 0.54
GRU-ODE 12 _._ 2498 _[±]_ [0] _[.]_ [0525] 0.082 0.64
mmRNN 7 _._ 5852 _[±]_ [0] _[.]_ [2785] 0.132 0.96
LTC 14 _._ 643 _[±]_ [0] _[.]_ [2445] 0.068 0.99
CfC 6 _._ 0988 _[±]_ [0] _[.]_ [2135] 0.164 0.76


Attention 0 _._ 0016 _[±]_ [0] _[.]_ [0001] 625.00 16.86
MHA 0 _._ 0041 _[±]_ [0] _[.]_ [0001] 243.90 69.05
mTAN 0 _._ 0272 _[±]_ [0] _[.]_ [0054] 36.76 790.16
ODEFormer 0 _._ 0317 _[±]_ [0] _[.]_ [0016] 31.55 67.71
CTA 8 _._ 5275 _[±]_ [0] _[.]_ [2355] 0.117 1.43
ContiFormer 0 _._ 066 _[±]_ [0] _[.]_ [0075] 15.15 67.71


NAC-2k 7 _._ 3071 _[±]_ [0] _[.]_ [1547] 0.137 44.75
NAC-32k 7 _._ 2313 _[±]_ [0] _[.]_ [219] 0.138 549.86
NAC-PW 8 _._ 5649 _[±]_ [0] _[.]_ [0203] 0.117 5042.09
NAC-FC 0 _._ 0195 ~~_[±]_~~ ~~[0]~~ ~~_[.]_~~ ~~[0002]~~ 51.28 29.92
NAC-02s 7 _._ 252 _[±]_ [0] _[.]_ [2018] 0.138 151.54
NAC-09s 7 _._ 222 _[±]_ [0] _[.]_ [176] 0.139 150.85
NAC-Exact/05s/8k 7 _._ 4101 ~~_[±]_~~ ~~[0]~~ ~~_[.]_~~ ~~[1586]~~ 0.135 151.50
NAC-Euler 7 _._ 3367 _[±]_ [0] _[.]_ [1719] 0.136 152.22
NAC-Steady 7 _._ 2942 _[±]_ [0] _[.]_ [1451] 0.137 150.86


from 151.54 MB to 150.85 MB. In constrast, decreasing
the Top- _K_ selection from _PW_ to _k_ = 2 drastically reduces
memory consumption from 5042 MB to 44.75 MB, demonstrating the flexibility of NAC.
**Interpreting the Results:** From the experiments, we observe that increasing the sparsity of the NAC layer improves
the robustness of the system and leads to higher overall accuracy. Similarly, increasing the Top- _K_ interactions enhances
accuracy too; however, the benefits diminish as memory
consumption grows. Using Exact mode, Top- _K_ =8 with 50%
sparsity achieves the best balance between accuracy and
efficiency. Steady mode is the fastest, while Euler mode
handles adaptive temporal dynamics.


**4. Discussions**



This research is part of ongoing work on biologically plausible attention mechanisms and represents a pioneering step,
with limitations to be addressed in future work.

**Architectural improvement:** Currently, NAC uses predetermined wiring (AutoNCP) requiring three inputs: number of units (sensory + interneuron + motor), output motor
neurons, and sparsity, with typically 60% of units assigned
to sensory neurons. To integrate with the attention mechanism while preserving wiring, sensory units for _NN_ sensory
are set as unitssensory = � _d_ model0 _.−_ 6 0 _._ 5 � and backbone units as



are set as unitssensory = � _d_ model0 _.−_ 6 0 _._ 5 � and backbone units as

units _backbone_ = _d_ model + � _d_ 0model _._ 6 �, where _⌈·⌉_ and _⌊·⌋_ denote



units _backbone_ = _d_ model + � _d_ 0model _._ 6 �, where _⌈·⌉_ and _⌊·⌋_ denote

the ceiling and floor functions, respectively. This results
in a larger overall architectural size and increased runtime.



Future work will support user-defined NCPs configurations
or randomized wiring to enable more efficient architectures.
**Learnable sparse Top-** _**K**_ **selection:** Sparse Top- _K_ attention can miss important context, is sensitive to _k_, and may
be harder to optimize. A further limitation is that it still
computes the full _QK_ _[⊤]_ matrix, which can dominate the
cost for very long sequences. Future work includes adaptive
or learnable Top- _K_ selection, improved key scoring, and
hardware-aware optimization to strengthen accuracy and
robustness.


**5. Conclusion**
","The Neuronal Attention Circuit (NAC) supports three attention logit computation modes:
1.  Explicit Euler integration
2.  Exact closed-form solution
3.  Steady-state approximation",99,2088,2187,,3,,2026-01-02T20:18:03.802042
What is the specific BLEU score achieved by the Transformer model on the WMT 2014 English-to-German translation task?,Single-Document Fact Lookup,"[Document 1]
Source: attention_is_all_you_need_raw_chunks_2k.jsonl
Chunk ID: attention_is_all_you_need_raw_chunks_2k_4
Content: **6.2** **Model Variations**


To evaluate the importance of different components of the Transformer, we varied our base model
in different ways, measuring the change in performance on English-to-German translation on the


5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.


8


Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base
model. All metrics are on the English-to-German translation development set, newstest2013. Listed
perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to
per-word perplexities.













|Col1|train<br>N d model d ff h d k d v P drop ϵls steps|PPL BLEU params<br>(dev) (dev) ×106|
|---|---|---|
|base|6<br>512<br>2048<br>8<br>64<br>64<br>0.1<br>0.1<br>100K|4.92<br>25.8<br>65|
|(A)|1<br>512<br>512<br>4<br>128<br>128<br>16<br>32<br>32<br>32<br>16<br>16|5.29<br>24.9<br>5.00<br>25.5<br>4.91<br>25.8<br>5.01<br>25.4|
|(B)|16<br>32|5.16<br>25.1<br>58<br>5.01<br>25.4<br>60|
|(C)|2<br>4<br>8<br>256<br>32<br>32<br>1024<br>128<br>128<br>1024<br>4096|6.11<br>23.7<br>36<br>5.19<br>25.3<br>50<br>4.88<br>25.5<br>80<br>5.75<br>24.5<br>28<br>4.66<br>26.0<br>168<br>5.12<br>25.4<br>53<br>4.75<br>26.2<br>90|
|(D)|0.0<br>0.2<br>0.0<br>0.2|5.77<br>24.6<br>4.95<br>25.5<br>4.67<br>25.3<br>5.47<br>25.7|
|(E)|positional embedding instead of sinusoids|4.92<br>25.7|
|big|6<br>1024<br>4096<br>16<br>0.3<br>300K|**4.33**<br>**26.4**<br>213|


development set, newstest2013. We used beam search as described in the previous section, but no
checkpoint averaging. We present these results in Table 3.


In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,
keeping the amount of computation constant, as described in Section 3.2.2. While single-head
attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.


In Table 3 rows (B), we observe that reducing the attention key size _dk_ hurts model quality. This
suggests that determining compatibility is not easy and that a more sophisticated compatibility
function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,
bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our
sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical
results to the base model.


**6.3** **English Constituency Parsing**


To evaluate if the Transformer can generalize to other tasks we performed experiments on English
constituency parsing. This task presents specific challenges: the output is subject to strong structural
constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence
models have not been able to attain state-of-the-art results in small-data regimes [37].


We trained a 4-layer transformer with _dmodel_ = 1024 on the Wall Street Journal (WSJ) portion of the
Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,
using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences

[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens
for the semi-supervised setting.


We performed only a small number of experiments to select the dropout, both attention and residual
(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters
remained unchanged from the English-to-German base translation model. During inference, we


9


Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23
of WSJ)

|Parser|Training|WSJ23F1|
|---|---|---|
|Vinyals & Kaiser el al. (2014) [37]<br>Petrov et al. (2006) [29]<br>Zhu et al. (2013) [40]<br>Dyer et al.(2016) [8]|WSJ only, discriminative<br>WSJ only, discriminative<br>WSJ only, discriminative<br>WSJ only, discriminative|88.3<br>90.4<br>90.4<br>91.7|
|Transformer(4 layers)|WSJ only, discriminative|91.3|
|Zhu et al. (2013) [40]<br>Huang & Harper (2009) [14]<br>McClosky et al. (2006) [26]<br>Vinyals & Kaiser el al.(2014) [37]|semi-supervised<br>semi-supervised<br>semi-supervised<br>semi-supervised|91.3<br>91.3<br>92.1<br>92.1|
|Transformer(4 layers)|semi-supervised|92.7|
|Luong et al. (2015) [23]<br>Dyer et al.(2016) [8]|multi-task<br>generative|93.0<br>93.3|



increased the maximum output length to input length + 300. We used a beam size of 21 and _α_ = 0 _._ 3
for both WSJ only and the semi-supervised setting.


Our results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the
Recurrent Neural Network Grammar [8].


In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the BerkeleyParser [29] even when training only on the WSJ training set of 40K sentences.


**7** **Conclusion**


In this work, we presented the Transformer, the first sequence transduction model based entirely on
attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with
multi-headed self-attention.


For translation tasks, the Transformer can be trained significantly faster than architectures based
on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014
English-to-French translation tasks, we achieve a new state of the art. In the former task our best
model outperforms even all previously reported ensembles.


We are excited about the future of attention-based models and plan to apply them to other tasks. We
plan to extend the Transformer to problems involving input and output modalities other than text and
to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs
such as images, audio and video. Making generation less sequential is another research goals of ours.


The code we used to train and evaluate our models is available at `[https://github.com/](https://github.com/tensorflow/tensor2tensor)`
`[tensorflow/tensor2tensor](https://github.com/tensorflow/tensor2tensor)` .

---

[Document 2]
Source: attention_is_all_you_need_raw_chunks_2k.jsonl
Chunk ID: attention_is_all_you_need_raw_chunks_2k_3
Content: As side benefit, self-attention could yield more interpretable models. We inspect attention distributions
from our models and present and discuss examples in the appendix. Not only do individual attention
heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic
and semantic structure of the sentences.


**5** **Training**


This section describes the training regime for our models.


**5.1** **Training Data and Batching**


We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million
sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared sourcetarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT
2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece
vocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training
batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000
target tokens.


**5.2** **Hardware and Schedule**


We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using
the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We
trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the
bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps
(3.5 days).


**5.3** **Optimizer**


We used the Adam optimizer [20] with _β_ 1 = 0 _._ 9, _β_ 2 = 0 _._ 98 and _ϵ_ = 10 _[−]_ [9] . We varied the learning
rate over the course of training, according to the formula:


_lrate_ = _d_ _[−]_ model [0] _[.]_ [5] _[·]_ [ min(] _[step]_ [_] _[num][−]_ [0] _[.]_ [5] _[, step]_ [_] _[num][ ·][ warmup]_ [_] _[steps][−]_ [1] _[.]_ [5][)] (3)


This corresponds to increasing the learning rate linearly for the first _warmup_ _ _steps_ training steps,
and decreasing it thereafter proportionally to the inverse square root of the step number. We used
_warmup_ _ _steps_ = 4000.


**5.4** **Regularization**


We employ three types of regularization during training:


7


Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the
English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.


BLEU Training Cost (FLOPs)
Model

EN-DE EN-FR EN-DE EN-FR
ByteNet [18] 23.75
Deep-Att + PosUnk [39] 39.2 1 _._ 0 _·_ 10 [20]

GNMT + RL [38] 24.6 39.92 2 _._ 3 _·_ 10 [19] 1 _._ 4 _·_ 10 [20]

ConvS2S [9] 25.16 40.46 9 _._ 6 _·_ 10 [18] 1 _._ 5 _·_ 10 [20]

MoE [32] 26.03 40.56 2 _._ 0 _·_ 10 [19] 1 _._ 2 _·_ 10 [20]

Deep-Att + PosUnk Ensemble [39] 40.4 8 _._ 0 _·_ 10 [20]

GNMT + RL Ensemble [38] 26.30 41.16 1 _._ 8 _·_ 10 [20] 1 _._ 1 _·_ 10 [21]

ConvS2S Ensemble [9] 26.36 **41.29** 7 _._ 7 _·_ 10 [19] 1 _._ 2 _·_ 10 [21]

Transformer (base model) 27.3 38.1 **3** _**.**_ **3** _**·**_ **10** **[18]**

Transformer (big) **28.4** **41.8** 2 _._ 3 _·_ 10 [19]


**Residual Dropout** We apply dropout [33] to the output of each sub-layer, before it is added to the
sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the
positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of
_Pdrop_ = 0 _._ 1.


**Label Smoothing** During training, we employed label smoothing of value _ϵls_ = 0 _._ 1 [36]. This
hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.


**6** **Results**


**6.1** **Machine Translation**


On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)
in Table 2) outperforms the best previously reported models (including ensembles) by more than 2 _._ 0
BLEU, establishing a new state-of-the-art BLEU score of 28 _._ 4. The configuration of this model is
listed in the bottom line of Table 3. Training took 3 _._ 5 days on 8 P100 GPUs. Even our base model
surpasses all previously published models and ensembles, at a fraction of the training cost of any of
the competitive models.


On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41 _._ 0,
outperforming all of the previously published single models, at less than 1 _/_ 4 the training cost of the
previous state-of-the-art model. The Transformer (big) model trained for English-to-French used
dropout rate _Pdrop_ = 0 _._ 1, instead of 0 _._ 3.


For the base models, we used a single model obtained by averaging the last 5 checkpoints, which
were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We
used beam search with a beam size of 4 and length penalty _α_ = 0 _._ 6 [38]. These hyperparameters
were chosen after experimentation on the development set. We set the maximum output length during
inference to input length + 50, but terminate early when possible [38].


Table 2 summarizes our results and compares our translation quality and training costs to other model
architectures from the literature. We estimate the number of floating point operations used to train a
model by multiplying the training time, the number of GPUs used, and an estimate of the sustained
single-precision floating-point capacity of each GPU [5] .


**6.2** **Model Variations**


To evaluate the importance of different components of the Transformer, we varied our base model
in different ways, measuring the change in performance on English-to-German translation on the


5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.


8


Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base
model. All metrics are on the English-to-German translation development set, newstest2013. Listed
perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to
per-word perplexities.

---

[Document 3]
Source: attention_is_all_you_need_raw_chunks_2k.jsonl
Chunk ID: attention_is_all_you_need_raw_chunks_2k_0
Content: Provided proper attribution is provided, Google hereby grants permission to
reproduce the tables and figures in this paper solely for use in journalistic or
scholarly works.

## **Attention Is All You Need**



**Niki Parmar** _[∗]_
Google Research
```
nikip@google.com

```


**Ashish Vaswani** _[∗]_
Google Brain
```
avaswani@google.com

```

**Llion Jones** _[∗]_
Google Research
```
 llion@google.com

```


**Noam Shazeer** _[∗]_
Google Brain
```
noam@google.com

```


**Jakob Uszkoreit** _[∗]_
Google Research
```
usz@google.com

```


**Aidan N. Gomez** _[∗†]_
University of Toronto
```
aidan@cs.toronto.edu

```


**Łukasz Kaiser** _[∗]_
Google Brain
```
lukaszkaiser@google.com

```


**Illia Polosukhin** _[∗‡]_

```
             illia.polosukhin@gmail.com

```

**Abstract**


The dominant sequence transduction models are based on complex recurrent or
convolutional neural networks that include an encoder and a decoder. The best
performing models also connect the encoder and decoder through an attention
mechanism. We propose a new simple network architecture, the Transformer,
based solely on attention mechanisms, dispensing with recurrence and convolutions
entirely. Experiments on two machine translation tasks show these models to
be superior in quality while being more parallelizable and requiring significantly
less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including
ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,
our model establishes a new single-model state-of-the-art BLEU score of 41.8 after
training for 3.5 days on eight GPUs, a small fraction of the training costs of the
best models from the literature. We show that the Transformer generalizes well to
other tasks by applying it successfully to English constituency parsing both with
large and limited training data.


_∗_ Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started
the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and
has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head
attention and the parameter-free position representation and became the other person involved in nearly every
detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and
tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and
efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and
implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating
our research.

_†_ Work performed while at Google Brain.

_‡_ Work performed while at Google Research.


31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.


**1** **Introduction**


Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks
in particular, have been firmly established as state of the art approaches in sequence modeling and
transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous
efforts have since continued to push the boundaries of recurrent language models and encoder-decoder
architectures [38, 24, 15].


Recurrent models typically factor computation along the symbol positions of the input and output
sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden
states _ht_, as a function of the previous hidden state _ht−_ 1 and the input for position _t_ . This inherently
sequential nature precludes parallelization within training examples, which becomes critical at longer
sequence lengths, as memory constraints limit batching across examples. Recent work has achieved
significant improvements in computational efficiency through factorization tricks [21] and conditional
computation [32], while also improving model performance in case of the latter. The fundamental
constraint of sequential computation, however, remains.


Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in
the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms
are used in conjunction with a recurrent network.


In this work we propose the Transformer, a model architecture eschewing recurrence and instead
relying entirely on an attention mechanism to draw global dependencies between input and output.
The Transformer allows for significantly more parallelization and can reach a new state of the art in
translation quality after being trained for as little as twelve hours on eight P100 GPUs.


**2** **Background**


The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU

[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building
block, computing hidden representations in parallel for all input and output positions. In these models,
the number of operations required to relate signals from two arbitrary input or output positions grows
in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes
it more difficult to learn dependencies between distant positions [12]. In the Transformer this is
reduced to a constant number of operations, albeit at the cost of reduced effective resolution due
to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as
described in section 3.2.


Self-attention, sometimes called intra-attention is an attention mechanism relating different positions
of a single sequence in order to compute a representation of the sequence. Self-attention has been
used successfully in a variety of tasks including reading comprehension, abstractive summarization,
textual entailment and learning task-independent sentence representations [4, 27, 28, 22].


End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering and
language modeling tasks [34].


To the best of our knowledge, however, the Transformer is the first transduction model relying
entirely on self-attention to compute representations of its input and output without using sequencealigned RNNs or convolution. In the following sections, we will describe the Transformer, motivate
self-attention and discuss its advantages over models such as [17, 18] and [9].


**3** **Model Architecture**


Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].
Here, the encoder maps an input sequence of symbol representations ( _x_ 1 _, ..., xn_ ) to a sequence
of continuous representations **z** = ( _z_ 1 _, ..., zn_ ). Given **z**, the decoder then generates an output
sequence ( _y_ 1 _, ..., ym_ ) of symbols one element at a time. At each step the model is auto-regressive

[10], consuming the previously generated symbols as additional input when generating the next.


2


![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_is_all_you_need.pdf-2-0.png)

Figure 1: The Transformer - model architecture.


The Transformer follows this overall architecture using stacked self-attention and point-wise, fully
connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,
respectively.


**3.1** **Encoder and Decoder Stacks**
",The Transformer model achieved a BLEU score of 28.4 on the WMT 2014 English-to-German translation task.,114,2227,2343,,3,,2026-01-02T20:18:06.146158
"How does the ""Full Attention Decode"" method in Sliding Window Attention Adaptation (SWAA) differ from standard SWA during the prefilling stage?",Single-Document Fact Lookup,"[Document 1]
Source: sliding_window_attention_annotated_chunks_2k.jsonl
Chunk ID: sliding_window_attention_annotated_chunks_2k_0
Content: ## **Sliding Window Attention Adaptation**

Yijiong Yu [a], Jiale Liu [b], Qingyun Wu [b], Huazheng Wang [a], and Ji Pei [c]


aOregon State University, {yuyiji, huazheng.wang}@oregonstate.edu
bPenn State University, {jiale.liu, qingyun.wu}@psu.edu
cDeepSolution, research@deepsolution.chat



**Abstract**


The self-attention mechanism in Transformer
based Large Language Models (LLMs) scales
quadratically with input length, making longcontext inference expensive. Sliding window attention (SWA) reduces this cost to linear complexity, but naively enabling complete
SWA at inference-time for models pretrained
with full attention (FA) causes severe longcontext performance degradation due to training–inference mismatch. This makes us wonder: _Can FA-pretrained LLMs be well adapted_
_to SWA without pretraining?_ We investigate
this by proposing Sliding Window Attention
Adaptation (SWAA), a set of practical recipes
that combine five methods for better adaptation: (i) applying SWA only during prefilling;
(ii) preserving “sink” tokens; (iii) interleaving
FA/SWA layers; (iv) chain-of-thought (CoT);
and (v) fine-tuning. Our experiments show that
SWA adaptation is feasible while non-trivial:
no single method suffices, yet specific synergistic combinations effectively recover the original
long-context performance. We further analyze
the performance-efficiency trade-offs of different SWAA configurations and provide recommended recipes for diverse scenarios. Our code
[is available at github.](https://github.com/yuyijiong/sliding-window-attention-adaptation)


**1** **Introduction**


Transformer-based Large Language Models
(LLMs) (Vaswani et al., 2017) demonstrate
remarkable capabilities, but their self-attention
scales quadratically with the input sequence length,
making long context processing inefficient. Sliding
Window Attention (SWA), the most straightforward and widely adopted sparse attention
pattern, which restricts each token’s attention to a
fixed-size local window, reduces the computational
complexity to linearity, along with some other
benefits (see Appendix A).
To apply SWA to LLMs, typical solutions involve training a model with SWA from scratch, but



are prohibitively costly and cannot match the performance of state-of-the-art full-causal-attention

models like Qwen3 (Team, 2025b), mainly due
to the inability to reproduce pretraining data.
Training-free methods like streaming attention
(Xiao et al., 2024) can stabilize LLM outputs by retaining “sink tokens” while applying SWA, which
greatly improve efficiency but inevitably suffer
from severe long-context performance degradation
possibly due to the inaccessibility of distant tokens’
information (Xiao, 2025). This motivates a critical,
unexplored question: _Can a full-attention model_
_be adapted to sliding window attention at low cost_
_while maintaining long-context performance?_
We answer Yes to this question by proposing
Sliding Window Attention Adaptation(SWAA), a
set of recipes for adapting FA-pretrained models to
SWA, which requires neither costly pretraining nor
modifications to the standard Transformer architecture. Specifically, it systematically combines five
practical and composable methods:


1. **Full Attention (FA) Decode** : applying SWA
only during the prefilling stage while switching back to full attention for decoding.


2. **Keep First** _k_ **Tokens** : explicitly preserving
attention to the first _k_ “sink” tokens.


3. **Interleaving FA/SWA layers** : mix fullattention and SWA layers (e.g., assigning
SWA to half layers).


4. **Chain-of-Thought (CoT)** : enforcing an explicit ""thinking"" process during decoding.


5. **Fine-tuning with SWA** : lightweight SWAaware supervised fine-tuning on long-context
data.


Among these, FA Decode is a novel method we
introduce. Keep First _k_ Tokens and FA/SWA Interleaving have been proven effective in prior work



1


(Xiao et al., 2024; Team, 2024a; Zhang et al., 2024),
while CoT and fine-tuning are common LLM techniques. However, how these methods should be
combined to be actually effective for SWA adaptation remains unexplored.
Therefore, in our experiments, we evaluate
SWAA on Qwen3 (Team, 2025b) and Llama3.1
(Team, 2024b) across several long-context benchmarks, measuring both performance and efficiency
under a wide range of SWAA recipes. First, we
find that each method makes a distinct contribution,
but no single ingredient suffices to make SWA competitive with full attention. Second, we show that
specific synergistic combinations of methods can
recover a large fraction of the original long-context
performance. Third, we analyze the performanceefficiency trade-offs of different SWAA recipes and
identify some recommended configurations suitable for different deployment scenarios.
Rather than proposing a single globally optimal
configuration, we view SWAA as a flexible toolkit
of practical recipes: practitioners can select SWAA
recipes that match their accuracy and efficiency
constraints, or compose their own SWA adaptation
strategies by combining the available ingredients.
Our key contributions are:


1. We perform the first systematic study on adapting FA-pretrained LLMs to SWA without pretraining, revealing novel insights about how
SWA impacts LLMs and providing a foundation for future research in efficient sparse
attention.


2. We propose SWAA, a set of practical
SWA adaptation recipes that offer a robust
performance-efficiency balance for various
use cases, accelerating LLM inference from
the bottom level.


3. We implement our methods with FlashAttention (Dao, 2024) and vLLM (Kwon
et al., 2023), making it plug-and-play and userfriendly for practical deployment.


**2** **Related Works**


The _O_ ( _N_ [2] ) complexity of self-attention in Transformers (Vaswani et al., 2017) has spurred a wide
field of research about more efficient language
model architectures. Among the two most popular technological routes are sparse attention and
linear attention.



**2.1** **Sparse Attention**


Our work falls in this category. Sliding Window
Attention (SWA) represents the most basic form
of local sparse attention, yet its performance is inherently limited. Therefore, model architectures
such as Longformer (Beltagy et al., 2020), BigBird
(Zaheer et al., 2020), and RATTENTION (Wang
et al., 2025) combine local SWA on most tokens
with special global attention on specific tokens to
create a more powerful, albeit still sparse, pattern.
Popular LLMs like Gemma2 (Team, 2024a) adopt
SWA in half of their layers to balance the efficiency
of SWA and peformance of FA. Sliding Window
Attention Training (SWAT) (Fu et al., 2025b) introduces architectural changes, such as sigmoid
activation and balanced position embeddings, to
stabilize SWA performance. More advanced methods like Deepseek-sparse-attention (Yuan et al.,
2025; DeepSeek-AI, 2025b), although achieving
excellent quality, involve more complicated implementation and optimization due to semantic-aware
attention operations (e.g., selecting the most important tokens based on attention weights). Regardless,
almost all of the above methods require pretraining with a specific sparse pattern, which is costly
and fails to leverage the advantages of existing pretrained models.


LightTransfer (Zhang et al., 2024) is a promising
attempt at adapting existing models to SWA without pretraining, which has the same motivation as
ours. But it may generalize poorly across model
families (see Appendix G).


**2.2** **Linear Attention**


An alternative approach involves reformulating
the attention mechanism entirely to achieve linear, _O_ ( _N_ ), complexity. This includes methods
such as RNN-like linear attention transformers

---

[Document 2]
Source: sliding_window_attention_annotated_chunks_2k.jsonl
Chunk ID: sliding_window_attention_annotated_chunks_2k_1
Content: LightTransfer (Zhang et al., 2024) is a promising
attempt at adapting existing models to SWA without pretraining, which has the same motivation as
ours. But it may generalize poorly across model
families (see Appendix G).


**2.2** **Linear Attention**


An alternative approach involves reformulating
the attention mechanism entirely to achieve linear, _O_ ( _N_ ), complexity. This includes methods
such as RNN-like linear attention transformers

(Katharopoulos et al., 2020; Peng et al., 2023;
Sun et al., 2023) and structured state-space models
(SSMs) like Mamba (Gu and Dao, 2023). Many
works such as Jamba and Nemotron-Flash(Lieber
et al., 2024; Linsong Chu et al., 2024; Team et al.,
2025; Fu et al., 2025a) interleave linear attention
layers with traditional attention layers to create hybrid model structures. While promising, they represent a fundamental architectural departure from the
standard Transformer, and their performance is generally weaker than traditional Transformer-based

LLMs.



2


**3** **Candidate Methods for SWA**

**Adaptation**


As established, a naive application of SWA leads
to severe long-context performance degradation.
Therefore, we investigate five methods that can potentially facilitate SWA adaptation from distinct
perspectives. However, every method except finetuning has some drawbacks to LLM inference efficiency, as discussed in Appendix A. Therefore,
although these methods are not mutually exclusive,
we should not indiscriminately adopt all of them.
Instead, we must evaluate various combinations to
identify the optimal trade-off between performance
and efficiency.


**3.1** **Full Attention Decode**


This method applies SWA **only** to the prefilling
stage. During the decoding (auto-regressive generation) stage, each token still employs full attention,
allowing access to all previous tokens in the context. The resulting attention mask is depicted in
Figure 1a.
This novel approach, first proposed in our study,
is inspired by human reading comprehension: humans typically scan a passage casually (prefilling)
before thinking deeply to formulate an answer (decoding) for a specific problem. We term this strategy ""reading casually, thinking carefully."" In our
design, the SWA-constrained prefilling stage corresponds to casual reading, while the full-attention
decoding stage enables careful thinking. This analogy also suggests that Chain-of-Thought (CoT)
during decoding may be particularly beneficial, as
extended generation (i.e. ""thinking"") could compensate for the insufficient contextual information
gathered during the prefilling stage.


(a) FA Decode (b) Keep First


Figure 1: (a) Attention mask for FA Decode. SWA is
used for prompt tokens (prefill), and full attention is
used for generated tokens (decode). (b) Attention mask
for SWA combined with Keep First _k_ Tokens.



**3.1.1** **Keep First k Tokens**

Xiao et al. demonstrate that models pretrained with
full attention allocate a disproportionate amount
of attention to the initial tokens (""attention sink""),
and removing the visibility of these tokens causes
performance collapse. Their solution, streaming
attention, involves permanently retaining the attention to these ""sink"" tokens while using SWA, which
successfully maintains the stability of the attention
distribution and the model’s output. Our method
is basically the same: as shown in Figure 1b, any
subsequent token can attend to its local window
**and** the initial _k_ tokens.

Notably, however, our method extends beyond
its original version. Streaming attention operates
only at the KV cache level; specifically, the KV
cache of the sink tokens is retained during decoding, while the prefilling stage is not modified or accelerated at all. In contrast, we directly customize
the Flash-Attention-2 (Dao, 2024) kernel to implement such attention mask, thereby accelerating
prefilling via SWA as well, and eliminating the
need to modify KV cache.


**3.2** **Interleaving Layers**


This method retains full attention on a subset of

layers while applying SWA to the remainder, providing a simple hybrid mechanism to balance the
performance of full attention with the efficiency
of pure SWA. A common strategy involves designating one in every _n_ layers to use full attention
(e.g., layers 0, 2, 4, . . . retain full attention, while all
others use SWA). For example, Gemma-2 (Team,
2024a) uses SWA only for layers [1, 3, 5, . . . ], and
Gemma-3 (Team, 2025a) uses SWA only for layers

[5, 11, 17, ...].
However, for an FA-pretrained model, layers
may exhibit distinct behaviors, suggesting it may
not be optimal to simply assign SWA to layers [1,
3, 5, ...]. Instead, it might be preferable to use
statistical features to select ""lazy"" (mostly focusing
on just recent tokens) layers, as adopted by LightTransfer (Zhang et al., 2024). However, we find
that LightTransfer is not consistently superior in
practice (see Appendix G). Therefore, for simplicity, we still choose the simplest fixed-interval layer
selections in our experiments, such as [0, 2, 4, . . . ]
and [1, 3, 5, ...].


**3.3** **Chain-of-Thought**


Chain-of-Thought (CoT) (Wei et al., 2022) is a
widely used technique for improving model ac


3



```
### Figure - Sliding Window Attention
![Sliding Window Attention](sliding_window_attention.pdf-2-0.png)

**Caption:**  
This figure illustrates the sliding window attention mechanism in a transformer model. The ""Prefill"" region represents the initial input, while the ""Decode"" region shows the subsequent processing.

**Description:**
- The image depicts a grid representing a sequence of data, with a sliding window moving across it.
- The green area indicates the window's current position, highlighting the portion of the sequence being attended to.
- The blue area represents the decoded output based on the attended input.
- The window progressively moves across the sequence, enabling contextual understanding.
```

```
### Figure - Sliding Window Attention
![Sliding Window Attention](sliding_window_attention.pdf-2-1.png)

**Caption:**  
The figure illustrates the sliding window attention mechanism, where attention is retained over a limited window. Attention decreases as the distance from the current position increases.

**Description:**
- The image depicts a grid representing a sequence of tokens.
- The blue squares indicate attention retained across the sliding window.
- The green squares represent attention calculated within the sliding window.
- Attention decreases progressively as the distance 'k' from the current position increases, reflecting a decaying attention mechanism.
- The sliding window size is decreasing as it moves along the sequence.
curacy via reasoning. With the advent of ""thinking"" models, such as DeepSeek-R1 (DeepSeek-AI,
2025a), CoT has evolved from a prompting strategy
to an intrinsic LLM capability. However, whether
CoT has a specific impact in SWA scenarios remains uninvestigated. We explore this by comparing a thinking model with its non-thinking variant,
e.g., Qwen3-4B-Thinking and Qwen3-4B-Instruct,
to verify the effect of CoT on SWA adaptation,
which can produce more notable differences compared to simply adding CoT prompting to the same
model.


**3.4** **Fine-tuning**

---

[Document 3]
Source: sliding_window_attention_annotated_chunks_2k.jsonl
Chunk ID: sliding_window_attention_annotated_chunks_2k_3
Content: **Combinations exhibit strong synergy.** Recipes
that combine multiple methods deliver large gains:


  - **FA Decode + Keep First** _k_ substantially improves over naive SWA (rows 7–9), recovering roughly half to two-thirds of the gap on
the thinking model as _k_ increases. However,
increasing _k_ from 100 to 1000 yields almost
no improvement, indicating that _k_ does not
need to be exceedingly large.


  - **Interleaving Layers + FA Decode** is
markedly stronger (row 13), recovering most
of the gap for the thinking model.


  - **FA Decode + Interleaving Layers + Keep**
**First** _k_ perform best (rows 18). The thinking
model recovers close to 90% of the FA gap
even at 2k window.


**CoT synergizes with FA Decode.** Under recipes
that include FA Decode, the thinking model consistently benefits more than the non-thinking model
(rows 13 and 18), suggesting that CoT synergizes
with FA Decode: preserving global attention at
decoding time enables longer reasoning traces to
capitalize on context processed by SWA, confirming our hypothesis in Section 3.1.


**Sliding window size affects, but is not the deci-**
**sive role.** With FA Decode + Keep First _k_, accuracy improves as the window increases (rows 7,
14, 15), though benefits are modest until 8k. When
added with interleaving FA layers, moving from a
2k to a 4k window is enough to close the remaining
gap on the thinking model (row 22 matches the FA
baseline), indicating that FA Decode and Interleaving are the dominant drivers, and larger windows
mainly smooth residual error.



5


Table 1: Qwen3-4B-Thinking and Qwen3-4B-Instruct results on LongMemEval without SFT


**No.** **window size** **FA layers** **keep first** **FA decode** **Acc think** **Acc non-think**


0 Full [] 0 False **73.0** **62.0**
1 2k [] 0 False 3.2 11.0
2 8k [] 0 False 13.2 19.8


3 2k [] 10 False 16.0 15.6
4 2k [] 0 True 11.8 14.2
5 2k [1, 3, 5, ...] 0 False 13.4 18.4
6 8k [] 0 True 26.2 25.0


7 2k [] 10 True 38.2 20.6
8 2k [] 100 True 50.0 17.8
9 2k [] 1000 True 50.0 20.2
10 2k [0, 2, 4, ...] 10 False 17.0 14.8
11 2k [0, 2, 4, ...] 0 True 32.2 26.0
12 2k [1, 3, 5, ...] 10 False 25.8 36.4
13 2k [1, 3, 5, ...] 0 True 59.2 34.8
14 4k [] 10 True 38.0 24.4
15 8k [] 10 True 49.2 35.2


16 2k [0, 2, 4, ...] 10 True 36.0 17.2
17 2k [1, 3, 5, ...] 10 True 65.0 **53.6**
18 2k [1, 3, 5, ...] 100 True **68.8** 50.6
19 2k [1, 5, 9, ...] 10 True 53.2 31.4
20 2k [1, 9, 17, ...] 10 True 36.4 18.8
21 2k [3, 7, 11, ...] 10 True 54.2 34.6
22 4k [1, 3, 5, ...] 100 True **73.0** **54.2**
23 8k [1, 3, 5, ...] 100 True **71.6** **56.6**



**SWA/FA layer selection has large impacts.** Selecting less layers for FA, e.g. only one fourth or
eighth (row 19, 20), though more efficient, will
significantly decrease the improvement brought by
Interleaving Layers. More importantly, for Qwen34B, configuring _odd-numbered_ layers with full attention is significantly better than _even-numbered_
layers (row 11, 13, 16, 17). However, surprisingly,
this result is reversed for Qwen3-30B-A3B and
Llama3.1-8B-Instruct (row 10 and 11 in Table 4
and 5). This suggests that layer functionalities differ across model families and sizes, necessitating
model-specific layer selection strategies, as discussed in Section 3.2.


Therefore, we answer RQ1: adapting an FA
LLM to SWA is feasible even without any training. But it requires specific combinations of at
least 2 methods, which could be less efficient for
inference.



**5.2** **SWA Adaptation With Fine-tuning**


We next evaluate SWA-aware supervised finetuning, which is expected to provide higher improvement. Table 2 reports LongMemEval_24k
accuracy after SFT under various SWAA configurations. The original full-attention model is also
fine-tuned as a baseline (row 0). Since training is
relatively time-consuming, we only test a representative subset of configurations. Our findings are as
follows:


**Fine-tuning substantially lifts all SWA config-**
**urations.** Comparing all the fine-tuning results
with non-training ones, it is clear that fine-tuning
consistently provides great improvement. However,
simply fine-tuning with naive SWA remains insufficient, only achieving 18.8% and 23.8% accuracy
(row 1).


**FA Decode and Interleaving Layers emerge as**
**dominant components.** After SFT, FA Decode
and Interleaving Layers provide the largest gains.



6


Table 2: Qwen3-4B-Thinking and Qwen3-4B-Instruct results on LongMemEval with SFT


**No.** **window size** **FA layers** **keep first** **FA decode** **Acc think** **Acc non-think**


0 Full [] 0 True **74.6** **63.4**
1 2k [] 0 False 18.8 23.8


2 2k [] 10 False 15.6 /
3 2k [] 0 True 57.9 42.0
4 2k [1, 3, 5, ...] 0 False 63.6 54.6
5 4k [] 0 True 62.6 /


6 2k [] 10 True 56.7 /
7 2k [] 100 True 62.2 42.6
8 2k [1, 3, 5, ...] 0 True **73.2** **58.8**
9 2k [0, 2, 4, ...] 0 True **66.0** /
10 2k [1, 5, 9, ...] 0 True **68.8** 47.0


11 2k [1, 3, 5, ...] 100 True **73.2** **61.4**


Table 3: Recommended SWA adaptation recipes for different needs and scenarios. � means optional.


**Training** ? **Thinking Model?** **Prefer?** **FA Decode** **Interleaving Layers** **Keep First**


No No Any � � �
No Yes Efficiency � � �
No Yes Accuracy � � �


Yes Any Efficiency � � �
Yes Any Accuracy � � �



Using FA Decode alone (row 3) or using Interleaving Layers alone (row 4) both get high accuracy.
And combining both (row 8) further boosts performance to 73 _._ 2 (think) and 58 _._ 8 (non-think), nearly
matching the full-attention SFT baseline in row 0.


**Keep First becomes optional rather than essen-**
**tial.** Before fine-tuning, Keep First is crucial for
stability under SWA. But after SFT, it only provides
minor additional improvement. With FA Decode,
adding _k_ = 100 (row 7) improves over _k_ = 0
(row 3) only 4.5%, and if further combining FA
layers, it almost offers no improvements (row 11
and row 8).


**Effect of sliding window size.** Row 3 and 5
shows that increasing the window from 2k to 4k
with FA Decode improves thinking-model accuracy from 57 _._ 9 to 62 _._ 6. This mirrors the non-SFT
trend that larger windows help, but the dominant
improvements still come from FA Decode and Interleaving Layers.
","In the ""Full Attention Decode"" (FA Decode) method of SWAA, Sliding Window Attention (SWA) is applied during the prefilling stage.

The key difference of FA Decode from standard SWA is not in the prefilling stage itself, but rather in the subsequent decoding stage. While standard SWA would continue to use a fixed-size local window for attention during decoding, FA Decode switches back to full attention for decoding, allowing each token to access all previous tokens in the context.",129,4608,4737,,3,,2026-01-02T20:18:10.884912
What are the eight cognitive functions defined in the CogVision dataset for classifying attention heads?,Single-Document Fact Lookup,"[Document 1]
Source: attention_functional_roles_raw_chunks_2k.jsonl
Chunk ID: attention_functional_roles_raw_chunks_2k_1
Content: Furthermore, we validate the functional importance of these heads by showing that their removal
degrades performance on complex tasks and leads to specific error patterns, while their enhancement improves reasoning capabilities. Our findings provide compelling evidence that these attention
heads play a critical role in multimodal reasoning. This insight not only deepens our understanding of the internal organization of VLMs but also suggests potential avenues for designing more
interpretable and cognitive-inspired multimodal AI systems.


2 COGVISION


In this section, we present our dataset Cognitive Vision (CogVision) that contains cognitive process
in multimodal reasoning. CogVision contains 1,409 main questions and 5,744 subquestions. Each


2


example comprises a main question and answer, its subquestions and subanswers and an annotation
specifying the receptive or cognitive function required for each subquestion.


2.1 COGNITIVE FUNCTIONS


To systematically capture the cognitive processes involved in complex reasoning tasks, we consider eight functions related to complex multimodal reasoning. These functions are inspired by
established frameworks in cognitive science (Anderson, 2014; Diamond, 2013), which highlight the
importance of perception, working memory, and reasoning in human cognition.


The functions related to vision include:


    - **Low-level Visual Reception** : Recognizing basic visual features such as color, shape, size,
position, motion.

    - **High-level Visual Reception** : Integrating visual information to recognize objects, patterns,
and scene structure.

    - **Visual Knowledge Recall** : Applying long-term visual knowledge related to visual concepts and their properties.


The functions related to language include:


    - **Language Information Extraction and Understanding** : locating and understanding relevant information from an external source or prior context.

    - **Language Knowledge Recall** : Accessing domain-specific knowledge without visual input.

    - **Math Reasoning** : Performing counting, arithmetic, comparison, and logic-based operations.

    - **Inference** : deriving implicit information that is not directly stated.

    - **Decision-Making** : Selecting the best outcome among alternatives based on reasoning.


This categorization reflects a natural progression from basic information processing to complex cognitive integration. Both the human brain and VLMs encompass a wide range of functional modules.
Our focus in this work is specifically on reasoning-related cognitive functions. By identifying and
organizing these eight core reasoning functions, we can more clearly examine how VLMs handle
different types of thinking steps, in a way that is both systematic and easy to interpret. Detail descriptions of each function and examples in CogVision can be found in Appendix A.1.


2.2 DATA COLLECTIONS


Based on our categorization of reasoning functions, we sampled 2000 diverse questions from existing commonly used visual reasoning benchmarks, selecting 200 examples each from ClevrMath (Lindstr¨om & Abraham, 2022), MathVision (Wang et al., 2024a), MathVista (Lu et al., 2024b),
MMMU (Yue et al., 2024), ScienceQA (Saikh et al., 2022), OKVQA (Marino et al., 2019) (400
examples), VCR (Zellers et al., 2019), VisuLogic (Xu et al., 2025) and Super-Clevr (Li et al.,
2023b) datasets. These datasets span a range of reasoning problems, including logical, mathematical, and commonsense reasoning. Using the chain-of-thought (CoT) paradigm, we prompted
GPT-4.1 (Achiam et al., 2023) to decompose each main question (with its final answer) into subquestions, each targeting a subanswer and a specific function. The prompt encourages structured,
step-by-step reasoning, ensuring that each subquestion is clear, answerable, and sequentially dependent. This process yields a set of subquestion–answer–function (subQAF) triples for each QA pair:
subQAFs = _{_ ( _qi, ai, fi_ ) _}_ _[k]_ _i_ =1 [, where each contains a subquestion] _[ q][i]_ [, its concise answer] _[ a][i]_ [, and the]
corresponding function label _fi_ . The prompt for generating subquestions are list in Appendix A.6.


2.3 DATA FILTERING AND ANNOTATION


Recent advances enable leveraging large pre-trained models for dataset construction, thanks to their
reasoning capabilities and ability to generate high-quality annotations at scale (Wang et al., 2024b).
While our dataset is automatically generated using a VLM to minimize manual effort, we employ a
rigorous two-stage human verification pipeline to ensure data quality and mitigate hallucination. In
the first stage, three expert annotator independently evaluate whether each subquestion is logically
structured and consistent with natural human reasoning. QA pairs with incoherent or inconsistent


3


decompositions are moved out. In the second stage, annotators verify and, if necessary, relabel the
cognitive function associated with each subquestion to ensure accurate alignment with the intended
mental process. Subanswers are further cross-checked with GPT-o3 (OpenAI, 2024) and adjudicated
by humans in cases of disagreement (details in Appendix A.5). This multi-step verification ensures
that each retained subQAF triplet represents a coherent, interpretable reasoning step grounded in
core cognitive functions. After this refinement, our final dataset comprises 1,409 main QA pairs
and 5,744 validated subQAF triplets. The detailed statistics of the dataset including the number of
triplet in training and testing set and distributions of each function can be found in Appendix A.3.
We analyze in-group subquestion diversity, and the results in the Appendix A.2 show that the eight
functions exhibit broad and overlapping distributions in phrasing patterns and token lengths, indicating no systematic surface-form differences.


3 DETECTION OF COGNITIVE FUNCTIONS


Using the CogVision dataset, we adopt a probing-based framework (Alain & Bengio, 2016; Belinkov, 2022; Tenney et al., 2019) to identify which attention heads in VLMs are associated with
specific functions in reasoning process. Specifically, for each functional annotated subquestion, we
extract head activations (see Subsection 3.1), train classifiers and compute accuracies to identify
contributing heads (see Subsection 3.2). Unlike prior work focusing on a single label, our formulation captures many-to-many relationships between heads and cognitive functions, enabling a more
nuanced analysis of functional specialization and overlap within the model.


3.1 HEAD FEATURE EXTRACTION


Given a large VLM _M_, we generate an answer _a_ _[M]_ _i_ for each subquestion _qi_ derived from a main
question _Qi_ . To support coherent multi-step reasoning, we include preceding subquestions and
their answers as contextual input, emulating the incremental reasoning process observed in human
cognition.


During inference, each input token is first mapped into an embedding and then propagated through
the transformer’s layers. At each layer, attention and feedforward operations update the residual
stream, which is ultimately decoded into token predictions. For each generated token _i_, we extract
attention head outputs _Xi_ = _{x_ _[m]_ _l_ _| l ∈_ 1 _, . . ., Nl, m ∈_ 1 _, . . ., Nh}_ across all layers, where _x_ _[m]_ _l_
denotes the value vector from the _m_ -th head in layer _l_ projected into the residual stream, with _Nh_
the number of heads per layer and _Nl_ the total number of layers.


Let _Nt_ denote the number of tokens in the generated answer _a_ _[M]_ _i_ [. To isolate semantically informative]
content relevant to reasoning, we select the top- _k_ most important tokens, determined by prompting
Qwen3-30B LLM (Yang et al., 2025), yielding an index set _Ik_ with _|Ik|_ = _k_ . For each index
_j ∈Ik_, we extract the corresponding attention head activations _Xj_, and compute the averaged
activation feature for the _m_ -th head in layer _l_ as ¯ _x_ _[m]_ _l_ = _k_ [1] � _j ∈Ikxml_ [. This results in a full set of]

---

[Document 2]
Source: attention_functional_roles_raw_chunks_2k.jsonl
Chunk ID: attention_functional_roles_raw_chunks_2k_2
Content: Let _Nt_ denote the number of tokens in the generated answer _a_ _[M]_ _i_ [. To isolate semantically informative]
content relevant to reasoning, we select the top- _k_ most important tokens, determined by prompting
Qwen3-30B LLM (Yang et al., 2025), yielding an index set _Ik_ with _|Ik|_ = _k_ . For each index
_j ∈Ik_, we extract the corresponding attention head activations _Xj_, and compute the averaged
activation feature for the _m_ -th head in layer _l_ as ¯ _x_ _[m]_ _l_ = _k_ [1] � _j ∈Ikxml_ [. This results in a full set of]

head-level features _X_ [¯] _i_ = _{x_ ¯ _[m]_ _l_ _[|][ l][ ∈]_ [1] _[, . . ., L, m][ ∈]_ [1] _[, . . ., M]_ _[}]_ [.]


3.2 FUNCTION PROBING


For the dataset with _N_ subQAF triplets, we collect all activations to construct the probing dataset:


_D_ probe = _{_ (¯ _x_ _[m]_ _l_ _[, c]_ [)] _[i][}][N]_ _i_ =1 _[, l][ ∈{]_ [1] _[, . . ., L][}][, m][ ∈{]_ [1] _[, . . ., M]_ _[}]_ (1)


For classification based on CogVision, the training set includes 1,124 main questions with 4,604
subQAF triplet, while the testing set has 285 main questions with 1,141 triplets. Our probe takes
the form _pθ_ ( _x_ _[m]_ _l_ [) = sigmoid (] _[⟨][θ, x][m]_ _l_ _[⟩]_ [)][. There is one probe per attention head per layer per function.]
For each target function, the probe is trained by treating the attention-head outputs that lead to
correct answers for that function as the positive class, and those associated with correct answers
from other functions as the negative class. To ensure data balance, we select an equal number of
negative samples to match the positive ones. Given prior findings suggesting that cognitive functions
may vary by layer depth (Zheng et al.), we incorporate layer-wise information by computing the
average activation ¯ _xl_ = _M_ 1 � _Mm_ =1 _[x]_ [¯] _l_ _[m]_ [for each layer. We then augment each head-level vector with]


4


its corresponding layer summary, resulting in enriched features _x_ _[m]_ _l_ _[′]_ = [¯ _x_ _[m]_ _l_ [; ¯] _[x][l]_ []][ for probing. The]
importance for each head are then calculated based on the accuracy of predicting target function. The
effectiveness of top-k tokens and layer information, as well as the sensitivity analysis with respect
to the parameter _k_ and the choice of LLM fused for top-k token extraction, and prompt format, can
be found in Appendix A.8.


4 EXPERIMENTS


We conduct a series of experiments on three VLM families across various model scales, including
Intern (Zhu et al., 2025) (InternVL3-8B and InternVL3-2B), Qwen (Yang et al., 2025) (Qwen2.5VL-7B and Qwen2.5-VL-3B), and Gemma (Team et al., 2025) (Gemma3-4B and Gemma3-2B).
We analyze the commonalities and differences of functional heads (Subsection 4.1), validate their
contributions (Subsection 4.2), and examine correlations, including cross-function interactions and
hierarchical organization (Subsection 4.3). We also assess their causal impact on downstream reasoning tasks (Subsection 4.4). Results confirm the existence of sparse, function-specific heads and
highlight their critical contribution to structured cognitive processing within VLMs.


4.1 PROPERTIES OF COGNITIVE HEADS


**Sparsity, Universality, and Intrinsic Organization:** Fig 2 shows the heatmap of attention head
accuracy across eight functions in Qwen2.5-VL-7B on the CogVision test set, revealing a sparse
distribution. In total, fewer than 7% of all heads achieve accuracies above 0.9 across the eight functions (about 2% for high-level visual reception and math reasoning, and less than 1% for the others),
suggesting that only a small subset of heads meaningfully contributes to different reasoning tasks.
These results demonstrate that VLMs rely on highly specialized, localized components for distinct
cognitive abilities. Pearson correlations between head-activation heatmaps across the eight functions
(Fig. 3) are generally low, confirming that different functions tend to depend on partially separable
subsets of heads. Moreover, this sparse functional organization is consistent across architectures
and scales: heatmaps for five additional models (Appendix A.4) confirm its universality, and the
relatively high Pearson correlation coefficients between models further verify this consistency (in
Appendix A.9). Within the same model family (e.g., Qwen2.5-VL-7B in Fig 2 vs. Qwen2.5-VL-3B
in Fig 8), we observe similar distributions, suggesting that such specialization is intrinsic to VLMs.


**Functional Personalization:** Beyond sparsity, attention heads exhibit a structured distribution
across model layers. Math-related heads are dispersed throughout the network, whereas inferencerelated heads appear more frequently in higher layers. This task-dependent localization suggests an
emergent modular organization in which different layers support distinct cognitive operations. We
also observe notable variation in head counts across functions. For example, in the Qwen family,
math reasoning and high-level visual reception heads are more prevalent than others, reflecting differences in representational and computational complexity. Smaller models contain fewer functional
heads compared to their larger counterparts.


4.2 FUNCTIONAL CONTRIBUTIONS OF COGNITIVE HEADS


After identifying the cognitive heads associated with each function, we examine their functional
roles by evaluating the model’s behavior on the CogVision test set under targeted interventions. We
perform head ablation by scaling the output of a specific attention head with a small factor _ϵ_ (e.g.,
0.001), effectively suppressing its contribution:



�



_x_ [mask] _i_ = Softmax



_Wq_ _[i][W][ iT]_ _k_
� ~~�~~ _dk/n_




_· ϵWv_ _[i]_ (2)



Specifically, we compare model performance when masking identified cognitive heads versus masking an equal number of randomly-selected heads. To quantify the impact, we employ both an LLMbased judge and an integrated accuracy metric. For LLM-based judge, we use LLM (Qwen3-30B
LLM (Yang et al., 2025)) to judge the correctness of the output. For the integrated accuracy metric,
an output is considered unaffected if its BLEU score (Papineni et al., 2002) exceeds 0.8, or if either


5


![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-5-0.png)

Figure 2: The existence of cognitive heads in Qwen2.5-VL-7B responsible for eight distinct functions in complex reasoning tasks. The x-axis represents the head index, while the y-axis indicates
the layer index. The values denote head importance scores, capped at a cutoff of 0.60.


Figure 3: Pearson Correlation between different functions across two models.


the ROUGE score (Chin-Yew, 2004) or the semantic similarity score surpasses 0.6. This provides a
comprehensive evaluation of performance degradation.

---

[Document 3]
Source: attention_functional_roles_raw_chunks_2k.jsonl
Chunk ID: attention_functional_roles_raw_chunks_2k_5
Content: pendix A.11 shows that masking the **math reasoning** heads leads to errors in arithmetic tasks, while
visual receptive functions remain largely unaffected. This confirms that these cognitive heads are
crucial for specific functions and highlights the robustness and generalizability of our method.


**Positive Intervention:** We calculate the activation directions of different functions using the CogVision dataset. For each function, the activation direction of a head at layer _l_ and index _h_ is computed

as:
dir _[h]_ _l_ [=][ E] _[i][∈D]_ correct � _x_ _[h]_ _l_ [(] _[i]_ [)] � _−_ E _i∈D_ incorrect � _x_ _[h]_ _l_ [(] _[i]_ [)] � (3)

where _x_ _[h]_ _l_ [(] _[i]_ [)][ denotes the activation of head at layer] _[ l]_ [ and index] _[ h]_ [, and] _[ D]_ [correct][ and] _[ D]_ [incorrect][ represent]
the sets of samples answered correctly and incorrectly, respectively. Then we estimate the standard
deviation of activations (Li et al., 2023a) along the cognitive function direction to be _σl_ _[h]_ [, and shift]
original head activation as _x_ _[h]_ _l_ [(] _[i]_ [)] _[ ←]_ _[x][h]_ _l_ [(] _[i]_ [) +] _[ ασ]_ _l_ _[h]_ [dir] _l_ _[h]_ [, where] _[ α]_ [ is a parameter.]


The experimental results in Table 5 show that enhancing the activation of functional heads along
their corresponding functional directions improves performance on the related tasks. For example,
positive intervention on vision knowledge recall heads in InternVL3-8B increased accuracy on the
corresponding CogVision question-answering task from 88.54% to 91.67%. Similarly, enhancing
function-specific heads can also boost performance on downstream tasks. Here, we set _α_ = 0 _._ 1
for all datasets, though tuning this parameter may further improve performance. Case analyses are
provided in Appendix A.11.


5 RELATED WORKS


**Neural Networks and the Brain.** Understanding the relationship between artificial neural networks (ANNs) and the biological brain has been a long-standing goal in both neuroscience and
machine learning. Early studies demonstrated that convolutional neural networks (CNNs) trained
on visual tasks develop hierarchical representations reminiscent of the ventral visual stream in primates (Yamins et al., 2014; Cadieu et al., 2014). Subsequent work extended this line of inquiry
to recurrent and transformer-based architectures, showing that attention mechanisms can emulate
aspects of selective processing observed in cortical circuits (Tsividis et al., 2017). More recently,
large language models (LLMs) have exhibited striking parallels with human brain activity during
language processing. In particular, transformer-based models such as GPT-2 produce internal representations that align with neural responses in language-selective brain regions (Caucheteux et al.,
2022; Schrimpf et al., 2021). Some works (Schulze Buschoff et al., 2025; Li et al., 2024) have
studied how VLMs perform differently from humans from a cognitive perspective. Furthermore, the


9



Table 4: Negative Intervention on Visual Question Answering task (OK-VQA and Clevr-Math). The scores
are based on LLM-Judge.








Table 5: Positive Intervention on CogVision test set and other Visual Question Answering benchmarks (OK-VQA, MathVista and Visulogic). The scores are based on LLM-Judge. For OK-VQA,
we perform positive intervention by masking 10% high-level visual reception head, MathVista for
math reasoning heads, and Visulogic for decision-making heads.

|In Domain Out of Domain<br>Model InterHead<br>Math Vision Recall Lang Recall Info Low-Level Inference High-Level Decision OK-VQA MathVista Visulogic|Col2|Col3|
|---|---|---|
|Qwen3B<br>before<br>after|46.40<br>82.29<br>81.37<br>48.03<br>78.26<br>65.87<br>77.38<br>29.73<br>46.40<br>**85.42**<br>**84.31**<br>44.74<br>78.26<br>**69.84**<br>**77.98**<br>**32.43**|62.50<br>60.00<br>26.50<br>62.00<br>60.00<br>**28.00**|
|Qwen7B<br>before<br>after|52.00<br>84.38<br>86.27<br>43.42<br>77.18<br>69.84<br>82.44<br>36.94<br>**52.80**<br>**85.42**<br>86.27<br>**46.05**<br>**82.61**<br>**74.60**<br>82.44<br>36.94|66.00<br>63.00<br>24.00<br>**67.50**<br>63.00<br>**24.50**|
|Intern2B<br>before<br>after|38.40<br>79.17<br>80.39<br>44.08<br>78.26<br>61.90<br>81.55<br>34.23<br>38.40<br>78.13<br>**84.31**<br>42.76<br>**80.43**<br>**64.29**<br>**82.44**<br>**35.14**|58.50<br>51.50<br>24.00<br>**61.50**<br>**52.00**<br>**26.00**|
|Intern8B<br>before<br>after|52.00<br>88.54<br>82.35<br>45.39<br>88.04<br>73.81<br>86.61<br>42.34<br>**52.80**<br>**91.67**<br>**84.31**<br>**46.71**<br>86.96<br>73.81<br>**87.80**<br>**43.24**|67.00<br>66.00<br>24.00<br>**67.50**<br>66.00<br>**26.00**|
|gemma2B<br>before<br>after|26.40<br>62.50<br>80.39<br>32.24<br>34.78<br>48.41<br>33.93<br>23.42<br>**28.00**<br>**64.58**<br>**84.31**<br>**38.16**<br>34.78<br>**50.00**<br>30.95<br>20.72|29.00<br>24.50<br>26.00<br>**29.50**<br>24.50<br>**29.00**|
|gemma4B<br>before<br>after|32.80<br>62.50<br>83.33<br>32.89<br>27.17<br>52.38<br>33.63<br>18.92<br>**35.20**<br>**65.63**<br>**85.29**<br>**35.53**<br>27.17<br>**53.97**<br>**33.63**<br>**19.82**|31.50<br>24.00<br>27.00<br>**33.00**<br>**26.50**<br>**27.50**|



chain-of-thought (CoT) paradigm has been argued to mirror step-by-step human reasoning, leading to improved problem-solving performance. These findings motivate the design of interpretable,
functionally specialized modules in artificial networks, bridging insights from neuroscience with
advances in multimodal reasoning.
","The eight cognitive functions defined in the CogVision dataset for classifying attention heads are:

1.  **Low-level Visual Reception**: Recognizing basic visual features such as color, shape, size, position, motion.
2.  **High-level Visual Reception**: Integrating visual information to recognize objects, patterns, and scene structure.
3.  **Visual Knowledge Recall**: Applying long-term visual knowledge related to visual concepts and their properties.
4.  **Language Information Extraction and Understanding**: Locating and understanding relevant information from an external source or prior context.
5.  **Language Knowledge Recall**: Accessing domain-specific knowledge without visual input.
6.  **Math Reasoning**: Performing counting, arithmetic, comparison, and logic-based operations.
7.  **Inference**: Deriving implicit information that is not directly stated.
8.  **Decision-Making**: Selecting the best outcome among alternatives based on reasoning.",121,2243,2365,,3,,2026-01-02T20:18:13.251672
"What is the computational complexity of a self-attention layer compared to a recurrent layer as described in ""Attention Is All You Need""?",Single-Document Fact Lookup,"[Document 1]
Source: attention_is_all_you_need_raw_chunks_2k.jsonl
Chunk ID: attention_is_all_you_need_raw_chunks_2k_2
Content: [38, 2, 9].


    - The encoder contains self-attention layers. In a self-attention layer all of the keys, values
and queries come from the same place, in this case, the output of the previous layer in the
encoder. Each position in the encoder can attend to all positions in the previous layer of the
encoder.


    - Similarly, self-attention layers in the decoder allow each position in the decoder to attend to
all positions in the decoder up to and including that position. We need to prevent leftward
information flow in the decoder to preserve the auto-regressive property. We implement this
inside of scaled dot-product attention by masking out (setting to _−∞_ ) all values in the input
of the softmax which correspond to illegal connections. See Figure 2.


**3.3** **Position-wise Feed-Forward Networks**


In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully
connected feed-forward network, which is applied to each position separately and identically. This
consists of two linear transformations with a ReLU activation in between.


FFN( _x_ ) = max(0 _, xW_ 1 + _b_ 1) _W_ 2 + _b_ 2 (2)


While the linear transformations are the same across different positions, they use different parameters
from layer to layer. Another way of describing this is as two convolutions with kernel size 1.
The dimensionality of input and output is _d_ model = 512, and the inner-layer has dimensionality
_dff_ = 2048.


**3.4** **Embeddings and Softmax**


Similarly to other sequence transduction models, we use learned embeddings to convert the input
tokens and output tokens to vectors of dimension _d_ model. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In
our model, we share the same weight matrix between the two embedding layers and the pre-softmax
linear transformation, similar to [30]. In the embedding layers, we multiply those weights by _[√]_ _d_ model.


5


Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations
for different layer types. _n_ is the sequence length, _d_ is the representation dimension, _k_ is the kernel
size of convolutions and _r_ the size of the neighborhood in restricted self-attention.


Layer Type Complexity per Layer Sequential Maximum Path Length
Operations
Self-Attention _O_ ( _n_ [2] _· d_ ) _O_ (1) _O_ (1)
Recurrent _O_ ( _n · d_ [2] ) _O_ ( _n_ ) _O_ ( _n_ )
Convolutional _O_ ( _k · n · d_ [2] ) _O_ (1) _O_ ( _logk_ ( _n_ ))
Self-Attention (restricted) _O_ ( _r · n · d_ ) _O_ (1) _O_ ( _n/r_ )


**3.5** **Positional Encoding**


Since our model contains no recurrence and no convolution, in order for the model to make use of the
order of the sequence, we must inject some information about the relative or absolute position of the
tokens in the sequence. To this end, we add ""positional encodings"" to the input embeddings at the
bottoms of the encoder and decoder stacks. The positional encodings have the same dimension _d_ model
as the embeddings, so that the two can be summed. There are many choices of positional encodings,
learned and fixed [9].


In this work, we use sine and cosine functions of different frequencies:


_PE_ ( _pos,_ 2 _i_ ) = _sin_ ( _pos/_ 10000 [2] _[i/d]_ [model] )

_PE_ ( _pos,_ 2 _i_ +1) = _cos_ ( _pos/_ 10000 [2] _[i/d]_ [model] )


where _pos_ is the position and _i_ is the dimension. That is, each dimension of the positional encoding
corresponds to a sinusoid. The wavelengths form a geometric progression from 2 _π_ to 10000 _·_ 2 _π_ . We
chose this function because we hypothesized it would allow the model to easily learn to attend by
relative positions, since for any fixed offset _k_, _PEpos_ + _k_ can be represented as a linear function of
_PEpos_ .


We also experimented with using learned positional embeddings [9] instead, and found that the two
versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version
because it may allow the model to extrapolate to sequence lengths longer than the ones encountered
during training.


**4** **Why Self-Attention**


In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations
( _x_ 1 _, ..., xn_ ) to another sequence of equal length ( _z_ 1 _, ..., zn_ ), with _xi, zi ∈_ R _[d]_, such as a hidden
layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we
consider three desiderata.


One is the total computational complexity per layer. Another is the amount of computation that can
be parallelized, as measured by the minimum number of sequential operations required.


The third is the path length between long-range dependencies in the network. Learning long-range
dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the
ability to learn such dependencies is the length of the paths forward and backward signals have to
traverse in the network. The shorter these paths between any combination of positions in the input
and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare
the maximum path length between any two input and output positions in networks composed of the
different layer types.


As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially
executed operations, whereas a recurrent layer requires _O_ ( _n_ ) sequential operations. In terms of
computational complexity, self-attention layers are faster than recurrent layers when the sequence


6


length _n_ is smaller than the representation dimensionality _d_, which is most often the case with
sentence representations used by state-of-the-art models in machine translations, such as word-piece

[38] and byte-pair [31] representations. To improve computational performance for tasks involving
very long sequences, self-attention could be restricted to considering only a neighborhood of size _r_ in
the input sequence centered around the respective output position. This would increase the maximum
path length to _O_ ( _n/r_ ). We plan to investigate this approach further in future work.


A single convolutional layer with kernel width _k < n_ does not connect all pairs of input and output
positions. Doing so requires a stack of _O_ ( _n/k_ ) convolutional layers in the case of contiguous kernels,
or _O_ ( _logk_ ( _n_ )) in the case of dilated convolutions [18], increasing the length of the longest paths
between any two positions in the network. Convolutional layers are generally more expensive than
recurrent layers, by a factor of _k_ . Separable convolutions [6], however, decrease the complexity
considerably, to _O_ ( _k · n · d_ + _n · d_ [2] ). Even with _k_ = _n_, however, the complexity of a separable
convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,
the approach we take in our model.


As side benefit, self-attention could yield more interpretable models. We inspect attention distributions
from our models and present and discuss examples in the appendix. Not only do individual attention
heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic
and semantic structure of the sentences.


**5** **Training**


This section describes the training regime for our models.


**5.1** **Training Data and Batching**

---

[Document 2]
Source: attention_is_all_you_need_raw_chunks_2k.jsonl
Chunk ID: attention_is_all_you_need_raw_chunks_2k_0
Content: Provided proper attribution is provided, Google hereby grants permission to
reproduce the tables and figures in this paper solely for use in journalistic or
scholarly works.

## **Attention Is All You Need**



**Niki Parmar** _[∗]_
Google Research
```
nikip@google.com

```


**Ashish Vaswani** _[∗]_
Google Brain
```
avaswani@google.com

```

**Llion Jones** _[∗]_
Google Research
```
 llion@google.com

```


**Noam Shazeer** _[∗]_
Google Brain
```
noam@google.com

```


**Jakob Uszkoreit** _[∗]_
Google Research
```
usz@google.com

```


**Aidan N. Gomez** _[∗†]_
University of Toronto
```
aidan@cs.toronto.edu

```


**Łukasz Kaiser** _[∗]_
Google Brain
```
lukaszkaiser@google.com

```


**Illia Polosukhin** _[∗‡]_

```
             illia.polosukhin@gmail.com

```

**Abstract**


The dominant sequence transduction models are based on complex recurrent or
convolutional neural networks that include an encoder and a decoder. The best
performing models also connect the encoder and decoder through an attention
mechanism. We propose a new simple network architecture, the Transformer,
based solely on attention mechanisms, dispensing with recurrence and convolutions
entirely. Experiments on two machine translation tasks show these models to
be superior in quality while being more parallelizable and requiring significantly
less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including
ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,
our model establishes a new single-model state-of-the-art BLEU score of 41.8 after
training for 3.5 days on eight GPUs, a small fraction of the training costs of the
best models from the literature. We show that the Transformer generalizes well to
other tasks by applying it successfully to English constituency parsing both with
large and limited training data.


_∗_ Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started
the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and
has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head
attention and the parameter-free position representation and became the other person involved in nearly every
detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and
tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and
efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and
implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating
our research.

_†_ Work performed while at Google Brain.

_‡_ Work performed while at Google Research.


31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.


**1** **Introduction**


Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks
in particular, have been firmly established as state of the art approaches in sequence modeling and
transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous
efforts have since continued to push the boundaries of recurrent language models and encoder-decoder
architectures [38, 24, 15].


Recurrent models typically factor computation along the symbol positions of the input and output
sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden
states _ht_, as a function of the previous hidden state _ht−_ 1 and the input for position _t_ . This inherently
sequential nature precludes parallelization within training examples, which becomes critical at longer
sequence lengths, as memory constraints limit batching across examples. Recent work has achieved
significant improvements in computational efficiency through factorization tricks [21] and conditional
computation [32], while also improving model performance in case of the latter. The fundamental
constraint of sequential computation, however, remains.


Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in
the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms
are used in conjunction with a recurrent network.


In this work we propose the Transformer, a model architecture eschewing recurrence and instead
relying entirely on an attention mechanism to draw global dependencies between input and output.
The Transformer allows for significantly more parallelization and can reach a new state of the art in
translation quality after being trained for as little as twelve hours on eight P100 GPUs.


**2** **Background**


The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU

[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building
block, computing hidden representations in parallel for all input and output positions. In these models,
the number of operations required to relate signals from two arbitrary input or output positions grows
in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes
it more difficult to learn dependencies between distant positions [12]. In the Transformer this is
reduced to a constant number of operations, albeit at the cost of reduced effective resolution due
to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as
described in section 3.2.


Self-attention, sometimes called intra-attention is an attention mechanism relating different positions
of a single sequence in order to compute a representation of the sequence. Self-attention has been
used successfully in a variety of tasks including reading comprehension, abstractive summarization,
textual entailment and learning task-independent sentence representations [4, 27, 28, 22].


End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering and
language modeling tasks [34].


To the best of our knowledge, however, the Transformer is the first transduction model relying
entirely on self-attention to compute representations of its input and output without using sequencealigned RNNs or convolution. In the following sections, we will describe the Transformer, motivate
self-attention and discuss its advantages over models such as [17, 18] and [9].


**3** **Model Architecture**


Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].
Here, the encoder maps an input sequence of symbol representations ( _x_ 1 _, ..., xn_ ) to a sequence
of continuous representations **z** = ( _z_ 1 _, ..., zn_ ). Given **z**, the decoder then generates an output
sequence ( _y_ 1 _, ..., ym_ ) of symbols one element at a time. At each step the model is auto-regressive

[10], consuming the previously generated symbols as additional input when generating the next.


2


![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_is_all_you_need.pdf-2-0.png)

Figure 1: The Transformer - model architecture.


The Transformer follows this overall architecture using stacked self-attention and point-wise, fully
connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,
respectively.


**3.1** **Encoder and Decoder Stacks**

---

[Document 3]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_7
Content: compact architecture suitable for resource-constrained devices, thereby supporting localized safety.
We utilized three benchmark datasets: (i) PRONOSTIA
(Nectoux et al., 2012), (ii) XJTU-SY (Wang et al., 2018),
and (iii) HUST (Thuan & Hong, 2023). Training is performed on PRONOSTIA, while XJTU-SY and HUST are

used to assess cross-validation. We used the Score met
ric (Nectoux et al., 2012) to assess the performance. We
evaluate generalization using _Bearing 1_ from the first operating condition of each dataset. Figure 3 visualizes the
expected degradation alongside the outputs of NAC. On the
PRONOSTIA dataset, ContiFormer performed the best with



**3.4. Run Time and Memory Experiments**


We evaluate computational requirements on fixed-length
sequences of 1024 steps, 64-dimensional features, 4 heads,
and a Batch size of 1. Each model is run for ten for
ward passes on Google Colab T4-GPU, and we report the
mean runtime with standard deviation, throughput, and peak
memory usage. NAC occupies an intermediate position
in runtime relative to several CT-RNN models, including
GRU-ODE, CfC, and LTC. In terms of memory consumption, NAC uses significantly less memory than mTAN, with
NAC-2k being the least memory-consuming among the CTAttention models. Reducing NAC sparsity from 90% to 20%
has minimal effect on memory, decreasing usage slightly



7


**Neuronal Attention Circuit (NAC) for Representation Learning**



_Table 2._ Run-Time and Memory Benchmark Results


**Run-Time** **Throughput** **Peak Memory**
**Model**
(s) (seq/s) (MB)


RNN 1 _._ 8392 _[±]_ [0] _[.]_ [1933] 0.544 0.29
CT-RNN 7 _._ 1097 _[±]_ [0] _[.]_ [3048] 0.141 0.67
LSTM 2 _._ 6241 _[±]_ [0] _[.]_ [2906] 0.381 0.42
PhasedLSTM 4 _._ 9812 _[±]_ [0] _[.]_ [272] 0.201 0.80
GRU 3 _._ 216 _[±]_ [0] _[.]_ [2566] 0.311 0.54
GRU-ODE 12 _._ 2498 _[±]_ [0] _[.]_ [0525] 0.082 0.64
mmRNN 7 _._ 5852 _[±]_ [0] _[.]_ [2785] 0.132 0.96
LTC 14 _._ 643 _[±]_ [0] _[.]_ [2445] 0.068 0.99
CfC 6 _._ 0988 _[±]_ [0] _[.]_ [2135] 0.164 0.76


Attention 0 _._ 0016 _[±]_ [0] _[.]_ [0001] 625.00 16.86
MHA 0 _._ 0041 _[±]_ [0] _[.]_ [0001] 243.90 69.05
mTAN 0 _._ 0272 _[±]_ [0] _[.]_ [0054] 36.76 790.16
ODEFormer 0 _._ 0317 _[±]_ [0] _[.]_ [0016] 31.55 67.71
CTA 8 _._ 5275 _[±]_ [0] _[.]_ [2355] 0.117 1.43
ContiFormer 0 _._ 066 _[±]_ [0] _[.]_ [0075] 15.15 67.71


NAC-2k 7 _._ 3071 _[±]_ [0] _[.]_ [1547] 0.137 44.75
NAC-32k 7 _._ 2313 _[±]_ [0] _[.]_ [219] 0.138 549.86
NAC-PW 8 _._ 5649 _[±]_ [0] _[.]_ [0203] 0.117 5042.09
NAC-FC 0 _._ 0195 ~~_[±]_~~ ~~[0]~~ ~~_[.]_~~ ~~[0002]~~ 51.28 29.92
NAC-02s 7 _._ 252 _[±]_ [0] _[.]_ [2018] 0.138 151.54
NAC-09s 7 _._ 222 _[±]_ [0] _[.]_ [176] 0.139 150.85
NAC-Exact/05s/8k 7 _._ 4101 ~~_[±]_~~ ~~[0]~~ ~~_[.]_~~ ~~[1586]~~ 0.135 151.50
NAC-Euler 7 _._ 3367 _[±]_ [0] _[.]_ [1719] 0.136 152.22
NAC-Steady 7 _._ 2942 _[±]_ [0] _[.]_ [1451] 0.137 150.86


from 151.54 MB to 150.85 MB. In constrast, decreasing
the Top- _K_ selection from _PW_ to _k_ = 2 drastically reduces
memory consumption from 5042 MB to 44.75 MB, demonstrating the flexibility of NAC.
**Interpreting the Results:** From the experiments, we observe that increasing the sparsity of the NAC layer improves
the robustness of the system and leads to higher overall accuracy. Similarly, increasing the Top- _K_ interactions enhances
accuracy too; however, the benefits diminish as memory
consumption grows. Using Exact mode, Top- _K_ =8 with 50%
sparsity achieves the best balance between accuracy and
efficiency. Steady mode is the fastest, while Euler mode
handles adaptive temporal dynamics.


**4. Discussions**



This research is part of ongoing work on biologically plausible attention mechanisms and represents a pioneering step,
with limitations to be addressed in future work.

**Architectural improvement:** Currently, NAC uses predetermined wiring (AutoNCP) requiring three inputs: number of units (sensory + interneuron + motor), output motor
neurons, and sparsity, with typically 60% of units assigned
to sensory neurons. To integrate with the attention mechanism while preserving wiring, sensory units for _NN_ sensory
are set as unitssensory = � _d_ model0 _.−_ 6 0 _._ 5 � and backbone units as



are set as unitssensory = � _d_ model0 _.−_ 6 0 _._ 5 � and backbone units as

units _backbone_ = _d_ model + � _d_ 0model _._ 6 �, where _⌈·⌉_ and _⌊·⌋_ denote



units _backbone_ = _d_ model + � _d_ 0model _._ 6 �, where _⌈·⌉_ and _⌊·⌋_ denote

the ceiling and floor functions, respectively. This results
in a larger overall architectural size and increased runtime.



Future work will support user-defined NCPs configurations
or randomized wiring to enable more efficient architectures.
**Learnable sparse Top-** _**K**_ **selection:** Sparse Top- _K_ attention can miss important context, is sensitive to _k_, and may
be harder to optimize. A further limitation is that it still
computes the full _QK_ _[⊤]_ matrix, which can dominate the
cost for very long sequences. Future work includes adaptive
or learnable Top- _K_ selection, improved key scoring, and
hardware-aware optimization to strengthen accuracy and
robustness.


**5. Conclusion**
","According to ""Attention Is All You Need"" (Document 1, Table 1):

*   **Self-Attention layer:** Has a computational complexity per layer of O(n² · d), where 'n' is the sequence length and 'd' is the representation dimension.
*   **Recurrent layer:** Has a computational complexity per layer of O(n · d²).

The paper states that self-attention layers are faster than recurrent layers when the sequence length 'n' is smaller than the representation dimensionality 'd' (Document 1, Section 4).",122,2430,2553,,3,,2026-01-02T20:18:15.807142
Which specific biological organism's nervous system inspired the wiring mechanism of the Neuronal Attention Circuit?,Single-Document Fact Lookup,"[Document 1]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_10
Content: Zhuang, J., Jia, M., Ding, Y., and Ding, P. Temporal
convolution-based transferable cross-domain adaptation
approach for remaining useful life estimation under variable failure behaviors. _Reliability Engineering & System_
_Safety_, 216:107946, 2021.



10


**Neuronal Attention Circuit (NAC) for Representation Learning**



**Appendix**


**A. Preliminaries**


**A.1. Attention Mechanism**


Attention mechanisms have become a cornerstone in

modern neural architectures, enabling models to dynamically focus on relevant parts of the input. The concept
was first introduced in the context of neural machine
translation, where it allowed the decoder to weight encoder
outputs according to their importance for generating each
target token. Formally, given a query vector _q ∈_ R _[d]_, key
vectors _K_ = [ _k_ 1 _, k_ 2 _, . . ., kn_ ] _∈_ R _[n][×][d]_, and value vectors
_V_ = [ _v_ 1 _, v_ 2 _, . . ., vn_ ] _∈_ R _[n][×][d]_, the attention mechanism can
be expressed in two steps:


1. Compute the scaled dot attention logits:


_ai_ = _[q][T][ k][i]_ (21)
~~_√_~~ _d_


2. Normalize the logits to get attention weights and compute the output:


_e_ _[a][i]_
_αi_ = softmax( _ai_ ) = ~~_n_~~ (22)
~~�~~ _j_ =1 _[e][a][j]_



exhibit a resting potential at _−_ 70 mV and an activation potential near _−_ 20 mV. Similarly, each _Nm_ is composed of
two subneurons, _Mp_ and _Mn_, and is driven by a controllable variable _y_, which also maps to a biologically plausible
range [ _−_ 70 mV _, −_ 20 mV]. The connections in the NCP
architecture are designed to reflect the biological sparsity
and abstraction of neural circuits. Specifically, connections
from _Ns_ to _Ni_ are feedforward, while those between _Nc_
and _Nm_ are highly recurrent (Lechner et al., 2018). Figure
1(a) illustrates the connectome of NCPs.


**B. Proofs**


In this section, we provide all the proofs.


**B.1. Deriving Closed-form (Exact) Solution**


Although _ϕ_ and _ωτ_ are nonlinear functions of the input
**u** = [ **q** ; **k** ], we derive closed-form solution by treating them
as locally constant over the pseudo-time integration interval
for each query–key pair based on frozen-coefficient approximation (John, 1952). This is accurate whenever the interval
is short or when input variations are slow compared with
the relaxation rate _ωτ_ . Under approximation assumption,
rewrite Eqn. 1 as


_dadtt_ [+] _[ ω][τ]_ _[a][t]_ [ =] _[ ϕ.]_ (24)


This is now a linear first-order ODE. The integrating factor
is



Attention( _q, k, v_ ) =



_n_
� _αivi_ (23)


_i_ =1



Here, _ai_ is the raw attention logit between the query and
each key, and the scaling factor _√d_ prevents large dot prod
ucts from destabilizing the softmax (Vaswani et al., 2017).


**A.2. Neuronal Circuit Policies (NCPs)**


NCPs represent a biologically inspired framework for developing interpretable neural control agents by adapting the
tap-withdrawal circuit found in the nematode _C. elegans_
(Lechner et al., 2018). Unlike traditional spiking neural
networks, the majority of neurons in this circuit exhibit
electronic dynamics, characterized by the passive flow of
electrical charges, resulting in graded potentials. NCPs
are structured as a four-layer hierarchical architecture comprising sensory neurons ( _Ns_ ), interneurons ( _Ni_ ), command
neurons ( _Nc_ ), and motor neurons ( _Nm_ ). The _Ns_ perceive
and respond to external stimulus inputs and are responsible for the initial signal transduction. Each _Ns_ consists
of subneurons _Sp_ and _Sn_ and a system variable _x_ . The
activation of _Sp_ and _Sn_ depends upon the sign of _x_ : _Sp_
becomes activated for _x >_ 0, whereas _Sn_ becomes activated for _x <_ 0. The variable _x_ is mapped to the membrane
potential range of [ _−_ 70 mV _, −_ 20 mV], which is consistent
with the biophysical behavior of nerve cells, which typically



� _ωτ dt_

_µ_ = _e_ � �


Multiply both sides by _µ_ ( _t_ ):



Substitute back:


_e_ _[ω][τ][ t]_ _at −_ _a_ 0 = _ϕ ·_ _[e][ω][τ][ t][ −]_ [1] _._ (30)

_ωτ_



= _e_ _[ω][τ][ t]_ _._ (25)




_[da][t]_
_e_ _[ω][τ][ t]_ _dt_ [+] _[ ω][τ]_ _[e][ω][τ][ t][a][t]_ [ =] _[ ϕe][ω][τ][ t][.]_ (26)


Recognize the left-hand side as the derivative of _e_ _[ω][τ][ t]_ _at_ :


_d_
� _e_ _[ω][τ][ t]_ _at_ � = _ϕe_ _[ω][τ][ t]_ _._ (27)
_dt_


Integrate from 0 to _t_ :


_t_
_e_ _[ω][τ][ t]_ _at −_ _e_ [0] _a_ 0 = _ϕ_ _e_ _[ω][τ][ s]_ _ds._ (28)
�0


Compute the integral (since _ωτ ̸_ = 0):



�0 _t_



_e_ _[ω][τ][ s]_ _ds_ = [1]
0 _ωτ_



_ωτ_



� _e_ _[ω][τ][ t]_ _−_ 1� _._ (29)



11


Rearrange:



**Neuronal Attention Circuit (NAC) for Representation Learning**


CASE 2: MULTIPLE CONNECTIONS ( _M >_ 1).



_e_ _[ω][τ][ t]_ _at_ = _a_ 0 + _[ϕ]_

_ωτ_


Divide both sides by _e_ _[ω][τ][ t]_ :


_at_ = _a_ 0 _e_ _[−][ω][τ][ t]_ + _[ϕ]_

_ωτ_



� _e_ _[ω][τ][ t]_ _−_ 1� _._ (31)


�1 _−_ _e_ _[−][ω][τ][ t]_ [�] _._ (32)



The ODE is



with per-connection equilibria _Aj_ = _ϕj/fj_ . The effective
equilibrium is



_da_

_dt_ [=] _[ −]_ � � _[M]_



_da_



� _fj_ � _a_ +

_j_ =1



_M_
� _fjAj,_ (37)

_j_ =1



Set _a_ _[∗]_ := _[ϕ]_ . Then _at_ = _a_ _[∗]_ + ( _a_ 0 _−_ _a_ _[∗]_ ) _e_ _[−][ω][τ][ t]_, proved.

_ωτ_


**B.2. Proof of Theorem 1**

---

[Document 2]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_0
Content: ## **Neuronal Attention Circuit (NAC) for Representation Learning**

**Waleed Razzaq** [1] **Izis Kankaraway** [1] **Yun-Bo Zhao** [1 2]



**Abstract**

Attention improves representation learning over
RNNs, but its discrete nature limits continuoustime (CT) modeling. We introduce Neuronal Attention Circuit (NAC), a novel, biologically plausible CT-Attention mechanism that reformulates

attention logits computation as the solution to a
linear first-order ODE with nonlinear interlinked
gates derived from repurposing _C. elegans_ Neuronal Circuit Policies (NCPs) wiring mechanism.
NAC replaces dense projections with sparse sensory gates for key-query projections and a sparse
backbone network with two heads for computing
_content-target_ and _learnable time-constant_ gates,
enabling efficient adaptive dynamics. NAC supports three attention logit computation modes: (i)
explicit Euler integration, (ii) exact closed-form
solution, and (iii) steady-state approximation. To
improve memory intensity, we implemented a
sparse Top- _K_ pairwise concatenation scheme that
selectively curates key-query interactions. We
provide rigorous theoretical guarantees, including
state stability, bounded approximation errors, and
universal approximation. Empirically, we implemented NAC in diverse domains, including irregular time-series classification, lane-keeping for
autonomous vehicles, and industrial prognostics.
We observed that NAC matches or outperforms
competing baselines in accuracy and occupies an
intermediate position in runtime and memory efficiency compared with several CT baselines.


**1. Introduction**


Learning representations of sequential data in temporal or
spatio-temporal domains is essential for capturing patterns
and enabling accurate forecasting. Discrete-time Recurrent neural networks (DT-RNNs) such as RNN (Rumelhart et al., 1985; Jordan, 1997), Long-short term memory


1Department of Automation, University of Science & Technology of China, Hefei, China [2] Institute of Artificial Intelligence,
Hefei Comprehensive National Science Center. Correspondence to:
Yun-Bo Zhao _<_ ybzhao@ustc.edu.cn _>_, Waleed Razzaq _<_ waleedrazzaq@mail.ustc.edu.cn _>_ .


_Preprint. December 12, 2025._



(LSTM) (Hochreiter & Schmidhuber, 1997), and Gated Recurrent Unit (GRU) (Cho et al., 2014) model sequential dependencies by iteratively updating hidden states to represent
or predict future elements in a sequence. While effective
for regularly sampled sequences, DT-RNNs face challenges
with irregularly sampled data because they assume uniform
time intervals. In addition, vanishing gradients can make
it difficult to capture long-term dependencies (Hochreiter,
1998).
Continuous-time RNNs (CT-RNNs) (Rubanova et al., 2019)
model hidden states as ordinary differential equations
(ODEs), allowing them to process inputs that arrive at arbitrary or irregular time intervals. Mixed-memory RNNs
(mmRNNs) (Lechner & Hasani, 2022) build on this idea
by separating memory compartments from time-continuous
states, helping maintain stable error propagation while capturing continuous-time dynamics. Liquid neural networks
(LNNs) (Hasani et al., 2021; 2022) take a biologically inspired approach by assigning variable time-constants to hidden states, improving adaptability and robustness, though
vanishing gradients can still pose challenges during training.
The attention mechanisms (Vaswani et al., 2017) mitigate
this limitation by treating all time steps equally and allowing models to focus on the most relevant observations. It
computes the similarity between queries ( _q_ ) and keys ( _k_ ),
scaling by the key dimension to keep gradients stable. MultiHead Attention (MHA) (Vaswani et al., 2017) extends this
by allowing the model to attend to different representation
subspaces in parallel. Variants like Sparse Attention (Tay
et al., 2020; Roy et al., 2021), BigBird (Zaheer et al., 2020),
and Longformer (Beltagy et al., 2020) modify the attention
pattern to reduce computational cost, particularly for long sequences, by attending only to selected positions rather than
all pairs. Even with these improvements, attention-based
methods still rely on discrete scaled dot-product operations,
limiting their ability to model continuous trajectories often
captured by CT counterparts.
Recent work has explored bridging this gap through NeuralODE (Chen et al., 2018) formulation. mTAN (Shukla &
Marlin, 2021) learns CT embeddings and uses time-based
attention to interpolate irregular observations into a fixedlength representation for downstream encoder-decoder modeling. ODEFormer (d’Ascoli et al., 2023) trains a sequenceto-sequence transformer on synthetic trajectories to directly
infer symbolic ODE systems from noisy, irregular data,
though it struggles with chaotic systems and generalization



1


**Neuronal Attention Circuit (NAC) for Representation Learning**



beyond observed conditions. Continuous-time Attention
(CTA) (Chien & Chen, 2021) embeds a continuous-time
attention mechanism within a Neural ODE, allowing attention weights and hidden states to evolve jointly over time.
Still, it remains computationally intensive and sensitive to
the accuracy of the ODE solver. ContiFormer (Chen et al.,
2023) builds a CT-transformer by pairing ODE-defined latent trajectories with a time-aware attention mechanism to
model dynamic relationships in data.
Despite these innovations, a persistent and underexplored
gap remains in developing a biologically plausible attention
mechanism that seamlessly integrates CT dynamics with the
abstraction of the brain’s connectome to handle irregular sequences without prohibitive computational costs. Building
on this, we propose a novel attention mechanism called the
_Neuronal Attention Circuit_ (NAC), in which attention logits
are computed as the solution to a first-order ODE modulated
by nonlinear, interlinked gates derived from repurposing
Neuronal Circuit Policies (NCPs) from the nervous system
of _C. elegans_ nematode (refer to Appendix A.2 for more
information). Unlike standard attention, which projects keyquery pairs through a dense layer, NAC employs a sensory
gate to transform input features and a backbone to model
nonlinear interactions, with multiple heads producing outputs structured for attention logits computation. Based on
the solutions to ODE, we define three computation modes:
(i) Exact, using the closed-form ODE solution; (ii) Euler,
approximating the solution via _explicit Euler_ integration;
and (iii) Steady, using only the steady-state solution, analogous to standard attention scores. To reduce computational
complexity, we implemented a sparse Top- _K_ pairwise concatenation algorithm that selectively curates key-query inputs. We evaluate NAC across multiple domains, including
irregularly sampled time series, autonomous vehicle lanekeeping, and Industry 4.0, comparing it to state-of-the-art
baselines. NAC consistently matches or outperforms these
models, while runtime and peak memory benchmarks place
it between CT-RNNs in terms of speed and CT-Attentions
in terms of memory requirements.


**2. Neuronal Attention Circuit (NAC)**


We propose a simple alternative formulation of the attention
logits _a_ (refer to Appendix A.1 for more information), interpreting them as the solution to a first-order linear ODE
modulated by nonlinear, interlinked gates:



from repurposing NCPs. We refer to this formulation as
the Neuronal Attention Circuit (NAC). It enables the logits
_at_ to evolve dynamically with input-dependent, variable
time constants, mirroring the adaptive temporal dynamics
found in _C. elegans_ nervous systems while improving
computational efficiency and expressiveness. Moreover, it
introduces continuous depth into the attention mechanism,
bridging discrete-layer computation with dynamic temporal
evolution.

**Motivation behind this formulation:** The proposed
formulation is loosely motivated by the input-dependent
time-constant mechanism of Liquid Neural Networks
(LNNs), a class of CT-RNNs inspired by biological nervous
systems and synaptic transmission. In this framework, the
dynamics of non-spiking neurons are described by a linear
ODE with nonlinear, interlinked gates: _ddt_ **xt** = **[x]** _τ_ **[t]** [+] **[ S][t]** _[,]_

---

[Document 3]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_8
Content: the ceiling and floor functions, respectively. This results
in a larger overall architectural size and increased runtime.



Future work will support user-defined NCPs configurations
or randomized wiring to enable more efficient architectures.
**Learnable sparse Top-** _**K**_ **selection:** Sparse Top- _K_ attention can miss important context, is sensitive to _k_, and may
be harder to optimize. A further limitation is that it still
computes the full _QK_ _[⊤]_ matrix, which can dominate the
cost for very long sequences. Future work includes adaptive
or learnable Top- _K_ selection, improved key scoring, and
hardware-aware optimization to strengthen accuracy and
robustness.


**5. Conclusion**


In this paper, we introduce the Neuronal Attention Circuit
(NAC), a biologically inspired attention mechanism that
reformulates attention logits as the solution to a first-order
ODE modulated by nonlinear, interlinked gates derived from
repurposing _C.elegans_ nematode NCPs. NAC bridges discrete attention with continuous-time dynamics, enabling
adaptive temporal processing without the limitations inherent to traditional scaled dot-product attention. Based on the
solution to ODE, we introduce three computational modes:
(i) Euler, based on _explicit Euler_ integration; (ii) Exact, providing closed-form solutions; and (iii) Steady, approximating equilibrium states. In addition, a sparse Top- _K_ pairwise
concatenation scheme is introduced to mitigate the memory intensity. Theoretically, we establish NAC’s log-state
stability, exponential error bounds, and universal approximation, thereby providing rigorous guarantees of convergence
and expressiveness. Empirical evaluations demonstrate that
NAC achieves state-of-the-art performance across diverse
tasks, including irregularly sampled time-series benchmarks,
autonomous vehicle lane-keeping, and industrial prognostics. Moreover, NAC occupies an intermediate position
between CT-RNNs and CT-Attention, offering robust temporal modeling while requiring less runtime than CT-RNNs
and less memory than CT-Attention models.


**Reproducibility Statement**


The code for reproducibility is available at
[https://github.com/itxwaleedrazzaq/](https://github.com/itxwaleedrazzaq/neuronal_attention_circuit)

[neuronal_attention_circuit](https://github.com/itxwaleedrazzaq/neuronal_attention_circuit)


**Impact Statement**


The work addresses the growing field of continuous-time
attention and pioneers a biologically plausible mechanism.
It encourages research into sparse, adaptive networks that
resemble natural wiring. From a societal perspective, it
supports more robust AI in resource-limited settings, but it
also raises ethical concerns when applied to areas such as
surveillance or autonomous systems.



8


**Neuronal Attention Circuit (NAC) for Representation Learning**



**References**


Introduction to self-driving cars. URL
[https://www.udacity.com/course/](https://www.udacity.com/course/intro-to-self-driving-cars--nd113)
[intro-to-self-driving-cars--nd113.](https://www.udacity.com/course/intro-to-self-driving-cars--nd113)


Aguiar-Conraria, L. and Soares, M. J. The continuous
wavelet transform: Moving beyond uni-and bivariate analysis. _Journal of economic surveys_, 28(2):344–375, 2014.


Beltagy, I., Peters, M. E., and Cohan, A. Longformer: The long-document transformer. _arXiv preprint_
_arXiv:2004.05150_, 2020.


Bojarski, M., Del Testa, D., Dworakowski, D., Firner, B.,
Flepp, B., Goyal, P., Jackel, L. D., Monfort, M., Muller,
U., Zhang, J., et al. End to end learning for self-driving
cars. _arXiv preprint arXiv:1604.07316_, 2016.


Brockman, G., Cheung, V., Pettersson, L., Schneider, J.,
Schulman, J., Tang, J., and Zaremba, W. Openai gym.
_arXiv preprint arXiv:1606.01540_, 2016.


Cao, Y., Li, S., Petzold, L., and Serban, R. Adjoint sensitivity analysis for differential-algebraic equations: The
adjoint dae system and its numerical solution. _SIAM_
_journal on scientific computing_, 24(3):1076–1089, 2003.


Chen, R. T., Rubanova, Y., Bettencourt, J., and Duvenaud,
D. K. Neural ordinary differential equations. _Advances_
_in neural information processing systems_, 31, 2018.


Chen, Y., Ren, K., Wang, Y., Fang, Y., Sun, W., and Li, D.
Contiformer: Continuous-time transformer for irregular
time series modeling. _Advances in Neural Information_
_Processing Systems_, 36:47143–47175, 2023.


Chien, J.-T. and Chen, Y.-H. Continuous-time attention for
sequential learning. In _Proceedings of the AAAI confer-_
_ence on artificial intelligence_, volume 35, pp. 7116–7124,
2021.


Cho, K., Van Merrienboer, B., Gulcehre, C., Bahdanau,¨
D., Bougares, F., Schwenk, H., and Bengio, Y. Learning phrase representations using rnn encoder-decoder
for statistical machine translation. _arXiv preprint_
_arXiv:1406.1078_, 2014.


d’Ascoli, S., Becker, S., Mathis, A., Schwaller, P., and
Kilbertus, N. Odeformer: Symbolic regression of dynamical systems with transformers. _arXiv preprint_
_arXiv:2310.05573_, 2023.


De Brouwer, E., Simm, J., Arany, A., and Moreau, Y.
Gru-ode-bayes: Continuous modeling of sporadicallyobserved time series. _Advances in neural information_
_processing systems_, 32, 2019.



Deng, L. The mnist database of handwritten digit images
for machine learning research [best of the web]. _IEEE_
_signal processing magazine_, 29(6):141–142, 2012.


Ding, Y., Jia, M., Miao, Q., and Huang, P. Remaining
useful life estimation using deep metric transfer learning
for kernel regression. _Reliability Engineering & System_
_Safety_, 212:107583, 2021.


Hasani, R., Lechner, M., Amini, A., Rus, D., and Grosu,
R. Liquid time-constant networks. In _Proceedings of the_
_AAAI Conference on Artificial Intelligence_, volume 35,
pp. 7657–7666, 2021.


Hasani, R., Lechner, M., Amini, A., Liebenwein, L., Ray,
A., Tschaikowski, M., Teschl, G., and Rus, D. Closed
form continuous-time neural networks. _Nature Machine_

_Intelligence_, 4(11):992–1003, 2022.


Hochreiter, S. The vanishing gradient problem during learning recurrent neural nets and problem solutions. _Interna-_
_tional Journal of Uncertainty, Fuzziness and Knowledge-_
_Based Systems_, 6(02):107–116, 1998.


Hochreiter, S. and Schmidhuber, J. Long short-term memory.
_Neural computation_, 9(8):1735–1780, 1997.


Hong, H. S. and Thuan, N. Hust bearing: a practical dataset
for ball bearing fault diagnosis. _Mendeley Data_, 3, 2023.


John, F. On integration of parabolic equations by difference methods: I. linear and quasi-linear equations for the
infinite interval. _Communications on Pure and Applied_
_Mathematics_, 5(2):155–211, 1952.
","The wiring mechanism of the Neuronal Attention Circuit (NAC) was inspired by the Neuronal Circuit Policies (NCPs) found in the nervous system of the nematode *C. elegans*. Specifically, it adapts the tap-withdrawal circuit of *C. elegans*.",122,2871,2993,,3,,2026-01-02T20:18:18.801921
"What is the ""attention sink"" phenomenon described in the context of the ""Keep First k Tokens"" method?",Single-Document Fact Lookup,"[Document 1]
Source: sliding_window_attention_annotated_chunks_2k.jsonl
Chunk ID: sliding_window_attention_annotated_chunks_2k_4
Content: **Keep First becomes optional rather than essen-**
**tial.** Before fine-tuning, Keep First is crucial for
stability under SWA. But after SFT, it only provides
minor additional improvement. With FA Decode,
adding _k_ = 100 (row 7) improves over _k_ = 0
(row 3) only 4.5%, and if further combining FA
layers, it almost offers no improvements (row 11
and row 8).


**Effect of sliding window size.** Row 3 and 5
shows that increasing the window from 2k to 4k
with FA Decode improves thinking-model accuracy from 57 _._ 9 to 62 _._ 6. This mirrors the non-SFT
trend that larger windows help, but the dominant
improvements still come from FA Decode and Interleaving Layers.

So, we answer RQ2: fine-tuning brings remarkably high performance restoration, provided we ap


ply **FA Decode**, **Interleaving Layers**, or a combination thereof, while **Keep First** becomes optional.
And the improvement brought by SFT under each
configuration varies significantly, meaning a nearoptimal training-free configuration need not remain
optimal after SFT, and vice versa.


**5.3** **Performance–efficiency Trade-offs and**
**Recommended Recipes**


Although integrating more methods can typically
achieve higher accuracy, it introduces more overhead, indicating that the efficiency of each recipe
must also be evaluated. To assess the performanceefficiency trade-off of different SWAA configurations, we evaluate time-to-first-token (TTFT), timeper-output-token (TPOT), total throughput, and average running time per request. Concretely, we
benchmark Qwen3-4B-Thinking on a single H100
GPU using vLLM’s bench_serve utility (Kwon
et al., 2023) with random input data and 100 total
requests. The prompt length and output length are
set to 128k and 512 tokens, respectively, representing a typical long-context QA setting.



7


```
### Figure - Accuracy vs. Time with SFT
![Accuracy graph](sliding_window_attention.pdf-7-0.png)

**Caption:**  
The figure shows the accuracy of a model with and without Sliding Factor Token (SFT) over time. Different configurations of window size, FA layers, and decode settings are compared.

**Description:**
- The x-axis represents time in seconds, ranging from 0.5 to 3.5 seconds.
- The y-axis represents accuracy in percentage.
- Two lines are plotted: a solid line represents the accuracy with SFT, and a dashed line represents the accuracy without SFT.
- Different colored markers indicate different configurations of the model:
  - Blue markers represent ""Full"" window, 0 FA layers, and False decode.
  - Green markers represent ""2k"" window, 0 FA layers, and False decode.
  - Orange markers represent ""2k"" window, 0 FA layers, and True decode.
  - Pink markers represent ""2k"" window, 1/2 FA layers, and False decode.
  - Light blue markers represent ""2k"" window, 1/2 FA layers, and True decode.
- The accuracy with SFT generally increases with time for all configurations.
- The accuracy without SFT is consistently lower than the accuracy with SFT.

```
### Figure - Accuracy vs. Time with and without SFT
![Accuracy graph](sliding_window_attention.pdf-7-1.png)

**Caption:**  
The figure shows the accuracy of a model with and without Sliding Factor Token (SFT) as a function of time window. Different configurations of FA layers and decode settings are also shown.

**Description:**
- The graph plots accuracy (%) on the y-axis against time (s) on the x-axis.
- Data points are color-coded to represent different configurations: w/ SFT (blue) and w/o SFT (black).
- Each color represents a specific combination of window size, FA layers, FA decode, and decode setting.
- The blue line represents the performance with SFT, while the black line represents the performance without SFT.
- The x-axis ranges from 0.5s to 3.5s, indicating the time window used for analysis.
- The y-axis ranges from 20% to 60%, representing the accuracy percentage.

(a) Qwen3-4B-Thinking (b) Qwen3-4B-Instruct


Figure 2: Accuracy and inference time of each configuration of Qwen3-4B on LongMemEval



To visualize the performance-efficiency tradeoff, Figure 2 plots each configuration’s accuracy
on LongMemEval_24k (Wu et al., 2024) against its
average running time, while detailed TTFT, TPOT,
and throughput statistics for each configuration are
provided in Appendix E. We draw a line between
the full-attention point and the naive-SWA point
as a baseline curve: configurations above this line
offer a better accuracy-latency balance intuitively.
For configurations with nearly identical time costs,
we display only the one with the highest accuracy.
Since **Keep First** _k_ has negligible impact on runtime (Appendix E), all plotted configurations fix
_k_ = 10.


From Figure 2, we observe that many configurations in Figure 2 achieve a clearly better
performance-efficiency ratio than baselines. And
for the thinking model, more points lie above the
baseline curve compared to non-thinking, indicating that **CoT** generally has a positive effect on improving the performance-efficiency ratio of SWAA.


Thus, we finally answer RQ3: many SWAA
configurations all reach excellent performanceefficiency trade-off, but there is no single metric to
quantify such trade-off to decide the globally optimal one. We therefore summarize **recommended**

**SWA adaptation recipes** tailored to various deployment scenarios in Table 3. And we must note
that specific parameters should be flexibly set to
meet application-specific requirements, without the
need to follow our experimental parameters (e.g.,



a 2k window, _k_ = 10). For example, you can increase the window size to 4k or _k_ to 128 for higher
accuracy and acceptable additional overhead.


**6** **Conclusion**


In this work, we validate the feasibility of adapting
full-attention pretrained LLMs to Sliding Window
Attention (SWA) for better efficiency, offering a
cost-effective alternative that avoids training sparseattention models from scratch. By systematically
deconstructing the adaptation process, we identify that the catastrophic degradation observed in
naive implementations can be effectively mitigated
through synergistic combinations of auxiliary methods. Our extensive experiments across the Qwen
and Llama families demonstrate that while trade
offs between computational overhead and model
performance are inevitable, optimized configurations can get an excellent performance-efficiency
balance.


**7** **Limitations**


We speculate that the ideal reasoning trajectory of
the model adapted to SWA should be longer than
the original model, to compensate for the information loss caused by SWA. That means, using
the answers generated by the original model as
fine-tuning data may not be the optimal training
method. Rather, RL methods like GRPO (Shao
et al., 2024) might further help the model adapted
to SWA learn a better reasoning trajectory. How


8


ever, we did not experiment with them since they
are too time-consuming and unstable.
We have not yet implemented the KV cache eviction (or overwriting) mechanism when using SWA;
that is, although the speed is improved, memory
usage is not effectively reduced.
Further experiments may be needed to confirm
whether our conclusions generalize to larger model
sizes, such as 70B.


**References**


Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei
Hou, Jie Tang, Yuxiao Dong, and Juanzi Li. 2024a.
[Longalign: A recipe for long context alignment of](https://arxiv.org/abs/2401.18058)
[large language models.](https://arxiv.org/abs/2401.18058)


Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu,
Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao
Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang,
[and Juanzi Li. 2023. Longbench: A bilingual, multi-](https://arxiv.org/abs/2308.14508)
[task benchmark for long context understanding.](https://arxiv.org/abs/2308.14508)

---

[Document 2]
Source: attention_functional_roles_raw_chunks_2k.jsonl
Chunk ID: attention_functional_roles_raw_chunks_2k_10
Content: 21


A.7 PROMPT FOR QUESTION ASKING



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-21-0.png)

A.8 ABLATION STUDY


In the main experiments, we use the top-k generated tokens and average their multi-head attention
vectors. Here are some examples of selected top-k tokens by llm:



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-21-1.png)



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-21-2.png)



We also explore alternative strategies for extracting representations, including using the first meaningful token, first token, last token and with or without layerbias. The corresponding results are
shown in Table 9.


Sensitivity to the choice of k: We vary _k ∈{_ 1 _,_ 3 _,_ 5 _}_ and compute Pearson correlations of the
resulting attention-head heatmaps. Figure 14 shows that the heatmaps remain highly correlated
across choices of k, demonstrating that our method is robust to the exact number of selected tokens.
This robustness arises because (1) the activation patterns associated with answering a subquestion
are reflected across multiple output tokens, and (2) VLM outputs are short, reducing variance from
token choice.


22


Table 9: Ablation experiment of topK tokens, first meaningful token, first token and last token, and
with layerbias vs without layerbias on Qwen2.5-VL-3B-Instruct.


Vision mainly Cognitive Functions Language mainly Cognitive Functions



Token LayerBias Inter ~~H~~ ead



Low-Level High-Level Recall Info Recall Math Inference Decision


llm acc llm acc llm acc llm acc llm acc llm acc llm acc llm acc



random 80.65 90.32 87.88 93.18 92.42 90.91 59.26 75.93 87.14 85.71 65.85 87.80 81.01 87.34 40.63 73.44
First without
cognitive 48.39 54.84 45.24 48.23 80.30 83.33 50.00 72.22 **0.00** **0.01** 82.93 95.12 55.70 63.29 64.06 70.31


random 51.61 67.74 81.82 88.64 92.42 95.45 64.81 81.48 88.57 85.71 78.05 92.68 83.54 87.34 65.63 75.00
with
cognitive 45.16 48.39 56.06 66.67 86.36 86.36 48.15 62.96 74.29 81.43 78.05 90.24 51.90 56.96 68.75 75.00


random 80.65 93.54 90.15 90.91 89.39 90.91 70.37 77.78 87.14 91.43 63.41 82.93 70.89 79.75 63.19 73.75
Last without
cognitive 12.90 12.90 66.94 67.12 86.36 86.36 38.89 64.81 15.71 18.57 82.93 95.12 6.33 7.59 48.44 50.00


random 90.32 100.0 89.39 93.18 95.45 92.42 61.11 72.22 38.57 38.57 85.37 92.68 84.81 81.61 75.00 78.13
with
cognitive 67.74 64.52 89.39 93.18 67.74 64.52 **0.06** **0.02** 68.57 64.29 82.93 95.12 43.04 46.84 65.63 70.31


random 77.42 80.65 81.06 83.33 84.85 86.36 68.52 59.26 75.71 88.57 51.22 65.85 77.22 79.75 73.44 70.32
Meaning ~~f~~ irst without cognitive 77.42 87.10 50.76 54.55 84.85 75.76 50.00 53.70 2.85 4.23 85.37 95.68 72.15 79.75 57.81 75.00


random 90.32 93.55 86.36 87.88 84.85 86.36 77.78 83.33 81.43 87.14 65.85 82.93 70.89 62.03 68.75 67.19
with
cognitive 67.74 64.52 68.18 72.73 84.85 75.76 59.26 62.96 2.85 4.23 7.31 9.76 72.15 79.75 48.44 57.82


random 83.87 90.32 82.58 83.33 84.85 86.36 64.81 70.37 87.14 94.29 78.05 92.68 58.23 91.14 78.13 90.63
TopK without cognitive 12.90 67.74 40.15 45.45 **27.27 28.79** 55.56 46.30 38.57 42.86 78.05 85.37 29.11 32.91 57.81 75.00


random 87.10 96.77 82.58 83.33 86.36 84.85 59.26 55.56 85.71 85.71 82.93 87.80 91.14 86.08 81.25 82.81
with
cognitive **6.45** **6.45** **16.67 18.94** 75.76 75.76 62.96 81.48 57.14 62.86 **2.43** **2.43** **0.00** **0.00** **3.13** **4.69**


Figure 13: Pearson Correlation of different prompt types across eight functions for Qwen2.5-VL3B-Instruct. Full Prompt: The prompt used in the main results, with given CoT, Main Question,
Current Question. w/o Main Question: Prompt without main question involved.


Sensitivity to the choice of LLM: We further experimented with alternative LLMs for token selection. As shown in the examples in Table 10, different LLMs consistently select highly similar
semantic tokens. The Pearson correlations of the resulting attention-head heatmaps (Fig. 15) are
likewise very high (almost 1), indicating that modern LLMs share a strong and consistent ability to
identify key semantic units.


**Prompt format:** We also examined the influence of the main question and contextual input (preceding subquestions and their answers). Fig. 13 shows that head importance maps vary noticeably
across these changes, highlighting the importance of including both the main question and contextual information when identifying cognitive heads.


A.9 MORE RESULTS


We conducted probing experiments using 3 random seeds. The results in Fig. 16 demonstrate that
our probing method is highly stable across seeds, with Pearson correlations of the heatmaps reaching
1 for all eight functions in Qwen2.5-VL-3B-Instruct.


23



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-22-0.png)
![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-23-0.png)

Figure 14: Pearson Correlation of different K of TopK token across eight functions for Qwen2.5VL-3B-Instruct.


Figure 15: Pearson Correlation of different LLM selection across eight functions for Qwen2.5-VL3B-Instruct.


24



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-23-1.png)
Table 10: Examples of topk token selection using different LLMs.

---

[Document 3]
Source: attention_is_all_you_need_raw_chunks_2k.jsonl
Chunk ID: attention_is_all_you_need_raw_chunks_2k_1
Content: [10], consuming the previously generated symbols as additional input when generating the next.


2


![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_is_all_you_need.pdf-2-0.png)

Figure 1: The Transformer - model architecture.


The Transformer follows this overall architecture using stacked self-attention and point-wise, fully
connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,
respectively.


**3.1** **Encoder and Decoder Stacks**


**Encoder:** The encoder is composed of a stack of _N_ = 6 identical layers. Each layer has two
sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network. We employ a residual connection [11] around each of
the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is
LayerNorm( _x_ + Sublayer( _x_ )), where Sublayer( _x_ ) is the function implemented by the sub-layer
itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding
layers, produce outputs of dimension _d_ model = 512.


**Decoder:** The decoder is also composed of a stack of _N_ = 6 identical layers. In addition to the two
sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head
attention over the output of the encoder stack. Similar to the encoder, we employ residual connections
around each of the sub-layers, followed by layer normalization. We also modify the self-attention
sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This
masking, combined with fact that the output embeddings are offset by one position, ensures that the
predictions for position _i_ can depend only on the known outputs at positions less than _i_ .


**3.2** **Attention**


An attention function can be described as mapping a query and a set of key-value pairs to an output,
where the query, keys, values, and output are all vectors. The output is computed as a weighted sum


3


Scaled Dot-Product Attention Multi-Head Attention


Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several
attention layers running in parallel.


of the values, where the weight assigned to each value is computed by a compatibility function of the
query with the corresponding key.


**3.2.1** **Scaled Dot-Product Attention**


We call our particular attention ""Scaled Dot-Product Attention"" (Figure 2). The input consists of
queries and keys of dimension _dk_, and values of dimension _dv_ . We compute the dot products of the
query with all keys, divide each by _[√]_ _dk_, and apply a softmax function to obtain the weights on the
values.


In practice, we compute the attention function on a set of queries simultaneously, packed together
into a matrix _Q_ . The keys and values are also packed together into matrices _K_ and _V_ . We compute
the matrix of outputs as:


Attention( _Q, K, V_ ) = softmax( _[Q][K]_ _[T]_ ) _V_ (1)

~~_√_~~ _dk_


The two most commonly used attention functions are additive attention [2], and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor
of ~~_√_~~ 1 _dk_ . Additive attention computes the compatibility function using a feed-forward network with
a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is
much faster and more space-efficient in practice, since it can be implemented using highly optimized
matrix multiplication code.


While for small values of _dk_ the two mechanisms perform similarly, additive attention outperforms
dot product attention without scaling for larger values of _dk_ [3]. We suspect that for large values of
_dk_, the dot products grow large in magnitude, pushing the softmax function into regions where it has
extremely small gradients [4] . To counteract this effect, we scale the dot products by ~~_√_~~ 1 _dk_ .


**3.2.2** **Multi-Head Attention**


Instead of performing a single attention function with _d_ model-dimensional keys, values and queries,
we found it beneficial to linearly project the queries, keys and values _h_ times with different, learned
linear projections to _dk_, _dk_ and _dv_ dimensions, respectively. On each of these projected versions of
queries, keys and values we then perform the attention function in parallel, yielding _dv_ -dimensional


4To illustrate why the dot products get large, assume that the components of _q_ and _k_ are independent random
variables with mean 0 and variance 1. Then their dot product, _q · k_ = [�] _[d]_ _i_ =1 _[k]_ _[q][i][k][i]_ [, has mean][ 0][ and variance] _[ d][k]_ [.]


4



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_is_all_you_need.pdf-3-0.png)

![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_is_all_you_need.pdf-3-1.png)
output values. These are concatenated and once again projected, resulting in the final values, as
depicted in Figure 2.


Multi-head attention allows the model to jointly attend to information from different representation
subspaces at different positions. With a single attention head, averaging inhibits this.


MultiHead( _Q, K, V_ ) = Concat(head1 _, ...,_ headh) _W_ _[O]_

where headi = Attention( _QWi_ _[Q][, KW][ K]_ _i_ _[, V W][ V]_ _i_ [)]


Where the projections are parameter matrices _Wi_ _[Q]_ _∈_ R _[d]_ [model] _[×][d][k]_, _Wi_ _[K]_ _∈_ R _[d]_ [model] _[×][d][k]_, _Wi_ _[V]_ _∈_ R _[d]_ [model] _[×][d][v]_
and _W_ _[O]_ _∈_ R _[hd][v][×][d]_ [model] .


In this work we employ _h_ = 8 parallel attention layers, or heads. For each of these we use
_dk_ = _dv_ = _d_ model _/h_ = 64. Due to the reduced dimension of each head, the total computational cost
is similar to that of single-head attention with full dimensionality.


**3.2.3** **Applications of Attention in our Model**


The Transformer uses multi-head attention in three different ways:


    - In ""encoder-decoder attention"" layers, the queries come from the previous decoder layer,
and the memory keys and values come from the output of the encoder. This allows every
position in the decoder to attend over all positions in the input sequence. This mimics the
typical encoder-decoder attention mechanisms in sequence-to-sequence models such as

[38, 2, 9].


    - The encoder contains self-attention layers. In a self-attention layer all of the keys, values
and queries come from the same place, in this case, the output of the previous layer in the
encoder. Each position in the encoder can attend to all positions in the previous layer of the
encoder.


    - Similarly, self-attention layers in the decoder allow each position in the decoder to attend to
all positions in the decoder up to and including that position. We need to prevent leftward
information flow in the decoder to preserve the auto-regressive property. We implement this
inside of scaled dot-product attention by masking out (setting to _−∞_ ) all values in the input
of the softmax which correspond to illegal connections. See Figure 2.
","The provided context discusses the ""Keep First k Tokens"" method, noting its importance for stability before fine-tuning and its minor impact after fine-tuning. However, it does not describe or mention an ""attention sink"" phenomenon in relation to this method or any other aspect. Therefore, the context does not contain enough information to answer what the ""attention sink"" phenomenon is.",106,3720,3827,,3,,2026-01-02T20:18:22.630033
How many main questions and subquestions are contained within the final CogVision dataset after filtering?,Single-Document Fact Lookup,"[Document 1]
Source: attention_functional_roles_raw_chunks_2k.jsonl
Chunk ID: attention_functional_roles_raw_chunks_2k_1
Content: Furthermore, we validate the functional importance of these heads by showing that their removal
degrades performance on complex tasks and leads to specific error patterns, while their enhancement improves reasoning capabilities. Our findings provide compelling evidence that these attention
heads play a critical role in multimodal reasoning. This insight not only deepens our understanding of the internal organization of VLMs but also suggests potential avenues for designing more
interpretable and cognitive-inspired multimodal AI systems.


2 COGVISION


In this section, we present our dataset Cognitive Vision (CogVision) that contains cognitive process
in multimodal reasoning. CogVision contains 1,409 main questions and 5,744 subquestions. Each


2


example comprises a main question and answer, its subquestions and subanswers and an annotation
specifying the receptive or cognitive function required for each subquestion.


2.1 COGNITIVE FUNCTIONS


To systematically capture the cognitive processes involved in complex reasoning tasks, we consider eight functions related to complex multimodal reasoning. These functions are inspired by
established frameworks in cognitive science (Anderson, 2014; Diamond, 2013), which highlight the
importance of perception, working memory, and reasoning in human cognition.


The functions related to vision include:


    - **Low-level Visual Reception** : Recognizing basic visual features such as color, shape, size,
position, motion.

    - **High-level Visual Reception** : Integrating visual information to recognize objects, patterns,
and scene structure.

    - **Visual Knowledge Recall** : Applying long-term visual knowledge related to visual concepts and their properties.


The functions related to language include:


    - **Language Information Extraction and Understanding** : locating and understanding relevant information from an external source or prior context.

    - **Language Knowledge Recall** : Accessing domain-specific knowledge without visual input.

    - **Math Reasoning** : Performing counting, arithmetic, comparison, and logic-based operations.

    - **Inference** : deriving implicit information that is not directly stated.

    - **Decision-Making** : Selecting the best outcome among alternatives based on reasoning.


This categorization reflects a natural progression from basic information processing to complex cognitive integration. Both the human brain and VLMs encompass a wide range of functional modules.
Our focus in this work is specifically on reasoning-related cognitive functions. By identifying and
organizing these eight core reasoning functions, we can more clearly examine how VLMs handle
different types of thinking steps, in a way that is both systematic and easy to interpret. Detail descriptions of each function and examples in CogVision can be found in Appendix A.1.


2.2 DATA COLLECTIONS


Based on our categorization of reasoning functions, we sampled 2000 diverse questions from existing commonly used visual reasoning benchmarks, selecting 200 examples each from ClevrMath (Lindstr¨om & Abraham, 2022), MathVision (Wang et al., 2024a), MathVista (Lu et al., 2024b),
MMMU (Yue et al., 2024), ScienceQA (Saikh et al., 2022), OKVQA (Marino et al., 2019) (400
examples), VCR (Zellers et al., 2019), VisuLogic (Xu et al., 2025) and Super-Clevr (Li et al.,
2023b) datasets. These datasets span a range of reasoning problems, including logical, mathematical, and commonsense reasoning. Using the chain-of-thought (CoT) paradigm, we prompted
GPT-4.1 (Achiam et al., 2023) to decompose each main question (with its final answer) into subquestions, each targeting a subanswer and a specific function. The prompt encourages structured,
step-by-step reasoning, ensuring that each subquestion is clear, answerable, and sequentially dependent. This process yields a set of subquestion–answer–function (subQAF) triples for each QA pair:
subQAFs = _{_ ( _qi, ai, fi_ ) _}_ _[k]_ _i_ =1 [, where each contains a subquestion] _[ q][i]_ [, its concise answer] _[ a][i]_ [, and the]
corresponding function label _fi_ . The prompt for generating subquestions are list in Appendix A.6.


2.3 DATA FILTERING AND ANNOTATION


Recent advances enable leveraging large pre-trained models for dataset construction, thanks to their
reasoning capabilities and ability to generate high-quality annotations at scale (Wang et al., 2024b).
While our dataset is automatically generated using a VLM to minimize manual effort, we employ a
rigorous two-stage human verification pipeline to ensure data quality and mitigate hallucination. In
the first stage, three expert annotator independently evaluate whether each subquestion is logically
structured and consistent with natural human reasoning. QA pairs with incoherent or inconsistent


3


decompositions are moved out. In the second stage, annotators verify and, if necessary, relabel the
cognitive function associated with each subquestion to ensure accurate alignment with the intended
mental process. Subanswers are further cross-checked with GPT-o3 (OpenAI, 2024) and adjudicated
by humans in cases of disagreement (details in Appendix A.5). This multi-step verification ensures
that each retained subQAF triplet represents a coherent, interpretable reasoning step grounded in
core cognitive functions. After this refinement, our final dataset comprises 1,409 main QA pairs
and 5,744 validated subQAF triplets. The detailed statistics of the dataset including the number of
triplet in training and testing set and distributions of each function can be found in Appendix A.3.
We analyze in-group subquestion diversity, and the results in the Appendix A.2 show that the eight
functions exhibit broad and overlapping distributions in phrasing patterns and token lengths, indicating no systematic surface-form differences.


3 DETECTION OF COGNITIVE FUNCTIONS


Using the CogVision dataset, we adopt a probing-based framework (Alain & Bengio, 2016; Belinkov, 2022; Tenney et al., 2019) to identify which attention heads in VLMs are associated with
specific functions in reasoning process. Specifically, for each functional annotated subquestion, we
extract head activations (see Subsection 3.1), train classifiers and compute accuracies to identify
contributing heads (see Subsection 3.2). Unlike prior work focusing on a single label, our formulation captures many-to-many relationships between heads and cognitive functions, enabling a more
nuanced analysis of functional specialization and overlap within the model.


3.1 HEAD FEATURE EXTRACTION


Given a large VLM _M_, we generate an answer _a_ _[M]_ _i_ for each subquestion _qi_ derived from a main
question _Qi_ . To support coherent multi-step reasoning, we include preceding subquestions and
their answers as contextual input, emulating the incremental reasoning process observed in human
cognition.


During inference, each input token is first mapped into an embedding and then propagated through
the transformer’s layers. At each layer, attention and feedforward operations update the residual
stream, which is ultimately decoded into token predictions. For each generated token _i_, we extract
attention head outputs _Xi_ = _{x_ _[m]_ _l_ _| l ∈_ 1 _, . . ., Nl, m ∈_ 1 _, . . ., Nh}_ across all layers, where _x_ _[m]_ _l_
denotes the value vector from the _m_ -th head in layer _l_ projected into the residual stream, with _Nh_
the number of heads per layer and _Nl_ the total number of layers.


Let _Nt_ denote the number of tokens in the generated answer _a_ _[M]_ _i_ [. To isolate semantically informative]
content relevant to reasoning, we select the top- _k_ most important tokens, determined by prompting
Qwen3-30B LLM (Yang et al., 2025), yielding an index set _Ik_ with _|Ik|_ = _k_ . For each index
_j ∈Ik_, we extract the corresponding attention head activations _Xj_, and compute the averaged
activation feature for the _m_ -th head in layer _l_ as ¯ _x_ _[m]_ _l_ = _k_ [1] � _j ∈Ikxml_ [. This results in a full set of]

---

[Document 2]
Source: attention_functional_roles_raw_chunks_2k.jsonl
Chunk ID: attention_functional_roles_raw_chunks_2k_9
Content: **Answer** B.


**Subquestion** **Answer** **Cognitive Label**



1. What visible features can be
observed in the cell image, such
as the presence of boundaries or
internal structures?


2. What do the observed
features (rectangular shape, defined boundaries, distinct internal
spots) correspond to in cellular
biology?


3. What cellular structures differentiate prokaryotic cells from eukaryotic cells?


4. Based on the cell’s visible
features and the definitions of
prokaryotic and eukaryotic cells,
which type of cell is shown in the
image?


A.3 COGVISION STATISTICS



Rectangular cells with defined boundaries
and distinct internal spots are visible.


Defined boundaries correspond to cell walls;
internal spots correspond to nuclei.


Eukaryotic cells have nuclei and may have
cell walls; prokaryotic cells lack nuclei.



Low-Level Vision Reception


Vision Knowledge
Recall


Language Knowledge
Recall



B: It is a eukaryotic cell. Decision-Making



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-15-0.png)

The statistics for the CogVision dataset is shown in Table 8


16


Table 8: Dataset Statistics


**Metric** **Training** **Testing**


Main Questions 1,124 285
Sub-questions 4,603 1,141


**Cognitive Skills Distribution**


High-Level Vision Reception 1,262 (27.42%) 336 (29.45%)
Math Reasoning 570 (12.38%) 125 (10.96%)
Semantic Understanding 545 (11.84%) 152 (13.32%)
Inference 544 (11.82%) 126 (11.04%)
Decision-Making 454 (9.86%) 111 (9.73%)
Language Knowledge Recall 424 (9.21%) 102 (8.94%)
Vision Knowledge Recall 403 (8.76%) 96 (8.41%)
Low-Level Vision Reception 401 (8.71%) 92 (8.06%)


A.4 THE COGNITIVE FUNCTION DISTRIBUTION OF OTHER MODELS


We present the heatmaps for the remaining five models in this subsection. The results reveal a
notable universality in the sparsity patterns of attention heads across different architectures.


Figure 8: The existence of cognitive heads in Qwen2.5-VL-3B responsible for eight distinct functions in complex reasoning tasks. The x-axis represents the head index, while the y-axis indicates
the layer index.


17



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-16-0.png)
![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-17-0.png)

Figure 9: InternVL3-2B Heatmap


Figure 10: InternVL3-8B Heatmap


18



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-17-1.png)
![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-18-0.png)

Figure 11: gemma-3n-e2b-it Heatmap


Figure 12: gemma-3n-e4b-it Heatmap


A.5 ANNOTATIONS


To ensure the quality and reliability of the decomposed subQAF triplets in the CogVision dataset,
we design a rigorous multi-stage annotation pipeline, combining expert review and model-based
verification. The goal is to verify the logical validity of subquestions, the correctness of their associated cognitive function labels, and the accuracy of the answers. Notably, for each subquestion, we
aim to align it with a single primary cognitive function. However, certain queries—such as “What is
the solvent volume and how many particles in each solution?”—may involve multiple abilities (e.g.,
object recognition and counting). In such cases, we assign the subquestion to its dominant function
in CogVision.


19



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-18-1.png)
**Stage 1: Validating Subquestion Decomposition** In the first stage, we evaluate whether the generated subquestions are logically sound and align with natural human reasoning. For each QA pair,
three expert annotators (with backgrounds in linguistics or cognitive science) independently assess
the validity of each subquestion. A subquestion is marked true if it meaningfully contributes to
answering the main question and follows a logical reasoning trajectory. Otherwise, it is marked
false.


We apply the following filtering criteria:


    - **AI-Human Agreement** : If any annotator considers fewer than 60% of the subquestions
valid, the entire QA decomposition is discarded.

    - **Inter-Annotator Agreement** : A subquestion is deemed invalid if at least two annotators
mark it as false. If over 40% of the subquestions in a QA pair are invalid under this rule,
the whole QA pair is removed.


This filtering ensures that the retained QA decompositions follow coherent, cognitively plausible
reasoning chains.


**Stage 2: Verifying Cognitive Function Labels** In the second stage, annotators evaluate the correctness of the function label _fi_ assigned to each subQAF triplet ( _qi, ai, fi_ ). Three annotators independently mark each label as true or false. When discrepancies occur, annotators collaboratively reassign the correct cognitive label to ensure alignment with the underlying mental operation.


This step ensures that the categorization of subquestions accurately reflects established distinctions
between information retrieval, semantic understanding, logical reasoning, and other cognitive pro
cesses.


**Stage 3: Answer Verification via Model and Human Review** In the final stage, we verify the
correctness of each answer _ai_ using both automated and manual procedures. We employ the GPTo3 model (OpenAI, 2024), known for its logical reasoning capabilities, to re-evaluate GPT-4.1generated answers, and approximately 38.78% were found to be in disagreement. If GPT-o3 disagrees with GPT-4.1, it provides an alternative answer. A human annotator then compares both
answers and resolves discrepancies by supplying the correct one when necessary. Given the generally objective nature of answers, only one annotator is required for this task.


**Annotation Outcome** Following this multi-stage process, we retain 1,409 validated QA pairs,
yielding a total of 5,744 high-quality subQAF triplets.


A.6 PROMPT FOR GENERATING COGVISION


We decompose the main question into subquestions through a two-step process: first, we prompt
GPT-4.1 to generate a chain-of-thought (CoT) for the main question; second, we use the main question together with the CoT to guide the model in generating subquestions.


Here is the prompt for generating subquestions:


20



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-19-0.png)
![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-20-0.png)



21


A.7 PROMPT FOR QUESTION ASKING



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-21-0.png)

A.8 ABLATION STUDY


In the main experiments, we use the top-k generated tokens and average their multi-head attention
vectors. Here are some examples of selected top-k tokens by llm:



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-21-1.png)

---

[Document 3]
Source: attention_functional_roles_raw_chunks_2k.jsonl
Chunk ID: attention_functional_roles_raw_chunks_2k_8
Content: An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu,
Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. _arXiv preprint_
_arXiv:2505.09388_, 2025.


Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens,
Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In _Proceedings of the IEEE/CVF_
_Conference on Computer Vision and Pattern Recognition_, pp. 9556–9567, 2024.


Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition: Visual
commonsense reasoning. In _Proceedings of the IEEE Conference on Computer Vision and Pattern_
_Recognition_, pp. 6720–6731, 2019.


Z Zheng, Y Wang, Y Huang, S Song, M Yang, B Tang, F Xiong, and Z Li. Attention heads of large
language models: A survey. arxiv 2024. _arXiv preprint arXiv:2409.03752_ .


Zhenhong Zhou, Haiyang Yu, Xinghua Zhang, Rongwu Xu, Fei Huang, Kun Wang, Yang Liu,
Junfeng Fang, and Yongbin Li. On the role of attention heads in large language model safety.
_arXiv preprint arXiv:2410.13708_, 2024.


Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. _arXiv preprint_
_arXiv:2304.10592_, 2023.


Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen
Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for
open-source multimodal models. _arXiv preprint arXiv:2504.10479_, 2025.


A APPENDIX


A.1 COGVISION FUNCTION DETAILS AND EXAMPLES


**Language Information Extraction and Understanding:** The ability to comprehend and extract
meaning from only language, including understanding word relationships, sentence structures, context, and intent within a given textual input.


13


**Low-Level Vision Reception:** The perception and interpretation of visual content, including recognizing low-level visual features such as number, color, shape, size and position.


**High-Level Vision Reception:** The perception and interpretation of visual content, including recognizing high-level visual features such as object recognition, the relationships, motion, spatial
arrangement, and scene-level understanding.


**Vision Knowledge Recall:** The access and application of long-term visual knowledge, such as
recognizing familiar objects, understanding occlusion, symmetry, physical structure, and part-whole
relationships (e.g., ”a cat has a tail”).


**Language Knowledge Recall:** The access and application of long-term domain-specific textual
knowledge, such as factual knowledge from science, history, or everyday concepts stored in memory.


**Math Reasoning:** The application of mathematical concepts and operations such as counting, comparison, arithmetic, and pattern-based quantitative reasoning.


**Inference:** The logical derivation of conclusions from given information, including deductive (guaranteed) reasoning and abductive (plausible) reasoning.


**Decision-Making:** The process of selecting the most appropriate option or answer based on prior
reasoning, evaluation of evidence, or predefined objectives.


Table 6 and Table 7 presents illustrative examples from the CogVision dataset. The main question
and its corresponding answer are taken from the original dataset. Based on an analysis of the main
question, a sequence of sub-questions, their answers, and associated cognitive function labels are
generated in order.


A.2 ANALYSES OF SURFACE-FORM VARIATION ACROSS COGNITIVE-FUNCTION GROUPS


We analyze the surface-form variation across cognitive-function groups. As shown in Figures 6
and 7, the eight functions exhibit wide and overlapping distributions in phrasing patterns and token
lengths, indicating no systematic surface-form differences. These results support that the cognitive
groups are not determined by trivial lexical or structural artifacts. About modality, as described
in Section 2.1, some functions (Low-level Visual Reception, High-level Visual Reception, Visual
Knowledge Recall) naturally involve vision, while others relate primarily to language. This modality
tendency is intrinsic to the underlying cognitive processes rather than an artifact of the pipeline.


Figure 6: Histogram of token length distribution for 8 functions.


14



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-13-0.png)
![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-14-0.png)

Figure 7: Word cloud distribution for 8 functions.


Table 6: One example from the CogVision dataset showing a main question, its final answer, and a
breakdown into subquestions with answers and their corresponding cognitive function labels.


**Example 1:**


**Main Question** In a case-control study, the results were shown in the table below. The OR was:
Choose one option from the following: A: 18 B: 16 C: 20D: 10


**Answer** B


**Subquestion** **Answer** **Cognitive Label**



1. What are the values in the 2x2

table for cases and controls with
and without a history of exposure
in the image?


2. What is the standard formula
to calculate the odds ratio (OR)
in a case-control 2x2 table?


3. What is the odds ratio (OR)
when you substitute the identified values into the formula?


4. Which option corresponds to
the calculated odds ratio?



Cases with exposure: 400, Cases without exposure: 100, Controls with exposure: 100,
Controls without exposure: 400


OR = (a*d) / (b*c), where a = cases with exposure, b = controls with exposure, c = cases
without exposure, d = controls without expo
sure


(400*400)/(100*100) = 160,000/10,000 =
16



B: 16 Decision-Making


15



High-Level Vision
Reception


Language Knowledge
Recall


Math Reasoning



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-14-1.png)
Table 7: One example from the CogQA dataset showing a main question, its final answer, and a
breakdown into subquestions with answers and their corresponding cognitive function labels.


**Example 2:**


**Main Question** How can you tell that this is a prokaryote or eukaryote cell? Choose one option
from the following: A: It is a prokaryote because it doesn’t have a nucleus B: It is a
eukaryotic cell because it has a cell wall and a nucleus C: It is eukaryotic because
it does not a nucleus nor a cell membrane D: It is prokaryote because it has a cell
wall


**Answer** B.


**Subquestion** **Answer** **Cognitive Label**



1. What visible features can be
observed in the cell image, such
as the presence of boundaries or
internal structures?


2. What do the observed
features (rectangular shape, defined boundaries, distinct internal
spots) correspond to in cellular
biology?


3. What cellular structures differentiate prokaryotic cells from eukaryotic cells?


4. Based on the cell’s visible
features and the definitions of
prokaryotic and eukaryotic cells,
which type of cell is shown in the
image?


A.3 COGVISION STATISTICS



Rectangular cells with defined boundaries
and distinct internal spots are visible.


Defined boundaries correspond to cell walls;
internal spots correspond to nuclei.
","The final CogVision dataset, after filtering, contains 1,409 main questions and 5,744 subquestions.",120,3070,3191,,3,,2026-01-02T20:18:25.823466
What are the two sub-layers present in each layer of the Transformer's encoder stack?,Single-Document Fact Lookup,"[Document 1]
Source: attention_is_all_you_need_raw_chunks_2k.jsonl
Chunk ID: attention_is_all_you_need_raw_chunks_2k_2
Content: [38, 2, 9].


    - The encoder contains self-attention layers. In a self-attention layer all of the keys, values
and queries come from the same place, in this case, the output of the previous layer in the
encoder. Each position in the encoder can attend to all positions in the previous layer of the
encoder.


    - Similarly, self-attention layers in the decoder allow each position in the decoder to attend to
all positions in the decoder up to and including that position. We need to prevent leftward
information flow in the decoder to preserve the auto-regressive property. We implement this
inside of scaled dot-product attention by masking out (setting to _−∞_ ) all values in the input
of the softmax which correspond to illegal connections. See Figure 2.


**3.3** **Position-wise Feed-Forward Networks**


In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully
connected feed-forward network, which is applied to each position separately and identically. This
consists of two linear transformations with a ReLU activation in between.


FFN( _x_ ) = max(0 _, xW_ 1 + _b_ 1) _W_ 2 + _b_ 2 (2)


While the linear transformations are the same across different positions, they use different parameters
from layer to layer. Another way of describing this is as two convolutions with kernel size 1.
The dimensionality of input and output is _d_ model = 512, and the inner-layer has dimensionality
_dff_ = 2048.


**3.4** **Embeddings and Softmax**


Similarly to other sequence transduction models, we use learned embeddings to convert the input
tokens and output tokens to vectors of dimension _d_ model. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In
our model, we share the same weight matrix between the two embedding layers and the pre-softmax
linear transformation, similar to [30]. In the embedding layers, we multiply those weights by _[√]_ _d_ model.


5


Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations
for different layer types. _n_ is the sequence length, _d_ is the representation dimension, _k_ is the kernel
size of convolutions and _r_ the size of the neighborhood in restricted self-attention.


Layer Type Complexity per Layer Sequential Maximum Path Length
Operations
Self-Attention _O_ ( _n_ [2] _· d_ ) _O_ (1) _O_ (1)
Recurrent _O_ ( _n · d_ [2] ) _O_ ( _n_ ) _O_ ( _n_ )
Convolutional _O_ ( _k · n · d_ [2] ) _O_ (1) _O_ ( _logk_ ( _n_ ))
Self-Attention (restricted) _O_ ( _r · n · d_ ) _O_ (1) _O_ ( _n/r_ )


**3.5** **Positional Encoding**


Since our model contains no recurrence and no convolution, in order for the model to make use of the
order of the sequence, we must inject some information about the relative or absolute position of the
tokens in the sequence. To this end, we add ""positional encodings"" to the input embeddings at the
bottoms of the encoder and decoder stacks. The positional encodings have the same dimension _d_ model
as the embeddings, so that the two can be summed. There are many choices of positional encodings,
learned and fixed [9].


In this work, we use sine and cosine functions of different frequencies:


_PE_ ( _pos,_ 2 _i_ ) = _sin_ ( _pos/_ 10000 [2] _[i/d]_ [model] )

_PE_ ( _pos,_ 2 _i_ +1) = _cos_ ( _pos/_ 10000 [2] _[i/d]_ [model] )


where _pos_ is the position and _i_ is the dimension. That is, each dimension of the positional encoding
corresponds to a sinusoid. The wavelengths form a geometric progression from 2 _π_ to 10000 _·_ 2 _π_ . We
chose this function because we hypothesized it would allow the model to easily learn to attend by
relative positions, since for any fixed offset _k_, _PEpos_ + _k_ can be represented as a linear function of
_PEpos_ .


We also experimented with using learned positional embeddings [9] instead, and found that the two
versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version
because it may allow the model to extrapolate to sequence lengths longer than the ones encountered
during training.


**4** **Why Self-Attention**


In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations
( _x_ 1 _, ..., xn_ ) to another sequence of equal length ( _z_ 1 _, ..., zn_ ), with _xi, zi ∈_ R _[d]_, such as a hidden
layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we
consider three desiderata.


One is the total computational complexity per layer. Another is the amount of computation that can
be parallelized, as measured by the minimum number of sequential operations required.


The third is the path length between long-range dependencies in the network. Learning long-range
dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the
ability to learn such dependencies is the length of the paths forward and backward signals have to
traverse in the network. The shorter these paths between any combination of positions in the input
and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare
the maximum path length between any two input and output positions in networks composed of the
different layer types.


As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially
executed operations, whereas a recurrent layer requires _O_ ( _n_ ) sequential operations. In terms of
computational complexity, self-attention layers are faster than recurrent layers when the sequence


6


length _n_ is smaller than the representation dimensionality _d_, which is most often the case with
sentence representations used by state-of-the-art models in machine translations, such as word-piece

[38] and byte-pair [31] representations. To improve computational performance for tasks involving
very long sequences, self-attention could be restricted to considering only a neighborhood of size _r_ in
the input sequence centered around the respective output position. This would increase the maximum
path length to _O_ ( _n/r_ ). We plan to investigate this approach further in future work.


A single convolutional layer with kernel width _k < n_ does not connect all pairs of input and output
positions. Doing so requires a stack of _O_ ( _n/k_ ) convolutional layers in the case of contiguous kernels,
or _O_ ( _logk_ ( _n_ )) in the case of dilated convolutions [18], increasing the length of the longest paths
between any two positions in the network. Convolutional layers are generally more expensive than
recurrent layers, by a factor of _k_ . Separable convolutions [6], however, decrease the complexity
considerably, to _O_ ( _k · n · d_ + _n · d_ [2] ). Even with _k_ = _n_, however, the complexity of a separable
convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,
the approach we take in our model.


As side benefit, self-attention could yield more interpretable models. We inspect attention distributions
from our models and present and discuss examples in the appendix. Not only do individual attention
heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic
and semantic structure of the sentences.


**5** **Training**


This section describes the training regime for our models.


**5.1** **Training Data and Batching**

---

[Document 2]
Source: attention_is_all_you_need_raw_chunks_2k.jsonl
Chunk ID: attention_is_all_you_need_raw_chunks_2k_1
Content: [10], consuming the previously generated symbols as additional input when generating the next.


2


![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_is_all_you_need.pdf-2-0.png)

Figure 1: The Transformer - model architecture.


The Transformer follows this overall architecture using stacked self-attention and point-wise, fully
connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,
respectively.


**3.1** **Encoder and Decoder Stacks**


**Encoder:** The encoder is composed of a stack of _N_ = 6 identical layers. Each layer has two
sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network. We employ a residual connection [11] around each of
the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is
LayerNorm( _x_ + Sublayer( _x_ )), where Sublayer( _x_ ) is the function implemented by the sub-layer
itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding
layers, produce outputs of dimension _d_ model = 512.


**Decoder:** The decoder is also composed of a stack of _N_ = 6 identical layers. In addition to the two
sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head
attention over the output of the encoder stack. Similar to the encoder, we employ residual connections
around each of the sub-layers, followed by layer normalization. We also modify the self-attention
sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This
masking, combined with fact that the output embeddings are offset by one position, ensures that the
predictions for position _i_ can depend only on the known outputs at positions less than _i_ .


**3.2** **Attention**


An attention function can be described as mapping a query and a set of key-value pairs to an output,
where the query, keys, values, and output are all vectors. The output is computed as a weighted sum


3


Scaled Dot-Product Attention Multi-Head Attention


Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several
attention layers running in parallel.


of the values, where the weight assigned to each value is computed by a compatibility function of the
query with the corresponding key.


**3.2.1** **Scaled Dot-Product Attention**


We call our particular attention ""Scaled Dot-Product Attention"" (Figure 2). The input consists of
queries and keys of dimension _dk_, and values of dimension _dv_ . We compute the dot products of the
query with all keys, divide each by _[√]_ _dk_, and apply a softmax function to obtain the weights on the
values.


In practice, we compute the attention function on a set of queries simultaneously, packed together
into a matrix _Q_ . The keys and values are also packed together into matrices _K_ and _V_ . We compute
the matrix of outputs as:


Attention( _Q, K, V_ ) = softmax( _[Q][K]_ _[T]_ ) _V_ (1)

~~_√_~~ _dk_


The two most commonly used attention functions are additive attention [2], and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor
of ~~_√_~~ 1 _dk_ . Additive attention computes the compatibility function using a feed-forward network with
a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is
much faster and more space-efficient in practice, since it can be implemented using highly optimized
matrix multiplication code.


While for small values of _dk_ the two mechanisms perform similarly, additive attention outperforms
dot product attention without scaling for larger values of _dk_ [3]. We suspect that for large values of
_dk_, the dot products grow large in magnitude, pushing the softmax function into regions where it has
extremely small gradients [4] . To counteract this effect, we scale the dot products by ~~_√_~~ 1 _dk_ .


**3.2.2** **Multi-Head Attention**


Instead of performing a single attention function with _d_ model-dimensional keys, values and queries,
we found it beneficial to linearly project the queries, keys and values _h_ times with different, learned
linear projections to _dk_, _dk_ and _dv_ dimensions, respectively. On each of these projected versions of
queries, keys and values we then perform the attention function in parallel, yielding _dv_ -dimensional


4To illustrate why the dot products get large, assume that the components of _q_ and _k_ are independent random
variables with mean 0 and variance 1. Then their dot product, _q · k_ = [�] _[d]_ _i_ =1 _[k]_ _[q][i][k][i]_ [, has mean][ 0][ and variance] _[ d][k]_ [.]


4



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_is_all_you_need.pdf-3-0.png)

![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_is_all_you_need.pdf-3-1.png)
output values. These are concatenated and once again projected, resulting in the final values, as
depicted in Figure 2.


Multi-head attention allows the model to jointly attend to information from different representation
subspaces at different positions. With a single attention head, averaging inhibits this.


MultiHead( _Q, K, V_ ) = Concat(head1 _, ...,_ headh) _W_ _[O]_

where headi = Attention( _QWi_ _[Q][, KW][ K]_ _i_ _[, V W][ V]_ _i_ [)]


Where the projections are parameter matrices _Wi_ _[Q]_ _∈_ R _[d]_ [model] _[×][d][k]_, _Wi_ _[K]_ _∈_ R _[d]_ [model] _[×][d][k]_, _Wi_ _[V]_ _∈_ R _[d]_ [model] _[×][d][v]_
and _W_ _[O]_ _∈_ R _[hd][v][×][d]_ [model] .


In this work we employ _h_ = 8 parallel attention layers, or heads. For each of these we use
_dk_ = _dv_ = _d_ model _/h_ = 64. Due to the reduced dimension of each head, the total computational cost
is similar to that of single-head attention with full dimensionality.


**3.2.3** **Applications of Attention in our Model**


The Transformer uses multi-head attention in three different ways:


    - In ""encoder-decoder attention"" layers, the queries come from the previous decoder layer,
and the memory keys and values come from the output of the encoder. This allows every
position in the decoder to attend over all positions in the input sequence. This mimics the
typical encoder-decoder attention mechanisms in sequence-to-sequence models such as

[38, 2, 9].


    - The encoder contains self-attention layers. In a self-attention layer all of the keys, values
and queries come from the same place, in this case, the output of the previous layer in the
encoder. Each position in the encoder can attend to all positions in the previous layer of the
encoder.


    - Similarly, self-attention layers in the decoder allow each position in the decoder to attend to
all positions in the decoder up to and including that position. We need to prevent leftward
information flow in the decoder to preserve the auto-regressive property. We implement this
inside of scaled dot-product attention by masking out (setting to _−∞_ ) all values in the input
of the softmax which correspond to illegal connections. See Figure 2.

---

[Document 3]
Source: attention_is_all_you_need_raw_chunks_2k.jsonl
Chunk ID: attention_is_all_you_need_raw_chunks_2k_0
Content: Provided proper attribution is provided, Google hereby grants permission to
reproduce the tables and figures in this paper solely for use in journalistic or
scholarly works.

## **Attention Is All You Need**



**Niki Parmar** _[∗]_
Google Research
```
nikip@google.com

```


**Ashish Vaswani** _[∗]_
Google Brain
```
avaswani@google.com

```

**Llion Jones** _[∗]_
Google Research
```
 llion@google.com

```


**Noam Shazeer** _[∗]_
Google Brain
```
noam@google.com

```


**Jakob Uszkoreit** _[∗]_
Google Research
```
usz@google.com

```


**Aidan N. Gomez** _[∗†]_
University of Toronto
```
aidan@cs.toronto.edu

```


**Łukasz Kaiser** _[∗]_
Google Brain
```
lukaszkaiser@google.com

```


**Illia Polosukhin** _[∗‡]_

```
             illia.polosukhin@gmail.com

```

**Abstract**


The dominant sequence transduction models are based on complex recurrent or
convolutional neural networks that include an encoder and a decoder. The best
performing models also connect the encoder and decoder through an attention
mechanism. We propose a new simple network architecture, the Transformer,
based solely on attention mechanisms, dispensing with recurrence and convolutions
entirely. Experiments on two machine translation tasks show these models to
be superior in quality while being more parallelizable and requiring significantly
less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including
ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,
our model establishes a new single-model state-of-the-art BLEU score of 41.8 after
training for 3.5 days on eight GPUs, a small fraction of the training costs of the
best models from the literature. We show that the Transformer generalizes well to
other tasks by applying it successfully to English constituency parsing both with
large and limited training data.


_∗_ Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started
the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and
has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head
attention and the parameter-free position representation and became the other person involved in nearly every
detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and
tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and
efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and
implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating
our research.

_†_ Work performed while at Google Brain.

_‡_ Work performed while at Google Research.


31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.


**1** **Introduction**


Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks
in particular, have been firmly established as state of the art approaches in sequence modeling and
transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous
efforts have since continued to push the boundaries of recurrent language models and encoder-decoder
architectures [38, 24, 15].


Recurrent models typically factor computation along the symbol positions of the input and output
sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden
states _ht_, as a function of the previous hidden state _ht−_ 1 and the input for position _t_ . This inherently
sequential nature precludes parallelization within training examples, which becomes critical at longer
sequence lengths, as memory constraints limit batching across examples. Recent work has achieved
significant improvements in computational efficiency through factorization tricks [21] and conditional
computation [32], while also improving model performance in case of the latter. The fundamental
constraint of sequential computation, however, remains.


Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in
the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms
are used in conjunction with a recurrent network.


In this work we propose the Transformer, a model architecture eschewing recurrence and instead
relying entirely on an attention mechanism to draw global dependencies between input and output.
The Transformer allows for significantly more parallelization and can reach a new state of the art in
translation quality after being trained for as little as twelve hours on eight P100 GPUs.


**2** **Background**


The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU

[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building
block, computing hidden representations in parallel for all input and output positions. In these models,
the number of operations required to relate signals from two arbitrary input or output positions grows
in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes
it more difficult to learn dependencies between distant positions [12]. In the Transformer this is
reduced to a constant number of operations, albeit at the cost of reduced effective resolution due
to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as
described in section 3.2.


Self-attention, sometimes called intra-attention is an attention mechanism relating different positions
of a single sequence in order to compute a representation of the sequence. Self-attention has been
used successfully in a variety of tasks including reading comprehension, abstractive summarization,
textual entailment and learning task-independent sentence representations [4, 27, 28, 22].


End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering and
language modeling tasks [34].


To the best of our knowledge, however, the Transformer is the first transduction model relying
entirely on self-attention to compute representations of its input and output without using sequencealigned RNNs or convolution. In the following sections, we will describe the Transformer, motivate
self-attention and discuss its advantages over models such as [17, 18] and [9].


**3** **Model Architecture**


Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].
Here, the encoder maps an input sequence of symbol representations ( _x_ 1 _, ..., xn_ ) to a sequence
of continuous representations **z** = ( _z_ 1 _, ..., zn_ ). Given **z**, the decoder then generates an output
sequence ( _y_ 1 _, ..., ym_ ) of symbols one element at a time. At each step the model is auto-regressive

[10], consuming the previously generated symbols as additional input when generating the next.


2


![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_is_all_you_need.pdf-2-0.png)

Figure 1: The Transformer - model architecture.


The Transformer follows this overall architecture using stacked self-attention and point-wise, fully
connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,
respectively.


**3.1** **Encoder and Decoder Stacks**
","Each layer in the Transformer's encoder stack has two sub-layers:
1.  A multi-head self-attention mechanism.
2.  A simple, position-wise fully connected feed-forward network.",104,2153,2258,,3,,2026-01-02T20:18:28.082786
Which specific metric was used to evaluate the Remaining Useful Life (RUL) estimation in the NAC experiments?,Single-Document Fact Lookup,"[Document 1]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_6
Content: MHA 95.94 _[±]_ [0.15] 88.36 _[±]_ [1.06] 79.99 _[±]_ [0.49] 0.0185 _[±]_ [0.0017] 45.36 _[±]_ [5.16] 37.31 _[±]_ [12.20] 41.40 _[±]_ [7.72]

mTAN 95.97 _[±]_ [0.25] 88.08 _[±]_ [0.94] 80.86 _[±]_ [0.22] 0.0178 _[±]_ [0.0005] 44.41 _[±]_ [7.15] 41.34 _[±]_ [3.72] 66.29 _[±]_ [4.25]

CTA 95.86 _[±]_ [0.14] 88.10 _[±]_ [1.10] 80.54 _[±]_ [0.40] 0.0197 _[±]_ [0.0016] 39.16 _[±]_ [3.54] **25.86** _[±]_ [1.47] 38.41 _[±]_ [4.51]

ODEFormer 95.62 _[±]_ [0.20] 88.25 _[±]_ [0.66] 80.54 _[±]_ [0.40] 0.0190 _[±]_ [0.0012] 42.42 _[±]_ [6.98] 35.63 _[±]_ [9.24] 40.60 _[±]_ [6.83]

ContiFormer **96.04** _[±]_ **[0.23]** 81.28 _[±]_ [0.85] 80.47 _[±]_ [0.50] **0.0174** _[±]_ **[0.01]** **27.82** _[±]_ **[7.09]** 34.71 _[±]_ [4.98] 43.81 _[±]_ [10.18]


NAC-2k 95.73 _[±]_ [0.07] 88.84% _[±]_ [0.81] 80.59 _[±]_ [0.46] 0.0208 _[±]_ [0.0015] 43.78 _[±]_ [2.71] 37.43 _[±]_ [9.28] 40.51 _[±]_ [6.61]

NAC-32k 95.15 _[±]_ [0.11] 88.80 _[±]_ [0.76] 80.38 _[±]_ [0.16] **0.0170** _[±]_ **[0.0007]** 49.53 _[±]_ [4.89] 32.45 _[±]_ [10.84] 39.17 _[±]_ [12.23]

NAC-PW **96.64** _[±]_ [0.12] **89.15** _[±]_ **[1.01]** **80.72** _[±]_ **[0.41]** 0.0177 _[±]_ [0.0008] **37.50** _[±]_ **[2.56]** 28.01 _[±]_ [4.93] **30.14** _[±]_ **[6.87]**


NAC-FC 95.31 _[±]_ [0.07] 88.45 _[±]_ [0.91] 80.49 _[±]_ [0.46] 0.0192 _[±]_ [0.0012] 40.36 _[±]_ [6.09] **24.89** _[±]_ **[5.30]** **35.35** _[±]_ **[6.64]**

NAC-02s 95.31 _[±]_ [0.07] 88.84 _[±]_ [1.33] 80.47 _[±]_ [0.27] 0.0188 _[±]_ [0.0013] 39.43 _[±]_ [5.94] 35.59 _[±]_ [3.86] 38.90 _[±]_ [6.43]

NAC-09s 95.86 _[±]_ [0.11] 88.61% _[±]_ [1.25] 80.43% _[±]_ [0.17] 0.0188 _[±]_ [0.0013] 47.29 _[±]_ [5.52] 40.40 _[±]_ [8.85] 44.39 _[±]_ [6.82]


NAC-Exact/05s/8k **96.12** _[±]_ [0.11] **89.01** _[±]_ **[1.01]** 80.59 _[±]_ [1.82] **0.0173** _[±]_ **[0.0006]** **37.75** _[±]_ **[4.72]** **19.87** _[±]_ **[1.75]** **27.82** _[±]_ **[7.09]**

NAC-Euler 95.67 _[±]_ [0.26] 88.52 _[±]_ [0.68] **80.61** _[±]_ [0.28] 0.0181 _[±]_ [0.0017] 42.08 _[±]_ [6.14] 28.46 _[±]_ [8.18] 39.32 _[±]_ [9.15]

NAC-Steady 95.75 _[±]_ [0.28] 88.36 _[±]_ [1.05] **80.62** _[±]_ **[0.26]** 0.0181 _[±]_ [0.0012] 40.95 _[±]_ [5.77] 26.76 _[±]_ [7.36] 37.12 _[±]_ [12.43]


**Note:** (↑) higher is better; (↓) lower is better.


the lowest score of 27.82. NAC-Exact/05s/8k and NAC-PW

achieved nearly identical scores, obtaining 37.75 and 37.50
on average, respectively. On the XJTU-SY dataset, NACExact/05s/8k has the lowest score of 19.87. NAC-FC ranked

second with a score of 24.89, followed by NAC-PW in third
place with an average score of 28.01. A similar trend was
observed on the HUST dataset, where NAC-Exact/05s/8k
achieved first place with a score of 27.82, NAC-PW ranked
second with 30.14, and NAC-FC ranked third with 35.35.
These results demonstrated the strong cross-validation capability of NAC.



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/neuronal_attention_circuits.pdf-6-0.png)

_Figure 3._ Degradation Estimation Results.


compact architecture suitable for resource-constrained devices, thereby supporting localized safety.
We utilized three benchmark datasets: (i) PRONOSTIA
(Nectoux et al., 2012), (ii) XJTU-SY (Wang et al., 2018),
and (iii) HUST (Thuan & Hong, 2023). Training is performed on PRONOSTIA, while XJTU-SY and HUST are

used to assess cross-validation. We used the Score met
ric (Nectoux et al., 2012) to assess the performance. We
evaluate generalization using _Bearing 1_ from the first operating condition of each dataset. Figure 3 visualizes the
expected degradation alongside the outputs of NAC. On the
PRONOSTIA dataset, ContiFormer performed the best with



**3.4. Run Time and Memory Experiments**

---

[Document 2]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_16
Content: _Condition 1_                - 0 W                -                - 1 _∼_ 5

**HUST** _Condition 2_       - 200 W       -       - 1 _∼_ 5

_Condition 3_                - 400 W                -                - 1 _∼_ 5


**Note:** The PRONOSTIA dataset is utilized for training and generalization testing, while the XJTU-SY and HUST datasets are employed
to evaluate cross-validation testing. (-) values are either not available or not utilized.



across three different operating settings: 1800 rpm with 4
kN radial load, 1650 rpm with a 4.2 kN load, and 1500 rpm
with a 5 kN load, all at a frequency of 100 Hz. Vibration
data were recorded using accelerometers placed along the
horizontal and vertical axes, which were sampled at 25.6
kHz. Additionally, temperature readings were collected at a
sampling rate of 10 Hz. The data distributions for training
and testing are provided in Table 5.
_XJTU-SY Dataset_ is another widely recognized benchmark
dataset developed through collaboration between Xi’an
Jiaotong University and Changxing Sumyoung Technology
(Wang et al., 2018). The dataset comprises 15 complete runto-failure experiments performed under accelerated degradation conditions with three distinct operational settings: 1200
rpm (35 Hz) with a 12 kN radial load, 2250 rpm (37.5 Hz)
with an 11 kN radial load, and 2400 rpm (40 Hz) with a 10
kN radial load. Vibrational signals were recorded using an
accelerometer mounted on the horizontal and vertical axes

and sampled at 25.6 kHz. This dataset is only used for the



cross-validation test.

_HUST Dataset_ is a practical dataset developed by Hanoi
University of Science and Technology to support research
on ball bearing fault diagnosis (Hong & Thuan, 2023). The
dataset includes vibration data collected from five bearing
types (6204, 6205, 6206, 6207, and 6208) under three different load conditions: 0 W, 200 W, and 400 W. Six fault
categories were introduced, consisting of single faults (inner race, outer race, and ball) and compound faults (inner–outer, inner–ball, and outer–ball). Faults were created
as early-stage defects in the form of 0.2 mm micro-cracks,
simulating real degradation scenarios. The vibration signals
were sampled at 51.2 kHz with approximately 10-second
recordings for each case. This dataset is only used for the
cross-validation test.

**Preprocess:** Condition monitoring data comprises 1D nonstationary vibrational signals collected from multiple sensors. To extract meaningful information, these signals must
be transformed into features that possess meaningful physi


17


**Neuronal Attention Circuit (NAC) for Representation Learning**


cal interpretability. We utilized the preprocessing proposed
in (Razzaq & Zhao, 2025a) and labels are generated according to (Razzaq & Zhao, 2025b). Initially, the signal
is segmented into small, rectangularized vectors using a
windowing technique ( _w_ ), enabling better localization of
transient characteristics. The continuous wavelet transform

(CWT) (Aguiar-Conraria & Soares, 2014) with the Morlet
wavelet (Lin & Qu, 2000) as the mother wavelet is then applied to obtain a time-frequency representation (TFR). The
CWT is defined as Γ( _a, b_ ) = � _−∞∞_ _[x][w]_ [(] _[t]_ [)] ~~_√_~~ [1] ~~_a_~~ _ψ_ _[∗]_ [�] _[t][−]_ _a_ _[b]_ � _dt_,

where _a_ and _b_ denote the scale and translation parameters,
respectively, and _ψ_ is the Morlet wavelet function. From
the resulting TFR, a compact set of statistical and domainspecific features is extracted to characterize the operational
condition of the bearing. The complete feature extraction
procedure is described in Algorithm 3.
**Neural Network Architecture:** The objective of this problem is to design a compact neural network that can effectively model degradation dynamics while remaining feasible
for deployment on resource-constrained devices, enabling
localized and personalized prognostics for individual machines. To achieve this, we combine a compact convolutional network with NAC. The CNN component extracts
spatial degradation features from the training data, while
NAC performs temporal filtering to emphasize informative
features. This architecture maintains a small model size

without sacrificing representational capacity. Full hyperparameter configurations are reported in Table 4.
**Evaluation Metric:** Score is a metric specifically designed
for RUL estimation in the IEEE PHM (Nectoux et al., 2012)
to score the estimates. The scoring function is asymmetric and penalizes overestimations more heavily than early
predictions. This reflects practical considerations, as late
maintenance prediction can lead to unexpected failures with
more severe consequences than early intervention can.



_yi_ ˆ _−yi_

10 _−_ 1
�


(48)



_Score_ = �

_i_ :ˆ _yi<yi_



_e_ _[−]_ _[yi]_ [ˆ] 13 _[−][yi]_ _−_ 1 +
� � �

_i_ :ˆ _yi≥yi_



_e_
�



18

---

[Document 3]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_4
Content: **2.3. NAC as Universal Approximator**


We now establish the universal approximation capability of
NAC by extending the classical Universal Approximation
Theorem (UAT) (Nishijima, 2021) to the proposed mechanism. For brevity, we consider a network with a single
NAC layer processing fixed-dimensional inputs, though the
argument generalizes to sequences.


**Theorem 2** (Universal Approximation by NAC) **.** _Let K ⊂_
R _[n]_ _be a compact set and f_ : _K →_ R _[m]_ _be a continuous_
_function. For any ϵ >_ 0 _, there exists a neural network_
_consisting of a single NAC layer, with sufficiently large_
_model dimension dmodel, number of heads H, sparsity s,_
_and nonlinear activations, such that the network’s output_
_g_ : R _[n]_ _→_ R _[m]_ _satisfies_


sup _∥f_ ( _x_ ) _−_ _g_ ( _x_ ) _∥_ _< ϵ._ (20)
_x∈K_


_The proof is provided in Appendix B.3._


**3. Evaluation**


We evaluate the proposed architecture against a range of
baselines, including (DT & CT) RNN, (DT & CT) attention,
and multiple NAC ablation configurations. Experiments
are conducted across diverse domains, including irregular
time-series modeling, lane keeping of autonomous vehicles,
and Industry 4.0 prognostics. All results are obtained using 5-fold cross-validation, where models are trained using
BPTT (see Appendix C.2) on each fold and evaluated across
all folds. We report the mean ( _µ_ ) and standard deviation ( _σ_ )
to capture variability and quantify uncertainty in the predictions. Table 1 provides results for all experiments, and the
details of the baselines, ablation, environment utilized, the
data curation and preprocessing, and neural network architectures for all experiments are provided in the Appendix
D.3.


**3.1. Irregular Time-series**


We evaluate the proposed architecture on two irregular timeseries datasets: (i) Event-based MNIST; and (ii) Person
Activity Recognition (PAR).



For finite _t_, the exponential factor (1 _−_ _e_ _[−][ω][τ][ t]_ ) regulates the
buildup of attention, giving _ωτ_ a temporal gating role. Normalizing across all keys via _softmax_ yields attention weights
_αt_ = softmax( _at_ ), defining a valid probability distribution
where _ϕ_ amplifies or suppresses content alignments, and _ωτ_
shapes both the speed and saturation of these preferences.
As _t →∞_, the trajectory converges to the steady state


_a_ _[∗]_ _t_ [=] _[ϕ]_ _≈_ _[q][⊤][k]_ _,_ (18)

_ωτ_ ~~_√_~~ _dk_


which is analogous to scaled-dot attention under specific
parameterization when the backbone _NN_ backbone is configured as a linear projection such that _ϕ_ ( **u** ) = _q_ _[⊤]_ _k_ and
_ωτ_ ( _u_ ) = _[√]_ _dk_ (e.g., by setting NCP weights to emulate
bilinear forms and disabling nonlinearities). In general, the
nonlinear backbone allows for more expressive similarities,
with the approximation holding when trained to mimic dot
products.
**Attention output:** Finally, the attention output is computed
by integrating the attention weights with the value matrix:


NAC( _q, k, v_ ) = _αtvtdt_ (19)
� _T_


In practice, the integration is approximated using a Riemannstyle approach, where the weighted elements are computed
by multiplying each _vt_ with its corresponding _αt_ . These are
then summed and multiplied by a fixed pseudo-time step
_δt_, chosen as a scalar (typically between 0.5–1.0) hyperparameter during layer initialization. This yields a continuous
analogue of standard weighted sums, giving finer resolution
of the attention trajectory without altering the underlying
values. Sensitivity to attention output w.r.t _δt_ is visualized
in Appendix D.2.


2.2.1. EXTENSION TO MULTI-HEAD


To scale this mechanism to multi-head attention, we project
the input sequence into _H_ independent subspaces (heads)
of dimension _d_ model _/H_, yielding query, key, and value tensors ( _q_ [(] _[h]_ [)] _, k_ [(] _[h]_ [)] _, v_ [(] _[h]_ [)] ) for _h ∈{_ 1 _, . . ., H}_ . For each head,
pairwise logits are computed according to Eqns. 2,3 or 18,



5


**Neuronal Attention Circuit (NAC) for Representation Learning**


_Figure 2._ Illustration of the architecture of **(a)** Neuronal Attention Circuit mechanism ; **(b)** Multi-Head Extension



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/neuronal_attention_circuits.pdf-5-0.png)

**Event-based MNIST:** Event-based MNIST is the trans
formation of the widely recognized MNIST dataset with
irregular sampling added originally proposed in (Lechner
& Hasani, 2022). The transformation was done in two
steps: (i) flattening each 28×28 image into a time series
of length 784, and (ii) encoding the binary time series into
an event-based format by collapsing consecutive identical
values (e.g., 1,1,1,1 → (1, t=4)). This representation requires models to handle temporal dependencies effectively.
NAC-PW achieved first place with an accuracy of 96.64%,
followed by NAC-Exact/05s/8k at 96.12%. GRU-ODE and
ContiFormer ranked third with 96.04%.

**Person Activity Recognition (PAR):** We employed the
Localized Person Activity dataset from UC Irvine (Vidulin
et al., 2010). The dataset contains data from five participants,
each equipped with inertial measurement sensors sampled
every 211 ms. The goal of this experiment is to predict a
person’s activity from a set of predefined actions, making it
a classification task. All models performed well on this task,
with NAC-PW achieving 89.15% accuracy and taking first
place. NAC-Exact/05s/8k and GRU-ODE ranked second
with 89.01% accuracy, while NAC-02s ranked third with
88.84% mean accuracy.


**3.2. Lane-Keeping of Autonomous Vehicles**


Lane keeping in autonomous vehicles (AVs) is a fundamental problem in robotics and AI. Early works (Tiang et al.,
2018; Park et al., 2021) primarily emphasized accuracy,
often relying on large models. More recent research (Lechner et al., 2020; Razzaq & Hongwei, 2023) has shifted toward designing compact architectures suitable for resourceconstrained devices. The goal of this experiment is to create
a long causal structure between the road’s horizon and the
","The specific metric used to evaluate the Remaining Useful Life (RUL) estimation in the NAC experiments was the **Score metric** (Nectoux et al., 2012).",112,3593,3706,,3,,2026-01-02T20:18:31.790954
"How does the ""Neuronal Attention Circuit"" fundamentally redefine the calculation of attention logits compared to the standard ""Scaled Dot-Product Attention""?",System Level / Architectural Understanding,"[Document 1]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_0
Content: ## **Neuronal Attention Circuit (NAC) for Representation Learning**

**Waleed Razzaq** [1] **Izis Kankaraway** [1] **Yun-Bo Zhao** [1 2]



**Abstract**

Attention improves representation learning over
RNNs, but its discrete nature limits continuoustime (CT) modeling. We introduce Neuronal Attention Circuit (NAC), a novel, biologically plausible CT-Attention mechanism that reformulates

attention logits computation as the solution to a
linear first-order ODE with nonlinear interlinked
gates derived from repurposing _C. elegans_ Neuronal Circuit Policies (NCPs) wiring mechanism.
NAC replaces dense projections with sparse sensory gates for key-query projections and a sparse
backbone network with two heads for computing
_content-target_ and _learnable time-constant_ gates,
enabling efficient adaptive dynamics. NAC supports three attention logit computation modes: (i)
explicit Euler integration, (ii) exact closed-form
solution, and (iii) steady-state approximation. To
improve memory intensity, we implemented a
sparse Top- _K_ pairwise concatenation scheme that
selectively curates key-query interactions. We
provide rigorous theoretical guarantees, including
state stability, bounded approximation errors, and
universal approximation. Empirically, we implemented NAC in diverse domains, including irregular time-series classification, lane-keeping for
autonomous vehicles, and industrial prognostics.
We observed that NAC matches or outperforms
competing baselines in accuracy and occupies an
intermediate position in runtime and memory efficiency compared with several CT baselines.


**1. Introduction**


Learning representations of sequential data in temporal or
spatio-temporal domains is essential for capturing patterns
and enabling accurate forecasting. Discrete-time Recurrent neural networks (DT-RNNs) such as RNN (Rumelhart et al., 1985; Jordan, 1997), Long-short term memory


1Department of Automation, University of Science & Technology of China, Hefei, China [2] Institute of Artificial Intelligence,
Hefei Comprehensive National Science Center. Correspondence to:
Yun-Bo Zhao _<_ ybzhao@ustc.edu.cn _>_, Waleed Razzaq _<_ waleedrazzaq@mail.ustc.edu.cn _>_ .


_Preprint. December 12, 2025._



(LSTM) (Hochreiter & Schmidhuber, 1997), and Gated Recurrent Unit (GRU) (Cho et al., 2014) model sequential dependencies by iteratively updating hidden states to represent
or predict future elements in a sequence. While effective
for regularly sampled sequences, DT-RNNs face challenges
with irregularly sampled data because they assume uniform
time intervals. In addition, vanishing gradients can make
it difficult to capture long-term dependencies (Hochreiter,
1998).
Continuous-time RNNs (CT-RNNs) (Rubanova et al., 2019)
model hidden states as ordinary differential equations
(ODEs), allowing them to process inputs that arrive at arbitrary or irregular time intervals. Mixed-memory RNNs
(mmRNNs) (Lechner & Hasani, 2022) build on this idea
by separating memory compartments from time-continuous
states, helping maintain stable error propagation while capturing continuous-time dynamics. Liquid neural networks
(LNNs) (Hasani et al., 2021; 2022) take a biologically inspired approach by assigning variable time-constants to hidden states, improving adaptability and robustness, though
vanishing gradients can still pose challenges during training.
The attention mechanisms (Vaswani et al., 2017) mitigate
this limitation by treating all time steps equally and allowing models to focus on the most relevant observations. It
computes the similarity between queries ( _q_ ) and keys ( _k_ ),
scaling by the key dimension to keep gradients stable. MultiHead Attention (MHA) (Vaswani et al., 2017) extends this
by allowing the model to attend to different representation
subspaces in parallel. Variants like Sparse Attention (Tay
et al., 2020; Roy et al., 2021), BigBird (Zaheer et al., 2020),
and Longformer (Beltagy et al., 2020) modify the attention
pattern to reduce computational cost, particularly for long sequences, by attending only to selected positions rather than
all pairs. Even with these improvements, attention-based
methods still rely on discrete scaled dot-product operations,
limiting their ability to model continuous trajectories often
captured by CT counterparts.
Recent work has explored bridging this gap through NeuralODE (Chen et al., 2018) formulation. mTAN (Shukla &
Marlin, 2021) learns CT embeddings and uses time-based
attention to interpolate irregular observations into a fixedlength representation for downstream encoder-decoder modeling. ODEFormer (d’Ascoli et al., 2023) trains a sequenceto-sequence transformer on synthetic trajectories to directly
infer symbolic ODE systems from noisy, irregular data,
though it struggles with chaotic systems and generalization



1


**Neuronal Attention Circuit (NAC) for Representation Learning**



beyond observed conditions. Continuous-time Attention
(CTA) (Chien & Chen, 2021) embeds a continuous-time
attention mechanism within a Neural ODE, allowing attention weights and hidden states to evolve jointly over time.
Still, it remains computationally intensive and sensitive to
the accuracy of the ODE solver. ContiFormer (Chen et al.,
2023) builds a CT-transformer by pairing ODE-defined latent trajectories with a time-aware attention mechanism to
model dynamic relationships in data.
Despite these innovations, a persistent and underexplored
gap remains in developing a biologically plausible attention
mechanism that seamlessly integrates CT dynamics with the
abstraction of the brain’s connectome to handle irregular sequences without prohibitive computational costs. Building
on this, we propose a novel attention mechanism called the
_Neuronal Attention Circuit_ (NAC), in which attention logits
are computed as the solution to a first-order ODE modulated
by nonlinear, interlinked gates derived from repurposing
Neuronal Circuit Policies (NCPs) from the nervous system
of _C. elegans_ nematode (refer to Appendix A.2 for more
information). Unlike standard attention, which projects keyquery pairs through a dense layer, NAC employs a sensory
gate to transform input features and a backbone to model
nonlinear interactions, with multiple heads producing outputs structured for attention logits computation. Based on
the solutions to ODE, we define three computation modes:
(i) Exact, using the closed-form ODE solution; (ii) Euler,
approximating the solution via _explicit Euler_ integration;
and (iii) Steady, using only the steady-state solution, analogous to standard attention scores. To reduce computational
complexity, we implemented a sparse Top- _K_ pairwise concatenation algorithm that selectively curates key-query inputs. We evaluate NAC across multiple domains, including
irregularly sampled time series, autonomous vehicle lanekeeping, and Industry 4.0, comparing it to state-of-the-art
baselines. NAC consistently matches or outperforms these
models, while runtime and peak memory benchmarks place
it between CT-RNNs in terms of speed and CT-Attentions
in terms of memory requirements.


**2. Neuronal Attention Circuit (NAC)**


We propose a simple alternative formulation of the attention
logits _a_ (refer to Appendix A.1 for more information), interpreting them as the solution to a first-order linear ODE
modulated by nonlinear, interlinked gates:



from repurposing NCPs. We refer to this formulation as
the Neuronal Attention Circuit (NAC). It enables the logits
_at_ to evolve dynamically with input-dependent, variable
time constants, mirroring the adaptive temporal dynamics
found in _C. elegans_ nervous systems while improving
computational efficiency and expressiveness. Moreover, it
introduces continuous depth into the attention mechanism,
bridging discrete-layer computation with dynamic temporal
evolution.

**Motivation behind this formulation:** The proposed
formulation is loosely motivated by the input-dependent
time-constant mechanism of Liquid Neural Networks
(LNNs), a class of CT-RNNs inspired by biological nervous
systems and synaptic transmission. In this framework, the
dynamics of non-spiking neurons are described by a linear
ODE with nonlinear, interlinked gates: _ddt_ **xt** = **[x]** _τ_ **[t]** [+] **[ S][t]** _[,]_

---

[Document 2]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_8
Content: the ceiling and floor functions, respectively. This results
in a larger overall architectural size and increased runtime.



Future work will support user-defined NCPs configurations
or randomized wiring to enable more efficient architectures.
**Learnable sparse Top-** _**K**_ **selection:** Sparse Top- _K_ attention can miss important context, is sensitive to _k_, and may
be harder to optimize. A further limitation is that it still
computes the full _QK_ _[⊤]_ matrix, which can dominate the
cost for very long sequences. Future work includes adaptive
or learnable Top- _K_ selection, improved key scoring, and
hardware-aware optimization to strengthen accuracy and
robustness.


**5. Conclusion**


In this paper, we introduce the Neuronal Attention Circuit
(NAC), a biologically inspired attention mechanism that
reformulates attention logits as the solution to a first-order
ODE modulated by nonlinear, interlinked gates derived from
repurposing _C.elegans_ nematode NCPs. NAC bridges discrete attention with continuous-time dynamics, enabling
adaptive temporal processing without the limitations inherent to traditional scaled dot-product attention. Based on the
solution to ODE, we introduce three computational modes:
(i) Euler, based on _explicit Euler_ integration; (ii) Exact, providing closed-form solutions; and (iii) Steady, approximating equilibrium states. In addition, a sparse Top- _K_ pairwise
concatenation scheme is introduced to mitigate the memory intensity. Theoretically, we establish NAC’s log-state
stability, exponential error bounds, and universal approximation, thereby providing rigorous guarantees of convergence
and expressiveness. Empirical evaluations demonstrate that
NAC achieves state-of-the-art performance across diverse
tasks, including irregularly sampled time-series benchmarks,
autonomous vehicle lane-keeping, and industrial prognostics. Moreover, NAC occupies an intermediate position
between CT-RNNs and CT-Attention, offering robust temporal modeling while requiring less runtime than CT-RNNs
and less memory than CT-Attention models.


**Reproducibility Statement**


The code for reproducibility is available at
[https://github.com/itxwaleedrazzaq/](https://github.com/itxwaleedrazzaq/neuronal_attention_circuit)

[neuronal_attention_circuit](https://github.com/itxwaleedrazzaq/neuronal_attention_circuit)


**Impact Statement**


The work addresses the growing field of continuous-time
attention and pioneers a biologically plausible mechanism.
It encourages research into sparse, adaptive networks that
resemble natural wiring. From a societal perspective, it
supports more robust AI in resource-limited settings, but it
also raises ethical concerns when applied to areas such as
surveillance or autonomous systems.



8


**Neuronal Attention Circuit (NAC) for Representation Learning**



**References**


Introduction to self-driving cars. URL
[https://www.udacity.com/course/](https://www.udacity.com/course/intro-to-self-driving-cars--nd113)
[intro-to-self-driving-cars--nd113.](https://www.udacity.com/course/intro-to-self-driving-cars--nd113)


Aguiar-Conraria, L. and Soares, M. J. The continuous
wavelet transform: Moving beyond uni-and bivariate analysis. _Journal of economic surveys_, 28(2):344–375, 2014.


Beltagy, I., Peters, M. E., and Cohan, A. Longformer: The long-document transformer. _arXiv preprint_
_arXiv:2004.05150_, 2020.


Bojarski, M., Del Testa, D., Dworakowski, D., Firner, B.,
Flepp, B., Goyal, P., Jackel, L. D., Monfort, M., Muller,
U., Zhang, J., et al. End to end learning for self-driving
cars. _arXiv preprint arXiv:1604.07316_, 2016.


Brockman, G., Cheung, V., Pettersson, L., Schneider, J.,
Schulman, J., Tang, J., and Zaremba, W. Openai gym.
_arXiv preprint arXiv:1606.01540_, 2016.


Cao, Y., Li, S., Petzold, L., and Serban, R. Adjoint sensitivity analysis for differential-algebraic equations: The
adjoint dae system and its numerical solution. _SIAM_
_journal on scientific computing_, 24(3):1076–1089, 2003.


Chen, R. T., Rubanova, Y., Bettencourt, J., and Duvenaud,
D. K. Neural ordinary differential equations. _Advances_
_in neural information processing systems_, 31, 2018.


Chen, Y., Ren, K., Wang, Y., Fang, Y., Sun, W., and Li, D.
Contiformer: Continuous-time transformer for irregular
time series modeling. _Advances in Neural Information_
_Processing Systems_, 36:47143–47175, 2023.


Chien, J.-T. and Chen, Y.-H. Continuous-time attention for
sequential learning. In _Proceedings of the AAAI confer-_
_ence on artificial intelligence_, volume 35, pp. 7116–7124,
2021.


Cho, K., Van Merrienboer, B., Gulcehre, C., Bahdanau,¨
D., Bougares, F., Schwenk, H., and Bengio, Y. Learning phrase representations using rnn encoder-decoder
for statistical machine translation. _arXiv preprint_
_arXiv:1406.1078_, 2014.


d’Ascoli, S., Becker, S., Mathis, A., Schwaller, P., and
Kilbertus, N. Odeformer: Symbolic regression of dynamical systems with transformers. _arXiv preprint_
_arXiv:2310.05573_, 2023.


De Brouwer, E., Simm, J., Arany, A., and Moreau, Y.
Gru-ode-bayes: Continuous modeling of sporadicallyobserved time series. _Advances in neural information_
_processing systems_, 32, 2019.



Deng, L. The mnist database of handwritten digit images
for machine learning research [best of the web]. _IEEE_
_signal processing magazine_, 29(6):141–142, 2012.


Ding, Y., Jia, M., Miao, Q., and Huang, P. Remaining
useful life estimation using deep metric transfer learning
for kernel regression. _Reliability Engineering & System_
_Safety_, 212:107583, 2021.


Hasani, R., Lechner, M., Amini, A., Rus, D., and Grosu,
R. Liquid time-constant networks. In _Proceedings of the_
_AAAI Conference on Artificial Intelligence_, volume 35,
pp. 7657–7666, 2021.


Hasani, R., Lechner, M., Amini, A., Liebenwein, L., Ray,
A., Tschaikowski, M., Teschl, G., and Rus, D. Closed
form continuous-time neural networks. _Nature Machine_

_Intelligence_, 4(11):992–1003, 2022.


Hochreiter, S. The vanishing gradient problem during learning recurrent neural nets and problem solutions. _Interna-_
_tional Journal of Uncertainty, Fuzziness and Knowledge-_
_Based Systems_, 6(02):107–116, 1998.


Hochreiter, S. and Schmidhuber, J. Long short-term memory.
_Neural computation_, 9(8):1735–1780, 1997.


Hong, H. S. and Thuan, N. Hust bearing: a practical dataset
for ball bearing fault diagnosis. _Mendeley Data_, 3, 2023.


John, F. On integration of parabolic equations by difference methods: I. linear and quasi-linear equations for the
infinite interval. _Communications on Pure and Applied_
_Mathematics_, 5(2):155–211, 1952.

---

[Document 3]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_3
Content: Scores: _S ←_ _Q · K_ _[⊤]_

Effective Top- _K_ : _K_ eff _←_ min( _K, Tk_ )
Indices: _I_ topk _←_ top ~~k~~ ( _S, K_ eff)
Gather: _K_ selected _←_ gather( _K, I_ topk) _∈_ R _[B][×][H][×][T][q]_ _[×][K]_ [eff] _[×][D]_

Tiled: _Q_ tiled _←_ tile( _Q, K_ eff) _∈_ R _[B][×][H][×][T][q]_ _[×][K]_ [eff] _[×][D]_

Concatenate: _U_ topk _←_ [ _Q_ tiled; _K_ selected ] _∈_ R _[B][×][H][×][T][q]_ _[×][K]_ [eff] _[×]_ [2] _[D]_

**return** _U_ topk


**2.2. Designing the Neural Network**


We now outline the design of a neural network layer guided
by the preceding analysis. The process involves five steps:
(i) repurposing NCPs; (ii) input curation; (iii) construction
of the time vector ( _t_ ); (iv) computing attention logits and
weights; and (v) generating the attention output. Figure 2
provides a graphical overview of NAC.
**Repurposing NCPs:** We repurpose the NCPs framework
by converting its fixed, biologically derived wiring (see Figure 1(a)) into a flexible recurrent architecture that allows
configurable input–output mappings. Instead of enforcing
a static connectome, our approach exposes adjacency matrices as modifiable structures defining sparse input and
recurrent connections. This enables selective information

routing across neuron groups while retaining the original circuit topology. Decoupling wiring specifications from model
instantiation allows dynamic connectivity adjustments to
accommodate different input modalities without full retraining. Algorithm 1 summarizes the steps for repurposing the
NCPs wiring mechanism. Key features include group-wise
masking for neuron isolation, adaptive remapping of inputs
and outputs for task-specific adaptation, and tunable sparsity
_s_ to balance expressiveness and efficiency.
In our implementation, the sensory neuron gate ( _NN_ sensory)
projects the _q_, _k_, and _v_ representations (see Figure 1(b)).
This enables sensory neurons to maintain structured, contextaware representations rather than collapsing inputs into fully



connected layers. As a result, the network preserves locality
and modularity, which improves information routing.


_NN_ sensory = NCPCell( _G_ input = [ _Ns_ ] _, G_ output = [ _Ns_ ] _,_

_D_ = [ _Ni, Nc, Nm_ ] _, s_ )
(13)
The inter-to-motor pathways form a backbone network
( _NN_ backbone) with branches that compute _ϕ_ and _ωτ_ (see
Figure 1(c)). Instead of learning _ϕ_ and _ωτ_ independently,
this backbone allows the model to learn shared representations, enabling multiple benefits: (i) separate head layers
enable the system to capture temporal and structural dependencies independently; (ii) accelerates convergence during
training.


_NN_ backbone = NCPCell( _G_ input = [ _Ni_ ] _, G_ output = [ _Nm_ ] _,_

_D_ = [ _Ns_ ] _, s_ )
(14)
The output heads are defined as:


_ϕ_ = _σ_ ( _NN_ backbone( **u** )) (15)

_ωτ_ = softplus( _NN_ backbone( **u** )) + _ε,_ _ε >_ 0 (16)


Here, _ϕ_ serves as a _content–target gate_ head, where the
sigmoid function _σ_ ( _·_ ) determines the target signal strength.
In contrast, _ωτ_ is a strictly positive _time–constant gate_ head
that controls the rate of convergence and the steady-state
amplitude. Conceptually, this parallels recurrent gating: _ϕ_
regulates _what_ content to emphasize, while _ωτ_ governs _how_
_quickly_ and _to what extent_ it is expressed.
**Input Curation:** We experimented with different
strategies for constructing query–key inputs. Initially, we implemented full pairwise concatenation,
where queries _Q ∈_ R _[B][×][H][×][T][q][×][D]_ are combined with
all keys _K_ _∈_ R _[B][×][H][×][T][k][×][D]_ to form a joint tensor
_U ∈_ R _[B][×][H][×][T][q][×][T][k][×]_ [2] _[D]_ . While this preserved complete
feature information and enabled expressive, learnable
similarity functions, it was memory-intensive, making it impractical for longer sequences. To mitigate this, we applied
a sparse Top- _K_ optimization: for each query, we compute
pairwise scores _S_ = _Q · K_ _[⊤]_ _∈_ R _[B][×][H][×][T][q][×][T][k]_, select the
Top- _K_ eff = min( _K, Tk_ ) keys, and construct concatenated
pairs _U_ topk _∈_ R _[B][×][H][×][T][q][×][K]_ [eff] _[×]_ [2] _[D]_ . This approach preserves
the most relevant interactions while substantially reducing
memory requirements in the concatenation and subsequent
backbone processing stages, allowing the method to scale
linearly with the sequence length in those components.
However, the initial computation of _S_ remains quadratic
(see Appendix C.3). Algorithm 2 outlines the steps required
for input curation.
**Time Vector:** NAC builds on continuous-depth models
as (Hasani et al., 2022) that adapt their temporal dynamics to the task. It constructs an internal, normalized
pseudo-time vector _t_ pseudo using a sigmoidal transformation,
_t_ pseudo = _σ_ ( _ta · t_ + _tb_ ), where _ta_ and _tb_ are learnable affine



4


**Neuronal Attention Circuit (NAC) for Representation Learning**



parameters and _σ_ is the sigmoid function. For time-varying
datasets (e.g., irregularly sampled series), each time point
_t_ is derived from the sample’s timestamp, while for tasks
without meaningful timing, _t_ is set to 1. The resulting _t_ pseudo
lies in [0 _,_ 1] and provides a smooth, bounded representation
of time for modulating the network’s dynamics.
**Attention logits and weights:** Starting from Eqn. 3,
consider the trajectory of a query–key pair with initial
condition _a_ 0 = 0:



_at_ = _[ϕ]_

_ωτ_



�1 _−_ _e_ _[−][ω][τ][ t]_ [�] _,_ (17)



followed by the _softmax_ normalization to calculate attention weights. The resulting attention weights _αt_ [(] _[h]_ [)] are then
used to integrate with the value vector _v_ [(] _[h]_ [)], producing headspecific attention outputs. Finally, these outputs are concatenated and linearly projected back into the model dimension.
This formulation ensures that each head learns distinct dynamic compatibilities governed by its own parameterization
of _ϕ_ and _ωτ_, while the aggregation across heads preserves
the expressive capacity of the standard multi-head attention
mechanism.


**2.3. NAC as Universal Approximator**


We now establish the universal approximation capability of
NAC by extending the classical Universal Approximation
Theorem (UAT) (Nishijima, 2021) to the proposed mechanism. For brevity, we consider a network with a single
NAC layer processing fixed-dimensional inputs, though the
argument generalizes to sequences.
","The ""Neuronal Attention Circuit"" (NAC) fundamentally redefines the calculation of attention logits by reformulating it as the solution to a linear first-order Ordinary Differential Equation (ODE). This ODE is modulated by nonlinear, interlinked gates derived from _C. elegans_ Neuronal Circuit Policies (NCPs) wiring mechanisms. This allows NAC logits to evolve dynamically with input-dependent, variable time constants, introducing continuous depth into the attention mechanism.

In contrast, the standard ""Scaled Dot-Product Attention"" computes attention logits by calculating the similarity between queries (_q_) and keys (_k_) through a discrete scaled dot-product operation (_Q · K_ _[⊤]_), which is then scaled by the key dimension.",120,4700,4821,,3,,2026-01-02T20:18:36.615426
"Critique the architectural decision to remove ""recurrence"" and ""convolutions"" in favor of ""self-attention"" as proposed in ""Attention Is All You Need"".",System Level / Architectural Understanding,"[Document 1]
Source: attention_is_all_you_need_raw_chunks_2k.jsonl
Chunk ID: attention_is_all_you_need_raw_chunks_2k_2
Content: [38, 2, 9].


    - The encoder contains self-attention layers. In a self-attention layer all of the keys, values
and queries come from the same place, in this case, the output of the previous layer in the
encoder. Each position in the encoder can attend to all positions in the previous layer of the
encoder.


    - Similarly, self-attention layers in the decoder allow each position in the decoder to attend to
all positions in the decoder up to and including that position. We need to prevent leftward
information flow in the decoder to preserve the auto-regressive property. We implement this
inside of scaled dot-product attention by masking out (setting to _−∞_ ) all values in the input
of the softmax which correspond to illegal connections. See Figure 2.


**3.3** **Position-wise Feed-Forward Networks**


In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully
connected feed-forward network, which is applied to each position separately and identically. This
consists of two linear transformations with a ReLU activation in between.


FFN( _x_ ) = max(0 _, xW_ 1 + _b_ 1) _W_ 2 + _b_ 2 (2)


While the linear transformations are the same across different positions, they use different parameters
from layer to layer. Another way of describing this is as two convolutions with kernel size 1.
The dimensionality of input and output is _d_ model = 512, and the inner-layer has dimensionality
_dff_ = 2048.


**3.4** **Embeddings and Softmax**


Similarly to other sequence transduction models, we use learned embeddings to convert the input
tokens and output tokens to vectors of dimension _d_ model. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In
our model, we share the same weight matrix between the two embedding layers and the pre-softmax
linear transformation, similar to [30]. In the embedding layers, we multiply those weights by _[√]_ _d_ model.


5


Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations
for different layer types. _n_ is the sequence length, _d_ is the representation dimension, _k_ is the kernel
size of convolutions and _r_ the size of the neighborhood in restricted self-attention.


Layer Type Complexity per Layer Sequential Maximum Path Length
Operations
Self-Attention _O_ ( _n_ [2] _· d_ ) _O_ (1) _O_ (1)
Recurrent _O_ ( _n · d_ [2] ) _O_ ( _n_ ) _O_ ( _n_ )
Convolutional _O_ ( _k · n · d_ [2] ) _O_ (1) _O_ ( _logk_ ( _n_ ))
Self-Attention (restricted) _O_ ( _r · n · d_ ) _O_ (1) _O_ ( _n/r_ )


**3.5** **Positional Encoding**


Since our model contains no recurrence and no convolution, in order for the model to make use of the
order of the sequence, we must inject some information about the relative or absolute position of the
tokens in the sequence. To this end, we add ""positional encodings"" to the input embeddings at the
bottoms of the encoder and decoder stacks. The positional encodings have the same dimension _d_ model
as the embeddings, so that the two can be summed. There are many choices of positional encodings,
learned and fixed [9].


In this work, we use sine and cosine functions of different frequencies:


_PE_ ( _pos,_ 2 _i_ ) = _sin_ ( _pos/_ 10000 [2] _[i/d]_ [model] )

_PE_ ( _pos,_ 2 _i_ +1) = _cos_ ( _pos/_ 10000 [2] _[i/d]_ [model] )


where _pos_ is the position and _i_ is the dimension. That is, each dimension of the positional encoding
corresponds to a sinusoid. The wavelengths form a geometric progression from 2 _π_ to 10000 _·_ 2 _π_ . We
chose this function because we hypothesized it would allow the model to easily learn to attend by
relative positions, since for any fixed offset _k_, _PEpos_ + _k_ can be represented as a linear function of
_PEpos_ .


We also experimented with using learned positional embeddings [9] instead, and found that the two
versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version
because it may allow the model to extrapolate to sequence lengths longer than the ones encountered
during training.


**4** **Why Self-Attention**


In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations
( _x_ 1 _, ..., xn_ ) to another sequence of equal length ( _z_ 1 _, ..., zn_ ), with _xi, zi ∈_ R _[d]_, such as a hidden
layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we
consider three desiderata.


One is the total computational complexity per layer. Another is the amount of computation that can
be parallelized, as measured by the minimum number of sequential operations required.


The third is the path length between long-range dependencies in the network. Learning long-range
dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the
ability to learn such dependencies is the length of the paths forward and backward signals have to
traverse in the network. The shorter these paths between any combination of positions in the input
and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare
the maximum path length between any two input and output positions in networks composed of the
different layer types.


As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially
executed operations, whereas a recurrent layer requires _O_ ( _n_ ) sequential operations. In terms of
computational complexity, self-attention layers are faster than recurrent layers when the sequence


6


length _n_ is smaller than the representation dimensionality _d_, which is most often the case with
sentence representations used by state-of-the-art models in machine translations, such as word-piece

[38] and byte-pair [31] representations. To improve computational performance for tasks involving
very long sequences, self-attention could be restricted to considering only a neighborhood of size _r_ in
the input sequence centered around the respective output position. This would increase the maximum
path length to _O_ ( _n/r_ ). We plan to investigate this approach further in future work.


A single convolutional layer with kernel width _k < n_ does not connect all pairs of input and output
positions. Doing so requires a stack of _O_ ( _n/k_ ) convolutional layers in the case of contiguous kernels,
or _O_ ( _logk_ ( _n_ )) in the case of dilated convolutions [18], increasing the length of the longest paths
between any two positions in the network. Convolutional layers are generally more expensive than
recurrent layers, by a factor of _k_ . Separable convolutions [6], however, decrease the complexity
considerably, to _O_ ( _k · n · d_ + _n · d_ [2] ). Even with _k_ = _n_, however, the complexity of a separable
convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,
the approach we take in our model.


As side benefit, self-attention could yield more interpretable models. We inspect attention distributions
from our models and present and discuss examples in the appendix. Not only do individual attention
heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic
and semantic structure of the sentences.


**5** **Training**


This section describes the training regime for our models.


**5.1** **Training Data and Batching**

---

[Document 2]
Source: attention_is_all_you_need_raw_chunks_2k.jsonl
Chunk ID: attention_is_all_you_need_raw_chunks_2k_0
Content: Provided proper attribution is provided, Google hereby grants permission to
reproduce the tables and figures in this paper solely for use in journalistic or
scholarly works.

## **Attention Is All You Need**



**Niki Parmar** _[∗]_
Google Research
```
nikip@google.com

```


**Ashish Vaswani** _[∗]_
Google Brain
```
avaswani@google.com

```

**Llion Jones** _[∗]_
Google Research
```
 llion@google.com

```


**Noam Shazeer** _[∗]_
Google Brain
```
noam@google.com

```


**Jakob Uszkoreit** _[∗]_
Google Research
```
usz@google.com

```


**Aidan N. Gomez** _[∗†]_
University of Toronto
```
aidan@cs.toronto.edu

```


**Łukasz Kaiser** _[∗]_
Google Brain
```
lukaszkaiser@google.com

```


**Illia Polosukhin** _[∗‡]_

```
             illia.polosukhin@gmail.com

```

**Abstract**


The dominant sequence transduction models are based on complex recurrent or
convolutional neural networks that include an encoder and a decoder. The best
performing models also connect the encoder and decoder through an attention
mechanism. We propose a new simple network architecture, the Transformer,
based solely on attention mechanisms, dispensing with recurrence and convolutions
entirely. Experiments on two machine translation tasks show these models to
be superior in quality while being more parallelizable and requiring significantly
less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including
ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,
our model establishes a new single-model state-of-the-art BLEU score of 41.8 after
training for 3.5 days on eight GPUs, a small fraction of the training costs of the
best models from the literature. We show that the Transformer generalizes well to
other tasks by applying it successfully to English constituency parsing both with
large and limited training data.


_∗_ Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started
the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and
has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head
attention and the parameter-free position representation and became the other person involved in nearly every
detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and
tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and
efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and
implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating
our research.

_†_ Work performed while at Google Brain.

_‡_ Work performed while at Google Research.


31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.


**1** **Introduction**


Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks
in particular, have been firmly established as state of the art approaches in sequence modeling and
transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous
efforts have since continued to push the boundaries of recurrent language models and encoder-decoder
architectures [38, 24, 15].


Recurrent models typically factor computation along the symbol positions of the input and output
sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden
states _ht_, as a function of the previous hidden state _ht−_ 1 and the input for position _t_ . This inherently
sequential nature precludes parallelization within training examples, which becomes critical at longer
sequence lengths, as memory constraints limit batching across examples. Recent work has achieved
significant improvements in computational efficiency through factorization tricks [21] and conditional
computation [32], while also improving model performance in case of the latter. The fundamental
constraint of sequential computation, however, remains.


Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in
the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms
are used in conjunction with a recurrent network.


In this work we propose the Transformer, a model architecture eschewing recurrence and instead
relying entirely on an attention mechanism to draw global dependencies between input and output.
The Transformer allows for significantly more parallelization and can reach a new state of the art in
translation quality after being trained for as little as twelve hours on eight P100 GPUs.


**2** **Background**


The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU

[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building
block, computing hidden representations in parallel for all input and output positions. In these models,
the number of operations required to relate signals from two arbitrary input or output positions grows
in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes
it more difficult to learn dependencies between distant positions [12]. In the Transformer this is
reduced to a constant number of operations, albeit at the cost of reduced effective resolution due
to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as
described in section 3.2.


Self-attention, sometimes called intra-attention is an attention mechanism relating different positions
of a single sequence in order to compute a representation of the sequence. Self-attention has been
used successfully in a variety of tasks including reading comprehension, abstractive summarization,
textual entailment and learning task-independent sentence representations [4, 27, 28, 22].


End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering and
language modeling tasks [34].


To the best of our knowledge, however, the Transformer is the first transduction model relying
entirely on self-attention to compute representations of its input and output without using sequencealigned RNNs or convolution. In the following sections, we will describe the Transformer, motivate
self-attention and discuss its advantages over models such as [17, 18] and [9].


**3** **Model Architecture**


Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].
Here, the encoder maps an input sequence of symbol representations ( _x_ 1 _, ..., xn_ ) to a sequence
of continuous representations **z** = ( _z_ 1 _, ..., zn_ ). Given **z**, the decoder then generates an output
sequence ( _y_ 1 _, ..., ym_ ) of symbols one element at a time. At each step the model is auto-regressive

[10], consuming the previously generated symbols as additional input when generating the next.


2


![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_is_all_you_need.pdf-2-0.png)

Figure 1: The Transformer - model architecture.


The Transformer follows this overall architecture using stacked self-attention and point-wise, fully
connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,
respectively.


**3.1** **Encoder and Decoder Stacks**

---

[Document 3]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_7
Content: compact architecture suitable for resource-constrained devices, thereby supporting localized safety.
We utilized three benchmark datasets: (i) PRONOSTIA
(Nectoux et al., 2012), (ii) XJTU-SY (Wang et al., 2018),
and (iii) HUST (Thuan & Hong, 2023). Training is performed on PRONOSTIA, while XJTU-SY and HUST are

used to assess cross-validation. We used the Score met
ric (Nectoux et al., 2012) to assess the performance. We
evaluate generalization using _Bearing 1_ from the first operating condition of each dataset. Figure 3 visualizes the
expected degradation alongside the outputs of NAC. On the
PRONOSTIA dataset, ContiFormer performed the best with



**3.4. Run Time and Memory Experiments**


We evaluate computational requirements on fixed-length
sequences of 1024 steps, 64-dimensional features, 4 heads,
and a Batch size of 1. Each model is run for ten for
ward passes on Google Colab T4-GPU, and we report the
mean runtime with standard deviation, throughput, and peak
memory usage. NAC occupies an intermediate position
in runtime relative to several CT-RNN models, including
GRU-ODE, CfC, and LTC. In terms of memory consumption, NAC uses significantly less memory than mTAN, with
NAC-2k being the least memory-consuming among the CTAttention models. Reducing NAC sparsity from 90% to 20%
has minimal effect on memory, decreasing usage slightly



7


**Neuronal Attention Circuit (NAC) for Representation Learning**



_Table 2._ Run-Time and Memory Benchmark Results


**Run-Time** **Throughput** **Peak Memory**
**Model**
(s) (seq/s) (MB)


RNN 1 _._ 8392 _[±]_ [0] _[.]_ [1933] 0.544 0.29
CT-RNN 7 _._ 1097 _[±]_ [0] _[.]_ [3048] 0.141 0.67
LSTM 2 _._ 6241 _[±]_ [0] _[.]_ [2906] 0.381 0.42
PhasedLSTM 4 _._ 9812 _[±]_ [0] _[.]_ [272] 0.201 0.80
GRU 3 _._ 216 _[±]_ [0] _[.]_ [2566] 0.311 0.54
GRU-ODE 12 _._ 2498 _[±]_ [0] _[.]_ [0525] 0.082 0.64
mmRNN 7 _._ 5852 _[±]_ [0] _[.]_ [2785] 0.132 0.96
LTC 14 _._ 643 _[±]_ [0] _[.]_ [2445] 0.068 0.99
CfC 6 _._ 0988 _[±]_ [0] _[.]_ [2135] 0.164 0.76


Attention 0 _._ 0016 _[±]_ [0] _[.]_ [0001] 625.00 16.86
MHA 0 _._ 0041 _[±]_ [0] _[.]_ [0001] 243.90 69.05
mTAN 0 _._ 0272 _[±]_ [0] _[.]_ [0054] 36.76 790.16
ODEFormer 0 _._ 0317 _[±]_ [0] _[.]_ [0016] 31.55 67.71
CTA 8 _._ 5275 _[±]_ [0] _[.]_ [2355] 0.117 1.43
ContiFormer 0 _._ 066 _[±]_ [0] _[.]_ [0075] 15.15 67.71


NAC-2k 7 _._ 3071 _[±]_ [0] _[.]_ [1547] 0.137 44.75
NAC-32k 7 _._ 2313 _[±]_ [0] _[.]_ [219] 0.138 549.86
NAC-PW 8 _._ 5649 _[±]_ [0] _[.]_ [0203] 0.117 5042.09
NAC-FC 0 _._ 0195 ~~_[±]_~~ ~~[0]~~ ~~_[.]_~~ ~~[0002]~~ 51.28 29.92
NAC-02s 7 _._ 252 _[±]_ [0] _[.]_ [2018] 0.138 151.54
NAC-09s 7 _._ 222 _[±]_ [0] _[.]_ [176] 0.139 150.85
NAC-Exact/05s/8k 7 _._ 4101 ~~_[±]_~~ ~~[0]~~ ~~_[.]_~~ ~~[1586]~~ 0.135 151.50
NAC-Euler 7 _._ 3367 _[±]_ [0] _[.]_ [1719] 0.136 152.22
NAC-Steady 7 _._ 2942 _[±]_ [0] _[.]_ [1451] 0.137 150.86


from 151.54 MB to 150.85 MB. In constrast, decreasing
the Top- _K_ selection from _PW_ to _k_ = 2 drastically reduces
memory consumption from 5042 MB to 44.75 MB, demonstrating the flexibility of NAC.
**Interpreting the Results:** From the experiments, we observe that increasing the sparsity of the NAC layer improves
the robustness of the system and leads to higher overall accuracy. Similarly, increasing the Top- _K_ interactions enhances
accuracy too; however, the benefits diminish as memory
consumption grows. Using Exact mode, Top- _K_ =8 with 50%
sparsity achieves the best balance between accuracy and
efficiency. Steady mode is the fastest, while Euler mode
handles adaptive temporal dynamics.


**4. Discussions**



This research is part of ongoing work on biologically plausible attention mechanisms and represents a pioneering step,
with limitations to be addressed in future work.

**Architectural improvement:** Currently, NAC uses predetermined wiring (AutoNCP) requiring three inputs: number of units (sensory + interneuron + motor), output motor
neurons, and sparsity, with typically 60% of units assigned
to sensory neurons. To integrate with the attention mechanism while preserving wiring, sensory units for _NN_ sensory
are set as unitssensory = � _d_ model0 _.−_ 6 0 _._ 5 � and backbone units as



are set as unitssensory = � _d_ model0 _.−_ 6 0 _._ 5 � and backbone units as

units _backbone_ = _d_ model + � _d_ 0model _._ 6 �, where _⌈·⌉_ and _⌊·⌋_ denote



units _backbone_ = _d_ model + � _d_ 0model _._ 6 �, where _⌈·⌉_ and _⌊·⌋_ denote

the ceiling and floor functions, respectively. This results
in a larger overall architectural size and increased runtime.



Future work will support user-defined NCPs configurations
or randomized wiring to enable more efficient architectures.
**Learnable sparse Top-** _**K**_ **selection:** Sparse Top- _K_ attention can miss important context, is sensitive to _k_, and may
be harder to optimize. A further limitation is that it still
computes the full _QK_ _[⊤]_ matrix, which can dominate the
cost for very long sequences. Future work includes adaptive
or learnable Top- _K_ selection, improved key scoring, and
hardware-aware optimization to strengthen accuracy and
robustness.


**5. Conclusion**
","The architectural decision to remove ""recurrence"" and ""convolutions"" in favor of ""self-attention"" in the Transformer model, as proposed in ""Attention Is All You Need,"" is critiqued based on several factors outlined in the provided context:

**Advantages of Self-Attention (and reasons for removal):**

1.  **Increased Parallelization:** Recurrent networks are inherently sequential (`O(n)` sequential operations), which limits parallelization within training examples. Self-attention, however, connects all positions with a constant number of sequentially executed operations (`O(1)`), allowing for significantly more parallelization and faster training times.
2.  **Improved Long-Range Dependency Learning:** Learning long-range dependencies is a key challenge in sequence tasks.
    *   Self-attention directly connects any two positions in the input or output sequence with a constant path length (`O(1)`), making it easier for signals to traverse and learn these dependencies.
    *   Recurrent layers have a maximum path length of `O(n)`.
    *   Convolutional layers require a stack of `O(n/k)` layers for contiguous kernels or `O(logk(n))` for dilated convolutions to connect all pairs of input and output positions, increasing the path length.
3.  **Computational Efficiency (under certain conditions):** Self-attention layers are faster than recurrent layers when the sequence length `n` is smaller than the representation dimensionality `d` (`O(n^2 · d)` vs. `O(n · d^2)`).
4.  **Superior Performance:** Experiments on machine translation tasks showed that models based solely on attention mechanisms achieved superior quality (e.g., higher BLEU scores) and reached state-of-the-art results with significantly less training time.
5.  **Interpretability:** Self-attention can yield more interpretable models, as individual attention heads appear to learn different tasks related to the syntactic and semantic structure of sentences.

**Consequences and Considerations of the Architectural Decision:**

1.  **Necessity of Positional Encodings:** Since the Transformer model contains no recurrence and no convolution, it inherently lacks information about the order of the sequence. To address this, positional encodings must be explicitly injected into the input embeddings to provide the model with relative or absolute position information.
2.  **Computational Complexity for Very Long Sequences:** While generally efficient, the `O(n^2 · d)` computational complexity of self-attention can become a bottleneck for very long sequences. The paper suggests restricting self-attention to a neighborhood (`O(r · n · d)`) to improve performance in such cases, though this would increase the maximum path length to `O(n/r)`.
3.  **Reduced Effective Resolution:** The constant number of operations in the Transformer, while beneficial for parallelization, comes ""at the cost of reduced effective resolution due to averaging attention-weighted positions."" This effect is counteracted by the use of Multi-Head Attention.",142,11844,11988,,3,,2026-01-02T20:18:48.604810
"Analyze the systemic trade-offs between ""accuracy"" and ""peak memory usage"" when deploying ""NAC-PW"" versus ""NAC-2k"".",System Level / Architectural Understanding,"[Document 1]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_7
Content: compact architecture suitable for resource-constrained devices, thereby supporting localized safety.
We utilized three benchmark datasets: (i) PRONOSTIA
(Nectoux et al., 2012), (ii) XJTU-SY (Wang et al., 2018),
and (iii) HUST (Thuan & Hong, 2023). Training is performed on PRONOSTIA, while XJTU-SY and HUST are

used to assess cross-validation. We used the Score met
ric (Nectoux et al., 2012) to assess the performance. We
evaluate generalization using _Bearing 1_ from the first operating condition of each dataset. Figure 3 visualizes the
expected degradation alongside the outputs of NAC. On the
PRONOSTIA dataset, ContiFormer performed the best with



**3.4. Run Time and Memory Experiments**


We evaluate computational requirements on fixed-length
sequences of 1024 steps, 64-dimensional features, 4 heads,
and a Batch size of 1. Each model is run for ten for
ward passes on Google Colab T4-GPU, and we report the
mean runtime with standard deviation, throughput, and peak
memory usage. NAC occupies an intermediate position
in runtime relative to several CT-RNN models, including
GRU-ODE, CfC, and LTC. In terms of memory consumption, NAC uses significantly less memory than mTAN, with
NAC-2k being the least memory-consuming among the CTAttention models. Reducing NAC sparsity from 90% to 20%
has minimal effect on memory, decreasing usage slightly



7


**Neuronal Attention Circuit (NAC) for Representation Learning**



_Table 2._ Run-Time and Memory Benchmark Results


**Run-Time** **Throughput** **Peak Memory**
**Model**
(s) (seq/s) (MB)


RNN 1 _._ 8392 _[±]_ [0] _[.]_ [1933] 0.544 0.29
CT-RNN 7 _._ 1097 _[±]_ [0] _[.]_ [3048] 0.141 0.67
LSTM 2 _._ 6241 _[±]_ [0] _[.]_ [2906] 0.381 0.42
PhasedLSTM 4 _._ 9812 _[±]_ [0] _[.]_ [272] 0.201 0.80
GRU 3 _._ 216 _[±]_ [0] _[.]_ [2566] 0.311 0.54
GRU-ODE 12 _._ 2498 _[±]_ [0] _[.]_ [0525] 0.082 0.64
mmRNN 7 _._ 5852 _[±]_ [0] _[.]_ [2785] 0.132 0.96
LTC 14 _._ 643 _[±]_ [0] _[.]_ [2445] 0.068 0.99
CfC 6 _._ 0988 _[±]_ [0] _[.]_ [2135] 0.164 0.76


Attention 0 _._ 0016 _[±]_ [0] _[.]_ [0001] 625.00 16.86
MHA 0 _._ 0041 _[±]_ [0] _[.]_ [0001] 243.90 69.05
mTAN 0 _._ 0272 _[±]_ [0] _[.]_ [0054] 36.76 790.16
ODEFormer 0 _._ 0317 _[±]_ [0] _[.]_ [0016] 31.55 67.71
CTA 8 _._ 5275 _[±]_ [0] _[.]_ [2355] 0.117 1.43
ContiFormer 0 _._ 066 _[±]_ [0] _[.]_ [0075] 15.15 67.71


NAC-2k 7 _._ 3071 _[±]_ [0] _[.]_ [1547] 0.137 44.75
NAC-32k 7 _._ 2313 _[±]_ [0] _[.]_ [219] 0.138 549.86
NAC-PW 8 _._ 5649 _[±]_ [0] _[.]_ [0203] 0.117 5042.09
NAC-FC 0 _._ 0195 ~~_[±]_~~ ~~[0]~~ ~~_[.]_~~ ~~[0002]~~ 51.28 29.92
NAC-02s 7 _._ 252 _[±]_ [0] _[.]_ [2018] 0.138 151.54
NAC-09s 7 _._ 222 _[±]_ [0] _[.]_ [176] 0.139 150.85
NAC-Exact/05s/8k 7 _._ 4101 ~~_[±]_~~ ~~[0]~~ ~~_[.]_~~ ~~[1586]~~ 0.135 151.50
NAC-Euler 7 _._ 3367 _[±]_ [0] _[.]_ [1719] 0.136 152.22
NAC-Steady 7 _._ 2942 _[±]_ [0] _[.]_ [1451] 0.137 150.86


from 151.54 MB to 150.85 MB. In constrast, decreasing
the Top- _K_ selection from _PW_ to _k_ = 2 drastically reduces
memory consumption from 5042 MB to 44.75 MB, demonstrating the flexibility of NAC.
**Interpreting the Results:** From the experiments, we observe that increasing the sparsity of the NAC layer improves
the robustness of the system and leads to higher overall accuracy. Similarly, increasing the Top- _K_ interactions enhances
accuracy too; however, the benefits diminish as memory
consumption grows. Using Exact mode, Top- _K_ =8 with 50%
sparsity achieves the best balance between accuracy and
efficiency. Steady mode is the fastest, while Euler mode
handles adaptive temporal dynamics.


**4. Discussions**



This research is part of ongoing work on biologically plausible attention mechanisms and represents a pioneering step,
with limitations to be addressed in future work.

**Architectural improvement:** Currently, NAC uses predetermined wiring (AutoNCP) requiring three inputs: number of units (sensory + interneuron + motor), output motor
neurons, and sparsity, with typically 60% of units assigned
to sensory neurons. To integrate with the attention mechanism while preserving wiring, sensory units for _NN_ sensory
are set as unitssensory = � _d_ model0 _.−_ 6 0 _._ 5 � and backbone units as



are set as unitssensory = � _d_ model0 _.−_ 6 0 _._ 5 � and backbone units as

units _backbone_ = _d_ model + � _d_ 0model _._ 6 �, where _⌈·⌉_ and _⌊·⌋_ denote



units _backbone_ = _d_ model + � _d_ 0model _._ 6 �, where _⌈·⌉_ and _⌊·⌋_ denote

the ceiling and floor functions, respectively. This results
in a larger overall architectural size and increased runtime.



Future work will support user-defined NCPs configurations
or randomized wiring to enable more efficient architectures.
**Learnable sparse Top-** _**K**_ **selection:** Sparse Top- _K_ attention can miss important context, is sensitive to _k_, and may
be harder to optimize. A further limitation is that it still
computes the full _QK_ _[⊤]_ matrix, which can dominate the
cost for very long sequences. Future work includes adaptive
or learnable Top- _K_ selection, improved key scoring, and
hardware-aware optimization to strengthen accuracy and
robustness.


**5. Conclusion**

---

[Document 2]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_4
Content: **2.3. NAC as Universal Approximator**


We now establish the universal approximation capability of
NAC by extending the classical Universal Approximation
Theorem (UAT) (Nishijima, 2021) to the proposed mechanism. For brevity, we consider a network with a single
NAC layer processing fixed-dimensional inputs, though the
argument generalizes to sequences.


**Theorem 2** (Universal Approximation by NAC) **.** _Let K ⊂_
R _[n]_ _be a compact set and f_ : _K →_ R _[m]_ _be a continuous_
_function. For any ϵ >_ 0 _, there exists a neural network_
_consisting of a single NAC layer, with sufficiently large_
_model dimension dmodel, number of heads H, sparsity s,_
_and nonlinear activations, such that the network’s output_
_g_ : R _[n]_ _→_ R _[m]_ _satisfies_


sup _∥f_ ( _x_ ) _−_ _g_ ( _x_ ) _∥_ _< ϵ._ (20)
_x∈K_


_The proof is provided in Appendix B.3._


**3. Evaluation**


We evaluate the proposed architecture against a range of
baselines, including (DT & CT) RNN, (DT & CT) attention,
and multiple NAC ablation configurations. Experiments
are conducted across diverse domains, including irregular
time-series modeling, lane keeping of autonomous vehicles,
and Industry 4.0 prognostics. All results are obtained using 5-fold cross-validation, where models are trained using
BPTT (see Appendix C.2) on each fold and evaluated across
all folds. We report the mean ( _µ_ ) and standard deviation ( _σ_ )
to capture variability and quantify uncertainty in the predictions. Table 1 provides results for all experiments, and the
details of the baselines, ablation, environment utilized, the
data curation and preprocessing, and neural network architectures for all experiments are provided in the Appendix
D.3.


**3.1. Irregular Time-series**


We evaluate the proposed architecture on two irregular timeseries datasets: (i) Event-based MNIST; and (ii) Person
Activity Recognition (PAR).



For finite _t_, the exponential factor (1 _−_ _e_ _[−][ω][τ][ t]_ ) regulates the
buildup of attention, giving _ωτ_ a temporal gating role. Normalizing across all keys via _softmax_ yields attention weights
_αt_ = softmax( _at_ ), defining a valid probability distribution
where _ϕ_ amplifies or suppresses content alignments, and _ωτ_
shapes both the speed and saturation of these preferences.
As _t →∞_, the trajectory converges to the steady state


_a_ _[∗]_ _t_ [=] _[ϕ]_ _≈_ _[q][⊤][k]_ _,_ (18)

_ωτ_ ~~_√_~~ _dk_


which is analogous to scaled-dot attention under specific
parameterization when the backbone _NN_ backbone is configured as a linear projection such that _ϕ_ ( **u** ) = _q_ _[⊤]_ _k_ and
_ωτ_ ( _u_ ) = _[√]_ _dk_ (e.g., by setting NCP weights to emulate
bilinear forms and disabling nonlinearities). In general, the
nonlinear backbone allows for more expressive similarities,
with the approximation holding when trained to mimic dot
products.
**Attention output:** Finally, the attention output is computed
by integrating the attention weights with the value matrix:


NAC( _q, k, v_ ) = _αtvtdt_ (19)
� _T_


In practice, the integration is approximated using a Riemannstyle approach, where the weighted elements are computed
by multiplying each _vt_ with its corresponding _αt_ . These are
then summed and multiplied by a fixed pseudo-time step
_δt_, chosen as a scalar (typically between 0.5–1.0) hyperparameter during layer initialization. This yields a continuous
analogue of standard weighted sums, giving finer resolution
of the attention trajectory without altering the underlying
values. Sensitivity to attention output w.r.t _δt_ is visualized
in Appendix D.2.


2.2.1. EXTENSION TO MULTI-HEAD


To scale this mechanism to multi-head attention, we project
the input sequence into _H_ independent subspaces (heads)
of dimension _d_ model _/H_, yielding query, key, and value tensors ( _q_ [(] _[h]_ [)] _, k_ [(] _[h]_ [)] _, v_ [(] _[h]_ [)] ) for _h ∈{_ 1 _, . . ., H}_ . For each head,
pairwise logits are computed according to Eqns. 2,3 or 18,



5


**Neuronal Attention Circuit (NAC) for Representation Learning**


_Figure 2._ Illustration of the architecture of **(a)** Neuronal Attention Circuit mechanism ; **(b)** Multi-Head Extension



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/neuronal_attention_circuits.pdf-5-0.png)

**Event-based MNIST:** Event-based MNIST is the trans
formation of the widely recognized MNIST dataset with
irregular sampling added originally proposed in (Lechner
& Hasani, 2022). The transformation was done in two
steps: (i) flattening each 28×28 image into a time series
of length 784, and (ii) encoding the binary time series into
an event-based format by collapsing consecutive identical
values (e.g., 1,1,1,1 → (1, t=4)). This representation requires models to handle temporal dependencies effectively.
NAC-PW achieved first place with an accuracy of 96.64%,
followed by NAC-Exact/05s/8k at 96.12%. GRU-ODE and
ContiFormer ranked third with 96.04%.

**Person Activity Recognition (PAR):** We employed the
Localized Person Activity dataset from UC Irvine (Vidulin
et al., 2010). The dataset contains data from five participants,
each equipped with inertial measurement sensors sampled
every 211 ms. The goal of this experiment is to predict a
person’s activity from a set of predefined actions, making it
a classification task. All models performed well on this task,
with NAC-PW achieving 89.15% accuracy and taking first
place. NAC-Exact/05s/8k and GRU-ODE ranked second
with 89.01% accuracy, while NAC-02s ranked third with
88.84% mean accuracy.


**3.2. Lane-Keeping of Autonomous Vehicles**


Lane keeping in autonomous vehicles (AVs) is a fundamental problem in robotics and AI. Early works (Tiang et al.,
2018; Park et al., 2021) primarily emphasized accuracy,
often relying on large models. More recent research (Lechner et al., 2020; Razzaq & Hongwei, 2023) has shifted toward designing compact architectures suitable for resourceconstrained devices. The goal of this experiment is to create
a long causal structure between the road’s horizon and the

---

[Document 3]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_6
Content: MHA 95.94 _[±]_ [0.15] 88.36 _[±]_ [1.06] 79.99 _[±]_ [0.49] 0.0185 _[±]_ [0.0017] 45.36 _[±]_ [5.16] 37.31 _[±]_ [12.20] 41.40 _[±]_ [7.72]

mTAN 95.97 _[±]_ [0.25] 88.08 _[±]_ [0.94] 80.86 _[±]_ [0.22] 0.0178 _[±]_ [0.0005] 44.41 _[±]_ [7.15] 41.34 _[±]_ [3.72] 66.29 _[±]_ [4.25]

CTA 95.86 _[±]_ [0.14] 88.10 _[±]_ [1.10] 80.54 _[±]_ [0.40] 0.0197 _[±]_ [0.0016] 39.16 _[±]_ [3.54] **25.86** _[±]_ [1.47] 38.41 _[±]_ [4.51]

ODEFormer 95.62 _[±]_ [0.20] 88.25 _[±]_ [0.66] 80.54 _[±]_ [0.40] 0.0190 _[±]_ [0.0012] 42.42 _[±]_ [6.98] 35.63 _[±]_ [9.24] 40.60 _[±]_ [6.83]

ContiFormer **96.04** _[±]_ **[0.23]** 81.28 _[±]_ [0.85] 80.47 _[±]_ [0.50] **0.0174** _[±]_ **[0.01]** **27.82** _[±]_ **[7.09]** 34.71 _[±]_ [4.98] 43.81 _[±]_ [10.18]


NAC-2k 95.73 _[±]_ [0.07] 88.84% _[±]_ [0.81] 80.59 _[±]_ [0.46] 0.0208 _[±]_ [0.0015] 43.78 _[±]_ [2.71] 37.43 _[±]_ [9.28] 40.51 _[±]_ [6.61]

NAC-32k 95.15 _[±]_ [0.11] 88.80 _[±]_ [0.76] 80.38 _[±]_ [0.16] **0.0170** _[±]_ **[0.0007]** 49.53 _[±]_ [4.89] 32.45 _[±]_ [10.84] 39.17 _[±]_ [12.23]

NAC-PW **96.64** _[±]_ [0.12] **89.15** _[±]_ **[1.01]** **80.72** _[±]_ **[0.41]** 0.0177 _[±]_ [0.0008] **37.50** _[±]_ **[2.56]** 28.01 _[±]_ [4.93] **30.14** _[±]_ **[6.87]**


NAC-FC 95.31 _[±]_ [0.07] 88.45 _[±]_ [0.91] 80.49 _[±]_ [0.46] 0.0192 _[±]_ [0.0012] 40.36 _[±]_ [6.09] **24.89** _[±]_ **[5.30]** **35.35** _[±]_ **[6.64]**

NAC-02s 95.31 _[±]_ [0.07] 88.84 _[±]_ [1.33] 80.47 _[±]_ [0.27] 0.0188 _[±]_ [0.0013] 39.43 _[±]_ [5.94] 35.59 _[±]_ [3.86] 38.90 _[±]_ [6.43]

NAC-09s 95.86 _[±]_ [0.11] 88.61% _[±]_ [1.25] 80.43% _[±]_ [0.17] 0.0188 _[±]_ [0.0013] 47.29 _[±]_ [5.52] 40.40 _[±]_ [8.85] 44.39 _[±]_ [6.82]


NAC-Exact/05s/8k **96.12** _[±]_ [0.11] **89.01** _[±]_ **[1.01]** 80.59 _[±]_ [1.82] **0.0173** _[±]_ **[0.0006]** **37.75** _[±]_ **[4.72]** **19.87** _[±]_ **[1.75]** **27.82** _[±]_ **[7.09]**

NAC-Euler 95.67 _[±]_ [0.26] 88.52 _[±]_ [0.68] **80.61** _[±]_ [0.28] 0.0181 _[±]_ [0.0017] 42.08 _[±]_ [6.14] 28.46 _[±]_ [8.18] 39.32 _[±]_ [9.15]

NAC-Steady 95.75 _[±]_ [0.28] 88.36 _[±]_ [1.05] **80.62** _[±]_ **[0.26]** 0.0181 _[±]_ [0.0012] 40.95 _[±]_ [5.77] 26.76 _[±]_ [7.36] 37.12 _[±]_ [12.43]


**Note:** (↑) higher is better; (↓) lower is better.


the lowest score of 27.82. NAC-Exact/05s/8k and NAC-PW

achieved nearly identical scores, obtaining 37.75 and 37.50
on average, respectively. On the XJTU-SY dataset, NACExact/05s/8k has the lowest score of 19.87. NAC-FC ranked

second with a score of 24.89, followed by NAC-PW in third
place with an average score of 28.01. A similar trend was
observed on the HUST dataset, where NAC-Exact/05s/8k
achieved first place with a score of 27.82, NAC-PW ranked
second with 30.14, and NAC-FC ranked third with 35.35.
These results demonstrated the strong cross-validation capability of NAC.



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/neuronal_attention_circuits.pdf-6-0.png)

_Figure 3._ Degradation Estimation Results.


compact architecture suitable for resource-constrained devices, thereby supporting localized safety.
We utilized three benchmark datasets: (i) PRONOSTIA
(Nectoux et al., 2012), (ii) XJTU-SY (Wang et al., 2018),
and (iii) HUST (Thuan & Hong, 2023). Training is performed on PRONOSTIA, while XJTU-SY and HUST are

used to assess cross-validation. We used the Score met
ric (Nectoux et al., 2012) to assess the performance. We
evaluate generalization using _Bearing 1_ from the first operating condition of each dataset. Figure 3 visualizes the
expected degradation alongside the outputs of NAC. On the
PRONOSTIA dataset, ContiFormer performed the best with



**3.4. Run Time and Memory Experiments**
","When comparing ""NAC-PW"" and ""NAC-2k"", there is a clear systemic trade-off between accuracy and peak memory usage:

*   **Accuracy:** NAC-PW generally achieves higher accuracy across various benchmarks. For instance, on Event-based MNIST, NAC-PW scored 96.64% (first place) compared to NAC-2k's 95.73%. Similarly, on Person Activity Recognition (PAR), NAC-PW achieved 89.15% (first place) while NAC-2k achieved 88.84%. For the Score metric on PRONOSTIA, XJTU-SY, and HUST datasets (where lower is better), NAC-PW consistently outperformed NAC-2k (e.g., PRONOSTIA: NAC-PW 0.0177 vs. NAC-2k 0.0208; XJTU-SY: NAC-PW 28.01 vs. NAC-2k 37.43; HUST: NAC-PW 30.14 vs. NAC-2k 40.51). The context also states that ""increasing the Top-K interactions enhances accuracy.""

*   **Peak Memory Usage:** NAC-PW demands significantly more memory than NAC-2k. NAC-PW has a peak memory usage of 5042.09 MB, whereas NAC-2k uses only 44.75 MB. This drastic reduction in memory consumption for NAC-2k is attributed to ""decreasing the Top-K selection from PW to k = 2.""

In summary, NAC-PW offers superior accuracy but at the cost of substantially higher peak memory usage, making it less suitable for resource-constrained environments. Conversely, NAC-2k provides a highly memory-efficient solution, albeit with a slight reduction in accuracy compared to NAC-PW.",126,5534,5661,,3,,2026-01-02T20:18:54.267994
"Evaluate the architectural implications of ""hybrid model structures"" that interleave ""linear attention"" with traditional attention, as discussed in the context of SWAA.",System Level / Architectural Understanding,"[Document 1]
Source: sliding_window_attention_annotated_chunks_2k.jsonl
Chunk ID: sliding_window_attention_annotated_chunks_2k_1
Content: LightTransfer (Zhang et al., 2024) is a promising
attempt at adapting existing models to SWA without pretraining, which has the same motivation as
ours. But it may generalize poorly across model
families (see Appendix G).


**2.2** **Linear Attention**


An alternative approach involves reformulating
the attention mechanism entirely to achieve linear, _O_ ( _N_ ), complexity. This includes methods
such as RNN-like linear attention transformers

(Katharopoulos et al., 2020; Peng et al., 2023;
Sun et al., 2023) and structured state-space models
(SSMs) like Mamba (Gu and Dao, 2023). Many
works such as Jamba and Nemotron-Flash(Lieber
et al., 2024; Linsong Chu et al., 2024; Team et al.,
2025; Fu et al., 2025a) interleave linear attention
layers with traditional attention layers to create hybrid model structures. While promising, they represent a fundamental architectural departure from the
standard Transformer, and their performance is generally weaker than traditional Transformer-based

LLMs.



2


**3** **Candidate Methods for SWA**

**Adaptation**


As established, a naive application of SWA leads
to severe long-context performance degradation.
Therefore, we investigate five methods that can potentially facilitate SWA adaptation from distinct
perspectives. However, every method except finetuning has some drawbacks to LLM inference efficiency, as discussed in Appendix A. Therefore,
although these methods are not mutually exclusive,
we should not indiscriminately adopt all of them.
Instead, we must evaluate various combinations to
identify the optimal trade-off between performance
and efficiency.


**3.1** **Full Attention Decode**


This method applies SWA **only** to the prefilling
stage. During the decoding (auto-regressive generation) stage, each token still employs full attention,
allowing access to all previous tokens in the context. The resulting attention mask is depicted in
Figure 1a.
This novel approach, first proposed in our study,
is inspired by human reading comprehension: humans typically scan a passage casually (prefilling)
before thinking deeply to formulate an answer (decoding) for a specific problem. We term this strategy ""reading casually, thinking carefully."" In our
design, the SWA-constrained prefilling stage corresponds to casual reading, while the full-attention
decoding stage enables careful thinking. This analogy also suggests that Chain-of-Thought (CoT)
during decoding may be particularly beneficial, as
extended generation (i.e. ""thinking"") could compensate for the insufficient contextual information
gathered during the prefilling stage.


(a) FA Decode (b) Keep First


Figure 1: (a) Attention mask for FA Decode. SWA is
used for prompt tokens (prefill), and full attention is
used for generated tokens (decode). (b) Attention mask
for SWA combined with Keep First _k_ Tokens.



**3.1.1** **Keep First k Tokens**

Xiao et al. demonstrate that models pretrained with
full attention allocate a disproportionate amount
of attention to the initial tokens (""attention sink""),
and removing the visibility of these tokens causes
performance collapse. Their solution, streaming
attention, involves permanently retaining the attention to these ""sink"" tokens while using SWA, which
successfully maintains the stability of the attention
distribution and the model’s output. Our method
is basically the same: as shown in Figure 1b, any
subsequent token can attend to its local window
**and** the initial _k_ tokens.

Notably, however, our method extends beyond
its original version. Streaming attention operates
only at the KV cache level; specifically, the KV
cache of the sink tokens is retained during decoding, while the prefilling stage is not modified or accelerated at all. In contrast, we directly customize
the Flash-Attention-2 (Dao, 2024) kernel to implement such attention mask, thereby accelerating
prefilling via SWA as well, and eliminating the
need to modify KV cache.


**3.2** **Interleaving Layers**


This method retains full attention on a subset of

layers while applying SWA to the remainder, providing a simple hybrid mechanism to balance the
performance of full attention with the efficiency
of pure SWA. A common strategy involves designating one in every _n_ layers to use full attention
(e.g., layers 0, 2, 4, . . . retain full attention, while all
others use SWA). For example, Gemma-2 (Team,
2024a) uses SWA only for layers [1, 3, 5, . . . ], and
Gemma-3 (Team, 2025a) uses SWA only for layers

[5, 11, 17, ...].
However, for an FA-pretrained model, layers
may exhibit distinct behaviors, suggesting it may
not be optimal to simply assign SWA to layers [1,
3, 5, ...]. Instead, it might be preferable to use
statistical features to select ""lazy"" (mostly focusing
on just recent tokens) layers, as adopted by LightTransfer (Zhang et al., 2024). However, we find
that LightTransfer is not consistently superior in
practice (see Appendix G). Therefore, for simplicity, we still choose the simplest fixed-interval layer
selections in our experiments, such as [0, 2, 4, . . . ]
and [1, 3, 5, ...].


**3.3** **Chain-of-Thought**


Chain-of-Thought (CoT) (Wei et al., 2022) is a
widely used technique for improving model ac


3



```
### Figure - Sliding Window Attention
![Sliding Window Attention](sliding_window_attention.pdf-2-0.png)

**Caption:**  
This figure illustrates the sliding window attention mechanism in a transformer model. The ""Prefill"" region represents the initial input, while the ""Decode"" region shows the subsequent processing.

**Description:**
- The image depicts a grid representing a sequence of data, with a sliding window moving across it.
- The green area indicates the window's current position, highlighting the portion of the sequence being attended to.
- The blue area represents the decoded output based on the attended input.
- The window progressively moves across the sequence, enabling contextual understanding.
```

```
### Figure - Sliding Window Attention
![Sliding Window Attention](sliding_window_attention.pdf-2-1.png)

**Caption:**  
The figure illustrates the sliding window attention mechanism, where attention is retained over a limited window. Attention decreases as the distance from the current position increases.

**Description:**
- The image depicts a grid representing a sequence of tokens.
- The blue squares indicate attention retained across the sliding window.
- The green squares represent attention calculated within the sliding window.
- Attention decreases progressively as the distance 'k' from the current position increases, reflecting a decaying attention mechanism.
- The sliding window size is decreasing as it moves along the sequence.
curacy via reasoning. With the advent of ""thinking"" models, such as DeepSeek-R1 (DeepSeek-AI,
2025a), CoT has evolved from a prompting strategy
to an intrinsic LLM capability. However, whether
CoT has a specific impact in SWA scenarios remains uninvestigated. We explore this by comparing a thinking model with its non-thinking variant,
e.g., Qwen3-4B-Thinking and Qwen3-4B-Instruct,
to verify the effect of CoT on SWA adaptation,
which can produce more notable differences compared to simply adding CoT prompting to the same
model.


**3.4** **Fine-tuning**

---

[Document 2]
Source: sliding_window_attention_annotated_chunks_2k.jsonl
Chunk ID: sliding_window_attention_annotated_chunks_2k_3
Content: **Combinations exhibit strong synergy.** Recipes
that combine multiple methods deliver large gains:


  - **FA Decode + Keep First** _k_ substantially improves over naive SWA (rows 7–9), recovering roughly half to two-thirds of the gap on
the thinking model as _k_ increases. However,
increasing _k_ from 100 to 1000 yields almost
no improvement, indicating that _k_ does not
need to be exceedingly large.


  - **Interleaving Layers + FA Decode** is
markedly stronger (row 13), recovering most
of the gap for the thinking model.


  - **FA Decode + Interleaving Layers + Keep**
**First** _k_ perform best (rows 18). The thinking
model recovers close to 90% of the FA gap
even at 2k window.


**CoT synergizes with FA Decode.** Under recipes
that include FA Decode, the thinking model consistently benefits more than the non-thinking model
(rows 13 and 18), suggesting that CoT synergizes
with FA Decode: preserving global attention at
decoding time enables longer reasoning traces to
capitalize on context processed by SWA, confirming our hypothesis in Section 3.1.


**Sliding window size affects, but is not the deci-**
**sive role.** With FA Decode + Keep First _k_, accuracy improves as the window increases (rows 7,
14, 15), though benefits are modest until 8k. When
added with interleaving FA layers, moving from a
2k to a 4k window is enough to close the remaining
gap on the thinking model (row 22 matches the FA
baseline), indicating that FA Decode and Interleaving are the dominant drivers, and larger windows
mainly smooth residual error.



5


Table 1: Qwen3-4B-Thinking and Qwen3-4B-Instruct results on LongMemEval without SFT


**No.** **window size** **FA layers** **keep first** **FA decode** **Acc think** **Acc non-think**


0 Full [] 0 False **73.0** **62.0**
1 2k [] 0 False 3.2 11.0
2 8k [] 0 False 13.2 19.8


3 2k [] 10 False 16.0 15.6
4 2k [] 0 True 11.8 14.2
5 2k [1, 3, 5, ...] 0 False 13.4 18.4
6 8k [] 0 True 26.2 25.0


7 2k [] 10 True 38.2 20.6
8 2k [] 100 True 50.0 17.8
9 2k [] 1000 True 50.0 20.2
10 2k [0, 2, 4, ...] 10 False 17.0 14.8
11 2k [0, 2, 4, ...] 0 True 32.2 26.0
12 2k [1, 3, 5, ...] 10 False 25.8 36.4
13 2k [1, 3, 5, ...] 0 True 59.2 34.8
14 4k [] 10 True 38.0 24.4
15 8k [] 10 True 49.2 35.2


16 2k [0, 2, 4, ...] 10 True 36.0 17.2
17 2k [1, 3, 5, ...] 10 True 65.0 **53.6**
18 2k [1, 3, 5, ...] 100 True **68.8** 50.6
19 2k [1, 5, 9, ...] 10 True 53.2 31.4
20 2k [1, 9, 17, ...] 10 True 36.4 18.8
21 2k [3, 7, 11, ...] 10 True 54.2 34.6
22 4k [1, 3, 5, ...] 100 True **73.0** **54.2**
23 8k [1, 3, 5, ...] 100 True **71.6** **56.6**



**SWA/FA layer selection has large impacts.** Selecting less layers for FA, e.g. only one fourth or
eighth (row 19, 20), though more efficient, will
significantly decrease the improvement brought by
Interleaving Layers. More importantly, for Qwen34B, configuring _odd-numbered_ layers with full attention is significantly better than _even-numbered_
layers (row 11, 13, 16, 17). However, surprisingly,
this result is reversed for Qwen3-30B-A3B and
Llama3.1-8B-Instruct (row 10 and 11 in Table 4
and 5). This suggests that layer functionalities differ across model families and sizes, necessitating
model-specific layer selection strategies, as discussed in Section 3.2.


Therefore, we answer RQ1: adapting an FA
LLM to SWA is feasible even without any training. But it requires specific combinations of at
least 2 methods, which could be less efficient for
inference.



**5.2** **SWA Adaptation With Fine-tuning**


We next evaluate SWA-aware supervised finetuning, which is expected to provide higher improvement. Table 2 reports LongMemEval_24k
accuracy after SFT under various SWAA configurations. The original full-attention model is also
fine-tuned as a baseline (row 0). Since training is
relatively time-consuming, we only test a representative subset of configurations. Our findings are as
follows:


**Fine-tuning substantially lifts all SWA config-**
**urations.** Comparing all the fine-tuning results
with non-training ones, it is clear that fine-tuning
consistently provides great improvement. However,
simply fine-tuning with naive SWA remains insufficient, only achieving 18.8% and 23.8% accuracy
(row 1).


**FA Decode and Interleaving Layers emerge as**
**dominant components.** After SFT, FA Decode
and Interleaving Layers provide the largest gains.



6


Table 2: Qwen3-4B-Thinking and Qwen3-4B-Instruct results on LongMemEval with SFT


**No.** **window size** **FA layers** **keep first** **FA decode** **Acc think** **Acc non-think**


0 Full [] 0 True **74.6** **63.4**
1 2k [] 0 False 18.8 23.8


2 2k [] 10 False 15.6 /
3 2k [] 0 True 57.9 42.0
4 2k [1, 3, 5, ...] 0 False 63.6 54.6
5 4k [] 0 True 62.6 /


6 2k [] 10 True 56.7 /
7 2k [] 100 True 62.2 42.6
8 2k [1, 3, 5, ...] 0 True **73.2** **58.8**
9 2k [0, 2, 4, ...] 0 True **66.0** /
10 2k [1, 5, 9, ...] 0 True **68.8** 47.0


11 2k [1, 3, 5, ...] 100 True **73.2** **61.4**


Table 3: Recommended SWA adaptation recipes for different needs and scenarios. � means optional.


**Training** ? **Thinking Model?** **Prefer?** **FA Decode** **Interleaving Layers** **Keep First**


No No Any � � �
No Yes Efficiency � � �
No Yes Accuracy � � �


Yes Any Efficiency � � �
Yes Any Accuracy � � �



Using FA Decode alone (row 3) or using Interleaving Layers alone (row 4) both get high accuracy.
And combining both (row 8) further boosts performance to 73 _._ 2 (think) and 58 _._ 8 (non-think), nearly
matching the full-attention SFT baseline in row 0.


**Keep First becomes optional rather than essen-**
**tial.** Before fine-tuning, Keep First is crucial for
stability under SWA. But after SFT, it only provides
minor additional improvement. With FA Decode,
adding _k_ = 100 (row 7) improves over _k_ = 0
(row 3) only 4.5%, and if further combining FA
layers, it almost offers no improvements (row 11
and row 8).


**Effect of sliding window size.** Row 3 and 5
shows that increasing the window from 2k to 4k
with FA Decode improves thinking-model accuracy from 57 _._ 9 to 62 _._ 6. This mirrors the non-SFT
trend that larger windows help, but the dominant
improvements still come from FA Decode and Interleaving Layers.

---

[Document 3]
Source: sliding_window_attention_annotated_chunks_2k.jsonl
Chunk ID: sliding_window_attention_annotated_chunks_2k_0
Content: ## **Sliding Window Attention Adaptation**

Yijiong Yu [a], Jiale Liu [b], Qingyun Wu [b], Huazheng Wang [a], and Ji Pei [c]


aOregon State University, {yuyiji, huazheng.wang}@oregonstate.edu
bPenn State University, {jiale.liu, qingyun.wu}@psu.edu
cDeepSolution, research@deepsolution.chat



**Abstract**


The self-attention mechanism in Transformer
based Large Language Models (LLMs) scales
quadratically with input length, making longcontext inference expensive. Sliding window attention (SWA) reduces this cost to linear complexity, but naively enabling complete
SWA at inference-time for models pretrained
with full attention (FA) causes severe longcontext performance degradation due to training–inference mismatch. This makes us wonder: _Can FA-pretrained LLMs be well adapted_
_to SWA without pretraining?_ We investigate
this by proposing Sliding Window Attention
Adaptation (SWAA), a set of practical recipes
that combine five methods for better adaptation: (i) applying SWA only during prefilling;
(ii) preserving “sink” tokens; (iii) interleaving
FA/SWA layers; (iv) chain-of-thought (CoT);
and (v) fine-tuning. Our experiments show that
SWA adaptation is feasible while non-trivial:
no single method suffices, yet specific synergistic combinations effectively recover the original
long-context performance. We further analyze
the performance-efficiency trade-offs of different SWAA configurations and provide recommended recipes for diverse scenarios. Our code
[is available at github.](https://github.com/yuyijiong/sliding-window-attention-adaptation)


**1** **Introduction**


Transformer-based Large Language Models
(LLMs) (Vaswani et al., 2017) demonstrate
remarkable capabilities, but their self-attention
scales quadratically with the input sequence length,
making long context processing inefficient. Sliding
Window Attention (SWA), the most straightforward and widely adopted sparse attention
pattern, which restricts each token’s attention to a
fixed-size local window, reduces the computational
complexity to linearity, along with some other
benefits (see Appendix A).
To apply SWA to LLMs, typical solutions involve training a model with SWA from scratch, but



are prohibitively costly and cannot match the performance of state-of-the-art full-causal-attention

models like Qwen3 (Team, 2025b), mainly due
to the inability to reproduce pretraining data.
Training-free methods like streaming attention
(Xiao et al., 2024) can stabilize LLM outputs by retaining “sink tokens” while applying SWA, which
greatly improve efficiency but inevitably suffer
from severe long-context performance degradation
possibly due to the inaccessibility of distant tokens’
information (Xiao, 2025). This motivates a critical,
unexplored question: _Can a full-attention model_
_be adapted to sliding window attention at low cost_
_while maintaining long-context performance?_
We answer Yes to this question by proposing
Sliding Window Attention Adaptation(SWAA), a
set of recipes for adapting FA-pretrained models to
SWA, which requires neither costly pretraining nor
modifications to the standard Transformer architecture. Specifically, it systematically combines five
practical and composable methods:


1. **Full Attention (FA) Decode** : applying SWA
only during the prefilling stage while switching back to full attention for decoding.


2. **Keep First** _k_ **Tokens** : explicitly preserving
attention to the first _k_ “sink” tokens.


3. **Interleaving FA/SWA layers** : mix fullattention and SWA layers (e.g., assigning
SWA to half layers).


4. **Chain-of-Thought (CoT)** : enforcing an explicit ""thinking"" process during decoding.


5. **Fine-tuning with SWA** : lightweight SWAaware supervised fine-tuning on long-context
data.


Among these, FA Decode is a novel method we
introduce. Keep First _k_ Tokens and FA/SWA Interleaving have been proven effective in prior work



1


(Xiao et al., 2024; Team, 2024a; Zhang et al., 2024),
while CoT and fine-tuning are common LLM techniques. However, how these methods should be
combined to be actually effective for SWA adaptation remains unexplored.
Therefore, in our experiments, we evaluate
SWAA on Qwen3 (Team, 2025b) and Llama3.1
(Team, 2024b) across several long-context benchmarks, measuring both performance and efficiency
under a wide range of SWAA recipes. First, we
find that each method makes a distinct contribution,
but no single ingredient suffices to make SWA competitive with full attention. Second, we show that
specific synergistic combinations of methods can
recover a large fraction of the original long-context
performance. Third, we analyze the performanceefficiency trade-offs of different SWAA recipes and
identify some recommended configurations suitable for different deployment scenarios.
Rather than proposing a single globally optimal
configuration, we view SWAA as a flexible toolkit
of practical recipes: practitioners can select SWAA
recipes that match their accuracy and efficiency
constraints, or compose their own SWA adaptation
strategies by combining the available ingredients.
Our key contributions are:


1. We perform the first systematic study on adapting FA-pretrained LLMs to SWA without pretraining, revealing novel insights about how
SWA impacts LLMs and providing a foundation for future research in efficient sparse
attention.


2. We propose SWAA, a set of practical
SWA adaptation recipes that offer a robust
performance-efficiency balance for various
use cases, accelerating LLM inference from
the bottom level.


3. We implement our methods with FlashAttention (Dao, 2024) and vLLM (Kwon
et al., 2023), making it plug-and-play and userfriendly for practical deployment.


**2** **Related Works**


The _O_ ( _N_ [2] ) complexity of self-attention in Transformers (Vaswani et al., 2017) has spurred a wide
field of research about more efficient language
model architectures. Among the two most popular technological routes are sparse attention and
linear attention.



**2.1** **Sparse Attention**


Our work falls in this category. Sliding Window
Attention (SWA) represents the most basic form
of local sparse attention, yet its performance is inherently limited. Therefore, model architectures
such as Longformer (Beltagy et al., 2020), BigBird
(Zaheer et al., 2020), and RATTENTION (Wang
et al., 2025) combine local SWA on most tokens
with special global attention on specific tokens to
create a more powerful, albeit still sparse, pattern.
Popular LLMs like Gemma2 (Team, 2024a) adopt
SWA in half of their layers to balance the efficiency
of SWA and peformance of FA. Sliding Window
Attention Training (SWAT) (Fu et al., 2025b) introduces architectural changes, such as sigmoid
activation and balanced position embeddings, to
stabilize SWA performance. More advanced methods like Deepseek-sparse-attention (Yuan et al.,
2025; DeepSeek-AI, 2025b), although achieving
excellent quality, involve more complicated implementation and optimization due to semantic-aware
attention operations (e.g., selecting the most important tokens based on attention weights). Regardless,
almost all of the above methods require pretraining with a specific sparse pattern, which is costly
and fails to leverage the advantages of existing pretrained models.


LightTransfer (Zhang et al., 2024) is a promising
attempt at adapting existing models to SWA without pretraining, which has the same motivation as
ours. But it may generalize poorly across model
families (see Appendix G).


**2.2** **Linear Attention**


An alternative approach involves reformulating
the attention mechanism entirely to achieve linear, _O_ ( _N_ ), complexity. This includes methods
such as RNN-like linear attention transformers
","Hybrid model structures that interleave linear attention layers with traditional attention layers, as seen in works like Jamba and Nemotron-Flash, represent a fundamental architectural departure from the standard Transformer. A key architectural implication is that their performance is generally weaker than traditional Transformer-based Large Language Models (LLMs).

It's important to note that the context discusses these hybrid linear/traditional attention models as an ""alternative approach"" to achieve linear complexity, distinct from the Sliding Window Attention Adaptation (SWAA) methods, which focus on adapting full-attention pretrained models to sliding window attention. SWAA itself includes a method of ""interleaving FA/SWA layers,"" which is different from interleaving linear attention with traditional attention.",125,5267,5393,,3,,2026-01-02T20:18:59.663000
"How does the ""interpretable functional organization"" of attention heads in VLMs challenge the view of these models as ""black boxes""?",System Level / Architectural Understanding,"[Document 1]
Source: attention_functional_roles_raw_chunks_2k.jsonl
Chunk ID: attention_functional_roles_raw_chunks_2k_0
Content: ## - INVESTIGATING THE FUNCTIONAL ROLES OF ATTEN ### TION HEADS IN VISION LANGUAGE MODELS: EVI DENCE FOR REASONING MODULES

**Yanbei Jiang** **[1]** _[∗]_ **Xueqi Ma** **[1]** _[∗]_ **Shu Liu** **[1]** **Sarah Monazam Erfani** **[1]**

**Tongliang Liu** **[2]** **James Bailey** **[1]** **Jey Han Lau** **[1]** **Krista A. Ehinger** **[1]**

1The University of Melbourne 2The University of Sydney
_{_ yanbeij, xueqim, shu6 _}_ @student.unimelb.edu.au
_{_ sarah.erfani, baileyj, jeyhan.lau, kris.ehinger _}_ @unimelb.edu.au
tongliang.liu@sydney.edu.au


ABSTRACT


Despite excelling on multimodal benchmarks, vision–language models (VLMs)
largely remain a black box. In this paper, we propose a novel interpretability framework to systematically analyze the internal mechanisms of VLMs, focusing on the functional roles of attention heads in multimodal reasoning. To
this end, we introduce CogVision, a dataset that decomposes complex multimodal questions into step-by-step subquestions designed to simulate human reasoning through a chain-of-thought paradigm, with each subquestion associated
with specific receptive or cognitive functions such as high-level visual reception and inference. Using a probing-based methodology, we identify attention
heads that specialize in these functions and characterize them as functional heads.
Our analysis across diverse VLM families reveals that these functional heads
are universally sparse, vary in number and distribution across functions, and
mediate interactions and hierarchical organization. Furthermore, intervention
experiments demonstrate their critical role in multimodal reasoning: removing
functional heads leads to performance degradation, while emphasizing them enhances accuracy. These findings provide new insights into the cognitive organization of VLMs and suggest promising directions for designing models with more
human-aligned perceptual and reasoning abilities. Code and data are available at
https://github.com/YanbeiJiang/CogVision.


1 INTRODUCTION


Large Vision-Language Models (VLMs) (Zhu et al., 2023; Liu et al., 2023; Lu et al., 2024a) have
demonstrated remarkable success across diverse multimodal tasks, ranging from image captioning
to visual question answering. Although VLMs can solve mathematical reasoning problems with
visual context (as shown in Fig. 1), their internal mechanisms remain poorly understood.


For humans, solving such complex problems (illustrated in Fig. 1) typically requires the collaboration of vision and language, engaging multiple brain regions (Barsalou, 2014): the occipital lobe
for visual reception, capturing and processing the content of the images; the temporal lobe supports
long-term memory and the recall of relevant factual knowledge, such as chemical concentration formulas (Wheeler et al., 1997); and the parietal and prefrontal cortices are involved in higher-order
reasoning (Hubbard et al., 2005), to produce the correct answer.


Recent research in interpretability has begun probing the internal organization of large language
models (LLMs), revealing specialized attention heads for specific functions (Wu et al.; Li et al.,
2023a; Zheng et al.). In the case of VLMs, several studies (Kang et al., 2025; Bi et al., 2025) have
identified sparse attention heads with special functional roles in tasks such as grounding. However,
studying VLMs in complex, multi-step reasoning scenarios remains underexplored. A deeper under

_∗_ Both authors contributed equally to this research.


1


![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-1-0.png)



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-1-1.png)

![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-1-2.png)



Decision making



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-1-3.png)

High-level Vision Reception



Language Knowledge Recall



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-1-5.png)



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-1-6.png)



Math Reasoning



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-1-7.png)



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-1-4.png)

Occipital lobe Temporal lobe Parietal lobe Frontal lobe









By dividing the number of Solution A has a higher

particles by the solvent concentration of A, Solution A.









![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-1-8.png)

![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-1-11.png)

Figure 1: To answer a complex question, the human brain engages multiple regions, each performing
distinct cognitive functions. We investigate whether specific attention heads in large vision language
models play analogous functional roles in generating responses.


standing of whether such specialized components exist, how they are organized, and what functional
roles they play in multimodal reasoning is therefore critical.


In this paper, we propose a novel interpretability framework for systematically analyzing the functional roles of attention heads-parallel units in transformer models that compute token-to-token
attention-an important component in VLMs, with a focus on their contributions to reception (perceptual processing) and cognition. To facilitate this, we introduce CogVision, a dataset that bridges the
gap between model analysis and human cognitive processes. CogVision decomposes multimodal
queries into step-by-step subquestions, each aligned with specific cognitive functions (such as math
reasoning, decision-masking), thus enabling a fine-grained evaluation of reasoning aligned with the
chain-of-thought (CoT) paradigm. Leveraging CogVision, we develop a probing method to identify and characterize attention heads responsible for distinct cognitive operations across vision and
language within the transformer architecture.


We conduct extensive experiments on three major VLM families, including Intern (Zhu et al., 2025),
Qwen (Yang et al., 2025), and Gemma (Team et al., 2025) with different model scales. Our results
reveal the existence of cognitive heads that consistently exhibit **universal**, **sparse**, and **intrinsic**
properties across architectures. Further analysis of the correlations among these functional heads
reveals **cross-function interactions**, where a single head may support multiple functions or modalities, and uncovers a **hierarchical structure** in which lower-level functional heads modulate higherlevel ones, showing the complexity of neural networks (Barsalou, 2014; Ono et al., 2022).


Furthermore, we validate the functional importance of these heads by showing that their removal
degrades performance on complex tasks and leads to specific error patterns, while their enhancement improves reasoning capabilities. Our findings provide compelling evidence that these attention
heads play a critical role in multimodal reasoning. This insight not only deepens our understanding of the internal organization of VLMs but also suggests potential avenues for designing more
interpretable and cognitive-inspired multimodal AI systems.


2 COGVISION


In this section, we present our dataset Cognitive Vision (CogVision) that contains cognitive process
in multimodal reasoning. CogVision contains 1,409 main questions and 5,744 subquestions. Each


2

---

[Document 2]
Source: attention_functional_roles_raw_chunks_2k.jsonl
Chunk ID: attention_functional_roles_raw_chunks_2k_6
Content: chain-of-thought (CoT) paradigm has been argued to mirror step-by-step human reasoning, leading to improved problem-solving performance. These findings motivate the design of interpretable,
functionally specialized modules in artificial networks, bridging insights from neuroscience with
advances in multimodal reasoning.


**Attention Heads in Vision–Language Models.** A growing body of interpretability research has
revealed that attention heads in LLMs exhibit functional specialization, such as pattern induction,
truthfulness, information retrieval, and safety alignment (Olsson et al., 2022; Li et al., 2023a; Wu
et al.; Zhou et al., 2024; Zheng et al.). Ma et al. further investigates the diverse cognitive roles that
attention heads play in supporting LLM reasoning.


In the multimodal domain, recent works (Li et al., 2020) have begun to explore the internal mechanisms of Vision–Language Models (VLMs). Studies have shown that certain sparse attention heads
play distinct roles in visual grounding, enabling alignment between textual tokens and image regions without additional fine-tuning (Kang et al., 2025; Bi et al., 2025). Similarly, probing studies
on multimodal pre-trained models (e.g., ViLBERT, LXMERT, UNITER) demonstrate that subsets
of attention heads encode cross-modal interactions and semantic alignment between vision and language (Cao et al., 2020). These works highlight the existence of specialized heads in VLMs but
largely focus on perception-oriented tasks such as grounding or alignment. In contrast, we investigate functionally specialized heads under more complex reasoning settings by aligning attention
head behavior with human cognitive functions.


6 CONCLUSION


We propose an interpretability framework that links attention heads in VLMs to human perceptual
and cognitive functions involved in multimodal reasoning. To enable this, we introduce CogVision, a cognitively grounded dataset that decomposes complex multimodal questions into functional
reasoning steps, and apply probing-based analyses to identify specialized heads supporting these
functions. Our study across diverse VLM families reveals that functional heads are sparse, universal, and intrinsic properties of the models, while varying in number, distribution, and hierarchical
organization. Moreover, we find that certain heads exhibit cross-modal interactions. Intervention
experiments further reveal their causal importance. Our insights into the functional organization
of attention mechanisms provide a foundation for developing more interpretable, robust, and cognitively inspired vision-language models. While our work provides a first step toward exploring
potential similarities between the cognitive processes of VLMs and those of the human brain, we do
not claim complete alignment, nor do we equate observations and analyses of attention heads with
the full scope of human reasoning.


10


**Limitations** While our study provides an initial framework for analyzing attention heads in VLMs,
several limitations remain. We focus on eight predefined cognitive functions, which may not cover
the full spectrum of LLM capabilities; future work could expand this taxonomy to include finergrained or emergent functions. Additionally, we concentrate on attention heads, leaving other components such as MLPs unexplored. Further exploring advanced probing methods and extending the
analysis to other model components, could provide further understandings.


REFERENCES


Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical
report. _arXiv preprint arXiv:2303.08774_, 2023.


Guillaume Alain and Yoshua Bengio. Understanding intermediate layers using linear classifier
probes. _arXiv preprint arXiv:1610.01644_, 2016.


John R Anderson. _Rules of the mind_ . Psychology Press, 2014.


Lawrence W Barsalou. _Cognitive psychology: An overview for cognitive scientists_ . Psychology
Press, 2014.


Yonatan Belinkov. Probing classifiers: Promises, shortcomings, and advances. _Computational_
_Linguistics_, 48(1):207–219, 2022.


Jing Bi, Junjia Guo, Yunlong Tang, Lianggong Bruce Wen, Zhang Liu, Bingjie Wang, and Chenliang
Xu. Unveiling visual perception in language models: An attention head analysis approach. In
_Proceedings of the Computer Vision and Pattern Recognition Conference_, pp. 4135–4144, 2025.


Charles F Cadieu, Ha Hong, Daniel LK Yamins, Nicolas Pinto, Diego Ardila, Ethan A Solomon,
Najib J Majaj, and James J DiCarlo. Deep neural networks rival the representation of primate it
cortex for core visual object recognition. _PLoS computational biology_, 10(12):e1003963, 2014.


Jize Cao, Zhe Gan, Yu Cheng, Licheng Yu, Yen-Chun Chen, and Jingjing Liu. Behind the scene:
Revealing the secrets of pre-trained vision-and-language models. In _European Conference on_
_Computer Vision_, pp. 565–580. Springer, 2020.


Charlotte Caucheteux, Alexandre Gramfort, and Jean-R´emi King. Deep language algorithms predict
semantic comprehension from brain activity. _Scientific reports_, 12(1):16327, 2022.


Lin Chin-Yew. Rouge: A package for automatic evaluation of summaries. In _Proceedings of the_
_Workshop on Text Summarization Branches Out, 2004_, 2004.


Adele Diamond. Executive functions. _Annual review of psychology_, 64(1):135–168, 2013.


Edward M Hubbard, Manuela Piazza, Philippe Pinel, and Stanislas Dehaene. Interactions between
number and space in parietal cortex. _Nature reviews neuroscience_, 6(6):435–448, 2005.


Seil Kang, Jinyeong Kim, Junhyeok Kim, and Seong Jae Hwang. Your large vision-language model
only needs a few attention heads for visual grounding. In _Proceedings of the Computer Vision_
_and Pattern Recognition Conference_, pp. 9339–9350, 2025.


Kenneth Li, Oam Patel, Fernanda Vi´egas, Hanspeter Pfister, and Martin Wattenberg. Inference-time
intervention: Eliciting truthful answers from a language model. _Advances in Neural Information_
_Processing Systems_, 36:41451–41530, 2023a.


Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. What does bert with
vision look at? In _Proceedings of the 58th annual meeting of the association for computational_
_linguistics_, pp. 5265–5275, 2020.


Yijiang Li, Qingying Gao, Tianwei Zhao, Bingyang Wang, Haoran Sun, Haiyun Lyu, Robert D
Hawkins, Nuno Vasconcelos, Tal Golan, Dezhi Luo, et al. Core knowledge deficits in multimodal language models. _arXiv preprint arXiv:2410.10855_, 2024.


11


Zhuowan Li, Xingrui Wang, Elias Stengel-Eskin, Adam Kortylewski, Wufei Ma, Benjamin
Van Durme, and Alan L Yuille. Super-clevr: A virtual benchmark to diagnose domain robustness in visual reasoning. In _Proceedings of the IEEE/CVF conference on computer vision and_
_pattern recognition_, pp. 14963–14973, 2023b.


Adam Dahlgren Lindstr¨om and Savitha Sam Abraham. Clevr-math: A dataset for compositional
language, visual and mathematical reasoning. _arXiv preprint arXiv:2208.05358_, 2022.


Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. _Advances_
_in neural information processing systems_, 36:34892–34916, 2023.


Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren,
Zhuoshu Li, Hao Yang, et al. Deepseek-vl: towards real-world vision-language understanding.
_arXiv preprint arXiv:2403.05525_, 2024a.

---

[Document 3]
Source: attention_functional_roles_raw_chunks_2k.jsonl
Chunk ID: attention_functional_roles_raw_chunks_2k_1
Content: Furthermore, we validate the functional importance of these heads by showing that their removal
degrades performance on complex tasks and leads to specific error patterns, while their enhancement improves reasoning capabilities. Our findings provide compelling evidence that these attention
heads play a critical role in multimodal reasoning. This insight not only deepens our understanding of the internal organization of VLMs but also suggests potential avenues for designing more
interpretable and cognitive-inspired multimodal AI systems.


2 COGVISION


In this section, we present our dataset Cognitive Vision (CogVision) that contains cognitive process
in multimodal reasoning. CogVision contains 1,409 main questions and 5,744 subquestions. Each


2


example comprises a main question and answer, its subquestions and subanswers and an annotation
specifying the receptive or cognitive function required for each subquestion.


2.1 COGNITIVE FUNCTIONS


To systematically capture the cognitive processes involved in complex reasoning tasks, we consider eight functions related to complex multimodal reasoning. These functions are inspired by
established frameworks in cognitive science (Anderson, 2014; Diamond, 2013), which highlight the
importance of perception, working memory, and reasoning in human cognition.


The functions related to vision include:


    - **Low-level Visual Reception** : Recognizing basic visual features such as color, shape, size,
position, motion.

    - **High-level Visual Reception** : Integrating visual information to recognize objects, patterns,
and scene structure.

    - **Visual Knowledge Recall** : Applying long-term visual knowledge related to visual concepts and their properties.


The functions related to language include:


    - **Language Information Extraction and Understanding** : locating and understanding relevant information from an external source or prior context.

    - **Language Knowledge Recall** : Accessing domain-specific knowledge without visual input.

    - **Math Reasoning** : Performing counting, arithmetic, comparison, and logic-based operations.

    - **Inference** : deriving implicit information that is not directly stated.

    - **Decision-Making** : Selecting the best outcome among alternatives based on reasoning.


This categorization reflects a natural progression from basic information processing to complex cognitive integration. Both the human brain and VLMs encompass a wide range of functional modules.
Our focus in this work is specifically on reasoning-related cognitive functions. By identifying and
organizing these eight core reasoning functions, we can more clearly examine how VLMs handle
different types of thinking steps, in a way that is both systematic and easy to interpret. Detail descriptions of each function and examples in CogVision can be found in Appendix A.1.


2.2 DATA COLLECTIONS


Based on our categorization of reasoning functions, we sampled 2000 diverse questions from existing commonly used visual reasoning benchmarks, selecting 200 examples each from ClevrMath (Lindstr¨om & Abraham, 2022), MathVision (Wang et al., 2024a), MathVista (Lu et al., 2024b),
MMMU (Yue et al., 2024), ScienceQA (Saikh et al., 2022), OKVQA (Marino et al., 2019) (400
examples), VCR (Zellers et al., 2019), VisuLogic (Xu et al., 2025) and Super-Clevr (Li et al.,
2023b) datasets. These datasets span a range of reasoning problems, including logical, mathematical, and commonsense reasoning. Using the chain-of-thought (CoT) paradigm, we prompted
GPT-4.1 (Achiam et al., 2023) to decompose each main question (with its final answer) into subquestions, each targeting a subanswer and a specific function. The prompt encourages structured,
step-by-step reasoning, ensuring that each subquestion is clear, answerable, and sequentially dependent. This process yields a set of subquestion–answer–function (subQAF) triples for each QA pair:
subQAFs = _{_ ( _qi, ai, fi_ ) _}_ _[k]_ _i_ =1 [, where each contains a subquestion] _[ q][i]_ [, its concise answer] _[ a][i]_ [, and the]
corresponding function label _fi_ . The prompt for generating subquestions are list in Appendix A.6.


2.3 DATA FILTERING AND ANNOTATION


Recent advances enable leveraging large pre-trained models for dataset construction, thanks to their
reasoning capabilities and ability to generate high-quality annotations at scale (Wang et al., 2024b).
While our dataset is automatically generated using a VLM to minimize manual effort, we employ a
rigorous two-stage human verification pipeline to ensure data quality and mitigate hallucination. In
the first stage, three expert annotator independently evaluate whether each subquestion is logically
structured and consistent with natural human reasoning. QA pairs with incoherent or inconsistent


3


decompositions are moved out. In the second stage, annotators verify and, if necessary, relabel the
cognitive function associated with each subquestion to ensure accurate alignment with the intended
mental process. Subanswers are further cross-checked with GPT-o3 (OpenAI, 2024) and adjudicated
by humans in cases of disagreement (details in Appendix A.5). This multi-step verification ensures
that each retained subQAF triplet represents a coherent, interpretable reasoning step grounded in
core cognitive functions. After this refinement, our final dataset comprises 1,409 main QA pairs
and 5,744 validated subQAF triplets. The detailed statistics of the dataset including the number of
triplet in training and testing set and distributions of each function can be found in Appendix A.3.
We analyze in-group subquestion diversity, and the results in the Appendix A.2 show that the eight
functions exhibit broad and overlapping distributions in phrasing patterns and token lengths, indicating no systematic surface-form differences.


3 DETECTION OF COGNITIVE FUNCTIONS


Using the CogVision dataset, we adopt a probing-based framework (Alain & Bengio, 2016; Belinkov, 2022; Tenney et al., 2019) to identify which attention heads in VLMs are associated with
specific functions in reasoning process. Specifically, for each functional annotated subquestion, we
extract head activations (see Subsection 3.1), train classifiers and compute accuracies to identify
contributing heads (see Subsection 3.2). Unlike prior work focusing on a single label, our formulation captures many-to-many relationships between heads and cognitive functions, enabling a more
nuanced analysis of functional specialization and overlap within the model.


3.1 HEAD FEATURE EXTRACTION


Given a large VLM _M_, we generate an answer _a_ _[M]_ _i_ for each subquestion _qi_ derived from a main
question _Qi_ . To support coherent multi-step reasoning, we include preceding subquestions and
their answers as contextual input, emulating the incremental reasoning process observed in human
cognition.


During inference, each input token is first mapped into an embedding and then propagated through
the transformer’s layers. At each layer, attention and feedforward operations update the residual
stream, which is ultimately decoded into token predictions. For each generated token _i_, we extract
attention head outputs _Xi_ = _{x_ _[m]_ _l_ _| l ∈_ 1 _, . . ., Nl, m ∈_ 1 _, . . ., Nh}_ across all layers, where _x_ _[m]_ _l_
denotes the value vector from the _m_ -th head in layer _l_ projected into the residual stream, with _Nh_
the number of heads per layer and _Nl_ the total number of layers.


Let _Nt_ denote the number of tokens in the generated answer _a_ _[M]_ _i_ [. To isolate semantically informative]
content relevant to reasoning, we select the top- _k_ most important tokens, determined by prompting
Qwen3-30B LLM (Yang et al., 2025), yielding an index set _Ik_ with _|Ik|_ = _k_ . For each index
_j ∈Ik_, we extract the corresponding attention head activations _Xj_, and compute the averaged
activation feature for the _m_ -th head in layer _l_ as ¯ _x_ _[m]_ _l_ = _k_ [1] � _j ∈Ikxml_ [. This results in a full set of]
","The ""interpretable functional organization"" of attention heads in Vision-Language Models (VLMs) challenges the view of these models as ""black boxes"" by:

1.  **Revealing Specialized Roles:** The research identifies specific attention heads that specialize in distinct receptive or cognitive functions, such as high-level visual reception, language knowledge recall, math reasoning, inference, and decision-making. This moves beyond a monolithic view of the model to show internal components with identifiable purposes.
2.  **Demonstrating Internal Structure:** Analysis reveals that these functional heads are universally sparse, vary in number and distribution, exhibit cross-function interactions, and form a hierarchical structure. This provides insights into the internal organization, rather than just observing input-output behavior.
3.  **Validating Functional Importance:** Intervention experiments show that removing these functional heads degrades performance on complex tasks, while emphasizing them enhances reasoning capabilities. This provides compelling evidence that these heads play a critical and understandable role in the model's operations.
4.  **Deepening Understanding:** By systematically analyzing and linking attention heads to human-aligned cognitive functions, the research provides new insights into the cognitive organization and internal mechanisms of VLMs, thereby deepening our understanding and suggesting pathways for designing more interpretable and cognitive-inspired AI systems.",126,7504,7631,,3,,2026-01-02T20:19:07.296304
"Discuss the role of ""universal approximation"" guarantees in validating the architectural design of NAC.",System Level / Architectural Understanding,"[Document 1]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_4
Content: **2.3. NAC as Universal Approximator**


We now establish the universal approximation capability of
NAC by extending the classical Universal Approximation
Theorem (UAT) (Nishijima, 2021) to the proposed mechanism. For brevity, we consider a network with a single
NAC layer processing fixed-dimensional inputs, though the
argument generalizes to sequences.


**Theorem 2** (Universal Approximation by NAC) **.** _Let K ⊂_
R _[n]_ _be a compact set and f_ : _K →_ R _[m]_ _be a continuous_
_function. For any ϵ >_ 0 _, there exists a neural network_
_consisting of a single NAC layer, with sufficiently large_
_model dimension dmodel, number of heads H, sparsity s,_
_and nonlinear activations, such that the network’s output_
_g_ : R _[n]_ _→_ R _[m]_ _satisfies_


sup _∥f_ ( _x_ ) _−_ _g_ ( _x_ ) _∥_ _< ϵ._ (20)
_x∈K_


_The proof is provided in Appendix B.3._


**3. Evaluation**


We evaluate the proposed architecture against a range of
baselines, including (DT & CT) RNN, (DT & CT) attention,
and multiple NAC ablation configurations. Experiments
are conducted across diverse domains, including irregular
time-series modeling, lane keeping of autonomous vehicles,
and Industry 4.0 prognostics. All results are obtained using 5-fold cross-validation, where models are trained using
BPTT (see Appendix C.2) on each fold and evaluated across
all folds. We report the mean ( _µ_ ) and standard deviation ( _σ_ )
to capture variability and quantify uncertainty in the predictions. Table 1 provides results for all experiments, and the
details of the baselines, ablation, environment utilized, the
data curation and preprocessing, and neural network architectures for all experiments are provided in the Appendix
D.3.


**3.1. Irregular Time-series**


We evaluate the proposed architecture on two irregular timeseries datasets: (i) Event-based MNIST; and (ii) Person
Activity Recognition (PAR).



For finite _t_, the exponential factor (1 _−_ _e_ _[−][ω][τ][ t]_ ) regulates the
buildup of attention, giving _ωτ_ a temporal gating role. Normalizing across all keys via _softmax_ yields attention weights
_αt_ = softmax( _at_ ), defining a valid probability distribution
where _ϕ_ amplifies or suppresses content alignments, and _ωτ_
shapes both the speed and saturation of these preferences.
As _t →∞_, the trajectory converges to the steady state


_a_ _[∗]_ _t_ [=] _[ϕ]_ _≈_ _[q][⊤][k]_ _,_ (18)

_ωτ_ ~~_√_~~ _dk_


which is analogous to scaled-dot attention under specific
parameterization when the backbone _NN_ backbone is configured as a linear projection such that _ϕ_ ( **u** ) = _q_ _[⊤]_ _k_ and
_ωτ_ ( _u_ ) = _[√]_ _dk_ (e.g., by setting NCP weights to emulate
bilinear forms and disabling nonlinearities). In general, the
nonlinear backbone allows for more expressive similarities,
with the approximation holding when trained to mimic dot
products.
**Attention output:** Finally, the attention output is computed
by integrating the attention weights with the value matrix:


NAC( _q, k, v_ ) = _αtvtdt_ (19)
� _T_


In practice, the integration is approximated using a Riemannstyle approach, where the weighted elements are computed
by multiplying each _vt_ with its corresponding _αt_ . These are
then summed and multiplied by a fixed pseudo-time step
_δt_, chosen as a scalar (typically between 0.5–1.0) hyperparameter during layer initialization. This yields a continuous
analogue of standard weighted sums, giving finer resolution
of the attention trajectory without altering the underlying
values. Sensitivity to attention output w.r.t _δt_ is visualized
in Appendix D.2.


2.2.1. EXTENSION TO MULTI-HEAD


To scale this mechanism to multi-head attention, we project
the input sequence into _H_ independent subspaces (heads)
of dimension _d_ model _/H_, yielding query, key, and value tensors ( _q_ [(] _[h]_ [)] _, k_ [(] _[h]_ [)] _, v_ [(] _[h]_ [)] ) for _h ∈{_ 1 _, . . ., H}_ . For each head,
pairwise logits are computed according to Eqns. 2,3 or 18,



5


**Neuronal Attention Circuit (NAC) for Representation Learning**


_Figure 2._ Illustration of the architecture of **(a)** Neuronal Attention Circuit mechanism ; **(b)** Multi-Head Extension



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/neuronal_attention_circuits.pdf-5-0.png)

**Event-based MNIST:** Event-based MNIST is the trans
formation of the widely recognized MNIST dataset with
irregular sampling added originally proposed in (Lechner
& Hasani, 2022). The transformation was done in two
steps: (i) flattening each 28×28 image into a time series
of length 784, and (ii) encoding the binary time series into
an event-based format by collapsing consecutive identical
values (e.g., 1,1,1,1 → (1, t=4)). This representation requires models to handle temporal dependencies effectively.
NAC-PW achieved first place with an accuracy of 96.64%,
followed by NAC-Exact/05s/8k at 96.12%. GRU-ODE and
ContiFormer ranked third with 96.04%.

**Person Activity Recognition (PAR):** We employed the
Localized Person Activity dataset from UC Irvine (Vidulin
et al., 2010). The dataset contains data from five participants,
each equipped with inertial measurement sensors sampled
every 211 ms. The goal of this experiment is to predict a
person’s activity from a set of predefined actions, making it
a classification task. All models performed well on this task,
with NAC-PW achieving 89.15% accuracy and taking first
place. NAC-Exact/05s/8k and GRU-ODE ranked second
with 89.01% accuracy, while NAC-02s ranked third with
88.84% mean accuracy.


**3.2. Lane-Keeping of Autonomous Vehicles**


Lane keeping in autonomous vehicles (AVs) is a fundamental problem in robotics and AI. Early works (Tiang et al.,
2018; Park et al., 2021) primarily emphasized accuracy,
often relying on large models. More recent research (Lechner et al., 2020; Razzaq & Hongwei, 2023) has shifted toward designing compact architectures suitable for resourceconstrained devices. The goal of this experiment is to create
a long causal structure between the road’s horizon and the

---

[Document 2]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_3
Content: Scores: _S ←_ _Q · K_ _[⊤]_

Effective Top- _K_ : _K_ eff _←_ min( _K, Tk_ )
Indices: _I_ topk _←_ top ~~k~~ ( _S, K_ eff)
Gather: _K_ selected _←_ gather( _K, I_ topk) _∈_ R _[B][×][H][×][T][q]_ _[×][K]_ [eff] _[×][D]_

Tiled: _Q_ tiled _←_ tile( _Q, K_ eff) _∈_ R _[B][×][H][×][T][q]_ _[×][K]_ [eff] _[×][D]_

Concatenate: _U_ topk _←_ [ _Q_ tiled; _K_ selected ] _∈_ R _[B][×][H][×][T][q]_ _[×][K]_ [eff] _[×]_ [2] _[D]_

**return** _U_ topk


**2.2. Designing the Neural Network**


We now outline the design of a neural network layer guided
by the preceding analysis. The process involves five steps:
(i) repurposing NCPs; (ii) input curation; (iii) construction
of the time vector ( _t_ ); (iv) computing attention logits and
weights; and (v) generating the attention output. Figure 2
provides a graphical overview of NAC.
**Repurposing NCPs:** We repurpose the NCPs framework
by converting its fixed, biologically derived wiring (see Figure 1(a)) into a flexible recurrent architecture that allows
configurable input–output mappings. Instead of enforcing
a static connectome, our approach exposes adjacency matrices as modifiable structures defining sparse input and
recurrent connections. This enables selective information

routing across neuron groups while retaining the original circuit topology. Decoupling wiring specifications from model
instantiation allows dynamic connectivity adjustments to
accommodate different input modalities without full retraining. Algorithm 1 summarizes the steps for repurposing the
NCPs wiring mechanism. Key features include group-wise
masking for neuron isolation, adaptive remapping of inputs
and outputs for task-specific adaptation, and tunable sparsity
_s_ to balance expressiveness and efficiency.
In our implementation, the sensory neuron gate ( _NN_ sensory)
projects the _q_, _k_, and _v_ representations (see Figure 1(b)).
This enables sensory neurons to maintain structured, contextaware representations rather than collapsing inputs into fully



connected layers. As a result, the network preserves locality
and modularity, which improves information routing.


_NN_ sensory = NCPCell( _G_ input = [ _Ns_ ] _, G_ output = [ _Ns_ ] _,_

_D_ = [ _Ni, Nc, Nm_ ] _, s_ )
(13)
The inter-to-motor pathways form a backbone network
( _NN_ backbone) with branches that compute _ϕ_ and _ωτ_ (see
Figure 1(c)). Instead of learning _ϕ_ and _ωτ_ independently,
this backbone allows the model to learn shared representations, enabling multiple benefits: (i) separate head layers
enable the system to capture temporal and structural dependencies independently; (ii) accelerates convergence during
training.


_NN_ backbone = NCPCell( _G_ input = [ _Ni_ ] _, G_ output = [ _Nm_ ] _,_

_D_ = [ _Ns_ ] _, s_ )
(14)
The output heads are defined as:


_ϕ_ = _σ_ ( _NN_ backbone( **u** )) (15)

_ωτ_ = softplus( _NN_ backbone( **u** )) + _ε,_ _ε >_ 0 (16)


Here, _ϕ_ serves as a _content–target gate_ head, where the
sigmoid function _σ_ ( _·_ ) determines the target signal strength.
In contrast, _ωτ_ is a strictly positive _time–constant gate_ head
that controls the rate of convergence and the steady-state
amplitude. Conceptually, this parallels recurrent gating: _ϕ_
regulates _what_ content to emphasize, while _ωτ_ governs _how_
_quickly_ and _to what extent_ it is expressed.
**Input Curation:** We experimented with different
strategies for constructing query–key inputs. Initially, we implemented full pairwise concatenation,
where queries _Q ∈_ R _[B][×][H][×][T][q][×][D]_ are combined with
all keys _K_ _∈_ R _[B][×][H][×][T][k][×][D]_ to form a joint tensor
_U ∈_ R _[B][×][H][×][T][q][×][T][k][×]_ [2] _[D]_ . While this preserved complete
feature information and enabled expressive, learnable
similarity functions, it was memory-intensive, making it impractical for longer sequences. To mitigate this, we applied
a sparse Top- _K_ optimization: for each query, we compute
pairwise scores _S_ = _Q · K_ _[⊤]_ _∈_ R _[B][×][H][×][T][q][×][T][k]_, select the
Top- _K_ eff = min( _K, Tk_ ) keys, and construct concatenated
pairs _U_ topk _∈_ R _[B][×][H][×][T][q][×][K]_ [eff] _[×]_ [2] _[D]_ . This approach preserves
the most relevant interactions while substantially reducing
memory requirements in the concatenation and subsequent
backbone processing stages, allowing the method to scale
linearly with the sequence length in those components.
However, the initial computation of _S_ remains quadratic
(see Appendix C.3). Algorithm 2 outlines the steps required
for input curation.
**Time Vector:** NAC builds on continuous-depth models
as (Hasani et al., 2022) that adapt their temporal dynamics to the task. It constructs an internal, normalized
pseudo-time vector _t_ pseudo using a sigmoidal transformation,
_t_ pseudo = _σ_ ( _ta · t_ + _tb_ ), where _ta_ and _tb_ are learnable affine



4


**Neuronal Attention Circuit (NAC) for Representation Learning**



parameters and _σ_ is the sigmoid function. For time-varying
datasets (e.g., irregularly sampled series), each time point
_t_ is derived from the sample’s timestamp, while for tasks
without meaningful timing, _t_ is set to 1. The resulting _t_ pseudo
lies in [0 _,_ 1] and provides a smooth, bounded representation
of time for modulating the network’s dynamics.
**Attention logits and weights:** Starting from Eqn. 3,
consider the trajectory of a query–key pair with initial
condition _a_ 0 = 0:



_at_ = _[ϕ]_

_ωτ_



�1 _−_ _e_ _[−][ω][τ][ t]_ [�] _,_ (17)



followed by the _softmax_ normalization to calculate attention weights. The resulting attention weights _αt_ [(] _[h]_ [)] are then
used to integrate with the value vector _v_ [(] _[h]_ [)], producing headspecific attention outputs. Finally, these outputs are concatenated and linearly projected back into the model dimension.
This formulation ensures that each head learns distinct dynamic compatibilities governed by its own parameterization
of _ϕ_ and _ωτ_, while the aggregation across heads preserves
the expressive capacity of the standard multi-head attention
mechanism.


**2.3. NAC as Universal Approximator**


We now establish the universal approximation capability of
NAC by extending the classical Universal Approximation
Theorem (UAT) (Nishijima, 2021) to the proposed mechanism. For brevity, we consider a network with a single
NAC layer processing fixed-dimensional inputs, though the
argument generalizes to sequences.

---

[Document 3]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_10
Content: Zhuang, J., Jia, M., Ding, Y., and Ding, P. Temporal
convolution-based transferable cross-domain adaptation
approach for remaining useful life estimation under variable failure behaviors. _Reliability Engineering & System_
_Safety_, 216:107946, 2021.



10


**Neuronal Attention Circuit (NAC) for Representation Learning**



**Appendix**


**A. Preliminaries**


**A.1. Attention Mechanism**


Attention mechanisms have become a cornerstone in

modern neural architectures, enabling models to dynamically focus on relevant parts of the input. The concept
was first introduced in the context of neural machine
translation, where it allowed the decoder to weight encoder
outputs according to their importance for generating each
target token. Formally, given a query vector _q ∈_ R _[d]_, key
vectors _K_ = [ _k_ 1 _, k_ 2 _, . . ., kn_ ] _∈_ R _[n][×][d]_, and value vectors
_V_ = [ _v_ 1 _, v_ 2 _, . . ., vn_ ] _∈_ R _[n][×][d]_, the attention mechanism can
be expressed in two steps:


1. Compute the scaled dot attention logits:


_ai_ = _[q][T][ k][i]_ (21)
~~_√_~~ _d_


2. Normalize the logits to get attention weights and compute the output:


_e_ _[a][i]_
_αi_ = softmax( _ai_ ) = ~~_n_~~ (22)
~~�~~ _j_ =1 _[e][a][j]_



exhibit a resting potential at _−_ 70 mV and an activation potential near _−_ 20 mV. Similarly, each _Nm_ is composed of
two subneurons, _Mp_ and _Mn_, and is driven by a controllable variable _y_, which also maps to a biologically plausible
range [ _−_ 70 mV _, −_ 20 mV]. The connections in the NCP
architecture are designed to reflect the biological sparsity
and abstraction of neural circuits. Specifically, connections
from _Ns_ to _Ni_ are feedforward, while those between _Nc_
and _Nm_ are highly recurrent (Lechner et al., 2018). Figure
1(a) illustrates the connectome of NCPs.


**B. Proofs**


In this section, we provide all the proofs.


**B.1. Deriving Closed-form (Exact) Solution**


Although _ϕ_ and _ωτ_ are nonlinear functions of the input
**u** = [ **q** ; **k** ], we derive closed-form solution by treating them
as locally constant over the pseudo-time integration interval
for each query–key pair based on frozen-coefficient approximation (John, 1952). This is accurate whenever the interval
is short or when input variations are slow compared with
the relaxation rate _ωτ_ . Under approximation assumption,
rewrite Eqn. 1 as


_dadtt_ [+] _[ ω][τ]_ _[a][t]_ [ =] _[ ϕ.]_ (24)


This is now a linear first-order ODE. The integrating factor
is



Attention( _q, k, v_ ) =



_n_
� _αivi_ (23)


_i_ =1



Here, _ai_ is the raw attention logit between the query and
each key, and the scaling factor _√d_ prevents large dot prod
ucts from destabilizing the softmax (Vaswani et al., 2017).


**A.2. Neuronal Circuit Policies (NCPs)**


NCPs represent a biologically inspired framework for developing interpretable neural control agents by adapting the
tap-withdrawal circuit found in the nematode _C. elegans_
(Lechner et al., 2018). Unlike traditional spiking neural
networks, the majority of neurons in this circuit exhibit
electronic dynamics, characterized by the passive flow of
electrical charges, resulting in graded potentials. NCPs
are structured as a four-layer hierarchical architecture comprising sensory neurons ( _Ns_ ), interneurons ( _Ni_ ), command
neurons ( _Nc_ ), and motor neurons ( _Nm_ ). The _Ns_ perceive
and respond to external stimulus inputs and are responsible for the initial signal transduction. Each _Ns_ consists
of subneurons _Sp_ and _Sn_ and a system variable _x_ . The
activation of _Sp_ and _Sn_ depends upon the sign of _x_ : _Sp_
becomes activated for _x >_ 0, whereas _Sn_ becomes activated for _x <_ 0. The variable _x_ is mapped to the membrane
potential range of [ _−_ 70 mV _, −_ 20 mV], which is consistent
with the biophysical behavior of nerve cells, which typically



� _ωτ dt_

_µ_ = _e_ � �


Multiply both sides by _µ_ ( _t_ ):



Substitute back:


_e_ _[ω][τ][ t]_ _at −_ _a_ 0 = _ϕ ·_ _[e][ω][τ][ t][ −]_ [1] _._ (30)

_ωτ_



= _e_ _[ω][τ][ t]_ _._ (25)




_[da][t]_
_e_ _[ω][τ][ t]_ _dt_ [+] _[ ω][τ]_ _[e][ω][τ][ t][a][t]_ [ =] _[ ϕe][ω][τ][ t][.]_ (26)


Recognize the left-hand side as the derivative of _e_ _[ω][τ][ t]_ _at_ :


_d_
� _e_ _[ω][τ][ t]_ _at_ � = _ϕe_ _[ω][τ][ t]_ _._ (27)
_dt_


Integrate from 0 to _t_ :


_t_
_e_ _[ω][τ][ t]_ _at −_ _e_ [0] _a_ 0 = _ϕ_ _e_ _[ω][τ][ s]_ _ds._ (28)
�0


Compute the integral (since _ωτ ̸_ = 0):



�0 _t_



_e_ _[ω][τ][ s]_ _ds_ = [1]
0 _ωτ_



_ωτ_



� _e_ _[ω][τ][ t]_ _−_ 1� _._ (29)



11


Rearrange:



**Neuronal Attention Circuit (NAC) for Representation Learning**


CASE 2: MULTIPLE CONNECTIONS ( _M >_ 1).



_e_ _[ω][τ][ t]_ _at_ = _a_ 0 + _[ϕ]_

_ωτ_


Divide both sides by _e_ _[ω][τ][ t]_ :


_at_ = _a_ 0 _e_ _[−][ω][τ][ t]_ + _[ϕ]_

_ωτ_



� _e_ _[ω][τ][ t]_ _−_ 1� _._ (31)


�1 _−_ _e_ _[−][ω][τ][ t]_ [�] _._ (32)



The ODE is



with per-connection equilibria _Aj_ = _ϕj/fj_ . The effective
equilibrium is



_da_

_dt_ [=] _[ −]_ � � _[M]_



_da_



� _fj_ � _a_ +

_j_ =1



_M_
� _fjAj,_ (37)

_j_ =1



Set _a_ _[∗]_ := _[ϕ]_ . Then _at_ = _a_ _[∗]_ + ( _a_ 0 _−_ _a_ _[∗]_ ) _e_ _[−][ω][τ][ t]_, proved.

_ωτ_


**B.2. Proof of Theorem 1**
","The universal approximation guarantee for NAC plays a crucial role in validating its architectural design by establishing its expressive power. By extending the classical Universal Approximation Theorem (UAT), the authors demonstrate that NAC possesses universal approximation capability.

Specifically, Theorem 2 states that for any continuous function on a compact set, there exists a neural network with a single NAC layer (given sufficient model dimension, number of heads, sparsity, and nonlinear activations) that can approximate the function to an arbitrary degree of accuracy. This theoretical guarantee confirms that the NAC architecture is fundamentally capable of learning and representing complex continuous functions, thereby validating its design as a powerful mechanism for representation learning.",105,4468,4574,,3,,2026-01-02T20:19:11.871443
"How does the ""auto-regressive"" property of the Transformer decoder influence the design of the ""masked multi-head attention"" mechanism?",System Level / Architectural Understanding,"[Document 1]
Source: attention_is_all_you_need_raw_chunks_2k.jsonl
Chunk ID: attention_is_all_you_need_raw_chunks_2k_1
Content: [10], consuming the previously generated symbols as additional input when generating the next.


2


![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_is_all_you_need.pdf-2-0.png)

Figure 1: The Transformer - model architecture.


The Transformer follows this overall architecture using stacked self-attention and point-wise, fully
connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,
respectively.


**3.1** **Encoder and Decoder Stacks**


**Encoder:** The encoder is composed of a stack of _N_ = 6 identical layers. Each layer has two
sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network. We employ a residual connection [11] around each of
the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is
LayerNorm( _x_ + Sublayer( _x_ )), where Sublayer( _x_ ) is the function implemented by the sub-layer
itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding
layers, produce outputs of dimension _d_ model = 512.


**Decoder:** The decoder is also composed of a stack of _N_ = 6 identical layers. In addition to the two
sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head
attention over the output of the encoder stack. Similar to the encoder, we employ residual connections
around each of the sub-layers, followed by layer normalization. We also modify the self-attention
sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This
masking, combined with fact that the output embeddings are offset by one position, ensures that the
predictions for position _i_ can depend only on the known outputs at positions less than _i_ .


**3.2** **Attention**


An attention function can be described as mapping a query and a set of key-value pairs to an output,
where the query, keys, values, and output are all vectors. The output is computed as a weighted sum


3


Scaled Dot-Product Attention Multi-Head Attention


Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several
attention layers running in parallel.


of the values, where the weight assigned to each value is computed by a compatibility function of the
query with the corresponding key.


**3.2.1** **Scaled Dot-Product Attention**


We call our particular attention ""Scaled Dot-Product Attention"" (Figure 2). The input consists of
queries and keys of dimension _dk_, and values of dimension _dv_ . We compute the dot products of the
query with all keys, divide each by _[√]_ _dk_, and apply a softmax function to obtain the weights on the
values.


In practice, we compute the attention function on a set of queries simultaneously, packed together
into a matrix _Q_ . The keys and values are also packed together into matrices _K_ and _V_ . We compute
the matrix of outputs as:


Attention( _Q, K, V_ ) = softmax( _[Q][K]_ _[T]_ ) _V_ (1)

~~_√_~~ _dk_


The two most commonly used attention functions are additive attention [2], and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor
of ~~_√_~~ 1 _dk_ . Additive attention computes the compatibility function using a feed-forward network with
a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is
much faster and more space-efficient in practice, since it can be implemented using highly optimized
matrix multiplication code.


While for small values of _dk_ the two mechanisms perform similarly, additive attention outperforms
dot product attention without scaling for larger values of _dk_ [3]. We suspect that for large values of
_dk_, the dot products grow large in magnitude, pushing the softmax function into regions where it has
extremely small gradients [4] . To counteract this effect, we scale the dot products by ~~_√_~~ 1 _dk_ .


**3.2.2** **Multi-Head Attention**


Instead of performing a single attention function with _d_ model-dimensional keys, values and queries,
we found it beneficial to linearly project the queries, keys and values _h_ times with different, learned
linear projections to _dk_, _dk_ and _dv_ dimensions, respectively. On each of these projected versions of
queries, keys and values we then perform the attention function in parallel, yielding _dv_ -dimensional


4To illustrate why the dot products get large, assume that the components of _q_ and _k_ are independent random
variables with mean 0 and variance 1. Then their dot product, _q · k_ = [�] _[d]_ _i_ =1 _[k]_ _[q][i][k][i]_ [, has mean][ 0][ and variance] _[ d][k]_ [.]


4



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_is_all_you_need.pdf-3-0.png)

![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_is_all_you_need.pdf-3-1.png)
output values. These are concatenated and once again projected, resulting in the final values, as
depicted in Figure 2.


Multi-head attention allows the model to jointly attend to information from different representation
subspaces at different positions. With a single attention head, averaging inhibits this.


MultiHead( _Q, K, V_ ) = Concat(head1 _, ...,_ headh) _W_ _[O]_

where headi = Attention( _QWi_ _[Q][, KW][ K]_ _i_ _[, V W][ V]_ _i_ [)]


Where the projections are parameter matrices _Wi_ _[Q]_ _∈_ R _[d]_ [model] _[×][d][k]_, _Wi_ _[K]_ _∈_ R _[d]_ [model] _[×][d][k]_, _Wi_ _[V]_ _∈_ R _[d]_ [model] _[×][d][v]_
and _W_ _[O]_ _∈_ R _[hd][v][×][d]_ [model] .


In this work we employ _h_ = 8 parallel attention layers, or heads. For each of these we use
_dk_ = _dv_ = _d_ model _/h_ = 64. Due to the reduced dimension of each head, the total computational cost
is similar to that of single-head attention with full dimensionality.


**3.2.3** **Applications of Attention in our Model**


The Transformer uses multi-head attention in three different ways:


    - In ""encoder-decoder attention"" layers, the queries come from the previous decoder layer,
and the memory keys and values come from the output of the encoder. This allows every
position in the decoder to attend over all positions in the input sequence. This mimics the
typical encoder-decoder attention mechanisms in sequence-to-sequence models such as

[38, 2, 9].


    - The encoder contains self-attention layers. In a self-attention layer all of the keys, values
and queries come from the same place, in this case, the output of the previous layer in the
encoder. Each position in the encoder can attend to all positions in the previous layer of the
encoder.


    - Similarly, self-attention layers in the decoder allow each position in the decoder to attend to
all positions in the decoder up to and including that position. We need to prevent leftward
information flow in the decoder to preserve the auto-regressive property. We implement this
inside of scaled dot-product attention by masking out (setting to _−∞_ ) all values in the input
of the softmax which correspond to illegal connections. See Figure 2.

---

[Document 2]
Source: attention_is_all_you_need_raw_chunks_2k.jsonl
Chunk ID: attention_is_all_you_need_raw_chunks_2k_2
Content: [38, 2, 9].


    - The encoder contains self-attention layers. In a self-attention layer all of the keys, values
and queries come from the same place, in this case, the output of the previous layer in the
encoder. Each position in the encoder can attend to all positions in the previous layer of the
encoder.


    - Similarly, self-attention layers in the decoder allow each position in the decoder to attend to
all positions in the decoder up to and including that position. We need to prevent leftward
information flow in the decoder to preserve the auto-regressive property. We implement this
inside of scaled dot-product attention by masking out (setting to _−∞_ ) all values in the input
of the softmax which correspond to illegal connections. See Figure 2.


**3.3** **Position-wise Feed-Forward Networks**


In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully
connected feed-forward network, which is applied to each position separately and identically. This
consists of two linear transformations with a ReLU activation in between.


FFN( _x_ ) = max(0 _, xW_ 1 + _b_ 1) _W_ 2 + _b_ 2 (2)


While the linear transformations are the same across different positions, they use different parameters
from layer to layer. Another way of describing this is as two convolutions with kernel size 1.
The dimensionality of input and output is _d_ model = 512, and the inner-layer has dimensionality
_dff_ = 2048.


**3.4** **Embeddings and Softmax**


Similarly to other sequence transduction models, we use learned embeddings to convert the input
tokens and output tokens to vectors of dimension _d_ model. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In
our model, we share the same weight matrix between the two embedding layers and the pre-softmax
linear transformation, similar to [30]. In the embedding layers, we multiply those weights by _[√]_ _d_ model.


5


Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations
for different layer types. _n_ is the sequence length, _d_ is the representation dimension, _k_ is the kernel
size of convolutions and _r_ the size of the neighborhood in restricted self-attention.


Layer Type Complexity per Layer Sequential Maximum Path Length
Operations
Self-Attention _O_ ( _n_ [2] _· d_ ) _O_ (1) _O_ (1)
Recurrent _O_ ( _n · d_ [2] ) _O_ ( _n_ ) _O_ ( _n_ )
Convolutional _O_ ( _k · n · d_ [2] ) _O_ (1) _O_ ( _logk_ ( _n_ ))
Self-Attention (restricted) _O_ ( _r · n · d_ ) _O_ (1) _O_ ( _n/r_ )


**3.5** **Positional Encoding**


Since our model contains no recurrence and no convolution, in order for the model to make use of the
order of the sequence, we must inject some information about the relative or absolute position of the
tokens in the sequence. To this end, we add ""positional encodings"" to the input embeddings at the
bottoms of the encoder and decoder stacks. The positional encodings have the same dimension _d_ model
as the embeddings, so that the two can be summed. There are many choices of positional encodings,
learned and fixed [9].


In this work, we use sine and cosine functions of different frequencies:


_PE_ ( _pos,_ 2 _i_ ) = _sin_ ( _pos/_ 10000 [2] _[i/d]_ [model] )

_PE_ ( _pos,_ 2 _i_ +1) = _cos_ ( _pos/_ 10000 [2] _[i/d]_ [model] )


where _pos_ is the position and _i_ is the dimension. That is, each dimension of the positional encoding
corresponds to a sinusoid. The wavelengths form a geometric progression from 2 _π_ to 10000 _·_ 2 _π_ . We
chose this function because we hypothesized it would allow the model to easily learn to attend by
relative positions, since for any fixed offset _k_, _PEpos_ + _k_ can be represented as a linear function of
_PEpos_ .


We also experimented with using learned positional embeddings [9] instead, and found that the two
versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version
because it may allow the model to extrapolate to sequence lengths longer than the ones encountered
during training.


**4** **Why Self-Attention**


In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations
( _x_ 1 _, ..., xn_ ) to another sequence of equal length ( _z_ 1 _, ..., zn_ ), with _xi, zi ∈_ R _[d]_, such as a hidden
layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we
consider three desiderata.


One is the total computational complexity per layer. Another is the amount of computation that can
be parallelized, as measured by the minimum number of sequential operations required.


The third is the path length between long-range dependencies in the network. Learning long-range
dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the
ability to learn such dependencies is the length of the paths forward and backward signals have to
traverse in the network. The shorter these paths between any combination of positions in the input
and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare
the maximum path length between any two input and output positions in networks composed of the
different layer types.


As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially
executed operations, whereas a recurrent layer requires _O_ ( _n_ ) sequential operations. In terms of
computational complexity, self-attention layers are faster than recurrent layers when the sequence


6


length _n_ is smaller than the representation dimensionality _d_, which is most often the case with
sentence representations used by state-of-the-art models in machine translations, such as word-piece

[38] and byte-pair [31] representations. To improve computational performance for tasks involving
very long sequences, self-attention could be restricted to considering only a neighborhood of size _r_ in
the input sequence centered around the respective output position. This would increase the maximum
path length to _O_ ( _n/r_ ). We plan to investigate this approach further in future work.


A single convolutional layer with kernel width _k < n_ does not connect all pairs of input and output
positions. Doing so requires a stack of _O_ ( _n/k_ ) convolutional layers in the case of contiguous kernels,
or _O_ ( _logk_ ( _n_ )) in the case of dilated convolutions [18], increasing the length of the longest paths
between any two positions in the network. Convolutional layers are generally more expensive than
recurrent layers, by a factor of _k_ . Separable convolutions [6], however, decrease the complexity
considerably, to _O_ ( _k · n · d_ + _n · d_ [2] ). Even with _k_ = _n_, however, the complexity of a separable
convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,
the approach we take in our model.


As side benefit, self-attention could yield more interpretable models. We inspect attention distributions
from our models and present and discuss examples in the appendix. Not only do individual attention
heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic
and semantic structure of the sentences.


**5** **Training**


This section describes the training regime for our models.


**5.1** **Training Data and Batching**

---

[Document 3]
Source: attention_is_all_you_need_raw_chunks_2k.jsonl
Chunk ID: attention_is_all_you_need_raw_chunks_2k_0
Content: Provided proper attribution is provided, Google hereby grants permission to
reproduce the tables and figures in this paper solely for use in journalistic or
scholarly works.

## **Attention Is All You Need**



**Niki Parmar** _[∗]_
Google Research
```
nikip@google.com

```


**Ashish Vaswani** _[∗]_
Google Brain
```
avaswani@google.com

```

**Llion Jones** _[∗]_
Google Research
```
 llion@google.com

```


**Noam Shazeer** _[∗]_
Google Brain
```
noam@google.com

```


**Jakob Uszkoreit** _[∗]_
Google Research
```
usz@google.com

```


**Aidan N. Gomez** _[∗†]_
University of Toronto
```
aidan@cs.toronto.edu

```


**Łukasz Kaiser** _[∗]_
Google Brain
```
lukaszkaiser@google.com

```


**Illia Polosukhin** _[∗‡]_

```
             illia.polosukhin@gmail.com

```

**Abstract**


The dominant sequence transduction models are based on complex recurrent or
convolutional neural networks that include an encoder and a decoder. The best
performing models also connect the encoder and decoder through an attention
mechanism. We propose a new simple network architecture, the Transformer,
based solely on attention mechanisms, dispensing with recurrence and convolutions
entirely. Experiments on two machine translation tasks show these models to
be superior in quality while being more parallelizable and requiring significantly
less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including
ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,
our model establishes a new single-model state-of-the-art BLEU score of 41.8 after
training for 3.5 days on eight GPUs, a small fraction of the training costs of the
best models from the literature. We show that the Transformer generalizes well to
other tasks by applying it successfully to English constituency parsing both with
large and limited training data.


_∗_ Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started
the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and
has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head
attention and the parameter-free position representation and became the other person involved in nearly every
detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and
tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and
efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and
implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating
our research.

_†_ Work performed while at Google Brain.

_‡_ Work performed while at Google Research.


31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.


**1** **Introduction**


Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks
in particular, have been firmly established as state of the art approaches in sequence modeling and
transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous
efforts have since continued to push the boundaries of recurrent language models and encoder-decoder
architectures [38, 24, 15].


Recurrent models typically factor computation along the symbol positions of the input and output
sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden
states _ht_, as a function of the previous hidden state _ht−_ 1 and the input for position _t_ . This inherently
sequential nature precludes parallelization within training examples, which becomes critical at longer
sequence lengths, as memory constraints limit batching across examples. Recent work has achieved
significant improvements in computational efficiency through factorization tricks [21] and conditional
computation [32], while also improving model performance in case of the latter. The fundamental
constraint of sequential computation, however, remains.


Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in
the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms
are used in conjunction with a recurrent network.


In this work we propose the Transformer, a model architecture eschewing recurrence and instead
relying entirely on an attention mechanism to draw global dependencies between input and output.
The Transformer allows for significantly more parallelization and can reach a new state of the art in
translation quality after being trained for as little as twelve hours on eight P100 GPUs.


**2** **Background**


The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU

[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building
block, computing hidden representations in parallel for all input and output positions. In these models,
the number of operations required to relate signals from two arbitrary input or output positions grows
in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes
it more difficult to learn dependencies between distant positions [12]. In the Transformer this is
reduced to a constant number of operations, albeit at the cost of reduced effective resolution due
to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as
described in section 3.2.


Self-attention, sometimes called intra-attention is an attention mechanism relating different positions
of a single sequence in order to compute a representation of the sequence. Self-attention has been
used successfully in a variety of tasks including reading comprehension, abstractive summarization,
textual entailment and learning task-independent sentence representations [4, 27, 28, 22].


End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering and
language modeling tasks [34].


To the best of our knowledge, however, the Transformer is the first transduction model relying
entirely on self-attention to compute representations of its input and output without using sequencealigned RNNs or convolution. In the following sections, we will describe the Transformer, motivate
self-attention and discuss its advantages over models such as [17, 18] and [9].


**3** **Model Architecture**


Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].
Here, the encoder maps an input sequence of symbol representations ( _x_ 1 _, ..., xn_ ) to a sequence
of continuous representations **z** = ( _z_ 1 _, ..., zn_ ). Given **z**, the decoder then generates an output
sequence ( _y_ 1 _, ..., ym_ ) of symbols one element at a time. At each step the model is auto-regressive

[10], consuming the previously generated symbols as additional input when generating the next.


2


![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_is_all_you_need.pdf-2-0.png)

Figure 1: The Transformer - model architecture.


The Transformer follows this overall architecture using stacked self-attention and point-wise, fully
connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,
respectively.


**3.1** **Encoder and Decoder Stacks**
","The ""auto-regressive"" property of the Transformer decoder means that at each step, the model generates an output symbol by consuming the previously generated symbols as additional input.

To preserve this auto-regressive property, the ""masked multi-head attention"" mechanism in the decoder's self-attention sub-layer is designed to prevent positions from attending to subsequent positions. This is implemented by masking out (setting to -infinity) all values in the input of the softmax function that correspond to illegal connections. This ensures that the predictions for a given position *i* can only depend on the known outputs at positions less than *i*, preventing ""leftward information flow"" and maintaining the sequential generation characteristic of an auto-regressive model.",129,4381,4511,,3,,2026-01-02T20:19:16.384573
"Analyze the ""performance-efficiency balance"" of the SWAA toolkit and how it allows for flexible deployment strategies.",System Level / Architectural Understanding,"[Document 1]
Source: sliding_window_attention_annotated_chunks_2k.jsonl
Chunk ID: sliding_window_attention_annotated_chunks_2k_1
Content: LightTransfer (Zhang et al., 2024) is a promising
attempt at adapting existing models to SWA without pretraining, which has the same motivation as
ours. But it may generalize poorly across model
families (see Appendix G).


**2.2** **Linear Attention**


An alternative approach involves reformulating
the attention mechanism entirely to achieve linear, _O_ ( _N_ ), complexity. This includes methods
such as RNN-like linear attention transformers

(Katharopoulos et al., 2020; Peng et al., 2023;
Sun et al., 2023) and structured state-space models
(SSMs) like Mamba (Gu and Dao, 2023). Many
works such as Jamba and Nemotron-Flash(Lieber
et al., 2024; Linsong Chu et al., 2024; Team et al.,
2025; Fu et al., 2025a) interleave linear attention
layers with traditional attention layers to create hybrid model structures. While promising, they represent a fundamental architectural departure from the
standard Transformer, and their performance is generally weaker than traditional Transformer-based

LLMs.



2


**3** **Candidate Methods for SWA**

**Adaptation**


As established, a naive application of SWA leads
to severe long-context performance degradation.
Therefore, we investigate five methods that can potentially facilitate SWA adaptation from distinct
perspectives. However, every method except finetuning has some drawbacks to LLM inference efficiency, as discussed in Appendix A. Therefore,
although these methods are not mutually exclusive,
we should not indiscriminately adopt all of them.
Instead, we must evaluate various combinations to
identify the optimal trade-off between performance
and efficiency.


**3.1** **Full Attention Decode**


This method applies SWA **only** to the prefilling
stage. During the decoding (auto-regressive generation) stage, each token still employs full attention,
allowing access to all previous tokens in the context. The resulting attention mask is depicted in
Figure 1a.
This novel approach, first proposed in our study,
is inspired by human reading comprehension: humans typically scan a passage casually (prefilling)
before thinking deeply to formulate an answer (decoding) for a specific problem. We term this strategy ""reading casually, thinking carefully."" In our
design, the SWA-constrained prefilling stage corresponds to casual reading, while the full-attention
decoding stage enables careful thinking. This analogy also suggests that Chain-of-Thought (CoT)
during decoding may be particularly beneficial, as
extended generation (i.e. ""thinking"") could compensate for the insufficient contextual information
gathered during the prefilling stage.


(a) FA Decode (b) Keep First


Figure 1: (a) Attention mask for FA Decode. SWA is
used for prompt tokens (prefill), and full attention is
used for generated tokens (decode). (b) Attention mask
for SWA combined with Keep First _k_ Tokens.



**3.1.1** **Keep First k Tokens**

Xiao et al. demonstrate that models pretrained with
full attention allocate a disproportionate amount
of attention to the initial tokens (""attention sink""),
and removing the visibility of these tokens causes
performance collapse. Their solution, streaming
attention, involves permanently retaining the attention to these ""sink"" tokens while using SWA, which
successfully maintains the stability of the attention
distribution and the model’s output. Our method
is basically the same: as shown in Figure 1b, any
subsequent token can attend to its local window
**and** the initial _k_ tokens.

Notably, however, our method extends beyond
its original version. Streaming attention operates
only at the KV cache level; specifically, the KV
cache of the sink tokens is retained during decoding, while the prefilling stage is not modified or accelerated at all. In contrast, we directly customize
the Flash-Attention-2 (Dao, 2024) kernel to implement such attention mask, thereby accelerating
prefilling via SWA as well, and eliminating the
need to modify KV cache.


**3.2** **Interleaving Layers**


This method retains full attention on a subset of

layers while applying SWA to the remainder, providing a simple hybrid mechanism to balance the
performance of full attention with the efficiency
of pure SWA. A common strategy involves designating one in every _n_ layers to use full attention
(e.g., layers 0, 2, 4, . . . retain full attention, while all
others use SWA). For example, Gemma-2 (Team,
2024a) uses SWA only for layers [1, 3, 5, . . . ], and
Gemma-3 (Team, 2025a) uses SWA only for layers

[5, 11, 17, ...].
However, for an FA-pretrained model, layers
may exhibit distinct behaviors, suggesting it may
not be optimal to simply assign SWA to layers [1,
3, 5, ...]. Instead, it might be preferable to use
statistical features to select ""lazy"" (mostly focusing
on just recent tokens) layers, as adopted by LightTransfer (Zhang et al., 2024). However, we find
that LightTransfer is not consistently superior in
practice (see Appendix G). Therefore, for simplicity, we still choose the simplest fixed-interval layer
selections in our experiments, such as [0, 2, 4, . . . ]
and [1, 3, 5, ...].


**3.3** **Chain-of-Thought**


Chain-of-Thought (CoT) (Wei et al., 2022) is a
widely used technique for improving model ac


3



```
### Figure - Sliding Window Attention
![Sliding Window Attention](sliding_window_attention.pdf-2-0.png)

**Caption:**  
This figure illustrates the sliding window attention mechanism in a transformer model. The ""Prefill"" region represents the initial input, while the ""Decode"" region shows the subsequent processing.

**Description:**
- The image depicts a grid representing a sequence of data, with a sliding window moving across it.
- The green area indicates the window's current position, highlighting the portion of the sequence being attended to.
- The blue area represents the decoded output based on the attended input.
- The window progressively moves across the sequence, enabling contextual understanding.
```

```
### Figure - Sliding Window Attention
![Sliding Window Attention](sliding_window_attention.pdf-2-1.png)

**Caption:**  
The figure illustrates the sliding window attention mechanism, where attention is retained over a limited window. Attention decreases as the distance from the current position increases.

**Description:**
- The image depicts a grid representing a sequence of tokens.
- The blue squares indicate attention retained across the sliding window.
- The green squares represent attention calculated within the sliding window.
- Attention decreases progressively as the distance 'k' from the current position increases, reflecting a decaying attention mechanism.
- The sliding window size is decreasing as it moves along the sequence.
curacy via reasoning. With the advent of ""thinking"" models, such as DeepSeek-R1 (DeepSeek-AI,
2025a), CoT has evolved from a prompting strategy
to an intrinsic LLM capability. However, whether
CoT has a specific impact in SWA scenarios remains uninvestigated. We explore this by comparing a thinking model with its non-thinking variant,
e.g., Qwen3-4B-Thinking and Qwen3-4B-Instruct,
to verify the effect of CoT on SWA adaptation,
which can produce more notable differences compared to simply adding CoT prompting to the same
model.


**3.4** **Fine-tuning**

---

[Document 2]
Source: sliding_window_attention_annotated_chunks_2k.jsonl
Chunk ID: sliding_window_attention_annotated_chunks_2k_2
Content: **3.4** **Fine-tuning**


This is the most natural way to mitigate traininginference mismatch. Apparently, the model should
be fine-tuned while SWA is applied, so that the
model’s parameters can be trained to better adapt
to SWA. Meanwhile, the training data must be longcontext tasks, where SWA actually works.
However, most available long-context datasets
only contain brief ground-truth answers, lacking the reasoning steps required for fine-tuning
a ""thinking"" model. Since our goal is to _restore_
the model’s original capabilities under SWA rather
than teach it new ones, instead of directly using the
original dataset, we adopt an approach similar to
self-distillation (Yang et al., 2024). Specifically, we
utilize the original full-attention model to generate
new answers for the dataset’s questions, and these
generated answers are then filtered for correctness
using GPT-5-Mini (OpenAI, 2025), to make up our
training dataset. For each question, we sample 4
answers with temperature 1, because we find this
strategy is slightly better than generating only one
answer with temperature 0.


**4** **Experiment Setup**


We organize our experiments around three research
questions:


**RQ1: Is SWA adaptation feasible without any**
**additional training?** We evaluate whether
an FA LLM can be adapted to SWA using
only inference-time modifications, and which
combinations of techniques are necessary.


**RQ2: How much does fine-tuning with SWA im-**
**prove performance?** We study the effect of
SWA-aware fine-tuning on long-context performance and identify which components of
SWAA are still required.



**RQ3: Which SWAA configurations achieves the**
**optimal performance-efficiency trade-offs?**
We evaluate how different SWAA configurations trade off accuracy against inference la
tency.


**4.1** **Models**


Our primary experiments use Qwen3-4B-Thinking
and Qwen3-4B-Instruct (Team, 2025b). The Thinking variant enforces chain-of-thought (CoT) style
reasoning, whereas the Instruct variant usually just
answers briefly. To ensure generality, we additionally evaluate Qwen3-30B-A3B-Thinking, Qwen330B-A3B-Instruct (Team, 2025b), and Llama3.18B-Instruct (Touvron et al., 2023).

All models are served with vLLM in float16

precision using a batch size of 64. We use greedy
decoding (temperature = 0) for all evaluations. In
preliminary experiments, we observed that vLLM
yields slightly lower (about 1% to 5%) scores
than HuggingFace Transformers due to precisionrelated discrepancies.


**4.2** **Evaluation Dataset**


SWA is identical to full attention when the context

length is within the window size. Even if the model
is fine-tuned, we can pre-calculate the prompt
length and simply disable the LoRA adapters for
short prompts to get completely the same response
as the original model. Therefore, our experiments
focus exclusively on long-context benchmarks with
inputs exceeding 16k tokens, as re-evaluating models on standard short-context benchmarks (e.g.,
MMLU (Hendrycks et al., 2021), GPQA (Rein
et al., 2023)) is completely unnecessary.
Since we find other long-context benchmarks are
either too easy or too difficult for 4B-level models (see Appendix B), we ultimately select LongMemEval (Wu et al., 2024), a benchmark consisting of various types of long-context QA tasks with
moderate difficulty, although it is originally designed for agent memory system evaluation. Its
context length is controllable by selecting a specific number of chat sessions to concatenate as the
context from a pool of hundreds of sessions (a session contains the chat history between user and
assistant within a day). To create a moderately difficult and discriminative evaluation, we construct
**LongMemEval_24k** by sampling 10 sessions, resulting in 500 samples ranging from 16k to 32k
with an average context length of 24k.



4


For additional validation of generalizability, as
shown in Appendix D, we also experiment on
LongBench-V2 (Bai et al., 2024b), a more modern and challenging benchmark that requires deep
reasoning across various real-world tasks.


**4.3** **Training Details**


For the fine-tuning dataset, we initially considered LongAlign (Bai et al., 2024a), a widely used
long-context fine-tuning dataset for adapt a regularlength model to long-context tasks. However, since

_∼_
its sample count ( 10,000) is insufficient, we incorporate an additional 6,000 samples from Fusangv1-long (Pan, 2024), a more comprehensive corpus
of over 40,000 long-context samples that includes
LongAlign as a subset.
We perform SWA-aware fine-tuning using LoRA
(Hu et al., 2022). Unless otherwise noted, we use
rank _r_ = 16 and _α_ = 128, and apply LoRA only
to the query, key, and value projection modules.
We adopt this parameter-efficient setting because
full-parameter fine-tuning often leads to overfitting
and degradation of the model’s original capabilities
in our preliminary experiments. We use a learning
rate of 1e-4 with a cosine decay schedule. Models
are fine-tuned for a single epoch on the sampled
long-context dataset since we observe no meaningful gains from additional epochs (see Appendix F).
Once training takes approximately 12 hours on an
8*H20 GPU server for Qwen3-4B and 30 hours for
Qwen3-30B-A3B.


**5** **Experiment Results**


**5.1** **SWA Adaptation Without Fine-tuning**


We first study SWA adaptation without any additional training. Table 1 reports LongMemEval_24k
accuracy for Qwen3-4B-Thinking (""think"") and
Qwen3-4B-Instruct (""non-think"") under different
combinations of SWAA components. In most settings, we use an aggressive 2k window to amplify
the impact of SWA. The configurations are ranked
by the number of methods applied (0, 1, 2, or 3 of
Interleaving Layers, Keep First and FA Decode).
Rows 1 (original model) and 2 (naive SWA) serve
as upper and lower baselines, respectively. In the
column ""FA layers"", the value records which layers use full attention, and [] means all the layers
use SWA, i.e., this method is not enabled. In the
column ""keep first"", the value is _k_ in Keep First
_k_ Tokens. When comparing results, an accuracy
difference of less than 5% is usually considered



statistically insignificant. From the results, we find
that:


**Naive SWA is not viable.** Naively replacing FA
with a 2k sliding window attention (row 1) drops
accuracy significantly to 3 _._ 2 and 11 _._ 0, respectively.
Even with an 8k window (row 2), accuracy only recovers to 13 _._ 2 and 19 _._ 8, far below the FA baseline.


**Single method helps, but cannot close the gap.**
Each method—Keep First, FA Decode, or Interleaving Layers—improves over naive SWA (rows 3–6),
yet each alone recovers only a small fraction of the
FA gap and remains well below the baseline. In
short, no single method is sufficient.


**Combinations exhibit strong synergy.** Recipes
that combine multiple methods deliver large gains:


  - **FA Decode + Keep First** _k_ substantially improves over naive SWA (rows 7–9), recovering roughly half to two-thirds of the gap on
the thinking model as _k_ increases. However,
increasing _k_ from 100 to 1000 yields almost
no improvement, indicating that _k_ does not
need to be exceedingly large.


  - **Interleaving Layers + FA Decode** is
markedly stronger (row 13), recovering most
of the gap for the thinking model.


  - **FA Decode + Interleaving Layers + Keep**
**First** _k_ perform best (rows 18). The thinking
model recovers close to 90% of the FA gap
even at 2k window.

---

[Document 3]
Source: sliding_window_attention_annotated_chunks_2k.jsonl
Chunk ID: sliding_window_attention_annotated_chunks_2k_3
Content: **Combinations exhibit strong synergy.** Recipes
that combine multiple methods deliver large gains:


  - **FA Decode + Keep First** _k_ substantially improves over naive SWA (rows 7–9), recovering roughly half to two-thirds of the gap on
the thinking model as _k_ increases. However,
increasing _k_ from 100 to 1000 yields almost
no improvement, indicating that _k_ does not
need to be exceedingly large.


  - **Interleaving Layers + FA Decode** is
markedly stronger (row 13), recovering most
of the gap for the thinking model.


  - **FA Decode + Interleaving Layers + Keep**
**First** _k_ perform best (rows 18). The thinking
model recovers close to 90% of the FA gap
even at 2k window.


**CoT synergizes with FA Decode.** Under recipes
that include FA Decode, the thinking model consistently benefits more than the non-thinking model
(rows 13 and 18), suggesting that CoT synergizes
with FA Decode: preserving global attention at
decoding time enables longer reasoning traces to
capitalize on context processed by SWA, confirming our hypothesis in Section 3.1.


**Sliding window size affects, but is not the deci-**
**sive role.** With FA Decode + Keep First _k_, accuracy improves as the window increases (rows 7,
14, 15), though benefits are modest until 8k. When
added with interleaving FA layers, moving from a
2k to a 4k window is enough to close the remaining
gap on the thinking model (row 22 matches the FA
baseline), indicating that FA Decode and Interleaving are the dominant drivers, and larger windows
mainly smooth residual error.



5


Table 1: Qwen3-4B-Thinking and Qwen3-4B-Instruct results on LongMemEval without SFT


**No.** **window size** **FA layers** **keep first** **FA decode** **Acc think** **Acc non-think**


0 Full [] 0 False **73.0** **62.0**
1 2k [] 0 False 3.2 11.0
2 8k [] 0 False 13.2 19.8


3 2k [] 10 False 16.0 15.6
4 2k [] 0 True 11.8 14.2
5 2k [1, 3, 5, ...] 0 False 13.4 18.4
6 8k [] 0 True 26.2 25.0


7 2k [] 10 True 38.2 20.6
8 2k [] 100 True 50.0 17.8
9 2k [] 1000 True 50.0 20.2
10 2k [0, 2, 4, ...] 10 False 17.0 14.8
11 2k [0, 2, 4, ...] 0 True 32.2 26.0
12 2k [1, 3, 5, ...] 10 False 25.8 36.4
13 2k [1, 3, 5, ...] 0 True 59.2 34.8
14 4k [] 10 True 38.0 24.4
15 8k [] 10 True 49.2 35.2


16 2k [0, 2, 4, ...] 10 True 36.0 17.2
17 2k [1, 3, 5, ...] 10 True 65.0 **53.6**
18 2k [1, 3, 5, ...] 100 True **68.8** 50.6
19 2k [1, 5, 9, ...] 10 True 53.2 31.4
20 2k [1, 9, 17, ...] 10 True 36.4 18.8
21 2k [3, 7, 11, ...] 10 True 54.2 34.6
22 4k [1, 3, 5, ...] 100 True **73.0** **54.2**
23 8k [1, 3, 5, ...] 100 True **71.6** **56.6**



**SWA/FA layer selection has large impacts.** Selecting less layers for FA, e.g. only one fourth or
eighth (row 19, 20), though more efficient, will
significantly decrease the improvement brought by
Interleaving Layers. More importantly, for Qwen34B, configuring _odd-numbered_ layers with full attention is significantly better than _even-numbered_
layers (row 11, 13, 16, 17). However, surprisingly,
this result is reversed for Qwen3-30B-A3B and
Llama3.1-8B-Instruct (row 10 and 11 in Table 4
and 5). This suggests that layer functionalities differ across model families and sizes, necessitating
model-specific layer selection strategies, as discussed in Section 3.2.


Therefore, we answer RQ1: adapting an FA
LLM to SWA is feasible even without any training. But it requires specific combinations of at
least 2 methods, which could be less efficient for
inference.



**5.2** **SWA Adaptation With Fine-tuning**


We next evaluate SWA-aware supervised finetuning, which is expected to provide higher improvement. Table 2 reports LongMemEval_24k
accuracy after SFT under various SWAA configurations. The original full-attention model is also
fine-tuned as a baseline (row 0). Since training is
relatively time-consuming, we only test a representative subset of configurations. Our findings are as
follows:


**Fine-tuning substantially lifts all SWA config-**
**urations.** Comparing all the fine-tuning results
with non-training ones, it is clear that fine-tuning
consistently provides great improvement. However,
simply fine-tuning with naive SWA remains insufficient, only achieving 18.8% and 23.8% accuracy
(row 1).


**FA Decode and Interleaving Layers emerge as**
**dominant components.** After SFT, FA Decode
and Interleaving Layers provide the largest gains.



6


Table 2: Qwen3-4B-Thinking and Qwen3-4B-Instruct results on LongMemEval with SFT


**No.** **window size** **FA layers** **keep first** **FA decode** **Acc think** **Acc non-think**


0 Full [] 0 True **74.6** **63.4**
1 2k [] 0 False 18.8 23.8


2 2k [] 10 False 15.6 /
3 2k [] 0 True 57.9 42.0
4 2k [1, 3, 5, ...] 0 False 63.6 54.6
5 4k [] 0 True 62.6 /


6 2k [] 10 True 56.7 /
7 2k [] 100 True 62.2 42.6
8 2k [1, 3, 5, ...] 0 True **73.2** **58.8**
9 2k [0, 2, 4, ...] 0 True **66.0** /
10 2k [1, 5, 9, ...] 0 True **68.8** 47.0


11 2k [1, 3, 5, ...] 100 True **73.2** **61.4**


Table 3: Recommended SWA adaptation recipes for different needs and scenarios. � means optional.


**Training** ? **Thinking Model?** **Prefer?** **FA Decode** **Interleaving Layers** **Keep First**


No No Any � � �
No Yes Efficiency � � �
No Yes Accuracy � � �


Yes Any Efficiency � � �
Yes Any Accuracy � � �



Using FA Decode alone (row 3) or using Interleaving Layers alone (row 4) both get high accuracy.
And combining both (row 8) further boosts performance to 73 _._ 2 (think) and 58 _._ 8 (non-think), nearly
matching the full-attention SFT baseline in row 0.


**Keep First becomes optional rather than essen-**
**tial.** Before fine-tuning, Keep First is crucial for
stability under SWA. But after SFT, it only provides
minor additional improvement. With FA Decode,
adding _k_ = 100 (row 7) improves over _k_ = 0
(row 3) only 4.5%, and if further combining FA
layers, it almost offers no improvements (row 11
and row 8).


**Effect of sliding window size.** Row 3 and 5
shows that increasing the window from 2k to 4k
with FA Decode improves thinking-model accuracy from 57 _._ 9 to 62 _._ 6. This mirrors the non-SFT
trend that larger windows help, but the dominant
improvements still come from FA Decode and Interleaving Layers.
","The SWAA toolkit is designed to achieve an ""optimal trade-off between performance and efficiency"" by offering various adaptation methods that can be combined. Naive application of Sliding Window Attention (SWA) leads to severe performance degradation, necessitating these methods.

**Performance-Efficiency Balance:**
*   The toolkit's methods (Full Attention Decode, Keep First k Tokens, Interleaving Layers, Chain-of-Thought, and Fine-tuning) each contribute to restoring performance lost by SWA, but some have drawbacks to LLM inference efficiency.
*   Individual methods improve performance over naive SWA but are insufficient alone.
*   Combinations of methods, such as ""FA Decode + Keep First k"" or ""Interleaving Layers + FA Decode,"" deliver significant performance gains, recovering a substantial portion of the full attention (FA) baseline accuracy. The combination of ""FA Decode + Interleaving Layers + Keep First k"" performs best for accuracy, even at aggressive window sizes like 2k.
*   However, these combinations ""could be less efficient for inference"" compared to pure SWA.
*   Fine-tuning with SWA-aware training further boosts performance across configurations. After fine-tuning, FA Decode and Interleaving Layers emerge as dominant components, while Keep First k becomes optional, providing only minor additional improvement. This suggests that fine-tuning can simplify the required configuration for performance, potentially improving efficiency by making some components less critical.
*   The sliding window size also impacts this balance; larger windows generally help, but the primary improvements come from FA Decode and Interleaving Layers.

**Flexible Deployment Strategies:**
The SWAA toolkit allows for flexible deployment strategies through the modularity and combinability of its methods, catering to different needs and scenarios:
*   **Combinatorial Approach:** Users can select and combine methods like Full Attention Decode, Keep First k Tokens, and Interleaving Layers based on their specific performance and efficiency requirements. For instance, without fine-tuning, combining all three methods yields the highest accuracy, while using fewer methods might prioritize efficiency.
*   **Training vs. Inference-time Adaptation:** The toolkit supports adaptation without any additional training (inference-time modifications only) or with SWA-aware fine-tuning. Fine-tuning can significantly improve performance and alter the necessity of certain components (e.g., making Keep First k optional).
*   **Model-Specific Layer Selection:** The optimal selection of layers for full attention in the ""Interleaving Layers"" method varies across different model families and sizes (e.g., odd-numbered layers for Qwen3-4B vs. even-numbered for Qwen3-30B-A3B and Llama3.1-8B-Instruct), necessitating model-specific strategies.
*   **Scenario-Based Recipes:** The toolkit provides recommended adaptation recipes tailored for different needs, such as:
    *   Whether fine-tuning is applied.
    *   Whether the model is a ""Thinking Model"" (utilizing Chain-of-Thought).
    *   Whether the preference is for ""Efficiency"" or ""Accuracy.""
    This allows users to choose a configuration that best fits their computational resources, desired output quality, and model characteristics.",108,12146,12256,,3,,2026-01-02T20:19:28.641638
"How does the ""CogVision"" framework's probing methodology reveal the ""hierarchical organization"" of cognitive processes in VLMs?",System Level / Architectural Understanding,"[Document 1]
Source: attention_functional_roles_raw_chunks_2k.jsonl
Chunk ID: attention_functional_roles_raw_chunks_2k_1
Content: Furthermore, we validate the functional importance of these heads by showing that their removal
degrades performance on complex tasks and leads to specific error patterns, while their enhancement improves reasoning capabilities. Our findings provide compelling evidence that these attention
heads play a critical role in multimodal reasoning. This insight not only deepens our understanding of the internal organization of VLMs but also suggests potential avenues for designing more
interpretable and cognitive-inspired multimodal AI systems.


2 COGVISION


In this section, we present our dataset Cognitive Vision (CogVision) that contains cognitive process
in multimodal reasoning. CogVision contains 1,409 main questions and 5,744 subquestions. Each


2


example comprises a main question and answer, its subquestions and subanswers and an annotation
specifying the receptive or cognitive function required for each subquestion.


2.1 COGNITIVE FUNCTIONS


To systematically capture the cognitive processes involved in complex reasoning tasks, we consider eight functions related to complex multimodal reasoning. These functions are inspired by
established frameworks in cognitive science (Anderson, 2014; Diamond, 2013), which highlight the
importance of perception, working memory, and reasoning in human cognition.


The functions related to vision include:


    - **Low-level Visual Reception** : Recognizing basic visual features such as color, shape, size,
position, motion.

    - **High-level Visual Reception** : Integrating visual information to recognize objects, patterns,
and scene structure.

    - **Visual Knowledge Recall** : Applying long-term visual knowledge related to visual concepts and their properties.


The functions related to language include:


    - **Language Information Extraction and Understanding** : locating and understanding relevant information from an external source or prior context.

    - **Language Knowledge Recall** : Accessing domain-specific knowledge without visual input.

    - **Math Reasoning** : Performing counting, arithmetic, comparison, and logic-based operations.

    - **Inference** : deriving implicit information that is not directly stated.

    - **Decision-Making** : Selecting the best outcome among alternatives based on reasoning.


This categorization reflects a natural progression from basic information processing to complex cognitive integration. Both the human brain and VLMs encompass a wide range of functional modules.
Our focus in this work is specifically on reasoning-related cognitive functions. By identifying and
organizing these eight core reasoning functions, we can more clearly examine how VLMs handle
different types of thinking steps, in a way that is both systematic and easy to interpret. Detail descriptions of each function and examples in CogVision can be found in Appendix A.1.


2.2 DATA COLLECTIONS


Based on our categorization of reasoning functions, we sampled 2000 diverse questions from existing commonly used visual reasoning benchmarks, selecting 200 examples each from ClevrMath (Lindstr¨om & Abraham, 2022), MathVision (Wang et al., 2024a), MathVista (Lu et al., 2024b),
MMMU (Yue et al., 2024), ScienceQA (Saikh et al., 2022), OKVQA (Marino et al., 2019) (400
examples), VCR (Zellers et al., 2019), VisuLogic (Xu et al., 2025) and Super-Clevr (Li et al.,
2023b) datasets. These datasets span a range of reasoning problems, including logical, mathematical, and commonsense reasoning. Using the chain-of-thought (CoT) paradigm, we prompted
GPT-4.1 (Achiam et al., 2023) to decompose each main question (with its final answer) into subquestions, each targeting a subanswer and a specific function. The prompt encourages structured,
step-by-step reasoning, ensuring that each subquestion is clear, answerable, and sequentially dependent. This process yields a set of subquestion–answer–function (subQAF) triples for each QA pair:
subQAFs = _{_ ( _qi, ai, fi_ ) _}_ _[k]_ _i_ =1 [, where each contains a subquestion] _[ q][i]_ [, its concise answer] _[ a][i]_ [, and the]
corresponding function label _fi_ . The prompt for generating subquestions are list in Appendix A.6.


2.3 DATA FILTERING AND ANNOTATION


Recent advances enable leveraging large pre-trained models for dataset construction, thanks to their
reasoning capabilities and ability to generate high-quality annotations at scale (Wang et al., 2024b).
While our dataset is automatically generated using a VLM to minimize manual effort, we employ a
rigorous two-stage human verification pipeline to ensure data quality and mitigate hallucination. In
the first stage, three expert annotator independently evaluate whether each subquestion is logically
structured and consistent with natural human reasoning. QA pairs with incoherent or inconsistent


3


decompositions are moved out. In the second stage, annotators verify and, if necessary, relabel the
cognitive function associated with each subquestion to ensure accurate alignment with the intended
mental process. Subanswers are further cross-checked with GPT-o3 (OpenAI, 2024) and adjudicated
by humans in cases of disagreement (details in Appendix A.5). This multi-step verification ensures
that each retained subQAF triplet represents a coherent, interpretable reasoning step grounded in
core cognitive functions. After this refinement, our final dataset comprises 1,409 main QA pairs
and 5,744 validated subQAF triplets. The detailed statistics of the dataset including the number of
triplet in training and testing set and distributions of each function can be found in Appendix A.3.
We analyze in-group subquestion diversity, and the results in the Appendix A.2 show that the eight
functions exhibit broad and overlapping distributions in phrasing patterns and token lengths, indicating no systematic surface-form differences.


3 DETECTION OF COGNITIVE FUNCTIONS


Using the CogVision dataset, we adopt a probing-based framework (Alain & Bengio, 2016; Belinkov, 2022; Tenney et al., 2019) to identify which attention heads in VLMs are associated with
specific functions in reasoning process. Specifically, for each functional annotated subquestion, we
extract head activations (see Subsection 3.1), train classifiers and compute accuracies to identify
contributing heads (see Subsection 3.2). Unlike prior work focusing on a single label, our formulation captures many-to-many relationships between heads and cognitive functions, enabling a more
nuanced analysis of functional specialization and overlap within the model.


3.1 HEAD FEATURE EXTRACTION


Given a large VLM _M_, we generate an answer _a_ _[M]_ _i_ for each subquestion _qi_ derived from a main
question _Qi_ . To support coherent multi-step reasoning, we include preceding subquestions and
their answers as contextual input, emulating the incremental reasoning process observed in human
cognition.


During inference, each input token is first mapped into an embedding and then propagated through
the transformer’s layers. At each layer, attention and feedforward operations update the residual
stream, which is ultimately decoded into token predictions. For each generated token _i_, we extract
attention head outputs _Xi_ = _{x_ _[m]_ _l_ _| l ∈_ 1 _, . . ., Nl, m ∈_ 1 _, . . ., Nh}_ across all layers, where _x_ _[m]_ _l_
denotes the value vector from the _m_ -th head in layer _l_ projected into the residual stream, with _Nh_
the number of heads per layer and _Nl_ the total number of layers.


Let _Nt_ denote the number of tokens in the generated answer _a_ _[M]_ _i_ [. To isolate semantically informative]
content relevant to reasoning, we select the top- _k_ most important tokens, determined by prompting
Qwen3-30B LLM (Yang et al., 2025), yielding an index set _Ik_ with _|Ik|_ = _k_ . For each index
_j ∈Ik_, we extract the corresponding attention head activations _Xj_, and compute the averaged
activation feature for the _m_ -th head in layer _l_ as ¯ _x_ _[m]_ _l_ = _k_ [1] � _j ∈Ikxml_ [. This results in a full set of]

---

[Document 2]
Source: attention_functional_roles_raw_chunks_2k.jsonl
Chunk ID: attention_functional_roles_raw_chunks_2k_2
Content: Let _Nt_ denote the number of tokens in the generated answer _a_ _[M]_ _i_ [. To isolate semantically informative]
content relevant to reasoning, we select the top- _k_ most important tokens, determined by prompting
Qwen3-30B LLM (Yang et al., 2025), yielding an index set _Ik_ with _|Ik|_ = _k_ . For each index
_j ∈Ik_, we extract the corresponding attention head activations _Xj_, and compute the averaged
activation feature for the _m_ -th head in layer _l_ as ¯ _x_ _[m]_ _l_ = _k_ [1] � _j ∈Ikxml_ [. This results in a full set of]

head-level features _X_ [¯] _i_ = _{x_ ¯ _[m]_ _l_ _[|][ l][ ∈]_ [1] _[, . . ., L, m][ ∈]_ [1] _[, . . ., M]_ _[}]_ [.]


3.2 FUNCTION PROBING


For the dataset with _N_ subQAF triplets, we collect all activations to construct the probing dataset:


_D_ probe = _{_ (¯ _x_ _[m]_ _l_ _[, c]_ [)] _[i][}][N]_ _i_ =1 _[, l][ ∈{]_ [1] _[, . . ., L][}][, m][ ∈{]_ [1] _[, . . ., M]_ _[}]_ (1)


For classification based on CogVision, the training set includes 1,124 main questions with 4,604
subQAF triplet, while the testing set has 285 main questions with 1,141 triplets. Our probe takes
the form _pθ_ ( _x_ _[m]_ _l_ [) = sigmoid (] _[⟨][θ, x][m]_ _l_ _[⟩]_ [)][. There is one probe per attention head per layer per function.]
For each target function, the probe is trained by treating the attention-head outputs that lead to
correct answers for that function as the positive class, and those associated with correct answers
from other functions as the negative class. To ensure data balance, we select an equal number of
negative samples to match the positive ones. Given prior findings suggesting that cognitive functions
may vary by layer depth (Zheng et al.), we incorporate layer-wise information by computing the
average activation ¯ _xl_ = _M_ 1 � _Mm_ =1 _[x]_ [¯] _l_ _[m]_ [for each layer. We then augment each head-level vector with]


4


its corresponding layer summary, resulting in enriched features _x_ _[m]_ _l_ _[′]_ = [¯ _x_ _[m]_ _l_ [; ¯] _[x][l]_ []][ for probing. The]
importance for each head are then calculated based on the accuracy of predicting target function. The
effectiveness of top-k tokens and layer information, as well as the sensitivity analysis with respect
to the parameter _k_ and the choice of LLM fused for top-k token extraction, and prompt format, can
be found in Appendix A.8.


4 EXPERIMENTS


We conduct a series of experiments on three VLM families across various model scales, including
Intern (Zhu et al., 2025) (InternVL3-8B and InternVL3-2B), Qwen (Yang et al., 2025) (Qwen2.5VL-7B and Qwen2.5-VL-3B), and Gemma (Team et al., 2025) (Gemma3-4B and Gemma3-2B).
We analyze the commonalities and differences of functional heads (Subsection 4.1), validate their
contributions (Subsection 4.2), and examine correlations, including cross-function interactions and
hierarchical organization (Subsection 4.3). We also assess their causal impact on downstream reasoning tasks (Subsection 4.4). Results confirm the existence of sparse, function-specific heads and
highlight their critical contribution to structured cognitive processing within VLMs.


4.1 PROPERTIES OF COGNITIVE HEADS


**Sparsity, Universality, and Intrinsic Organization:** Fig 2 shows the heatmap of attention head
accuracy across eight functions in Qwen2.5-VL-7B on the CogVision test set, revealing a sparse
distribution. In total, fewer than 7% of all heads achieve accuracies above 0.9 across the eight functions (about 2% for high-level visual reception and math reasoning, and less than 1% for the others),
suggesting that only a small subset of heads meaningfully contributes to different reasoning tasks.
These results demonstrate that VLMs rely on highly specialized, localized components for distinct
cognitive abilities. Pearson correlations between head-activation heatmaps across the eight functions
(Fig. 3) are generally low, confirming that different functions tend to depend on partially separable
subsets of heads. Moreover, this sparse functional organization is consistent across architectures
and scales: heatmaps for five additional models (Appendix A.4) confirm its universality, and the
relatively high Pearson correlation coefficients between models further verify this consistency (in
Appendix A.9). Within the same model family (e.g., Qwen2.5-VL-7B in Fig 2 vs. Qwen2.5-VL-3B
in Fig 8), we observe similar distributions, suggesting that such specialization is intrinsic to VLMs.


**Functional Personalization:** Beyond sparsity, attention heads exhibit a structured distribution
across model layers. Math-related heads are dispersed throughout the network, whereas inferencerelated heads appear more frequently in higher layers. This task-dependent localization suggests an
emergent modular organization in which different layers support distinct cognitive operations. We
also observe notable variation in head counts across functions. For example, in the Qwen family,
math reasoning and high-level visual reception heads are more prevalent than others, reflecting differences in representational and computational complexity. Smaller models contain fewer functional
heads compared to their larger counterparts.


4.2 FUNCTIONAL CONTRIBUTIONS OF COGNITIVE HEADS


After identifying the cognitive heads associated with each function, we examine their functional
roles by evaluating the model’s behavior on the CogVision test set under targeted interventions. We
perform head ablation by scaling the output of a specific attention head with a small factor _ϵ_ (e.g.,
0.001), effectively suppressing its contribution:



�



_x_ [mask] _i_ = Softmax



_Wq_ _[i][W][ iT]_ _k_
� ~~�~~ _dk/n_




_· ϵWv_ _[i]_ (2)



Specifically, we compare model performance when masking identified cognitive heads versus masking an equal number of randomly-selected heads. To quantify the impact, we employ both an LLMbased judge and an integrated accuracy metric. For LLM-based judge, we use LLM (Qwen3-30B
LLM (Yang et al., 2025)) to judge the correctness of the output. For the integrated accuracy metric,
an output is considered unaffected if its BLEU score (Papineni et al., 2002) exceeds 0.8, or if either


5


![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-5-0.png)

Figure 2: The existence of cognitive heads in Qwen2.5-VL-7B responsible for eight distinct functions in complex reasoning tasks. The x-axis represents the head index, while the y-axis indicates
the layer index. The values denote head importance scores, capped at a cutoff of 0.60.


Figure 3: Pearson Correlation between different functions across two models.


the ROUGE score (Chin-Yew, 2004) or the semantic similarity score surpasses 0.6. This provides a
comprehensive evaluation of performance degradation.

---

[Document 3]
Source: attention_functional_roles_raw_chunks_2k.jsonl
Chunk ID: attention_functional_roles_raw_chunks_2k_0
Content: ## - INVESTIGATING THE FUNCTIONAL ROLES OF ATTEN ### TION HEADS IN VISION LANGUAGE MODELS: EVI DENCE FOR REASONING MODULES

**Yanbei Jiang** **[1]** _[∗]_ **Xueqi Ma** **[1]** _[∗]_ **Shu Liu** **[1]** **Sarah Monazam Erfani** **[1]**

**Tongliang Liu** **[2]** **James Bailey** **[1]** **Jey Han Lau** **[1]** **Krista A. Ehinger** **[1]**

1The University of Melbourne 2The University of Sydney
_{_ yanbeij, xueqim, shu6 _}_ @student.unimelb.edu.au
_{_ sarah.erfani, baileyj, jeyhan.lau, kris.ehinger _}_ @unimelb.edu.au
tongliang.liu@sydney.edu.au


ABSTRACT


Despite excelling on multimodal benchmarks, vision–language models (VLMs)
largely remain a black box. In this paper, we propose a novel interpretability framework to systematically analyze the internal mechanisms of VLMs, focusing on the functional roles of attention heads in multimodal reasoning. To
this end, we introduce CogVision, a dataset that decomposes complex multimodal questions into step-by-step subquestions designed to simulate human reasoning through a chain-of-thought paradigm, with each subquestion associated
with specific receptive or cognitive functions such as high-level visual reception and inference. Using a probing-based methodology, we identify attention
heads that specialize in these functions and characterize them as functional heads.
Our analysis across diverse VLM families reveals that these functional heads
are universally sparse, vary in number and distribution across functions, and
mediate interactions and hierarchical organization. Furthermore, intervention
experiments demonstrate their critical role in multimodal reasoning: removing
functional heads leads to performance degradation, while emphasizing them enhances accuracy. These findings provide new insights into the cognitive organization of VLMs and suggest promising directions for designing models with more
human-aligned perceptual and reasoning abilities. Code and data are available at
https://github.com/YanbeiJiang/CogVision.


1 INTRODUCTION


Large Vision-Language Models (VLMs) (Zhu et al., 2023; Liu et al., 2023; Lu et al., 2024a) have
demonstrated remarkable success across diverse multimodal tasks, ranging from image captioning
to visual question answering. Although VLMs can solve mathematical reasoning problems with
visual context (as shown in Fig. 1), their internal mechanisms remain poorly understood.


For humans, solving such complex problems (illustrated in Fig. 1) typically requires the collaboration of vision and language, engaging multiple brain regions (Barsalou, 2014): the occipital lobe
for visual reception, capturing and processing the content of the images; the temporal lobe supports
long-term memory and the recall of relevant factual knowledge, such as chemical concentration formulas (Wheeler et al., 1997); and the parietal and prefrontal cortices are involved in higher-order
reasoning (Hubbard et al., 2005), to produce the correct answer.


Recent research in interpretability has begun probing the internal organization of large language
models (LLMs), revealing specialized attention heads for specific functions (Wu et al.; Li et al.,
2023a; Zheng et al.). In the case of VLMs, several studies (Kang et al., 2025; Bi et al., 2025) have
identified sparse attention heads with special functional roles in tasks such as grounding. However,
studying VLMs in complex, multi-step reasoning scenarios remains underexplored. A deeper under

_∗_ Both authors contributed equally to this research.


1


![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-1-0.png)



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-1-1.png)

![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-1-2.png)



Decision making



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-1-3.png)

High-level Vision Reception



Language Knowledge Recall



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-1-5.png)



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-1-6.png)



Math Reasoning



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-1-7.png)



![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-1-4.png)

Occipital lobe Temporal lobe Parietal lobe Frontal lobe









By dividing the number of Solution A has a higher

particles by the solvent concentration of A, Solution A.









![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-1-8.png)

![](E:/Python Stuff/MAS-for-multimodal-knowledge-graph/markdown_outputs/images/attention_functional_roles.pdf-1-11.png)

Figure 1: To answer a complex question, the human brain engages multiple regions, each performing
distinct cognitive functions. We investigate whether specific attention heads in large vision language
models play analogous functional roles in generating responses.


standing of whether such specialized components exist, how they are organized, and what functional
roles they play in multimodal reasoning is therefore critical.


In this paper, we propose a novel interpretability framework for systematically analyzing the functional roles of attention heads-parallel units in transformer models that compute token-to-token
attention-an important component in VLMs, with a focus on their contributions to reception (perceptual processing) and cognition. To facilitate this, we introduce CogVision, a dataset that bridges the
gap between model analysis and human cognitive processes. CogVision decomposes multimodal
queries into step-by-step subquestions, each aligned with specific cognitive functions (such as math
reasoning, decision-masking), thus enabling a fine-grained evaluation of reasoning aligned with the
chain-of-thought (CoT) paradigm. Leveraging CogVision, we develop a probing method to identify and characterize attention heads responsible for distinct cognitive operations across vision and
language within the transformer architecture.


We conduct extensive experiments on three major VLM families, including Intern (Zhu et al., 2025),
Qwen (Yang et al., 2025), and Gemma (Team et al., 2025) with different model scales. Our results
reveal the existence of cognitive heads that consistently exhibit **universal**, **sparse**, and **intrinsic**
properties across architectures. Further analysis of the correlations among these functional heads
reveals **cross-function interactions**, where a single head may support multiple functions or modalities, and uncovers a **hierarchical structure** in which lower-level functional heads modulate higherlevel ones, showing the complexity of neural networks (Barsalou, 2014; Ono et al., 2022).


Furthermore, we validate the functional importance of these heads by showing that their removal
degrades performance on complex tasks and leads to specific error patterns, while their enhancement improves reasoning capabilities. Our findings provide compelling evidence that these attention
heads play a critical role in multimodal reasoning. This insight not only deepens our understanding of the internal organization of VLMs but also suggests potential avenues for designing more
interpretable and cognitive-inspired multimodal AI systems.


2 COGVISION


In this section, we present our dataset Cognitive Vision (CogVision) that contains cognitive process
in multimodal reasoning. CogVision contains 1,409 main questions and 5,744 subquestions. Each


2
","The ""CogVision"" framework's probing methodology first identifies attention heads in Vision-Language Models (VLMs) that specialize in specific cognitive functions by extracting head activations for functionally annotated subquestions and training classifiers to associate heads with these functions.

The ""hierarchical organization"" of cognitive processes is then revealed through **further analysis of the correlations among these identified functional heads**. This analysis uncovers a hierarchical structure where lower-level functional heads modulate higher-level ones.",104,4953,5058,,3,,2026-01-02T20:19:33.700462
"Evaluate the ""biologically plausible"" claims of NAC in relation to the actual mechanisms of ""synaptic transmission"" and ""neuronal dynamics"".",System Level / Architectural Understanding,"[Document 1]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_10
Content: Zhuang, J., Jia, M., Ding, Y., and Ding, P. Temporal
convolution-based transferable cross-domain adaptation
approach for remaining useful life estimation under variable failure behaviors. _Reliability Engineering & System_
_Safety_, 216:107946, 2021.



10


**Neuronal Attention Circuit (NAC) for Representation Learning**



**Appendix**


**A. Preliminaries**


**A.1. Attention Mechanism**


Attention mechanisms have become a cornerstone in

modern neural architectures, enabling models to dynamically focus on relevant parts of the input. The concept
was first introduced in the context of neural machine
translation, where it allowed the decoder to weight encoder
outputs according to their importance for generating each
target token. Formally, given a query vector _q ∈_ R _[d]_, key
vectors _K_ = [ _k_ 1 _, k_ 2 _, . . ., kn_ ] _∈_ R _[n][×][d]_, and value vectors
_V_ = [ _v_ 1 _, v_ 2 _, . . ., vn_ ] _∈_ R _[n][×][d]_, the attention mechanism can
be expressed in two steps:


1. Compute the scaled dot attention logits:


_ai_ = _[q][T][ k][i]_ (21)
~~_√_~~ _d_


2. Normalize the logits to get attention weights and compute the output:


_e_ _[a][i]_
_αi_ = softmax( _ai_ ) = ~~_n_~~ (22)
~~�~~ _j_ =1 _[e][a][j]_



exhibit a resting potential at _−_ 70 mV and an activation potential near _−_ 20 mV. Similarly, each _Nm_ is composed of
two subneurons, _Mp_ and _Mn_, and is driven by a controllable variable _y_, which also maps to a biologically plausible
range [ _−_ 70 mV _, −_ 20 mV]. The connections in the NCP
architecture are designed to reflect the biological sparsity
and abstraction of neural circuits. Specifically, connections
from _Ns_ to _Ni_ are feedforward, while those between _Nc_
and _Nm_ are highly recurrent (Lechner et al., 2018). Figure
1(a) illustrates the connectome of NCPs.


**B. Proofs**


In this section, we provide all the proofs.


**B.1. Deriving Closed-form (Exact) Solution**


Although _ϕ_ and _ωτ_ are nonlinear functions of the input
**u** = [ **q** ; **k** ], we derive closed-form solution by treating them
as locally constant over the pseudo-time integration interval
for each query–key pair based on frozen-coefficient approximation (John, 1952). This is accurate whenever the interval
is short or when input variations are slow compared with
the relaxation rate _ωτ_ . Under approximation assumption,
rewrite Eqn. 1 as


_dadtt_ [+] _[ ω][τ]_ _[a][t]_ [ =] _[ ϕ.]_ (24)


This is now a linear first-order ODE. The integrating factor
is



Attention( _q, k, v_ ) =



_n_
� _αivi_ (23)


_i_ =1



Here, _ai_ is the raw attention logit between the query and
each key, and the scaling factor _√d_ prevents large dot prod
ucts from destabilizing the softmax (Vaswani et al., 2017).


**A.2. Neuronal Circuit Policies (NCPs)**


NCPs represent a biologically inspired framework for developing interpretable neural control agents by adapting the
tap-withdrawal circuit found in the nematode _C. elegans_
(Lechner et al., 2018). Unlike traditional spiking neural
networks, the majority of neurons in this circuit exhibit
electronic dynamics, characterized by the passive flow of
electrical charges, resulting in graded potentials. NCPs
are structured as a four-layer hierarchical architecture comprising sensory neurons ( _Ns_ ), interneurons ( _Ni_ ), command
neurons ( _Nc_ ), and motor neurons ( _Nm_ ). The _Ns_ perceive
and respond to external stimulus inputs and are responsible for the initial signal transduction. Each _Ns_ consists
of subneurons _Sp_ and _Sn_ and a system variable _x_ . The
activation of _Sp_ and _Sn_ depends upon the sign of _x_ : _Sp_
becomes activated for _x >_ 0, whereas _Sn_ becomes activated for _x <_ 0. The variable _x_ is mapped to the membrane
potential range of [ _−_ 70 mV _, −_ 20 mV], which is consistent
with the biophysical behavior of nerve cells, which typically



� _ωτ dt_

_µ_ = _e_ � �


Multiply both sides by _µ_ ( _t_ ):



Substitute back:


_e_ _[ω][τ][ t]_ _at −_ _a_ 0 = _ϕ ·_ _[e][ω][τ][ t][ −]_ [1] _._ (30)

_ωτ_



= _e_ _[ω][τ][ t]_ _._ (25)




_[da][t]_
_e_ _[ω][τ][ t]_ _dt_ [+] _[ ω][τ]_ _[e][ω][τ][ t][a][t]_ [ =] _[ ϕe][ω][τ][ t][.]_ (26)


Recognize the left-hand side as the derivative of _e_ _[ω][τ][ t]_ _at_ :


_d_
� _e_ _[ω][τ][ t]_ _at_ � = _ϕe_ _[ω][τ][ t]_ _._ (27)
_dt_


Integrate from 0 to _t_ :


_t_
_e_ _[ω][τ][ t]_ _at −_ _e_ [0] _a_ 0 = _ϕ_ _e_ _[ω][τ][ s]_ _ds._ (28)
�0


Compute the integral (since _ωτ ̸_ = 0):



�0 _t_



_e_ _[ω][τ][ s]_ _ds_ = [1]
0 _ωτ_



_ωτ_



� _e_ _[ω][τ][ t]_ _−_ 1� _._ (29)



11


Rearrange:



**Neuronal Attention Circuit (NAC) for Representation Learning**


CASE 2: MULTIPLE CONNECTIONS ( _M >_ 1).



_e_ _[ω][τ][ t]_ _at_ = _a_ 0 + _[ϕ]_

_ωτ_


Divide both sides by _e_ _[ω][τ][ t]_ :


_at_ = _a_ 0 _e_ _[−][ω][τ][ t]_ + _[ϕ]_

_ωτ_



� _e_ _[ω][τ][ t]_ _−_ 1� _._ (31)


�1 _−_ _e_ _[−][ω][τ][ t]_ [�] _._ (32)



The ODE is



with per-connection equilibria _Aj_ = _ϕj/fj_ . The effective
equilibrium is



_da_

_dt_ [=] _[ −]_ � � _[M]_



_da_



� _fj_ � _a_ +

_j_ =1



_M_
� _fjAj,_ (37)

_j_ =1



Set _a_ _[∗]_ := _[ϕ]_ . Then _at_ = _a_ _[∗]_ + ( _a_ 0 _−_ _a_ _[∗]_ ) _e_ _[−][ω][τ][ t]_, proved.

_ωτ_


**B.2. Proof of Theorem 1**

---

[Document 2]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_1
Content: **Motivation behind this formulation:** The proposed
formulation is loosely motivated by the input-dependent
time-constant mechanism of Liquid Neural Networks
(LNNs), a class of CT-RNNs inspired by biological nervous
systems and synaptic transmission. In this framework, the
dynamics of non-spiking neurons are described by a linear
ODE with nonlinear, interlinked gates: _ddt_ **xt** = **[x]** _τ_ **[t]** [+] **[ S][t]** _[,]_

where **xt** denotes the hidden state and **St** _∈_ R _[M]_ represents
a nonlinear contribution defined as _f_ ( **xt** _,_ **u** _, t, θ_ )( _A −_ **xt** ).
Here, _A_ and _θ_ are learnable parameters. Plugging **St**
yields _[d]_ **[x][t]** [=] _[ −]_ � _−_ [1] _[−]_ _[f]_ [(] **[x][t]** _[,]_ **[ u]** _[, t, θ]_ [)] � **xt** + _f_ ( **xt** _,_ **u** _, t, θ_ ) _A_ .



_an_ +1 = _an_ + ∆ _t_ ( _−ωτ_ _an_ + _ϕ_ ) _._ (2)


**Closed-form (Exact) Computation of NAC:** We now devise the analytical solution for Eqn. 1. Let both _ωτ_ and
_ϕ_ be fixed in pseudo-time interval (frozen-coefficient approximation (John, 1952)) with initial condition _a_ 0, then
closed-form solution is:



**xt** = **[x][t]**

_dt_ _τ_




**[x][t]** _−_ [1]

_dt_ [=] _[ −]_ � _τ_



yields _[d]_ _dt_ **[x][t]** [=] _[ −]_ � _−_ _τ_ [1] _[−]_ _[f]_ [(] **[x][t]** _[,]_ **[ u]** _[, t, θ]_ [)] � **xt** + _f_ ( **xt** _,_ **u** _, t, θ_ ) _A_ .

LNNs are known for their strong expressivity, stability, and
performance in irregularly sampled time-series modeling
(Hasani et al., 2021; 2022).
**NAC’s forward-pass update using ODE solver:** The
state of NAC at time _t_ can be computed using a numerical
ODE solver that simulates the dynamics from an initial
state _a_ 0 to _at_ . The solver discretizes the continuous interval

[0 _, T_ ] into steps [ _t_ 0 _, t_ 1 _, t_ 2 _, . . ., tn_ ], with each step updating
the state from _ti_ to _ti_ +1. For our purposes, we use the
_explicit Euler_ solver, which is simple, efficient, and easy to
implement. Although methods such as Runge-Kutta may
offer higher accuracy, their computational overhead makes
them less suitable for large-scale neural simulations that
require numerous updates, especially since the logits are
normalized, and exact precision is not necessary. Let the
step size be ∆ _t_, with discrete times _tn_ = _n_ ∆ _t_ and logit
states _an_ = _a_ ( _tn_ ). Using the _explicit Euler_ method, the
update is



_dat_

_dt_ [=] _[ −]_ ~~�~~ _[f][ω][τ]_ [ ([] **[q]** � [;] **[ k]** ~~�~~ []] _[, θ][ω][τ]_ [ )] ~~�~~
_ωτ_ ( **u** )



_at_ + _fϕ_ ([ **q** ; **k** ] _, θϕ_ ) _,_ (1)
� ~~�~~ � ~~�~~
_ϕ_ ( **u** )



_at_ = _a_ _[∗]_ + ( _a_ 0 _−_ _a_ _[∗]_ ) _e_ _[−][ω][τ][ t]_
���� ~~�~~ ~~��~~ ~~�~~
steady-state transient



(3)



where **u** = [ **q** ; **k** ] denotes the sparse Top- _K_ concatenated
query–key input. _ωτ_ represents a learnable time-constant
gate head, _ϕ_ denotes a nonlinear content-target head. Both
gates are parameterized by a backbone network derived



Here, _a_ _[∗]_ = _ϕ/ωτ_ is the steady-state solution. The full
derivation is provided in Appendix B.1.



2


**Neuronal Attention Circuit (NAC) for Representation Learning**



**2.1. Stability Analysis of NAC**


We now investigate the stability bounds of _NAC_ under both
the ODE-based and the Closed-Form formulations.


2.1.1. STATE STABILITY


We analyze state stability in both single-connection and
multi-connection settings. This analysis establishes the
boundedness of the attention logit state trajectory, ensuring that, under positive decay rates, the dynamics remain
well-behaved without divergence or overshoot.

**Theorem 1** (State Stability) **.** _Let a_ [(] _t_ _[i]_ [)] _denote the state of_
_the i-th attention logit governed by da_ [(] _t_ _[i]_ [)] _[/dt]_ [ =] _[ −][ω][τ]_ _[a]_ [(] _t_ _[i]_ [)] +
_ϕ. Assume that ϕ and ωτ decompose across M incom-_
_ing connections as ϕ_ = [�] _[M]_ _j_ =1 _[f][ϕ]_ [([] **[q]** _[i]_ [;] **[ k]** _[j]_ [])] _[, and][ ω][τ]_ [ =]
_M_
� _j_ =1 _[f][ω]_ _τ_ [([] **[q]** _[i]_ [;] **[ k]** _[j]_ [])] _[,][ with][ f][ω]_ _τ_ _>_ 0 _._ _Define the per-_
_connection equilibrium Ai,j_ = _fϕ_ ([ **q** _i_ ; **k** _j_ ]) _/fωτ_ ([ **q** _i_ ; **k** _j_ ]) _,_
_and let A_ [min] _i_ = min _j Ai,j and A_ [max] _i_ = max _j Ai,j. Then_
_for any finite horizon t ∈_ [0 _, T_ ] _, the state trajectory satisfies_


min(0 _, A_ [min] _i_ ) _≤_ _a_ [(] _t_ _[i]_ [)] _≤_ max(0 _, A_ [max] _i_ ) _,_ (4)


_provided the initial condition ai_ (0) _lies within this range. In_
_the special case of a single connection (M_ = 1 _), the bounds_
_collapse to_


min(0 _, Ai_ ) _≤_ _a_ [(] _t_ _[i]_ [)] _≤_ max(0 _, Ai_ ) _,_ (5)


_where Ai_ = _fϕ/ωτ is exactly the steady-state solution from_
_Eqn. 3. The proof is provided in the Appendix B.2._


2.1.2. CLOSED-FORM ERROR & EXPONENTIAL BOUNDS

---

[Document 3]
Source: neuronal_attention_circuits_raw_chunks_2k.jsonl
Chunk ID: neuronal_attention_circuits_raw_chunks_2k_12
Content: _a_ ( _t_ + ∆ _t_ ) = _at_ + ∆ _t ·_ _[da]_ (36)

_dt_ _[.]_



At _a_ = _M_, _[da]_ _dt_ _[≤]_ [0 =] _[⇒]_ _[a]_ [(] _[t]_ [ + ∆] _[t]_ [)] _[ ≤]_ _[M]_ [. At] _[ a]_ [ =] _[ m]_ [,]

_dadt_ _[≥]_ [0 =] _[⇒]_ _[a]_ [(] _[t]_ [ + ∆] _[t]_ [)] _[ ≥]_ _[m]_ [. By induction over steps,]

_at ∈_ [ _m, M_ ] for all _t ∈_ [0 _, T_ ].



12


**Neuronal Attention Circuit (NAC) for Representation Learning**



multi-head, scale _H_ proportionally to target complexity,
with output projection _Wo_ aggregating as in classical UAT
proofs (Stinchcomb, 1989).
**Input Projections:** The input _x_ is projected via NCPbased sensory projections to obtain query _q_ = _q_ proj( _x_ ), key
_k_ = _k_ proj( _x_ ), and value _v_ = _v_ proj( _x_ ), each in R _[d]_ [model] . For
emulation, set _q_ proj = _k_ proj = _In_ (identity on R _[n]_ ) and adjust


**Head Splitting and Sparse Top-** _**k**_ **Pairwise Computation:**
Split into _H_ heads, yielding _q_ [(] _[h]_ [)] _, k_ [(] _[h]_ [)] _∈_ R _[d]_ per head _h_,
where _d_ = _d_ model _/H_ . For _T_ = 1, compute sparse top- _k_
pairs, but since _T_ = 1, _K_ eff = 1, yielding concatenated
pair _u_ [(] _[h]_ [)] = [ _q_ [(] _[h]_ [)] ; _k_ [(] _[h]_ [)] ] _∈_ R [2] _[d]_ . Since _q_ [(] _[h]_ [)] = _k_ [(] _[h]_ [)], this is

[ _x_ [(] _[h]_ [)] ; _x_ [(] _[h]_ [)] ], but the NCP processes it generally.
**Computation of** _ϕ_ [(] _[h]_ [)] **and** _ωτ_ [(] _[h]_ [)] **:** The scalar _ϕ_ [(] _[h]_ [)] is computed via the NCP-based inter-to-motor projection on the
pair:
_ϕ_ [(] _[h]_ [)] = _σ_ ( _NN_ backbone( _u_ [(] _[h]_ [)] )) (42)


where _σ_ ( _z_ ) = (1 + _e_ _[−][z]_ ) _[−]_ [1] is the sigmoid. This NCP, with
sufficiently large units and low sparsity, approximates any
continuous scalar function _ϕ_ [˜] : R [2] _[d]_ _→_ [0 _,_ 1] to arbitrary precision on compact sets (by the UAT for multi-layer networks
(Stinchcomb, 1989)). Similarly, _ωτ_ [(] _[h]_ [)] is computed via:


_ωτ_ [(] _[h]_ [)] = softplus( _NN_ backbone( _u_ [(] _[h]_ [)] )) + _ε,_ _ε >_ 0 (43)


By setting weights to make _ωτ_ [(] _[h]_ [)] _≡_ 1 (constant), the steadymode logit simplifies to _a_ [(] _[h]_ [)] = _ϕ_ [(] _[h]_ [)] _/ωτ_ [(] _[h]_ [)] = _ϕ_ [(] _[h]_ [)] . Thus,
_a_ [(] _[h]_ [)] _≈_ _σ_ � _w_ [(] _[h]_ [)] _x_ + _b_ [(] _[h]_ [)][�] for chosen weights _w_ [(] _[h]_ [)] _, b_ [(] _[h]_ [)], emulating a sigmoid hidden unit.
**Attention Weights and output:** For _T_ = 1, the softmax
over one “key” yields _α_ [(] _[h]_ [)] = exp( _a_ [(] _[h]_ [)] ) _/_ exp( _a_ [(] _[h]_ [)] ) = 1.
The head output is _y_ [(] _[h]_ [)] = � _T_ _[α]_ [(] _[h]_ [)] _[v]_ [(] _[h]_ [)] _[dt]_ [. Set] _[ v]_ [proj][ such that]

_v_ [(] _[h]_ [)] = 1 (scalar), yielding _y_ [(] _[h]_ [)] _≈_ _σ_ � _w_ [(] _[h]_ [)] _x_ + _b_ [(] _[h]_ [)][�] . For
vector-valued _v_ [(] _[h]_ [)], more complex combinations are possible, but scalars suffice here.
**Output Projection:** Concatenate head outputs: _Y_ =

[ _y_ [(1)] ; _y_ [(2)] ; _. . ._ ; _y_ [(] _[H]_ [)] ] _∈_ R _[H]_ . Apply the final dense layer:


_g_ ( _x_ ) = ( _Y · Wo_ ) + _bo ∈_ R _[m]_ _._ (44)


With _y_ [(] _[h]_ [)] _≈_ _σ_ � _w_ [(] _[h]_ [)] _x_ + _b_ [(] _[h]_ [)][�], this matches a single-hiddenlayer network with _H_ units. By the UAT, for large _H_, such
networks approximate any continuous _f_ on compact _K_ to
accuracy _ϵ_, by choosing appropriate _w_ [(] _[h]_ [)] _, b_ [(] _[h]_ [)] _, Wo, bo_ .


**C. Training, Gradients and Complexity**


**C.1. Gradient Characterization**


We analyze the sensitivity of the dynamics with respect to
the underlying learnable parameters. Specifically, we compute closed-form derivatives of both the steady state and the
full trajectory _at_ with respect to the parameters _ϕ_ and _ωτ_ .
These expressions illuminate how gradients flow through



the system, and provide guidance for selecting parameterizations that avoid vanishing or exploding gradients.


C.1.1. TRAJECTORY SENSITIVITIES FOR CLOSED-FORM


FORMULATION


The trajectory is given by


_at_ = _a_ _[∗]_ + ( _a_ 0 _−_ _a_ _[∗]_ ) _e_ _[−][ω][τ][ t]_ _,_ (45)


which depends on ( _ϕ, ωτ_ ) both through the equilibrium _a_ _[∗]_

and the exponential term.
**Derivative with respect to** _ϕ_ **:** We obtain



_Interpretation_ : The gradient with respect to _ωτ_ contains
a transient term proportional to _te_ _[−][ω][τ][ t]_, which dominates
at intermediate times, and a steady-state contribution proportional to _−ϕ/ωτ_ [2][, which persists asymptotically. Thus,]
sensitivity to _ωτ_ is time-dependent, peaking before vanishing exponentially in the transient component.


**C.2. Gradient-Based Training**
","The ""biologically plausible"" claims of the Neuronal Attention Circuit (NAC) are primarily rooted in its foundational inspiration and the components it integrates:

1.  **Inspiration from Liquid Neural Networks (LNNs) and Synaptic Transmission**: The formulation of NAC is ""loosely motivated by the input-dependent time-constant mechanism of Liquid Neural Networks (LNNs),"" which are themselves ""inspired by biological nervous systems and synaptic transmission."" This connection suggests that NAC's underlying dynamics, described by an Ordinary Differential Equation (ODE) (e.g., `dat/dt = -ωτ(u) at + ϕ(u)`), are intended to reflect the continuous-time dynamics of non-spiking neurons and the way signals are processed across synapses. The `ωτ` and `ϕ` parameters represent learnable time-constant and content-target gates, respectively, influencing these dynamics.

2.  **Integration of Neuronal Circuit Policies (NCPs)**: NAC utilizes NCPs for its input projections and the computation of its `ϕ` and `ωτ` parameters. NCPs are described as a ""biologically inspired framework"" adapted from the *C. elegans* tap-withdrawal circuit.
    *   **Neuronal Dynamics**: NCPs model neurons exhibiting ""electronic dynamics, characterized by the passive flow of electrical charges, resulting in graded potentials,"" rather than the discrete spikes of traditional spiking neural networks. This reflects a specific type of biological neuronal activity.
    *   **Membrane Potentials**: Variables within NCPs (e.g., `x` for sensory neurons, `y` for motor neurons) are mapped to a ""biologically plausible range [−70 mV, −20 mV],"" which is consistent with the typical resting potential (around -70 mV) and activation potential (near -20 mV) of biological nerve cells.
    *   **Circuit Structure**: The connections within the NCP architecture are designed to reflect ""biological sparsity and abstraction of neural circuits,"" including feedforward connections from sensory to interneurons and highly recurrent connections between command and motor neurons.

In essence, NAC's claims of biological plausibility are based on its continuous-time ODE formulation, which draws inspiration from biological nervous systems and synaptic transmission, and its use of NCPs, which incorporate biologically consistent membrane potential ranges, graded potentials, and circuit connectivity patterns.",117,8413,8531,,3,,2026-01-02T20:19:42.232666
